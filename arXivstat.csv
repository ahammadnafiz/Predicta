id,title,categories,abstract,doi,created,updated,authors
0803.0966,new probabilistic interest measures for association rules,cs.db stat.ml,"mining association rules is an important technique for discovering meaningful patterns in transaction databases. many different measures of interestingness have been proposed for association rules. however, these measures fail to take the probabilistic properties of the mined data into account. in this paper, we start with presenting a simple probabilistic framework for transaction data which can be used to simulate transaction data when no associations are present. we use such data and a real-world database from a grocery outlet to explore the behavior of confidence and lift, two popular interest measures used for rule mining. the results show that confidence is systematically influenced by the frequency of the items in the left hand side of rules and that lift performs poorly to filter random noise in transaction data. based on the probabilistic framework we develop two new interest measures, hyper-lift and hyper-confidence, which can be used to filter or order mined association rules. the new measures show significantly better performance than lift for applications where spurious rules are problematic.",10.3233/ida-2007-11502,2008-03-06,,"['michael hahsler', 'kurt hornik']"
1301.1954,on the incommensurability phenomenon,stat.ml,"suppose that two large, multi-dimensional data sets are each noisy measurements of the same underlying random process, and principle components analysis is performed separately on the data sets to reduce their dimensionality. in some circumstances it may happen that the two lower-dimensional data sets have an inordinately large procrustean fitting-error between them. the purpose of this manuscript is to quantify this ""incommensurability phenomenon."" in particular, under specified conditions, the square procrustean fitting-error of the two normalized lower-dimensional data sets is (asymptotically) a convex combination (via a correlation parameter) of the hausdorff distance between the projection subspaces and the maximum possible value of the square procrustean fitting-error for normalized data. we show how this gives rise to the incommensurability phenomenon, and we employ illustrative simulations as well as a real data experiment to explore how the incommensurability phenomenon may have an appreciable impact.",10.1007/s00357-016-9203-9,2013-01-09,2015-02-06,"['donniell e. fishkind', 'cencheng shen', 'youngser park', 'carey e. priebe']"
1304.7981,generalized canonical correlation analysis for classification,stat.ml,"for multiple multivariate data sets, we derive conditions under which generalized canonical correlation analysis (gcca) improves classification performance of the projected datasets, compared to standard canonical correlation analysis (cca) using only two data sets. we illustrate our theoretical results with simulations and a real data experiment.",10.1016/j.jmva.2014.05.011,2013-04-30,2014-06-26,"['cencheng shen', 'ming sun', 'minh tang', 'carey e. priebe']"
1311.5954,robust vertex classification,stat.ml,"for random graphs distributed according to stochastic blockmodels, a special case of latent position graphs, adjacency spectral embedding followed by appropriate vertex classification is asymptotically bayes optimal; but this approach requires knowledge of and critically depends on the model dimension. in this paper, we propose a sparse representation vertex classifier which does not require information about the model dimension. this classifier represents a test vertex as a sparse combination of the vertices in the training set and uses the recovered coefficients to classify the test vertex. we prove consistency of our proposed classifier for stochastic blockmodels, and demonstrate that the sparse representation classifier can predict vertex labels with higher accuracy than adjacency spectral embedding approaches via both simulation studies and real data experiments. our results demonstrate the robustness and effectiveness of our proposed vertex classifier when the model dimension is unknown.",10.1109/tpami.2015.2456913,2013-11-22,2015-04-22,"['li chen', 'cencheng shen', 'joshua vogelstein', 'carey priebe']"
1506.03410,sparse projection oblique randomer forests,stat.ml cs.lg,"decision forests, including random forests and gradient boosting trees, have recently demonstrated state-of-the-art performance in a variety of machine learning settings. decision forests are typically ensembles of axis-aligned decision trees; that is, trees that split only along feature dimensions. in contrast, many recent extensions to decision forests are based on axis-oblique splits. unfortunately, these extensions forfeit one or more of the favorable properties of decision forests based on axis-aligned splits, such as robustness to many noise dimensions, interpretability, or computational efficiency. we introduce yet another decision forest, called ""sparse projection oblique randomer forests"" (sporf). sporf uses very sparse random projections, i.e., linear combinations of a small subset of features. sporf significantly improves accuracy over existing state-of-the-art algorithms on a standard benchmark suite for classification with >100 problems of varying dimension, sample size, and number of classes. to illustrate how sporf addresses the limitations of both axis-aligned and existing oblique decision forest methods, we conduct extensive simulated experiments. sporf typically yields improved performance over existing decision forests, while mitigating computational efficiency and scalability and maintaining interpretability. sporf can easily be incorporated into other ensemble methods such as boosting to obtain potentially similar gains.",,2015-06-10,2019-10-03,"['tyler m. tomita', 'james browne', 'cencheng shen', 'jaewon chung', 'jesse l. patsolic', 'benjamin falk', 'jason yim', 'carey e. priebe', 'randal burns', 'mauro maggioni', 'joshua t. vogelstein']"
1701.07275,"universal representations:the missing link between faces, text,   planktons, and cat breeds",cs.cv stat.ml,"with the advent of large labelled datasets and high-capacity models, the performance of machine vision systems has been improving rapidly. however, the technology has still major limitations, starting from the fact that different vision problems are still solved by different models, trained from scratch or fine-tuned on the target data. the human visual system, in stark contrast, learns a universal representation for vision in the early life of an individual. this representation works well for an enormous variety of vision problems, with little or no change, with the major advantage of requiring little training data to solve any of them. in this paper we investigate whether neural networks may work as universal representations by studying their capacity in relation to the âsizeâ of a large combination of vision problems. we do so by showing that a single neural network can learn simultaneously several very different visual domains (from sketches to planktons and mnist digits) as well as, or better than, a number of specialized networks. however, we also show that this requires to carefully normalize the information in the network, by using domain-specific scaling factors or, more generically, by using an instance normalization layer.",,2017-01-25,,"['hakan bilen', 'andrea vedaldi']"
1710.00095,user-friendly guarantees for the langevin monte carlo with inaccurate   gradient,math.st cs.lg math.pr stat.co stat.ml stat.th,"in this paper, we study the problem of sampling from a given probability density function that is known to be smooth and strongly log-concave. we analyze several methods of approximate sampling based on discretizations of the (highly overdamped) langevin diffusion and establish guarantees on its error measured in the wasserstein-2 distance. our guarantees improve or extend the state-of-the-art results in three directions. first, we provide an upper bound on the error of the first-order langevin monte carlo (lmc) algorithm with optimized varying step-size. this result has the advantage of being horizon free (we do not need to know in advance the target precision) and to improve by a logarithmic factor the corresponding result for the constant step-size. second, we study the case where accurate evaluations of the gradient of the log-density are unavailable, but one can have access to approximations of the aforementioned gradient. in such a situation, we consider both deterministic and stochastic approximations of the gradient and provide an upper bound on the sampling error of the first-order lmc that quantifies the impact of the gradient evaluation inaccuracies. third, we establish upper bounds for two versions of the second-order lmc, which leverage the hessian of the log-density. we provide nonasymptotic guarantees on the sampling error of these second-order lmcs. these guarantees reveal that the second-order lmc algorithms improve on the first-order lmc in ill-conditioned settings.",10.1016/j.spa.2019.02.016,2017-09-29,2024-02-23,"['arnak s. dalalyan', 'avetik g. karagulyan']"
1712.02195,approximations in the homogeneous ising model,stat.me stat.ap stat.co stat.ml,"the ising model is important in statistical modeling and inference in many applications, however its normalizing constant, mean number of active vertices and mean spin interaction -- quantities needed in inference -- are computationally intractable. we provide accurate approximations that make it possible to numerically calculate these quantities in the homogeneous case. simulation studies indicate good performance of our approximation formulae that are scalable and unfazed by the size (number of nodes, degree of graph) of the markov random field. the practical import of our approximation formulae is illustrated in performing bayesian inference in a functional magnetic resonance imaging activation detection experiment, and also in likelihood ratio testing for anisotropy in the spatial patterns of yearly increases in pistachio tree yields.",,2017-12-06,2024-01-20,"['alejandro murua-sazo', 'ranjan maitra']"
1801.07683,discovering the signal subgraph: an iterative screening approach on   graphs,stat.me,"supervised learning on graphs is a challenging task due to the high dimensionality and inherent structural dependencies in the data, where each edge depends on a pair of vertices. existing conventional methods designed for euclidean data do not account for this graph dependency structure. to address this issue, this paper proposes an iterative vertex screening method to identify the signal subgraph that is most informative for the given graph attributes. the method screens the rows and columns of the adjacency matrix concurrently and stops when the resulting distance correlation is maximized. we establish the theoretical foundation of our method by proving that it estimates the true signal subgraph with high probability. additionally, we establish the convergence rate of classification error under the erdos-renyi random graph model and prove that the subsequent classification can be asymptotically optimal, outperforming the entire graph under high-dimensional conditions. our method is evaluated on various simulated datasets and real-world human and murine graphs derived from functional and structural magnetic resonance images. the results demonstrate its excellent performance in estimating the ground-truth signal subgraph and achieving superior classification accuracy.",,2018-01-23,2024-02-05,"['cencheng shen', 'shangsi wang', 'alexandra badea', 'carey e. priebe', 'joshua t. vogelstein']"
1801.09386,tournament leave-pair-out cross-validation for receiver operating   characteristic (roc) analysis,stat.ml,"receiver operating characteristic (roc) analysis is widely used for evaluating diagnostic systems. recent studies have shown that estimating an area under roc curve (auc) with standard cross-validation methods suffers from a large bias. the leave-pair-out (lpo) cross-validation has been shown to correct this bias. however, while lpo produces an almost unbiased estimate of auc, it does not provide a ranking of the data needed for plotting and analyzing the roc curve. in this study, we propose a new method called tournament leave-pair-out (tlpo) cross-validation. this method extends lpo by creating a tournament from pair comparisons to produce a ranking for the data. tlpo preserves the advantage of lpo for estimating auc, while it also allows performing roc analyses. we have shown using both synthetic and real world data that tlpo is as reliable as lpo for auc estimation, and confirmed the bias in leave-one-out cross-validation on low-dimensional data. as a case study on roc analysis, we also evaluate how reliably sensitivity and specificity can be estimated from tlpo roc curves.",10.1177/0962280218795190,2018-01-29,2024-01-24,"['ileana montoya perez', 'antti airola', 'peter j. boström', 'ivan jambor', 'tapio pahikkala']"
1802.05319,500+ times faster than deep learning (a case study exploring faster   methods for text mining stackoverflow),cs.se cs.lg stat.ml,"deep learning methods are useful for high-dimensional data and are becoming widely used in many areas of software engineering. deep learners utilizes extensive computational power and can take a long time to train-- making it difficult to widely validate and repeat and improve their results. further, they are not the best solution in all domains. for example, recent results show that for finding related stack overflow posts, a tuned svm performs similarly to a deep learner, but is significantly faster to train. this paper extends that recent result by clustering the dataset, then tuning very learners within each cluster. this approach is over 500 times faster than deep learning (and over 900 times faster if we use all the cores on a standard laptop computer). significantly, this faster approach generates classifiers nearly as good (within 2\% f1 score) as the much slower deep learning method. hence we recommend this faster methods since it is much easier to reproduce and utilizes far fewer cpu resources. more generally, we recommend that before researchers release research results, that they compare their supposedly sophisticated methods against simpler alternatives (e.g applying simpler learners to build local models).",10.1145/3196398.3196424,2018-02-14,,"['suvodeep majumder', 'nikhila balaji', 'katie brey', 'wei fu', 'tim menzies']"
1803.00679,matrices with gaussian noise: optimal estimates for singular subspace   perturbation,stat.ml cs.it cs.lg math.it math.pr,"the davis-kahan-wedin $\sin \theta$ theorem describes how the singular subspaces of a matrix change when subjected to a small perturbation. this classic result is sharp in the worst case scenario. in this paper, we prove a stochastic version of the davis-kahan-wedin $\sin \theta$ theorem when the perturbation is a gaussian random matrix. under certain structural assumptions, we obtain an optimal bound that significantly improves upon the classic davis-kahan-wedin $\sin \theta$ theorem. one of our key tools is a new perturbation bound for the singular values, which may be of independent interest.",,2018-03-01,2023-12-29,"[""sean o'rourke"", 'van vu', 'ke wang']"
1806.05451,the committee machine: computational to statistical gaps in learning a   two-layers neural network,cs.lg cond-mat.dis-nn cond-mat.stat-mech physics.comp-ph stat.ml,"heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. in this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. we also introduce a version of the approximate message passing (amp) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. we find that there are regimes in which a low generalization error is information-theoretically achievable while the amp algorithm fails to deliver it, strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.",10.1088/1742-5468/ab43d2,2018-06-14,2024-02-29,"['benjamin aubin', 'antoine maillard', 'jean barbier', 'florent krzakala', 'nicolas macris', 'lenka zdeborová']"
1809.04564,on the generalization of stochastic gradient descent with momentum,cs.lg stat.ml,"while momentum-based accelerated variants of stochastic gradient descent (sgd) are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. in this work, we first show that there exists a convex loss function for which the stability gap for multiple epochs of sgd with standard heavy-ball momentum (sgdm) becomes unbounded. then, for smooth lipschitz loss functions, we analyze a modified momentum-based update rule, i.e., sgd with early momentum (sgdem) under a broad range of step-sizes, and show that it can train machine learning models for multiple epochs with a guarantee for generalization. finally, for the special case of strongly convex loss functions, we find a range of momentum such that multiple epochs of standard sgdm, as a special form of sgdem, also generalizes. extending our results on generalization, we also develop an upper bound on the expected true risk, in terms of the number of training steps, sample size, and momentum. our experimental evaluations verify the consistency between the numerical results and our theoretical bounds. sgdem improves the generalization error of sgdm when training resnet-18 on imagenet in practical distributed settings.",,2018-09-12,2024-01-15,"['ali ramezani-kebrya', 'kimon antonakopoulos', 'volkan cevher', 'ashish khisti', 'ben liang']"
1810.01864,agnostic sample compression schemes for regression,cs.lg cs.it math.it math.st stat.ml stat.th,"we obtain the first positive results for bounded sample compression in the agnostic regression setting with the $\ell_p$ loss, where $p\in [1,\infty]$. we construct a generic approximate sample compression scheme for real-valued function classes exhibiting exponential size in the fat-shattering dimension but independent of the sample size. notably, for linear regression, an approximate compression of size linear in the dimension is constructed. moreover, for $\ell_1$ and $\ell_\infty$ losses, we can even exhibit an efficient exact sample compression scheme of size linear in the dimension. we further show that for every other $\ell_p$ loss, $p\in (1,\infty)$, there does not exist an exact agnostic compression scheme of bounded size. this refines and generalizes a negative result of david, moran, and yehudayoff for the $\ell_2$ loss. we close by posing general open questions: for agnostic regression with $\ell_1$ loss, does every function class admits an exact compression scheme of size equal to its pseudo-dimension? for the $\ell_2$ loss, does every function class admit an approximate compression scheme of polynomial size in the fat-shattering dimension? these questions generalize warmuth's classic sample compression conjecture for realizable-case classification.",,2018-10-03,2024-02-03,"['idan attias', 'steve hanneke', 'aryeh kontorovich', 'menachem sadigurschi']"
1810.02897,cdf transform-and-shift: an effective way to deal with datasets of   inhomogeneous cluster densities,cs.lg cs.ai cs.cv stat.ml,"the problem of inhomogeneous cluster densities has been a long-standing issue for distance-based and density-based algorithms in clustering and anomaly detection. these algorithms implicitly assume that all clusters have approximately the same density. as a result, they often exhibit a bias towards dense clusters in the presence of sparse clusters. many remedies have been suggested; yet, we show that they are partial solutions which do not address the issue satisfactorily. to match the implicit assumption, we propose to transform a given dataset such that the transformed clusters have approximately the same density while all regions of locally low density become globally low density -- homogenising cluster density while preserving the cluster structure of the dataset. we show that this can be achieved by using a new multi-dimensional cumulative distribution function in a transform-and-shift method. the method can be applied to every dataset, before the dataset is used in many existing algorithms to match their implicit assumption without algorithmic modification. we show that the proposed method performs better than existing remedies.",10.1016/j.patcog.2021.107977,2018-10-05,2021-04-12,"['ye zhu', 'kai ming ting', 'mark carman', 'maia angelova']"
1810.03393,hierarchical clustering that takes advantage of both density-peak and   density-connectivity,cs.lg cs.ai stat.ml,"this paper focuses on density-based clustering, particularly the density peak (dp) algorithm and the one based on density-connectivity dbscan; and proposes a new method which takes advantage of the individual strengths of these two methods to yield a density-based hierarchical clustering algorithm. our investigation begins with formally defining the types of clusters dp and dbscan are designed to detect; and then identifies the kinds of distributions that dp and dbscan individually fail to detect all clusters in a dataset. these identified weaknesses inspire us to formally define a new kind of clusters and propose a new method called dc-hdp to overcome these weaknesses to identify clusters with arbitrary shapes and varied densities. in addition, the new method produces a richer clustering result in terms of hierarchy or dendrogram for better cluster structures understanding. our empirical evaluation results show that dc-hdp produces the best clustering results on 14 datasets in comparison with 7 state-of-the-art clustering algorithms.",10.1016/j.is.2021.101871,2018-10-08,2021-09-20,"['ye zhu', 'kai ming ting', 'yuan jin', 'maia angelova']"
1810.13306,automated machine learning: from principles to practices,cs.ai cs.lg stat.ml,"machine learning (ml) methods have been developing rapidly, but configuring and selecting proper methods to achieve a desired performance is increasingly difficult and tedious. to address this challenge, automated machine learning (automl) has emerged, which aims to generate satisfactory ml configurations for given tasks in a data-driven way. in this paper, we provide a comprehensive survey on this topic. we begin with the formal definition of automl and then introduce its principles, including the bi-level learning objective, the learning strategy, and the theoretical interpretation. then, we summarize the automl practices by setting up the taxonomy of existing works based on three main factors: the search space, the search algorithm, and the evaluation strategy. each category is also explained with the representative methods. then, we illustrate the principles and practices with exemplary applications from configuring ml pipeline, one-shot neural architecture search, and integration with foundation models. finally, we highlight the emerging directions of automl and conclude the survey.",,2018-10-31,2024-02-27,"['zhenqian shen', 'yongqi zhang', 'lanning wei', 'huan zhao', 'quanming yao']"
1812.02207,better trees: an empirical study on hyperparameter tuning of   classification decision tree induction algorithms,cs.lg stat.ml,"machine learning algorithms often contain many hyperparameters (hps) whose values affect the predictive performance of the induced models in intricate ways. due to the high number of possibilities for these hp configurations and their complex interactions, it is common to use optimization techniques to find settings that lead to high predictive performance. however, insights into efficiently exploring this vast space of configurations and dealing with the trade-off between predictive and runtime performance remain challenging. furthermore, there are cases where the default hps fit the suitable configuration. additionally, for many reasons, including model validation and attendance to new legislation, there is an increasing interest in interpretable models, such as those created by the decision tree (dt) induction algorithms. this paper provides a comprehensive approach for investigating the effects of hyperparameter tuning for the two dt induction algorithms most often used, cart and c4.5. dt induction algorithms present high predictive performance and interpretable classification models, though many hps need to be adjusted. experiments were carried out with different tuning strategies to induce models and to evaluate hps' relevance using 94 classification datasets from openml. the experimental results point out that different hp profiles for the tuning of each algorithm provide statistically significant improvements in most of the datasets for cart, but only in one-third for c4.5. although different algorithms may present different tuning scenarios, the tuning techniques generally required few evaluations to find accurate solutions. furthermore, the best technique for all the algorithms was the irace. finally, we found out that tuning a specific small subset of hps is a good alternative for achieving optimal predictive performance.",10.1007/s10618-024-01002-5,2018-12-05,2023-12-21,"['rafael gomes mantovani', 'tomáš horváth', 'andré l. d. rossi', 'ricardo cerri', 'sylvio barbon junior', 'joaquin vanschoren', 'andré carlos ponce de leon ferreira de carvalho']"
1902.08412,adversarial attacks on graph neural networks via meta learning,cs.lg cs.cr stat.ml,"deep learning models for graphs have advanced the state of the art on many tasks. despite their recent success, little is known about their robustness. we investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. our attacks do not assume any knowledge about or access to the target classifiers.",,2019-02-22,2024-01-28,"['daniel zügner', 'stephan günnemann']"
1905.09239,optimal decision making under strategic behavior,cs.lg cs.cy stat.ml,"we are witnessing an increasing use of data-driven predictive models to inform decisions. as decisions have implications for individuals and society, there is increasing pressure on decision makers to be transparent about their decision policies. at the same time, individuals may use knowledge, gained by transparency, to invest effort strategically in order to maximize their chances of receiving a beneficial decision. our goal is to find decision policies that are optimal in terms of utility in such a strategic setting. to this end, we first characterize how strategic investment of effort by individuals leads to a change in the feature distribution. using this characterization, we first show that, in general, we cannot expect to find optimal decision policies in polynomial time and there are cases in which deterministic policies are suboptimal. then, we demonstrate that, if the cost individuals pay to change their features satisfies a natural monotonicity assumption, we can narrow down the search for the optimal policy to a particular family of decision policies with a set of desirable properties, which allow for a highly effective polynomial time heuristic search algorithm using dynamic programming. finally, under no assumptions on the cost individuals pay to change their features, we develop an iterative search algorithm that is guaranteed to find locally optimal decision policies also in polynomial time. experiments on synthetic and real credit card data illustrate our theoretical findings and show that the decision policies found by our algorithms achieve higher utility than those that do not account for strategic behavior.",10.1287/mnsc.2021.02567,2019-05-22,2020-09-21,"['stratis tsirtsis', 'behzad tabibian', 'moein khajehnejad', 'adish singla', 'bernhard schölkopf', 'manuel gomez-rodriguez']"
1905.10917,temporal-difference learning with nonlinear function approximation: lazy   training and mean field regimes,cs.lg stat.ml,"we discuss the approximation of the value function for infinite-horizon discounted markov reward processes (mrp) with nonlinear functions trained with the temporal-difference (td) learning algorithm. we first consider this problem under a certain scaling of the approximating function, leading to a regime called lazy training. in this regime, the parameters of the model vary only slightly during the learning process, a feature that has recently been observed in the training of neural networks, where the scaling we study arises naturally, implicit in the initialization of their parameters. both in the under- and over-parametrized frameworks, we prove exponential convergence to local, respectively global minimizers of the above algorithm in the lazy training regime. we then compare this scaling of the parameters to the mean-field regime, where the approximately linear behavior of the model is lost. under this alternative scaling we prove that all fixed points of the dynamics in parameter space are global minimizers. we finally give examples of our convergence results in the case of models that diverge if trained with non-lazy td learning, and in the case of neural networks.",,2019-05-26,2021-08-11,"['andrea agazzi', 'jianfeng lu']"
1906.01741,fr\'echet random forests for metric space valued regression with non   euclidean predictors,stat.ml cs.lg stat.me,"random forests are a statistical learning method widely used in many areas of scientific research because of its ability to learn complex relationships between input and output variables and also its capacity to handle high-dimensional data. however, current random forest approaches are not flexible enough to handle heterogeneous data such as curves, images and shapes. in this paper, we introduce fr\'echet trees and fr\'echet random forests, which allow to handle data for which input and output variables take values in general metric spaces. to this end, a new way of splitting the nodes of trees is introduced and the prediction procedures of trees and forests are generalized. then, random forests out-of-bag error and variable importance score are naturally adapted. a consistency theorem for fr\'echet regressogram predictor using data-driven partitions is given and applied to fr\'echet purely uniformly random trees. the method is studied through several simulation scenarios on heterogeneous data combining longitudinal, image and scalar data. finally, one real dataset about air quality is used to illustrate the use of the proposed method in practice.",,2019-06-04,2024-02-16,"['louis capitaine', 'jérémie bigot', 'rodolphe thiébaut', 'robin genuer']"
1906.09744,improving the effectiveness and efficiency of stochastic neighbour   embedding with isolation kernel,cs.lg cs.ai cs.cv stat.ml,"this paper presents a new insight into improving the performance of stochastic neighbour embedding (t-sne) by using isolation kernel instead of gaussian kernel. isolation kernel outperforms gaussian kernel in two aspects. first, the use of isolation kernel in t-sne overcomes the drawback of misrepresenting some structures in the data, which often occurs when gaussian kernel is applied in t-sne. this is because gaussian kernel determines each local bandwidth based on one local point only, while isolation kernel is derived directly from the data based on space partitioning. second, the use of isolation kernel yields a more efficient similarity computation because data-dependent isolation kernel has only one parameter that needs to be tuned. in contrast, the use of data-independent gaussian kernel increases the computational cost by determining n bandwidths for a dataset of n points. as the root cause of these deficiencies in t-sne is gaussian kernel, we show that simply replacing gaussian kernel with isolation kernel in t-sne significantly improves the quality of the final visualisation output (without creating misrepresented structures) and removes one key obstacle that prevents t-sne from processing large datasets. moreover, isolation kernel enables t-sne to deal with large-scale datasets in less runtime without trading off accuracy, unlike existing methods in speeding up t-sne.",10.1613/jair.1.12904,2019-06-24,2021-07-08,"['ye zhu', 'kai ming ting']"
1907.00378,nearest-neighbour-induced isolation similarity and its impact on   density-based clustering,cs.lg stat.ml,"a recent proposal of data dependent similarity called isolation kernel/similarity has enabled svm to produce better classification accuracy. we identify shortcomings of using a tree method to implement isolation similarity; and propose a nearest neighbour method instead. we formally prove the characteristic of isolation similarity with the use of the proposed method. the impact of isolation similarity on density-based clustering is studied here. we show for the first time that the clustering performance of the classic density-based clustering algorithm dbscan can be significantly uplifted to surpass that of the recent density-peak clustering algorithm dp. this is achieved by simply replacing the distance measure with the proposed nearest-neighbour-induced isolation similarity in dbscan, leaving the rest of the procedure unchanged. a new type of clusters called mass-connected clusters is formally defined. we show that dbscan, which detects density-connected clusters, becomes one which detects mass-connected clusters, when the distance measure is replaced with the proposed similarity. we also provide the condition under which mass-connected clusters can be detected, while density-connected clusters cannot.",10.1609/aaai.v33i01.33014755,2019-06-30,,"['xiaoyu qin', 'kai ming ting', 'ye zhu', 'vincent cs lee']"
1908.00286,reinforcement learning for personalized dialogue management,cs.lg cs.ai cs.cl cs.hc stat.ml,"language systems have been of great interest to the research community and have recently reached the mass market through various assistant platforms on the web. reinforcement learning methods that optimize dialogue policies have seen successes in past years and have recently been extended into methods that personalize the dialogue, e.g. take the personal context of users into account. these works, however, are limited to personalization to a single user with whom they require multiple interactions and do not generalize the usage of context across users. this work introduces a problem where a generalized usage of context is relevant and proposes two reinforcement learning (rl)-based approaches to this problem. the first approach uses a single learner and extends the traditional pomdp formulation of dialogue state with features that describe the user context. the second approach segments users by context and then employs a learner per context. we compare these approaches in a benchmark of existing non-rl and rl-based methods in three established and one novel application domain of financial product recommendation. we compare the influence of context and training experiences on performance and find that learning approaches generally outperform a handcrafted gold standard.",10.1145/3350546.3352501,2019-08-01,,"['floris den hengst', 'mark hoogendoorn', 'frank van harmelen', 'joost bosman']"
1908.06486,independence testing for temporal data,stat.ml cs.lg stat.me,"temporal data are increasingly prevalent in modern data science. a fundamental question is whether two time-series are related or not. existing approaches often have limitations, such as relying on parametric assumptions, detecting only linear associations, and requiring multiple tests and corrections. while many non-parametric and universally consistent dependence measures have recently been proposed, directly applying them to temporal data can inflate the p-value and result in invalid test. to address these challenges, this paper introduces the temporal dependence statistic with block permutation to test independence between temporal data. under proper assumptions, the proposed procedure is asymptotically valid and universally consistent for testing independence between stationary time-series, and capable of estimating the optimal dependence lag that maximizes the dependence. notably, it is compatible with a rich family of distance and kernel based dependence measures, eliminates the need for multiple testing, and demonstrates superior power in multivariate, low sample size, and nonlinear settings. an analysis of neural connectivity with fmri data reveals various temporal dependence among signals within the visual network and default mode network.",,2019-08-18,2024-02-05,"['cencheng shen', 'jaewon chung', 'ronak mehta', 'ting xu', 'joshua t. vogelstein']"
1908.08600,online causal inference for advertising in real-time bidding auctions,cs.lg cs.gt econ.em stat.ml,"real-time bidding (rtb) systems, which utilize auctions to allocate user impressions to competing advertisers, continue to enjoy success in digital advertising. assessing the effectiveness of such advertising remains a challenge in research and practice. this paper proposes a new approach to perform causal inference on advertising bought through such mechanisms. leveraging the economic structure of first- and second-price auctions, we first show that the effects of advertising are identified by the optimal bids. hence, since these optimal bids are the only objects that need to be recovered, we introduce an adapted thompson sampling (ts) algorithm to solve a multi-armed bandit problem that succeeds in recovering such bids and, consequently, the effects of advertising while minimizing the costs of experimentation. we derive a regret bound for our algorithm which is order optimal and use data from rtb auctions to show that it outperforms commonly used methods that estimate the effects of advertising.",,2019-08-22,2024-02-25,"['caio waisman', 'harikesh s. nair', 'carlos carrion']"
1910.06002,optimal clustering from noisy binary feedback,stat.ml cs.lg,"we study the problem of clustering a set of items from binary user feedback. such a problem arises in crowdsourcing platforms solving large-scale labeling tasks with minimal effort put on the users. for example, in some of the recent recaptcha systems, users clicks (binary answers) can be used to efficiently label images. in our inference problem, items are grouped into initially unknown non-overlapping clusters. to recover these clusters, the learner sequentially presents to users a finite list of items together with a question with a binary answer selected from a fixed finite set. for each of these items, the user provides a noisy answer whose expectation is determined by the item cluster and the question and by an item-specific parameter characterizing the {\it hardness} of classifying the item. the objective is to devise an algorithm with a minimal cluster recovery error rate. we derive problem-specific information-theoretical lower bounds on the error rate satisfied by any algorithm, for both uniform and adaptive (list, question) selection strategies. for uniform selection, we present a simple algorithm built upon the k-means algorithm and whose performance almost matches the fundamental limits. for adaptive selection, we develop an adaptive algorithm that is inspired by the derivation of the information-theoretical error lower bounds, and in turn allocates the budget in an efficient way. the algorithm learns to select items hard to cluster and relevant questions more often. we compare the performance of our algorithms with or without the adaptive selection strategy numerically and illustrate the gain achieved by being adaptive.",,2019-10-14,2024-02-05,"['kaito ariu', 'jungseul ok', 'alexandre proutiere', 'se-young yun']"
1910.08597,robust learning rate selection for stochastic optimization via splitting   diagnostic,stat.ml cs.lg math.oc stat.me,"this paper proposes splitsgd, a new dynamic learning rate schedule for stochastic optimization. this method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. the detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. owing to this simple yet provably valid stationarity detection, splitsgd is easy-to-implement and essentially does not incur additional computational cost than standard sgd. through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. importantly, this method is observed to be very robust with a set of default parameters for a wide range of problems and, moreover, can yield better generalization performance than other adaptive gradient methods such as adam.",,2019-10-18,2024-02-16,"['matteo sordello', 'niccolò dalmasso', 'hangfeng he', 'weijie su']"
1910.10596,sparse orthogonal variational inference for gaussian processes,stat.ml cs.lg,"we introduce a new interpretation of sparse variational approximations for gaussian processes using inducing points, which can lead to more scalable algorithms than previous methods. it is based on decomposing a gaussian process as a sum of two independent processes: one spanned by a finite basis of inducing points and the other capturing the remaining variation. we show that this formulation recovers existing approximations and at the same time allows to obtain tighter lower bounds on the marginal likelihood and new stochastic variational inference algorithms. we demonstrate the efficiency of these algorithms in several gaussian process models ranging from standard regression to multi-class classification using (deep) convolutional gaussian processes and report state-of-the-art results on cifar-10 among purely gp-based models.",,2019-10-23,2024-02-24,"['jiaxin shi', 'michalis k. titsias', 'andriy mnih']"
1911.08756,classification with costly features in hierarchical deep sets,cs.lg cs.ai stat.ml,"classification with costly features (cwcf) is a classification problem that includes the cost of features in the optimization criteria. individually for each sample, its features are sequentially acquired to maximize accuracy while minimizing the acquired features' cost. however, existing approaches can only process data that can be expressed as vectors of fixed length. in real life, the data often possesses rich and complex structure, which can be more precisely described with formats such as xml or json. the data is hierarchical and often contains nested lists of objects. in this work, we extend an existing deep reinforcement learning-based algorithm with hierarchical deep sets and hierarchical softmax, so that it can directly process this data. the extended method has greater control over which features it can acquire and, in experiments with seven datasets, we show that this leads to superior performance. to showcase the real usage of the new method, we apply it to a real-life problem of classifying malicious web domains, using an online service.",,2019-11-20,2024-02-29,"['jaromír janisch', 'tomáš pevný', 'viliam lisý']"
1912.05638,end-to-end learning of geometrical shaping maximizing generalized mutual   information,eess.sp cs.ai cs.it math.it stat.ml,gmi-based end-to-end learning is shown to be highly nonconvex. we apply gradient descent initialized with gray-labeled apsk constellations directly to the constellation coordinates. state-of-the-art constellations in 2d and 4d are found providing reach increases up to 26\% w.r.t. to qam.,10.1364/ofc.2020.w3d.4,2019-12-11,,"['kadir gümüs', 'alex alvarado', 'bin chen', 'christian häger', 'erik agrell']"
1912.09818,when explanations lie: why many modified bp attributions fail,cs.lg stat.ml,"attribution methods aim to explain a neural network's prediction by highlighting the most relevant image areas. a popular approach is to backpropagate (bp) a custom relevance score using modified rules, rather than the gradient. we analyze an extensive set of modified bp methods: deep taylor decomposition, layer-wise relevance propagation (lrp), excitation bp, patternattribution, deeplift, deconv, rectgrad, and guided bp. we find empirically that the explanations of all mentioned methods, except for deeplift, are independent of the parameters of later layers. we provide theoretical insights for this surprising behavior and also analyze why deeplift does not suffer from this limitation. empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (csc). the paper provides a framework to assess the faithfulness of new and existing modified bp methods theoretically and empirically. for code see: https://github.com/berleon/when-explanations-lie",,2019-12-20,2024-02-19,"['leon sixt', 'maximilian granz', 'tim landgraf']"
1912.11119,mm for penalized estimation,stat.co stat.ml,"penalized estimation can conduct variable selection and parameter estimation simultaneously. the general framework is to minimize a loss function subject to a penalty designed to generate sparse variable selection. the majorization-minimization (mm) algorithm is a computational scheme for stability and simplicity, and the mm algorithm has been widely applied in penalized estimation. much of the previous work have focused on convex loss functions such as generalized linear models. when data are contaminated with outliers, robust loss functions can generate more reliable estimates. recent literature has witnessed a growing impact of nonconvex loss-based methods, which can generate robust estimation for data contaminated with outliers. this article investigates mm algorithm for penalized estimation, provide innovative optimality conditions and establish convergence theory with both convex and nonconvex loss functions. with respect to applications, we focus on several nonconvex loss functions, which were formerly studied in machine learning for regression and classification problems. performance of the proposed algorithms are evaluated on simulated and real data including healthcare costs and cancer clinical status. efficient implementations of the algorithms are available in the r package mpath in cran.",10.1007/s11749-021-00770-2,2019-12-23,2022-03-01,['zhu wang']
2001.01095,high-dimensional independence testing via maximum and average distance   correlations,stat.ml cs.lg stat.me,"this paper introduces and investigates the utilization of maximum and average distance correlations for multivariate independence testing. we characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, assess the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. the resulting tests are non-parametric and applicable to both euclidean distance and the gaussian kernel as the underlying metric. to better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma.",,2020-01-04,2024-02-05,"['cencheng shen', 'yuexiao dong']"
2001.03710,prediction with eventual almost sure guarantees,math.st stat.th,"we study the problem of sequentially predicting properties of a probabilistic model and its next outcome over an infinite horizon, with the goal of ensuring that the predictions incur only finitely many errors with probability 1. we introduce a general framework that models such prediction problems, provide general characterizations for the existence of successful prediction rules, and demonstrate the application of these characterizations through several concrete problem setups, including hypothesis testing, online learning, and risk domination. in particular, our characterizations allow us to recover the findings of dembo and peres (1994) with simple and elementary proofs, and offer a partial resolution to an open problem posed therein.",,2020-01-10,2024-02-06,"['changlong wu', 'narayana santhanam']"
2001.07026,leveraging tensor kernels to reduce objective function mismatch in deep   clustering,stat.ml cs.cv cs.lg,"objective function mismatch (ofm) occurs when the optimization of one objective has a negative impact on the optimization of another objective. in this work we study ofm in deep clustering, and find that the popular autoencoder-based approach to deep clustering can lead to both reduced clustering performance, and a significant amount of ofm between the reconstruction and clustering objectives. to reduce the mismatch, while maintaining the structure-preserving property of an auxiliary objective, we propose a set of new auxiliary objectives for deep clustering, referred to as the unsupervised companion objectives (ucos). the ucos rely on a kernel function to formulate a clustering objective on intermediate representations in the network. generally, intermediate representations can include other dimensions, for instance spatial or temporal, in addition to the feature dimension. we therefore argue that the na\""ive approach of vectorizing and applying a vector kernel is suboptimal for such representations, as it ignores the information contained in the other dimensions. to address this drawback, we equip the ucos with structure-exploiting tensor kernels, designed for tensors of arbitrary rank. the ucos can thus be adapted to a broad class of network architectures. we also propose a novel, regression-based measure of ofm, allowing us to accurately quantify the amount of ofm observed during training. our experiments show that the ofm between the ucos and the main clustering objective is lower, compared to a similar autoencoder-based model. further, we illustrate that the ucos improve the clustering performance of the model, in contrast to the autoencoder-based approach. the code for our experiments is available at https://github.com/danieltrosten/tk-uco.",10.1016/j.patcog.2023.110229,2020-01-20,2024-02-13,"['daniel j. trosten', 'sigurd løkse', 'robert jenssen', 'michael kampffmeyer']"
2001.11704,boosting simple learners,cs.lg stat.ml,"boosting is a celebrated machine learning approach which is based on the idea of combining weak and moderately inaccurate hypotheses to a strong and accurate one. we study boosting under the assumption that the weak hypotheses belong to a class of bounded capacity. this assumption is inspired by the common convention that weak hypotheses are ""rules-of-thumbs"" from an ""easy-to-learn class"". (schapire and freund~'12, shalev-shwartz and ben-david '14.) formally, we assume the class of weak hypotheses has a bounded vc dimension. we focus on two main questions: (i) oracle complexity: how many weak hypotheses are needed to produce an accurate hypothesis? we design a novel boosting algorithm and demonstrate that it circumvents a classical lower bound by freund and schapire ('95, '12). whereas the lower bound shows that $\omega({1}/{\gamma^2})$ weak hypotheses with $\gamma$-margin are sometimes necessary, our new method requires only $\tilde{o}({1}/{\gamma})$ weak hypothesis, provided that they belong to a class of bounded vc dimension. unlike previous boosting algorithms which aggregate the weak hypotheses by majority votes, the new boosting algorithm uses more complex (""deeper"") aggregation rules. we complement this result by showing that complex aggregation rules are in fact necessary to circumvent the aforementioned lower bound. (ii) expressivity: which tasks can be learned by boosting weak hypotheses from a bounded vc class? can complex concepts that are ""far away"" from the class be learned? towards answering the first question we {introduce combinatorial-geometric parameters which capture expressivity in boosting.} as a corollary we provide an affirmative answer to the second question for well-studied classes, including half-spaces and decision stumps. along the way, we establish and exploit connections with discrepancy theory.",10.46298/theoretics.23.8,2020-01-31,2023-06-16,"['noga alon', 'alon gonen', 'elad hazan', 'shay moran']"
2002.00178,an equivalence between bayesian priors and penalties in variational   inference,cs.lg math.st stat.ml stat.th,"in machine learning, it is common to optimize the parameters of a probabilistic model, modulated by an ad hoc regularization term that penalizes some values of the parameters. regularization terms appear naturally in variational inference, a tractable way to approximate bayesian posteriors: the loss to optimize contains a kullback--leibler divergence term between the approximate posterior and a bayesian prior. we fully characterize the regularizers that can arise according to this procedure, and provide a systematic way to compute the prior corresponding to a given penalty. such a characterization can be used to discover constraints over the penalty function, so that the overall procedure remains bayesian.",,2020-02-01,2024-02-07,"['pierre wolinski', 'guillaume charpiat', 'yann ollivier']"
2002.01431,mean shift cluster recognition method implementation in the nested   sampling algorithm,stat.co astro-ph.im physics.comp-ph stat.ml,"nested sampling is an efficient algorithm for the calculation of the bayesian evidence and posterior parameter probability distributions. it is based on the step-by-step exploration of the parameter space by monte carlo sampling with a series of values sets called live points that evolve towards the region of interest, i.e. where the likelihood function is maximal. in presence of several local likelihood maxima, the algorithm converges with difficulty. some systematic errors can also be introduced by unexplored parameter volume regions. in order to avoid this, different methods are proposed in the literature for an efficient search of new live points, even in presence of local maxima. here we present a new solution based on the mean shift cluster recognition method implemented in a random walk search algorithm. the clustering recognition is integrated within the bayesian analysis program nestedfit. it is tested with the analysis of some difficult cases. compared to the analysis results without cluster recognition, the computation time is considerably reduced. at the same time, the entire parameter space is efficiently explored, which translates into a smaller uncertainty of the extracted value of the bayesian evidence.",10.3390/e22020185,2020-01-31,,"['m. trassinelli', 'pierre ciccodicola']"
2002.01444,learning of linear dynamical systems as a non-commutative polynomial   optimization problem,math.oc cs.lg cs.sy eess.sy stat.ml,"there has been much recent progress in forecasting the next observation of a linear dynamical system (lds), which is known as the improper learning, as well as in the estimation of its system matrices, which is known as the proper learning of lds. we present an approach to proper learning of lds, which in spite of the non-convexity of the problem, guarantees global convergence of numerical solutions to a least-squares estimator. we present promising computational results.",10.1109/tac.2023.3313351,2020-02-04,2024-02-27,"['quan zhou', 'jakub marecek']"
2002.03339,input validation for neural networks via runtime local robustness   verification,cs.lg stat.ml,"local robustness verification can verify that a neural network is robust wrt. any perturbation to a specific input within a certain distance. we call this distance robustness radius. we observe that the robustness radii of correctly classified inputs are much larger than that of misclassified inputs which include adversarial examples, especially those from strong adversarial attacks. another observation is that the robustness radii of correctly classified inputs often follow a normal distribution. based on these two observations, we propose to validate inputs for neural networks via runtime local robustness verification. experiments show that our approach can protect neural networks from adversarial examples and improve their accuracies.",,2020-02-09,2024-02-13,"['jiangchao liu', 'liqian chen', 'antoine mine', 'ji wang']"
2002.05465,nonasymptotic analysis of stochastic gradient hamiltonian monte carlo   under local conditions for nonconvex optimization,math.oc stat.co stat.ml,"we provide a nonasymptotic analysis of the convergence of the stochastic gradient hamiltonian monte carlo (sghmc) to a target measure in wasserstein-2 distance without assuming log-concavity. our analysis quantifies key theoretical properties of the sghmc as a sampler under local conditions which significantly improves the findings of previous results. in particular, we prove that the wasserstein-2 distance between the target and the law of the sghmc is uniformly controlled by the step-size of the algorithm, therefore demonstrate that the sghmc can provide high-precision results uniformly in the number of iterations. the analysis also allows us to obtain nonasymptotic bounds for nonconvex optimization problems under local conditions and implies that the sghmc, when viewed as a nonconvex optimizer, converges to a global minimum with the best known rates. we apply our results to obtain nonasymptotic bounds for scalable bayesian inference and nonasymptotic generalization bounds.",,2020-02-13,2024-01-28,"['ömer deniz akyildiz', 'sotirios sabanis']"
2002.07756,hierarchical correlation clustering and tree preserving embedding,cs.lg stat.ml,"we propose a hierarchical correlation clustering method that extends the well-known correlation clustering to produce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. then, in the following, we study unsupervised representation learning with such hierarchical correlation clustering. for this purpose, we first investigate embedding the respective hierarchy to be used for tree-preserving embedding and feature extraction. thereafter, we study the extension of minimax distance measures to correlation clustering, as another representation learning paradigm. finally, we demonstrate the performance of our methods on several datasets.",,2020-02-18,2024-01-09,"['morteza haghir chehreghani', 'mostafa haghir chehreghani']"
2002.10046,permutation inference for canonical correlation analysis,stat.me math.st stat.ap stat.co stat.ml stat.th,"canonical correlation analysis (cca) has become a key tool for population neuroimaging, allowing investigation of associations between many imaging and non-imaging measurements. as other variables are often a source of variability not of direct interest, previous work has used cca on residuals from a model that removes these effects, then proceeded directly to permutation inference. we show that such a simple permutation test leads to inflated error rates. the reason is that residualisation introduces dependencies among the observations that violate the exchangeability assumption. even in the absence of nuisance variables, however, a simple permutation test for cca also leads to excess error rates for all canonical correlations other than the first. the reason is that a simple permutation scheme does not ignore the variability already explained by previous canonical variables. here we propose solutions for both problems: in the case of nuisance variables, we show that transforming the residuals to a lower dimensional basis where exchangeability holds results in a valid permutation test; for more general cases, with or without nuisance variables, we propose estimating the canonical correlations in a stepwise manner, removing at each iteration the variance already explained, while dealing with different number of variables in both sides. we also discuss how to address the multiplicity of tests, proposing an admissible test that is not conservative, and provide a complete algorithm for permutation inference for cca.",10.1016/j.neuroimage.2020.117065,2020-02-23,2020-06-17,"['anderson m. winkler', 'olivier renaud', 'stephen m. smith', 'thomas e. nichols']"
2002.10940,stochastic-sign sgd for federated learning with theoretical guarantees,cs.lg stat.ml,"federated learning (fl) has emerged as a prominent distributed learning paradigm. fl entails some pressing needs for developing novel parameter estimation approaches with theoretical guarantees of convergence, which are also communication efficient, differentially private and byzantine resilient in the heterogeneous data distribution settings. quantization-based sgd solvers have been widely adopted in fl and the recently proposed signsgd with majority vote shows a promising direction. however, no existing methods enjoy all the aforementioned properties. in this paper, we propose an intuitively-simple yet theoretically-sound method based on signsgd to bridge the gap. we present stochastic-sign sgd which utilizes novel stochastic-sign based gradient compressors enabling the aforementioned properties in a unified framework. we also present an error-feedback variant of the proposed stochastic-sign sgd which further improves the learning performance in fl. we test the proposed method with extensive experiments using deep neural networks on the mnist dataset and the cifar-10 dataset. the experimental results corroborate the effectiveness of the proposed method.",10.1109/tnnls.2023.3345367,2020-02-25,2021-09-27,"['richeng jin', 'yufan huang', 'xiaofan he', 'huaiyu dai', 'tianfu wu']"
2002.12410,on biased compression for distributed learning,cs.lg cs.dc math.oc stat.ml,"in the last few years, various communication compression techniques have emerged as an indispensable tool helping to alleviate the communication bottleneck in distributed learning. however, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors, very little is known about them. in this work we study three classes of biased compression operators, two of which are new, and their performance when applied to (stochastic) gradient descent and distributed (stochastic) gradient descent. we show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings. we prove that distributed compressed sgd method, employed with error feedback mechanism, enjoys the ergodic rate $o\left( \delta l \exp \left[-\frac{\mu k}{\delta l}\right] + \frac{(c + \delta d)}{k\mu}\right)$, where $\delta\ge 1$ is a compression parameter which grows when more compression is applied, $l$ and $\mu$ are the smoothness and strong convexity constants, $c$ captures stochastic gradient noise ($c=0$ if full gradients are computed on each node) and $d$ captures the variance of the gradients at the optimum ($d=0$ for over-parameterized models). further, via a theoretical study of several synthetic and empirical distributions of communicated gradients, we shed light on why and by how much biased compressors outperform their unbiased variants. finally, we propose several new biased compressors with promising theoretical guarantees and practical performance.",,2020-02-27,2024-01-14,"['aleksandr beznosikov', 'samuel horváth', 'peter richtárik', 'mher safaryan']"
2003.03546,adversarial machine learning: bayesian perspectives,cs.ai cs.lg stat.co stat.ml,"adversarial machine learning (aml) is emerging as a major field aimed at protecting machine learning (ml) systems against security threats: in certain scenarios there may be adversaries that actively manipulate input data to fool learning systems. this creates a new class of security vulnerabilities that ml systems may face, and a new desirable property called adversarial robustness essential to trust operations based on ml outputs. most work in aml is built upon a game-theoretic modelling of the conflict between a learning system and an adversary, ready to manipulate input data. this assumes that each agent knows their opponent's interests and uncertainty judgments, facilitating inferences based on nash equilibria. however, such common knowledge assumption is not realistic in the security scenarios typical of aml. after reviewing such game-theoretic approaches, we discuss the benefits that bayesian perspectives provide when defending ml-based systems. we demonstrate how the bayesian approach allows us to explicitly model our uncertainty about the opponent's beliefs and interests, relaxing unrealistic assumptions, and providing more robust inferences. we illustrate this approach in supervised learning settings, and identify relevant future research problems.",10.1080/01621459.2023.2183129,2020-03-07,2024-02-22,"['david rios insua', 'roi naveiro', 'victor gallego', 'jason poulos']"
2003.06281,bayesflow: learning complex stochastic models with invertible neural   networks,stat.ml cs.lg,"estimating the parameters of mathematical models is a common problem in almost all branches of science. however, this problem can prove notably difficult when processes and model descriptions become increasingly complex and an explicit likelihood function is not available. with this work, we propose a novel method for globally amortized bayesian inference based on invertible neural networks which we call bayesflow. the method uses simulation to learn a global estimator for the probabilistic mapping from observed data to underlying model parameters. a neural network pre-trained in this way can then, without additional training or optimization, infer full posteriors on arbitrary many real datasets involving the same model family. in addition, our method incorporates a summary network trained to embed the observed data into maximally informative summary statistics. learning summary statistics from data makes the method applicable to modeling scenarios where standard inference techniques with hand-crafted summary statistics fail. we demonstrate the utility of bayesflow on challenging intractable models from population dynamics, epidemiology, cognitive science and ecology. we argue that bayesflow provides a general framework for building amortized bayesian parameter estimation machines for any forward model from which data can be simulated.",,2020-03-13,2020-12-01,"['stefan t. radev', 'ulf k. mertens', 'andreas voss', 'lynton ardizzone', 'ullrich köthe']"
2003.13119,"statistical quantile learning for large, nonlinear, and additive latent   variable models",stat.me,"the studies of large-scale, high-dimensional data in fields such as genomics and neuroscience have injected new insights into science. yet, despite advances, they are confronting several challenges, often simultaneously: lack of interpretability, nonlinearity, slow computation, inconsistency and uncertain convergence, and small sample sizes compared to high feature dimensions. here, we propose a relatively simple, scalable, and consistent nonlinear dimension reduction method that can potentially address these issues in unsupervised settings. we call this method statistical quantile learning (sql) because, methodologically, it leverages on a quantile approximation of the latent variables together with standard nonparametric techniques (sieve or penalyzed methods). we show that estimating the model simplifies into a convex assignment matching problem; we derive its asymptotic properties; we show that the model is identifiable under few conditions. compared to its linear competitors, sql explains more variance, yields better separation and explanation, and delivers more accurate outcome prediction. compared to its nonlinear competitors, sql shows considerable advantage in interpretability, ease of use and computations in large-dimensional settings. finally, we apply sql to high-dimensional gene expression data (consisting of 20,263 genes from 801 subjects), where the proposed method identified latent factors predictive of five cancer types. the sql package is available at https://github.com/jbodelet/sql.",,2020-03-29,2023-12-29,"['julien bodelet', 'guillaume blanc', 'jiajun shan', 'graciela muniz terrera', 'oliver y. chen']"
2004.05839,a theory of the risk for optimization with relaxation and its   application to support vector machines,cs.lg cs.sy eess.sy math.oc stat.ml,"in this paper we consider optimization with relaxation, an ample paradigm to make data-driven designs. this approach was previously considered by the same authors of this work in garatti and campi (2019), a study that revealed a deep-seated connection between two concepts: risk (probability of not satisfying a new, out-of-sample, constraint) and complexity (according to a definition introduced in paper garatti and campi (2019)). this connection was shown to have profound implications in applications because it implied that the risk can be estimated from the complexity, a quantity that can be measured from the data without any knowledge of the data-generation mechanism. in the present work we establish new results. first, we expand the scope of garatti and campi (2019) so as to embrace a more general setup that covers various algorithms in machine learning. then, we study classical support vector methods - including svm (support vector machine), svr (support vector regression) and svdd (support vector data description) - and derive new results for the ability of these methods to generalize. all results are valid for any finite size of the data set. when the sample size tends to infinity, we establish the unprecedented result that the risk approaches the ratio between the complexity and the cardinality of the data sample, regardless of the value of the complexity.",,2020-04-13,2024-01-08,"['marco c. campi', 'simone garatti']"
2004.12908,representation ensembling for synergistic lifelong learning with   quasilinear complexity,cs.ai cs.lg stat.ml,"in lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. while typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. but striving to avoid forgetting sets the goal unnecessarily low. the goal of lifelong learning should be to use data to improve performance on both future tasks (forward transfer) and past tasks (backward transfer). our key insight is that we can ensemble representations that were learned independently on disparate tasks to enable both forward and backward transfer, with algorithms that run in quasilinear time. our algorithms demonstrate both forward and backward transfer in a variety of simulated and benchmark data scenarios, including tabular, vision (cifar-100, 5-dataset, split mini-imagenet, and food1k), and audition (spoken digit), including adversarial tasks, in contrast to various reference algorithms, which typically failed to transfer either forward or backward, or both.",,2020-04-27,2024-02-02,"['joshua t. vogelstein', 'jayanta dey', 'hayden s. helm', 'will levine', 'ronak d. mehta', 'tyler m. tomita', 'haoyin xu', 'ali geisa', 'qingyang wang', 'gido m. van de ven', 'chenyu gao', 'weiwei yang', 'bryan tower', 'jonathan larson', 'christopher m. white', 'carey e. priebe']"
2005.03625,know your clients' behaviours: a cluster analysis of financial   transactions,econ.em stat.ap stat.ml,"in canada, financial advisors and dealers are required by provincial securities commissions and self-regulatory organizations--charged with direct regulation over investment dealers and mutual fund dealers--to respectively collect and maintain know your client (kyc) information, such as their age or risk tolerance, for investor accounts. with this information, investors, under their advisor's guidance, make decisions on their investments which are presumed to be beneficial to their investment goals. our unique dataset is provided by a financial investment dealer with over 50,000 accounts for over 23,000 clients. we use a modified behavioural finance recency, frequency, monetary model for engineering features that quantify investor behaviours, and machine learning clustering algorithms to find groups of investors that behave similarly. we show that the kyc information collected does not explain client behaviours, whereas trade and transaction frequency and volume are most informative. we believe the results shown herein encourage financial regulators and advisors to use more advanced metrics to better understand and predict investor behaviours.",10.3390/jrfm14020050,2020-05-07,2020-05-14,"['john r. j. thompson', 'longlong feng', 'r. mark reesor', 'chuck grace']"
2005.07972,conformal prediction: a unified review of theory and new challenges,cs.lg econ.em stat.me stat.ml,"in this work we provide a review of basic ideas and novel developments about conformal prediction -- an innovative distribution-free, non-parametric forecasting method, based on minimal assumptions -- that is able to yield in a very straightforward way predictions sets that are valid in a statistical sense also in in the finite sample case. the in-depth discussion provided in the paper covers the theoretical underpinnings of conformal prediction, and then proceeds to list the more advanced developments and adaptations of the original idea.",10.3150/21-bej1447,2020-05-16,2022-07-29,"['matteo fontana', 'gianluca zeni', 'simone vantini']"
2005.09218,large margin mechanism and pseudo query set on cross-domain few-shot   learning,cs.lg stat.ml,"in recent years, few-shot learning problems have received a lot of attention. while methods in most previous works were trained and tested on datasets in one single domain, cross-domain few-shot learning is a brand-new branch of few-shot learning problems, where models handle datasets in different domains between training and testing phases. in this paper, to solve the problem that the model is pre-trained (meta-trained) on a single dataset while fine-tuned on datasets in four different domains, including common objects, satellite images, and medical images, we propose a novel large margin fine-tuning method (lmm-pqs), which generates pseudo query images from support images and fine-tunes the feature extraction modules with a large margin mechanism inspired by methods in face recognition. according to the experiment results, lmm-pqs surpasses the baseline models by a significant margin and demonstrates that our approach is robust and can easily adapt pre-trained models to new domains with few data.",,2020-05-19,2024-02-06,"['jia-fong yeh', 'hsin-ying lee', 'bing-chen tsai', 'yi-rong chen', 'ping-chia huang', 'winston h. hsu']"
2005.10902,global optimization of gaussian processes,math.oc cs.lg stat.ml,"gaussian processes~(kriging) are interpolating data-driven models that are frequently applied in various disciplines. often, gaussian processes are trained on datasets and are subsequently embedded as surrogate models in optimization problems. these optimization problems are nonconvex and global optimization is desired. however, previous literature observed computational burdens limiting deterministic global optimization to gaussian processes trained on few data points. we propose a reduced-space formulation for deterministic global optimization with trained gaussian processes embedded. for optimization, the branch-and-bound solver branches only on the degrees of freedom and mccormick relaxations are propagated through explicit gaussian process models. the approach also leads to significantly smaller and computationally cheaper subproblems for lower and upper bounding. to further accelerate convergence, we derive envelopes of common covariance functions for gps and tight relaxations of acquisition functions used in bayesian optimization including expected improvement, probability of improvement, and lower confidence bound. in total, we reduce computational time by orders of magnitude compared to state-of-the-art methods, thus overcoming previous computational burdens. we demonstrate the performance and scaling of the proposed method and apply it to bayesian optimization with global optimization of the acquisition function and chance-constrained programming. the gaussian process models, acquisition functions, and training scripts are available open-source within the ""melon - machine learning models for optimization"" toolbox~(https://git.rwth-aachen.de/avt.svt/public/melon).",10.1007/s12532-021-00204-y,2020-05-21,,"['artur m. schweidtmann', 'dominik bongartz', 'daniel grothe', 'tim kerkenhoff', 'xiaopeng lin', 'jaromil najman', 'alexander mitsos']"
2006.05259,wavelet networks: scale-translation equivariant learning from raw   time-series,cs.lg stat.ml,"leveraging the symmetries inherent to specific data domains for the construction of equivariant neural networks has lead to remarkable improvements in terms of data efficiency and generalization. however, most existing research focuses on symmetries arising from planar and volumetric data, leaving a crucial data source largely underexplored: time-series. in this work, we fill this gap by leveraging the symmetries inherent to time-series for the construction of equivariant neural network. we identify two core symmetries: *scale and translation*, and construct scale-translation equivariant neural networks for time-series learning. intriguingly, we find that scale-translation equivariant mappings share strong resemblance with the wavelet transform. inspired by this resemblance, we term our networks wavelet networks, and show that they perform nested non-linear wavelet-like time-frequency transforms. empirical results show that wavelet networks outperform conventional cnns on raw waveforms, and match strongly engineered spectrogram techniques across several tasks and time-series types, including audio, environmental sounds, and electrical signals. our code is publicly available at https://github.com/dwromero/wavelet_networks.",,2020-06-09,2024-01-21,"['david w. romero', 'erik j. bekkers', 'jakub m. tomczak', 'mark hoogendoorn']"
2006.06530,age-structured estimation of covid-19 icu demand from low quality data,physics.soc-ph q-bio.pe stat.ml,"we sample aggravated cases following age-structured probabilities from confirmed cases and use icu occupation data to find a subnotification factor. a logistic fit is then employed to project the progression of the covid-19 epidemic with plateau scenarios taken from locations that have reached this stage. finally, the logistic curve found is corrected by the subnotification factor and sampled to project the future demand for icu beds.",,2020-06-08,2020-06-15,"['rodrigo veiga', 'rodrigo murta', 'renato vicente']"
2006.07796,structure by architecture: structured representations without   regularization,cs.lg cs.cv stat.ml,"we study the problem of self-supervised structured representation learning using autoencoders for downstream tasks such as generative modeling. unlike most methods which rely on matching an arbitrary, relatively unstructured, prior distribution for sampling, we propose a sampling technique that relies solely on the independence of latent variables, thereby avoiding the trade-off between reconstruction quality and generative performance typically observed in vaes. we design a novel autoencoder architecture capable of learning a structured representation without the need for aggressive regularization. our structural decoders learn a hierarchy of latent variables, thereby ordering the information without any additional regularization or supervision. we demonstrate how these models learn a representation that improves results in a variety of downstream tasks including generation, disentanglement, and extrapolation using several challenging and natural image datasets.",,2020-06-14,2024-02-15,"['felix leeb', 'guilia lanzillotta', 'yashas annadani', 'michel besserve', 'stefan bauer', 'bernhard schölkopf']"
2006.07841,classify and generate reciprocally: simultaneous positive-unlabelled   learning and conditional generation with extra data,cs.lg stat.ml,"the scarcity of class-labeled data is a ubiquitous bottleneck in many machine learning problems. while abundant unlabeled data typically exist and provide a potential solution, it is highly challenging to exploit them. in this paper, we address this problem by leveraging positive-unlabeled~(pu) classification and the conditional generation with extra unlabeled data \emph{simultaneously}. in particular, we present a novel training framework to jointly target both pu classification and conditional generation when exposed to extra data, especially out-of-distribution unlabeled data, by exploring the interplay between them: 1) enhancing the performance of pu classifiers with the assistance of a novel classifier-noise-invariant conditional gan~(cni-cgan) that is robust to noisy labels, 2) leveraging extra data with predicted labels from a pu classifier to help the generation. theoretically, we prove the optimal condition of cni-cgan, and experimentally, we conducted extensive evaluations on diverse datasets, verifying the simultaneous improvements in both classification and generation.",,2020-06-14,2024-02-08,"['bing yu', 'ke sun', 'he wang', 'zhouchen lin', 'zhanxing zhu']"
2006.09587,"adaptive, rate-optimal hypothesis testing in nonparametric iv models",econ.em stat.me stat.ml,"we propose a new adaptive hypothesis test for inequality (e.g., monotonicity, convexity) and equality (e.g., parametric, semiparametric) restrictions on a structural function in a nonparametric instrumental variables (npiv) model. our test statistic is based on a modified leave-one-out sample analog of a quadratic distance between the restricted and unrestricted sieve npiv estimators. we provide computationally simple, data-driven choices of sieve tuning parameters and bonferroni adjusted chi-squared critical values. our test adapts to the unknown smoothness of alternative functions in the presence of unknown degree of endogeneity and unknown strength of the instruments. it attains the adaptive minimax rate of testing in $l^2$.   that is, the sum of its type i error uniformly over the composite null and its type ii error uniformly over nonparametric alternative models cannot be improved by any other hypothesis test for npiv models of unknown regularities. confidence sets in $l^2$ are obtained by inverting the adaptive test. simulations confirm that our adaptive test controls size and its finite-sample power greatly exceeds existing non-adaptive tests for monotonicity and parametric restrictions in npiv models. empirical applications to test for shape restrictions of differentiated products demand and of engel curves are presented.",,2020-06-16,2024-02-06,"['christoph breunig', 'xiaohong chen']"
2006.10628,offline detection of change-points in the mean for stationary graph   signals,cs.lg stat.ap stat.ml,"this paper addresses the problem of segmenting a stream of graph signals: we aim to detect changes in the mean of a multivariate signal defined over the nodes of a known graph. we propose an offline method that relies on the concept of graph signal stationarity and allows the convenient translation of the problem from the original vertex domain to the spectral domain (graph fourier transform), where it is much easier to solve. although the obtained spectral representation is sparse in real applications, to the best of our knowledge this property has not been sufficiently exploited in the existing related literature. our change-point detection method adopts a model selection approach that takes into account the sparsity of the spectral representation and determines automatically the number of change-points. our detector comes with a proof of a non-asymptotic oracle inequality. numerical experiments demonstrate the performance of the proposed method.",,2020-06-18,2024-02-29,"['alejandro de la concha', 'nicolas vayatis', 'argyris kalogeratos']"
2006.13309,fast deep mixtures of gaussian process experts,cs.lg stat.ml,"mixtures of experts have become an indispensable tool for flexible modelling in a supervised learning context, allowing not only the mean function but the entire density of the output to change with the inputs. sparse gaussian processes (gp) have shown promise as a leading candidate for the experts in such models, and in this article, we propose to design the gating network for selecting the experts from such mixtures of sparse gps using a deep neural network (dnn). furthermore, a fast one pass algorithm called cluster-classify-regress (ccr) is leveraged to approximate the maximum a posteriori (map) estimator extremely quickly. this powerful combination of model and algorithm together delivers a novel method which is flexible, robust, and extremely efficient. in particular, the method is able to outperform competing methods in terms of accuracy and uncertainty quantification. the cost is competitive on low-dimensional and small data sets, but is significantly lower for higher-dimensional and big data sets. iteratively maximizing the distribution of experts given allocations and allocations given experts does not provide significant improvement, which indicates that the algorithm achieves a good approximation to the local map estimator very fast. this insight can be useful also in the context of other mixture of experts models.",10.1007/s10994-023-06491-x,2020-06-11,2023-11-30,"['clement etienam', 'kody law', 'sara wade', 'vitaly zankin']"
2006.14266,intrinsic gaussian processes on manifolds and their accelerations by   symmetry,math.oc stat.ml,"amidst the growing interest in nonparametric regression, we address a significant challenge in gaussian processes(gp) applied to manifold-based predictors. existing methods primarily focus on low dimensional constrained domains for heat kernel estimation, limiting their effectiveness in higher-dimensional manifolds. our research proposes an intrinsic approach for constructing gp on general manifolds such as orthogonal groups, unitary groups, stiefel manifolds and grassmannian manifolds. our methodology estimates the heat kernel by simulating brownian motion sample paths using the exponential map, ensuring independence from the manifold's embedding. the introduction of our strip algorithm, tailored for manifolds with extra symmetries, and the ball algorithm, designed for arbitrary manifolds, constitutes our significant contribution. both algorithms are rigorously substantiated through theoretical proofs and numerical testing, with the strip algorithm showcasing remarkable efficiency gains over traditional methods. this intrinsic approach delivers several key advantages, including applicability to high dimensional manifolds, eliminating the requirement for global parametrization or embedding. we demonstrate its practicality through regression case studies (torus knots and eight dimensional projective spaces) and by developing binary classifiers for real world datasets (gorilla skulls planar images and diffusion tensor images). these classifiers outperform traditional methods, particularly in limited data scenarios.",,2020-06-25,2024-01-31,"['ke ye', 'mu niu', 'pokman cheung', 'zhenwen dai', 'yuan liu']"
2007.02192,tail-adaptive bayesian shrinkage,math.st stat.ap stat.co stat.me stat.ml stat.th,"robust bayesian methods for high-dimensional regression problems under diverse sparse regimes are studied. traditional shrinkage priors are primarily designed to detect a handful of signals from tens of thousands of predictors in the so-called ultra-sparsity domain. however, they may not perform desirably when the degree of sparsity is moderate. in this paper, we propose a robust sparse estimation method under diverse sparsity regimes, which has a tail-adaptive shrinkage property. in this property, the tail-heaviness of the prior adjusts adaptively, becoming larger or smaller as the sparsity level increases or decreases, respectively, to accommodate more or fewer signals, a posteriori. we propose a global-local-tail (glt) gaussian mixture distribution that ensures this property. we examine the role of the tail-index of the prior in relation to the underlying sparsity level and demonstrate that the glt posterior contracts at the minimax optimal rate for sparse normal mean models. we apply both the glt prior and the horseshoe prior to a real data problem and simulation examples. our findings indicate that the varying tail rule based on the glt prior offers advantages over a fixed tail rule based on the horseshoe prior in diverse sparsity regimes.",,2020-07-04,2024-02-19,"['se yoon lee', 'peng zhao', 'debdeep pati', 'bani k. mallick']"
2007.05690,a unified linear speedup analysis of federated averaging and nesterov   fedavg,cs.lg stat.ml,"federated learning (fl) learns a model jointly from a set of participating devices without sharing each other's privately held data. the characteristics of non-i.i.d. data across the network, low device participation, high communication costs, and the mandate that data remain private bring challenges in understanding the convergence of fl algorithms, particularly regarding how convergence scales with the number of participating devices. in this paper, we focus on federated averaging (fedavg), one of the most popular and effective fl algorithms in use today, as well as its nesterov accelerated variant, and conduct a systematic study of how their convergence scale with the number of participating devices under non-i.i.d. data and partial participation in convex settings. we provide a unified analysis that establishes convergence guarantees for fedavg under strongly convex, convex, and overparameterized strongly convex problems. we show that fedavg enjoys linear speedup in each case, although with different convergence rates and communication efficiencies. for strongly convex and convex problems, we also characterize the corresponding convergence rates for the nesterov accelerated fedavg algorithm, which are the first linear speedup guarantees for momentum variants of fedavg in convex settings. empirical studies of the algorithms in various settings have supported our theoretical results.",,2020-07-11,2023-12-31,"['zhaonan qu', 'kaixiang lin', 'zhaojian li', 'jiayu zhou', 'zhengyuan zhou']"
2007.05943,on the generalization of tanimoto-type kernels to real valued functions,cs.lg stat.ml,"the tanimoto kernel (jaccard index) is a well known tool to describe the similarity between sets of binary attributes. it has been extended to the case when the attributes are nonnegative real values. this paper introduces a more general tanimoto kernel formulation which allows to measure the similarity of arbitrary real-valued functions. this extension is constructed by unifying the representation of the attributes via properly chosen sets. after deriving the general form of the kernel, explicit feature representation is extracted from the kernel function, and a simply way of including general kernels into the tanimoto kernel is shown. finally, the kernel is also expressed as a quotient of piecewise linear functions, and a smooth approximation is provided.",,2020-07-12,2024-02-26,"['sandor szedmak', 'eric bach']"
2007.06007,universal approximation power of deep residual neural networks via   nonlinear control theory,cs.lg cs.sy eess.sy math.oc stat.ml,"in this paper, we explain the universal approximation capabilities of deep residual neural networks through geometric nonlinear control. inspired by recent work establishing links between residual networks and control systems, we provide a general sufficient condition for a residual network to have the power of universal approximation by asking the activation function, or one of its derivatives, to satisfy a quadratic differential equation. many activation functions used in practice satisfy this assumption, exactly or approximately, and we show this property to be sufficient for an adequately deep neural network with $n+1$ neurons per layer to approximate arbitrarily well, on a compact set and with respect to the supremum norm, any continuous function from $\mathbb{r}^n$ to $\mathbb{r}^n$. we further show this result to hold for very simple architectures for which the weights only need to assume two values. the first key technical contribution consists of relating the universal approximation problem to controllability of an ensemble of control systems corresponding to a residual network and to leverage classical lie algebraic techniques to characterize controllability. the second technical contribution is to identify monotonicity as the bridge between controllability of finite ensembles and uniform approximability on compact sets.",,2020-07-12,2024-02-09,"['paulo tabuada', 'bahman gharesifard']"
2007.06169,an adversarial approach to structural estimation,econ.em cs.lg math.st stat.me stat.ml stat.th,"we propose a new simulation-based estimation method, adversarial estimation, for structural models. the estimator is formulated as the solution to a minimax problem between a generator (which generates simulated observations using the structural model) and a discriminator (which classifies whether an observation is simulated). the discriminator maximizes the accuracy of its classification while the generator minimizes it. we show that, with a sufficiently rich discriminator, the adversarial estimator attains parametric efficiency under correct specification and the parametric rate under misspecification. we advocate the use of a neural network as a discriminator that can exploit adaptivity properties and attain fast rates of convergence. we apply our method to the elderly's saving decision model and show that our estimator uncovers the bequest motive as an important source of saving across the wealth distribution, not only for the rich.",10.3982/ecta18707,2020-07-12,2022-10-31,"['tetsuya kaji', 'elena manresa', 'guillaume pouliot']"
2007.12882,a finite sample analysis of the benign overfitting phenomenon for ridge   function estimation,stat.ml cs.lg,"recent extensive numerical experiments in high scale machine learning have allowed to uncover a quite counterintuitive phase transition, as a function of the ratio between the sample size and the number of parameters in the model. as the number of parameters $p$ approaches the sample size $n$, the generalisation error increases, but surprisingly, it starts decreasing again past the threshold $p=n$. this phenomenon, brought to the theoretical community attention in \cite{belkin2019reconciling}, has been thoroughly investigated lately, more specifically for simpler models than deep neural networks, such as the linear model when the parameter is taken to be the minimum norm solution to the least-squares problem, firstly in the asymptotic regime when $p$ and $n$ tend to infinity, see e.g. \cite{hastie2019surprises}, and recently in the finite dimensional regime and more specifically for linear models \cite{bartlett2020benign}, \cite{tsigler2020benign}, \cite{lecue2022geometrical}. in the present paper, we propose a finite sample analysis of non-linear models of \textit{ridge} type, where we investigate the \textit{overparametrised regime} of the double descent phenomenon for both the \textit{estimation problem} and the \textit{prediction} problem. our results provide a precise analysis of the distance of the best estimator from the true parameter as well as a generalisation bound which complements recent works of \cite{bartlett2020benign} and \cite{chinot2020benign}. our analysis is based on tools closely related to the continuous newton method \cite{neuberger2007continuous} and a refined quantitative analysis of the performance in prediction of the minimum $\ell_2$-norm solution.",,2020-07-25,2024-01-12,"['emmanuel caron', 'stephane chretien']"
2007.13553,sequential design of multi-fidelity computer experiments: maximizing the   rate of stepwise uncertainty reduction,stat.ap stat.me stat.ml,"this article deals with the sequential design of experiments for (deterministic or stochastic) multi-fidelity numerical simulators, that is, simulators that offer control over the accuracy of simulation of the physical phenomenon or system under study. very often, accurate simulations correspond to high computational efforts whereas coarse simulations can be obtained at a smaller cost. in this setting, simulation results obtained at several levels of fidelity can be combined in order to estimate quantities of interest (the optimal value of the output, the probability that the output exceeds a given threshold...) in an efficient manner. to do so, we propose a new bayesian sequential strategy called maximal rate of stepwise uncertainty reduction (mr-sur), that selects additional simulations to be performed by maximizing the ratio between the expected reduction of uncertainty and the cost of simulation. this generic strategy unifies several existing methods, and provides a principled approach to develop new ones. we assess its performance on several examples, including a computationally intensive problem of fire safety analysis where the quantity of interest is the probability of exceeding a tenability threshold during a building fire.",10.1080/00401706.2021.1935324,2020-07-27,2021-05-28,"['rémi stroh', 'julien bect', 'séverine demeyer', 'nicolas fischer', 'damien marquis', 'emmanuel vazquez']"
2007.15788,stochastic low-rank tensor bandits for multi-dimensional online decision   making,stat.ml cs.lg,"multi-dimensional online decision making plays a crucial role in many real applications such as online recommendation and digital marketing. in these problems, a decision at each time is a combination of choices from different types of entities. to solve it, we introduce stochastic low-rank tensor bandits, a class of bandits whose mean rewards can be represented as a low-rank tensor. we consider two settings, tensor bandits without context and tensor bandits with context. in the first setting, the platform aims to find the optimal decision with the highest expected reward, a.k.a, the largest entry of true reward tensor. in the second setting, some modes of the tensor are contexts and the rest modes are decisions, and the goal is to find the optimal decision given the contextual information. we propose two learning algorithms tensor elimination and tensor epoch-greedy for tensor bandits without context, and derive finite-time regret bounds for them. comparing with existing competitive methods, tensor elimination has the best overall regret bound and tensor epoch-greedy has a sharper dependency on dimensions of the reward tensor. furthermore, we develop a practically effective bayesian algorithm called tensor ensemble sampling for tensor bandits with context. extensive simulations and real analysis in online advertising data back up our theoretical findings and show that our algorithms outperform various state-of-the-art approaches that ignore the tensor low-rank structure.",,2020-07-30,2024-02-13,"['jie zhou', 'botao hao', 'zheng wen', 'jingfei zhang', 'will wei sun']"
2008.05966,deep-lock: secure authorization for deep neural networks,cs.lg cs.cr stat.ml,"trained deep neural network (dnn) models are considered valuable intellectual properties (ip) in several business models. prevention of ip theft and unauthorized usage of such dnn models has been raised as of significant concern by industry. in this paper, we address the problem of preventing unauthorized usage of dnn models by proposing a generic and lightweight key-based model-locking scheme, which ensures that a locked model functions correctly only upon applying the correct secret key. the proposed scheme, known as deep-lock, utilizes s-boxes with good security properties to encrypt each parameter of a trained dnn model with secret keys generated from a master key via a key scheduling algorithm. the resulting dense network of encrypted weights is found robust against model fine-tuning attacks. finally, deep-lock does not require any intervention in the structure and training of the dnn models, making it applicable for all existing software and hardware implementations of dnn.",,2020-08-13,2024-02-18,"['manaar alam', 'sayandeep saha', 'debdeep mukhopadhyay', 'sandip kundu']"
2009.01959,concra: a convolutional neural network code retrieval approach,cs.lg cs.cl cs.se stat.ml,"software developers routinely search for code using general-purpose search engines. however, these search engines cannot find code semantically unless it has an accompanying description. we propose a technique for semantic code search: a convolutional neural network approach to code retrieval (concra). our technique aims to find the code snippet that most closely matches the developer's intent, expressed in natural language. we evaluated our approach's efficacy on a dataset composed of questions and code snippets collected from stack overflow. our preliminary results showed that our technique, which prioritizes local interactions (words nearby), improved the state-of-the-art (sota) by 5% on average, retrieving the most relevant code snippets in the top 3 (three) positions by almost 80% of the time. therefore, our technique is promising and can improve the efficacy of semantic code retrieval.",10.1145/3422392.3422462,2020-09-03,,"['marcelo de rezende martins', 'marco a. gerosa']"
2009.04614,end-to-end kernel learning via generative random fourier features,cs.lg stat.ml,"random fourier features (rffs) provide a promising way for kernel learning in a spectral case. current rffs-based kernel learning methods usually work in a two-stage way. in the first-stage process, learning the optimal feature map is often formulated as a target alignment problem, which aims to align the learned kernel with the pre-defined target kernel (usually the ideal kernel). in the second-stage process, a linear learner is conducted with respect to the mapped random features. nevertheless, the pre-defined kernel in target alignment is not necessarily optimal for the generalization of the linear learner. instead, in this paper, we consider a one-stage process that incorporates the kernel learning and linear learner into a unifying framework. to be specific, a generative network via rffs is devised to implicitly learn the kernel, followed by a linear classifier parameterized as a full-connected layer. then the generative network and the classifier are jointly trained by solving the empirical risk minimization (erm) problem to reach a one-stage solution. this end-to-end scheme naturally allows deeper features, in correspondence to a multi-layer structure, and shows superior generalization performance over the classical two-stage, rffs-based methods in real-world classification tasks. moreover, inspired by the randomized resampling mechanism of the proposed method, its enhanced adversarial robustness is investigated and experimentally verified.",10.1016/j.patcog.2022.109057,2020-09-09,2024-01-15,"['kun fang', 'fanghui liu', 'xiaolin huang', 'jie yang']"
2009.10303,on the representation and learning of monotone triangular transport maps,stat.ml cs.lg math.fa stat.co stat.me,"transportation of measure provides a versatile approach for modeling complex probability distributions, with applications in density estimation, bayesian inference, generative modeling, and beyond. monotone triangular transport maps$\unicode{x2014}$approximations of the knothe$\unicode{x2013}$rosenblatt (kr) rearrangement$\unicode{x2014}$are a canonical choice for these tasks. yet the representation and parameterization of such maps have a significant impact on their generality and expressiveness, and on properties of the optimization problem that arises in learning a map from data (e.g., via maximum likelihood estimation). we present a general framework for representing monotone triangular maps via invertible transformations of smooth functions. we establish conditions on the transformation such that the associated infinite-dimensional minimization problem has no spurious local minima, i.e., all local minima are global minima; and we show for target distributions satisfying certain tail conditions that the unique global minimizer corresponds to the kr map. given a sample from the target, we then propose an adaptive algorithm that estimates a sparse semi-parametric approximation of the underlying kr map. we demonstrate how this framework can be applied to joint and conditional density estimation, likelihood-free inference, and structure learning of directed graphical models, with stable generalization performance across a range of sample sizes.",10.1007/s10208-023-09630-x,2020-09-21,2024-02-24,"['ricardo baptista', 'youssef marzouk', 'olivier zahm']"
2009.10629,accelerated gradient methods for sparse statistical learning with   nonconvex penalties,math.oc stat.co stat.ml,"nesterov's accelerated gradient (ag) is a popular technique to optimize objective functions comprising two components: a convex loss and a penalty function. while ag methods perform well for convex penalties, such as the lasso, convergence issues may arise when it is applied to nonconvex penalties, such as scad. a recent proposal generalizes nesterov's ag method to the nonconvex setting. the proposed algorithm requires specification of several hyperparameters for its practical application. aside from some general conditions, there is no explicit rule for selecting the hyperparameters, and how different selection can affect convergence of the algorithm. in this article, we propose a hyperparameter setting based on the complexity upper bound to accelerate convergence, and consider the application of this nonconvex ag algorithm to high-dimensional linear and logistic sparse learning problems. we further establish the rate of convergence and present a simple and useful bound to characterize our proposed optimal damping sequence. simulation studies show that convergence can be made, on average, considerably faster than that of the conventional proximal gradient algorithm. our experiments also show that the proposed method generally outperforms the current state-of-the-art methods in terms of signal recovery.",10.1007/s11222-023-10371-8,2020-09-22,2022-11-28,"['kai yang', 'masoud asgharian', 'sahir bhatnagar']"
2009.10862,an intuitive tutorial to gaussian process regression,stat.ml cs.lg cs.ro,"this tutorial aims to provide an intuitive introduction to gaussian process regression (gpr). gpr models have been widely used in machine learning applications due to their representation flexibility and inherent capability to quantify uncertainty over predictions. the tutorial starts with explaining the basic concepts that a gaussian process is built on, including multivariate normal distribution, kernels, non-parametric models, and joint and conditional probability. it then provides a concise description of gpr and an implementation of a standard gpr algorithm. in addition, the tutorial reviews packages for implementing state-of-the-art gaussian process algorithms. this tutorial is accessible to a broad audience, including those new to machine learning, ensuring a clear understanding of gpr fundamentals.",10.1109/mcse.2023.3342149,2020-09-22,2024-01-27,['jie wang']
2010.07067,machine learning force fields,physics.chem-ph stat.ml,"in recent years, the use of machine learning (ml) in computational chemistry has enabled numerous advances previously out of reach due to the computational complexity of traditional electronic-structure methods. one of the most promising applications is the construction of ml-based force fields (ffs), with the aim to narrow the gap between the accuracy of ab initio methods and the efficiency of classical ffs. the key idea is to learn the statistical relation between chemical structure and potential energy without relying on a preconceived notion of fixed chemical bonds or knowledge about the relevant interactions. such universal ml approximations are in principle only limited by the quality and quantity of the reference data used to train them. this review gives an overview of applications of ml-ffs and the chemical insights that can be obtained from them. the core concepts underlying ml-ffs are described in detail and a step-by-step guide for constructing and testing them from scratch is given. the text concludes with a discussion of the challenges that remain to be overcome by the next generation of ml-ffs.",10.1021/acs.chemrev.0c01111,2020-10-14,2021-01-12,"['oliver t. unke', 'stefan chmiela', 'huziel e. sauceda', 'michael gastegger', 'igor poltavsky', 'kristof t. schütt', 'alexandre tkatchenko', 'klaus-robert müller']"
2010.16271,view selection in multi-view stacking: choosing the meta-learner,stat.ml cs.lg stat.me,"multi-view stacking is a framework for combining information from different views (i.e. different feature sets) describing the same set of objects. in this framework, a base-learner algorithm is trained on each view separately, and their predictions are then combined by a meta-learner algorithm. in a previous study, stacked penalized logistic regression, a special case of multi-view stacking, has been shown to be useful in identifying which views are most important for prediction. in this article we expand this research by considering seven different algorithms to use as the meta-learner, and evaluating their view selection and classification performance in simulations and two applications on real gene-expression data sets. our results suggest that if both view selection and classification accuracy are important to the research at hand, then the nonnegative lasso, nonnegative adaptive lasso and nonnegative elastic net are suitable meta-learners. exactly which among these three is to be preferred depends on the research context. the remaining four meta-learners, namely nonnegative ridge regression, nonnegative forward selection, stability selection and the interpolating predictor, show little advantages in order to be preferred over the other three.",,2020-10-30,2024-01-29,"['wouter van loon', 'marjolein fokkema', 'botond szabo', 'mark de rooij']"
2011.11877,instahide's sample complexity when mixing two private images,cs.lg cs.cc cs.cr cs.ds stat.ml,"training neural networks usually require large numbers of sensitive training data, and how to protect the privacy of training data has thus become a critical topic in deep learning research. instahide is a state-of-the-art scheme to protect training data privacy with only minor effects on test accuracy, and its security has become a salient question. in this paper, we systematically study recent attacks on instahide and present a unified framework to understand and analyze these attacks. we find that existing attacks either do not have a provable guarantee or can only recover a single private image. on the current instahide challenge setup, where each instahide image is a mixture of two private images, we present a new algorithm to recover all the private images with a provable guarantee and optimal sample complexity. in addition, we also provide a computational hardness result on retrieving all instahide images. our results demonstrate that instahide is not information-theoretically secure but computationally secure in the worst case, even when mixing two private images.",,2020-11-23,2024-02-05,"['baihe huang', 'zhao song', 'runzhou tao', 'junze yin', 'ruizhe zhang', 'danyang zhuo']"
2011.14238,approximate cross-validated mean estimates for bayesian hierarchical   regression models,stat.ml cs.lg stat.co,"we introduce a novel procedure for obtaining cross-validated predictive estimates for bayesian hierarchical regression models (bhrms). bayesian hierarchical models are popular for their ability to model complex dependence structures and provide probabilistic uncertainty estimates, but can be computationally expensive to run. cross-validation (cv) is therefore not a common practice to evaluate the predictive performance of bhrms. our method circumvents the need to re-run computationally costly estimation methods for each cross-validation fold and makes cv more feasible for large bhrms. by conditioning on the variance-covariance parameters, we shift the cv problem from probability-based sampling to a simple and familiar optimization problem. in many cases, this produces estimates which are equivalent to full cv. we provide theoretical results and demonstrate its efficacy on publicly available data and in simulations.",,2020-11-28,2024-01-17,"['amy x. zhang', 'le bao', 'changcheng li', 'michael j. daniels']"
2012.14331,methods to integrate multinormals and compute classification measures,stat.ml cs.cv cs.lg,"univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. besides some special cases, there exist no general analytical expressions, standard numerical methods or software for these integrals. here we present mathematical results and open-source software that provide (i) the probability in any domain of a normal in any dimensions with any parameters, (ii) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector, (iii) the classification errors among any number of normal distributions, the bayes-optimal discriminability index and relation to the operating characteristic, (iv) dimension reduction and visualizations for such problems, and (v) tests for how reliably these methods may be used on given data. we demonstrate these tools with vision research applications of detecting occluding objects in natural scenes, and detecting camouflage.",,2020-12-23,2024-01-27,"['abhranil das', 'wilson s geisler']"
2101.00009,adversarial estimation of riesz representers,econ.em cs.lg stat.ml,"many causal and structural parameters are linear functionals of an underlying regression. the riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. we propose an adversarial framework to estimate the riesz representer using general function spaces. we prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel hilbert spaces as leading cases. furthermore, we use critical radius theory -- in place of donsker theory -- to prove asymptotic normality without sample splitting, uncovering a ``complexity-rate robustness'' condition. this condition has practical consequences: inference without sample splitting is possible in several machine learning settings, which may improve finite sample performance compared to sample splitting. our estimators achieve nominal coverage in highly nonlinear simulations where previous methods break down. they shed new light on the heterogeneous effects of matching grants.",,2020-12-30,2024-01-15,"['victor chernozhukov', 'whitney newey', 'rahul singh', 'vasilis syrgkanis']"
2101.12365,"sharp bounds on the approximation rates, metric entropy, and $n$-widths   of shallow neural networks",stat.ml cs.it cs.lg math.it,"in this article, we study approximation properties of the variation spaces corresponding to shallow neural networks with a variety of activation functions. we introduce two main tools for estimating the metric entropy, approximation rates, and $n$-widths of these spaces. first, we introduce the notion of a smoothly parameterized dictionary and give upper bounds on the non-linear approximation rates, metric entropy and $n$-widths of their absolute convex hull. the upper bounds depend upon the order of smoothness of the parameterization. this result is applied to dictionaries of ridge functions corresponding to shallow neural networks, and they improve upon existing results in many cases. next, we provide a method for lower bounding the metric entropy and $n$-widths of variation spaces which contain certain classes of ridge functions. this result gives sharp lower bounds on the $l^2$-approximation rates, metric entropy, and $n$-widths for variation spaces corresponding to neural networks with a range of important activation functions, including relu$^k$ activation functions and sigmoidal activation functions with bounded variation.",10.1007/s10208-022-09595-3,2021-01-28,2024-02-22,"['jonathan w. siegel', 'jinchao xu']"
2102.00199,rates of convergence for density estimation with generative adversarial   networks,math.st stat.ml stat.th,"in this work we undertake a thorough study of the non-asymptotic properties of the vanilla generative adversarial networks (gans). we prove an oracle inequality for the jensen-shannon (js) divergence between the underlying density $\mathsf{p}^*$ and the gan estimate with a significantly better statistical error term compared to the previously known results. the advantage of our bound becomes clear in application to nonparametric density estimation. we show that the js-divergence between the gan estimate and $\mathsf{p}^*$ decays as fast as $(\log{n}/n)^{2\beta/(2\beta + d)}$, where $n$ is the sample size and $\beta$ determines the smoothness of $\mathsf{p}^*$. this rate of convergence coincides (up to logarithmic factors) with minimax optimal for the considered class of densities.",,2021-01-30,2024-01-25,"['nikita puchkin', 'sergey samsonov', 'denis belomestny', 'eric moulines', 'alexey naumov']"
2102.08232,the melodic family for simultaneous binary logistic regression in a   reduced space,stat.me stat.co stat.ml,"logistic regression is a commonly used method for binary classification. researchers often have more than a single binary response variable and simultaneous analysis is beneficial because it provides insight into the dependencies among response variables as well as between the predictor variables and the responses. moreover, in such a simultaneous analysis the equations can lend each other strength, which might increase predictive accuracy. in this paper, we propose the melodic family for simultaneous binary logistic regression modeling. in this family, the regression models are defined in a euclidean space of reduced dimension, based on a distance rule. the model may be interpreted in terms of logistic regression coefficients or in terms of a biplot. we discuss a fast iterative majorization (or mm) algorithm for parameter estimation. two applications are shown in detail: one relating personality characteristics to drug consumption profiles and one relating personality characteristics to depressive and anxiety disorders. we present a thorough comparison of our melodic family with alternative approaches for multivariate binary data.",10.1007/978-981-99-2240-6_4,2021-02-16,2022-06-24,"['mark de rooij', 'patrick j. f. groenen']"
2102.08754,a regret analysis of bilateral trade,cs.lg econ.th stat.ml,"bilateral trade, a fundamental topic in economics, models the problem of intermediating between two strategic agents, a seller and a buyer, willing to trade a good for which they hold private valuations. despite the simplicity of this problem, a classical result by myerson and satterthwaite (1983) affirms the impossibility of designing a mechanism which is simultaneously efficient, incentive compatible, individually rational, and budget balanced. this impossibility result fostered an intense investigation of meaningful trade-offs between these desired properties. much work has focused on approximately efficient fixed-price mechanisms, i.e., blumrosen and dobzinski (2014; 2016), colini-baldeschi et al. (2016), which have been shown to fully characterize strong budget balanced and ex-post individually rational direct revelation mechanisms. all these results, however, either assume some knowledge on the priors of the seller/buyer valuations, or a black box access to some samples of the distributions, as in d{\""u}tting et al. (2021). in this paper, we cast for the first time the bilateral trade problem in a regret minimization framework over rounds of seller/buyer interactions, with no prior knowledge on the private seller/buyer valuations. our main contribution is a complete characterization of the regret regimes for fixed-price mechanisms with different models of feedback and private valuations, using as benchmark the best fixed price in hindsight. more precisely, we prove the following bounds on the regret:   $\bullet$ $\widetilde{\theta}(\sqrt{t})$ for full-feedback (i.e., direct revelation mechanisms);   $\bullet$ $\widetilde{\theta}(t^{2/3})$ for realistic feedback (i.e., posted-price mechanisms) and independent seller/buyer valuations with bounded densities;   $\bullet$ $\theta(t)$ for realistic feedback and seller/buyer valuations with bounded densities;   $\bullet$ $\theta(t)$ for realistic feedback and independent seller/buyer valuations;   $\bullet$ $\theta(t)$ for the adversarial setting.",10.1145/3465456.3467645,2021-02-16,,"['nicolò cesa-bianchi', 'tommaso cesari', 'roberto colomboni', 'federico fusco', 'stefano leonardi']"
2102.10019,the disparate impact of uncertainty: affirmative action vs. affirmative   information,stat.ml cs.lg econ.th,"critical decisions like hiring, college admissions, and loan approvals are guided by predictions made in the presence of uncertainty. while uncertainty imparts errors across all demographic groups, this paper shows that the types of errors vary systematically: groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. we characterize the conditions that give rise to this disparate impact and explain why the intuitive remedy to omit demographic variables from datasets does not correct it. instead of data omission, this paper examines how data enrichment can broaden access to opportunity. the strategy, which we call ""affirmative information,"" could stand as an alternative to affirmative action.",,2021-02-19,2024-02-08,['claire lazar reich']
2103.01280,dynamic covariate balancing: estimating treatment effects over time with   potential local projections,econ.em math.st stat.me stat.ml stat.th,"this paper studies the estimation and inference of treatment histories in panel data settings when treatments change dynamically over time.   we propose a method that allows for (i) treatments to be assigned dynamically over time based on high-dimensional covariates, past outcomes and treatments; (ii) outcomes and time-varying covariates to depend on treatment trajectories; (iii) heterogeneity of treatment effects.   our approach recursively projects potential outcomes' expectations on past histories. it then controls the bias by balancing dynamically observable characteristics. we study the asymptotic and numerical properties of the estimator and illustrate the benefits of the procedure in an empirical application.",,2021-03-01,2024-01-26,"['davide viviano', 'jelena bradic']"
2103.05161,the efficient shrinkage path: maximum likelihood of minimum mse risk,stat.me stat.co stat.ml,a new generalized ridge regression shrinkage path is proposed that is as short as possible under the restriction that it must pass through the vector of regression coefficient estimators that make the overall optimal variance-bias trade-off under normal distribution-theory. five distinct types of ridge trace displays plus other graphics for this efficient path are motivated and illustrated here. these visualizations provide invaluable data-analytic insights and improved self-confidence to researchers and data scientists fitting linear models to ill-conditioned (confounded) data.,10.1515/stat-2022-0108,2021-03-08,2024-02-15,['robert l. obenchain']
2103.05371,exploring coronal heating using unsupervised machine-learning,astro-ph.sr astro-ph.im stat.ml,"the perplexing mystery of what maintains the solar coronal temperature at about a million k, while the visible disc of the sun is only at 5800 k, has been a long standing problem in solar physics. a recent study by mondal(2020) has provided the first evidence for the presence of numerous ubiquitous impulsive emissions at low radio frequencies from the quiet sun regions, which could hold the key to solving this mystery. these features occur at rates of about five hundred events per minute, and their strength is only a few percent of the background steady emission. one of the next steps for exploring the feasibility of this resolution to the coronal heating problem is to understand the morphology of these emissions. to meet this objective we have developed a technique based on an unsupervised machine learning approach for characterising the morphology of these impulsive emissions. here we present the results of application of this technique to over 8000 images spanning 70 minutes of data in which about 34,500 features could robustly be characterised as 2d elliptical gaussians.",,2021-03-09,,"['shabbir bawaji', 'ujjaini alam', 'surajit mondal', 'divya oberoi']"
2103.07020,max-linear regression by convex programming,stat.ml cs.it cs.lg math.it math.st stat.th,"we consider the multivariate max-linear regression problem where the model parameters $\boldsymbol{\beta}_{1},\dotsc,\boldsymbol{\beta}_{k}\in\mathbb{r}^{p}$ need to be estimated from $n$ independent samples of the (noisy) observations $y = \max_{1\leq j \leq k} \boldsymbol{\beta}_{j}^{\mathsf{t}} \boldsymbol{x} + \mathrm{noise}$. the max-linear model vastly generalizes the conventional linear model, and it can approximate any convex function to an arbitrary accuracy when the number of linear models $k$ is large enough. however, the inherent nonlinearity of the max-linear model renders the estimation of the regression parameters computationally challenging. particularly, no estimator based on convex programming is known in the literature. we formulate and analyze a scalable convex program given by anchored regression (ar) as the estimator for the max-linear regression problem. under the standard gaussian observation setting, we present a non-asymptotic performance guarantee showing that the convex program recovers the parameters with high probability. when the $k$ linear components are equally likely to achieve the maximum, our result shows a sufficient number of noise-free observations for exact recovery scales as {$k^{4}p$} up to a logarithmic factor. { this sample complexity coincides with that by alternating minimization (ghosh et al., {2021}). moreover, the same sample complexity applies when the observations are corrupted with arbitrary deterministic noise. we provide empirical results that show that our method performs as our theoretical result predicts, and is competitive with the alternating minimization algorithm particularly in presence of multiplicative bernoulli noise. furthermore, we also show empirically that a recursive application of ar can significantly improve the estimation accuracy.}",,2021-03-11,2024-02-23,"['seonho kim', 'sohail bahmani', 'kiryung lee']"
2103.09603,doubleml -- an object-oriented implementation of double machine learning   in r,stat.ml cs.lg econ.em,"the r package doubleml implements the double/debiased machine learning framework of chernozhukov et al. (2018). it provides functionalities to estimate parameters in causal models based on machine learning methods. the double machine learning framework consist of three key ingredients: neyman orthogonality, high-quality machine learning estimation and sample splitting. estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. doubleml makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. the object-oriented implementation of doubleml enables a high flexibility for the model specification and makes it easily extendable. this paper serves as an introduction to the double machine learning framework and the r package doubleml. in reproducible code examples with simulated and real data sets, we demonstrate how doubleml users can perform valid inference based on machine learning methods.",10.18637/jss.v108.i03,2021-03-17,2024-02-27,"['philipp bach', 'victor chernozhukov', 'malte s. kurz', 'martin spindler', 'sven klaassen']"
2104.03158,simple imputation rules for prediction with missing data: contrasting   theoretical guarantees with empirical performance,stat.ml cs.lg,"missing data is a common issue in real-world datasets. this paper studies the performance of impute-then-regress pipelines by contrasting theoretical and empirical evidence. we establish the asymptotic consistency of such pipelines for a broad family of imputation methods. while common sense suggests that a `good' imputation method produces datasets that are plausible, we show, on the contrary, that, as far as prediction is concerned, crude can be good. among others, we find that mode-impute is asymptotically sub-optimal, while mean-impute is asymptotically optimal. we then exhaustively assess the validity of these theoretical conclusions on a large corpus of synthetic, semi-real, and real datasets. while the empirical evidence we collect mostly supports our theoretical findings, it also highlights gaps between theory and practice and opportunities for future research, regarding the relevance of the mar assumption, the complex interdependency between the imputation and regression tasks, and the need for realistic synthetic data generation models.",,2021-04-07,2024-02-02,"['dimitris bertsimas', 'arthur delarue', 'jean pauphilet']"
2104.03527,sparse nmf with archetypal regularization: computational and robustness   properties,stat.ml cs.lg,"we consider the problem of sparse nonnegative matrix factorization (nmf) using archetypal regularization. the goal is to represent a collection of data points as nonnegative linear combinations of a few nonnegative sparse factors with appealing geometric properties, arising from the use of archetypal regularization. we generalize the notion of robustness studied in javadi and montanari (2019) (without sparsity) to the notions of (a) strong robustness that implies each estimated archetype is close to the underlying archetypes and (b) weak robustness that implies there exists at least one recovered archetype that is close to the underlying archetypes. our theoretical results on robustness guarantees hold under minimal assumptions on the underlying data, and applies to settings where the underlying archetypes need not be sparse. we present theoretical results and illustrative examples to strengthen the insights underlying the notions of robustness. we propose new algorithms for our optimization problem; and present numerical experiments on synthetic and real data sets that shed further insights into our proposed framework and theoretical developments.",,2021-04-08,2024-02-10,"['kayhan behdin', 'rahul mazumder']"
2104.06667,double robust semi-supervised inference for the mean: selection bias   under mar labeling with decaying overlap,stat.me math.st stat.ml stat.th,"semi-supervised (ss) inference has received much attention in recent years. apart from a moderate-sized labeled data, l, the ss setting is characterized by an additional, much larger sized, unlabeled data, u. the setting of |u| >> |l|, makes ss inference unique and different from the standard missing data problems, owing to natural violation of the so-called ""positivity"" or ""overlap"" assumption. however, most of the ss literature implicitly assumes l and u to be equally distributed, i.e., no selection bias in the labeling. inferential challenges in missing at random (mar) type labeling allowing for selection bias, are inevitably exacerbated by the decaying nature of the propensity score (ps). we address this gap for a prototype problem, the estimation of the response's mean. we propose a double robust ss (drss) mean estimator and give a complete characterization of its asymptotic properties. the proposed estimator is consistent as long as either the outcome or the ps model is correctly specified. when both models are correctly specified, we provide inference results with a non-standard consistency rate that depends on the smaller size |l|. the results are also extended to causal inference with imbalanced treatment groups. further, we provide several novel choices of models and estimators of the decaying ps, including a novel offset logistic model and a stratified labeling model. we present their properties under both high and low dimensional settings. these may be of independent interest. lastly, we present extensive simulations and also a real data application.",10.1093/imaiai/iaad021,2021-04-14,2023-05-18,"['yuqian zhang', 'abhishek chakrabortty', 'jelena bradic']"
2104.06685,broadcast: reducing both stochastic and compression noise to robustify   communication-efficient federated learning,cs.lg cs.dc math.oc stat.ml,"communication between workers and the master node to collect local stochastic gradients is a key bottleneck in a large-scale federated learning system. various recent works have proposed to compress the local stochastic gradients to mitigate the communication overhead. however, robustness to malicious attacks is rarely considered in such a setting. in this work, we investigate the problem of byzantine-robust compressed federated learning, where the attacks from byzantine workers can be arbitrarily malicious. we theoretically point out that different to the attacks-free compressed stochastic gradient descent (sgd), its vanilla combination with geometric median-based robust aggregation seriously suffers from the compression noise in the presence of byzantine attacks. in light of this observation, we propose to reduce the compression noise with gradient difference compression so as to improve the byzantine-robustness. we also observe the impact of the intrinsic stochastic noise caused by selecting random samples, and adopt the stochastic average gradient algorithm (saga) to gradually eliminate the inner variations of regular workers. we theoretically prove that the proposed algorithm reaches a neighborhood of the optimal solution at a linear convergence rate, and the asymptotic learning error is in the same order as that of the state-of-the-art uncompressed method. finally, numerical experiments demonstrate the effectiveness of the proposed method.",10.1109/tsipn.2023.3265892,2021-04-14,2022-04-11,"['heng zhu', 'qing ling']"
2104.08928,group-sparse matrix factorization for transfer learning of word   embeddings,stat.ml cs.cl cs.lg,"unstructured text provides decision-makers with a rich data source in many domains, ranging from product reviews in retail to nursing notes in healthcare. to leverage this information, words are typically translated into word embeddings -- vectors that encode the semantic relationships between words -- through unsupervised learning algorithms such as matrix factorization. however, learning word embeddings from new domains with limited training data can be challenging, because the meaning/usage may be different in the new domain, e.g., the word ``positive'' typically has positive sentiment, but often has negative sentiment in medical notes since it may imply that a patient tested positive for a disease. in practice, we expect that only a small number of domain-specific words may have new meanings. we propose an intuitive two-stage estimator that exploits this structure via a group-sparse penalty to efficiently transfer learn domain-specific word embeddings by combining large-scale text corpora (such as wikipedia) with limited domain-specific text data. we bound the generalization error of our transfer learning estimator, proving that it can achieve high accuracy with substantially less domain-specific data when only a small number of embeddings are altered between domains. furthermore, we prove that all local minima identified by our nonconvex objective function are statistically indistinguishable from the global minimum under standard regularization conditions, implying that our estimator can be computed efficiently. our results provide the first bounds on group-sparse matrix factorization, which may be of independent interest. we empirically evaluate our approach compared to state-of-the-art fine-tuning heuristics from natural language processing.",,2021-04-18,2024-02-17,"['kan xu', 'xuanyi zhao', 'hamsa bastani', 'osbert bastani']"
2104.10746,automatic model training under restrictive time constraints,stat.ml cs.lg,"we develop a hyperparameter optimisation algorithm, automated budget constrained training (autobct), which balances the quality of a model with the computational cost required to tune it. the relationship between hyperparameters, model quality and computational cost must be learnt and this learning is incorporated directly into the optimisation problem. at each training epoch, the algorithm decides whether to terminate or continue training, and, in the latter case, what values of hyperparameters to use. this decision weighs optimally potential improvements in the quality with the additional training time and the uncertainty about the learnt quantities. the performance of our algorithm is verified on a number of machine learning problems encompassing random forests and neural networks. our approach is rooted in the theory of markov decision processes with partial information and we develop a numerical method to compute the value function and an optimal strategy.",10.1007/s11222-022-10166-3,2021-04-21,,"['lukas cironis', 'jan palczewski', 'georgios aivaliotis']"
2105.02487,high-dimensional functional graphical model structure learning via   neighborhood selection approach,stat.ml cs.lg stat.me,"undirected graphical models are widely used to model the conditional independence structure of vector-valued data. however, in many modern applications, for example those involving eeg and fmri data, observations are more appropriately modeled as multivariate random functions rather than vectors. functional graphical models have been proposed to model the conditional independence structure of such functional data. we propose a neighborhood selection approach to estimate the structure of gaussian functional graphical models, where we first estimate the neighborhood of each node via a function-on-function regression and subsequently recover the entire graph structure by combining the estimated neighborhoods. our approach only requires assumptions on the conditional distributions of random functions, and we estimate the conditional independence structure directly. we thus circumvent the need for a well-defined precision operator that may not exist when the functions are infinite dimensional. additionally, the neighborhood selection approach is computationally efficient and can be easily parallelized. the statistical consistency of the proposed method in the high-dimensional setting is supported by both theory and experimental results. in addition, we study the effect of the choice of the function basis used for dimensionality reduction in an intermediate step. we give a heuristic criterion for choosing a function basis and motivate two practically useful choices, which we justify by both theory and experiments.",,2021-05-06,2024-01-25,"['boxin zhao', 'percy s. zhai', 'y. samuel wang', 'mladen kolar']"
2105.02569,machine collaboration,stat.ml cs.lg econ.em,"we propose a new ensemble framework for supervised learning, called machine collaboration (mac), using a collection of base machines for prediction tasks. unlike bagging/stacking (a parallel & independent framework) and boosting (a sequential & top-down framework), mac is a type of circular & interactive learning framework. the circular & interactive feature helps the base machines to transfer information circularly and update their structures and parameters accordingly. the theoretical result on the risk bound of the estimator from mac reveals that the circular & interactive feature can help mac reduce risk via a parsimonious ensemble. we conduct extensive experiments on mac using both simulated data and 119 benchmark real datasets. the results demonstrate that in most cases, mac performs significantly better than several other state-of-the-art methods, including classification and regression trees, neural networks, stacking, and boosting.",,2021-05-06,2024-02-11,"['qingfeng liu', 'yang feng']"
2105.03425,kernel two-sample tests for manifold data,stat.ml cs.lg math.st stat.th,"we present a study of a kernel-based two-sample test statistic related to the maximum mean discrepancy (mmd) in the manifold data setting, assuming that high-dimensional observations are close to a low-dimensional manifold. we characterize the test level and power in relation to the kernel bandwidth, the number of samples, and the intrinsic dimensionality of the manifold. specifically, when data densities $p$ and $q$ are supported on a $d$-dimensional sub-manifold ${m}$ embedded in an $m$-dimensional space and are h\""older with order $\beta$ (up to 2) on ${m}$, we prove a guarantee of the test power for finite sample size $n$ that exceeds a threshold depending on $d$, $\beta$, and $\delta_2$ the squared $l^2$-divergence between $p$ and $q$ on the manifold, and with a properly chosen kernel bandwidth $\gamma$. for small density departures, we show that with large $n$ they can be detected by the kernel test when $\delta_2$ is greater than $n^{- { 2 \beta/( d + 4 \beta ) }}$ up to a certain constant and $\gamma$ scales as $n^{-1/(d+4\beta)}$. the analysis extends to cases where the manifold has a boundary and the data samples contain high-dimensional additive noise. our results indicate that the kernel two-sample test has no curse-of-dimensionality when the data lie on or near a low-dimensional manifold. we validate our theory and the properties of the kernel test for manifold data through a series of numerical experiments.",,2021-05-07,2024-02-26,"['xiuyuan cheng', 'yao xie']"
2105.04240,a rigorous introduction to linear models,cs.lg stat.ml,"this book is meant to provide an introduction to linear models and the theories behind them. our goal is to give a rigorous introduction to the readers with prior exposure to ordinary least squares. in machine learning, the output is usually a nonlinear function of the input. deep learning even aims to find a nonlinear dependence with many layers, which require a large amount of computation. however, most of these algorithms build upon simple linear models. we then describe linear models from different perspectives and find the properties and theories behind the models. the linear model is the main technique in regression problems, and the primary tool for it is the least squares approximation, which minimizes a sum of squared errors. this is a natural choice when we're interested in finding the regression function which minimizes the corresponding expected squared error. this book is primarily a summary of purpose, significance of important theories behind linear models, e.g., distribution theory and the minimum variance estimator. we first describe ordinary least squares from three different points of view, upon which we disturb the model with random noise and gaussian noise. through gaussian noise, the model gives rise to the likelihood so that we introduce a maximum likelihood estimator. it also develops some distribution theories via this gaussian disturbance. the distribution theory of least squares will help us answer various questions and introduce related applications. we then prove least squares is the best unbiased linear model in the sense of mean squared error, and most importantly, it actually approaches the theoretical limit. we end up with linear models with the bayesian approach and beyond.",,2021-05-10,2024-02-04,['jun lu']
2105.08620,adversarial examples detection with bayesian neural network,stat.ml cs.cv cs.lg,"in this paper, we propose a new framework to detect adversarial examples motivated by the observations that random components can improve the smoothness of predictors and make it easier to simulate the output distribution of a deep neural network. with these observations, we propose a novel bayesian adversarial example detector, short for bater, to improve the performance of adversarial example detection. specifically, we study the distributional difference of hidden layer output between natural and adversarial examples, and propose to use the randomness of the bayesian neural network to simulate hidden layer output distribution and leverage the distribution dispersion to detect adversarial examples. the advantage of a bayesian neural network is that the output is stochastic while a deep neural network without random components does not have such characteristics. empirical results on several benchmark datasets against popular attacks show that the proposed bater outperforms the state-of-the-art detectors in adversarial example detection.",,2021-05-18,2024-02-22,"['yao li', 'tongyi tang', 'cho-jui hsieh', 'thomas c. m. lee']"
2105.09254,multiply robust causal mediation analysis with continuous treatments,math.st cs.lg econ.em stat.ml stat.th,"in many applications, researchers are interested in the direct and indirect causal effects of a treatment or exposure on an outcome of interest. mediation analysis offers a rigorous framework for identifying and estimating these causal effects. for binary treatments, efficient estimators for the direct and indirect effects are presented in tchetgen tchetgen and shpitser (2012) based on the influence function of the parameter of interest. these estimators possess desirable properties, such as multiple-robustness and asymptotic normality, while allowing for slower than root-n rates of convergence for the nuisance parameters. however, in settings involving continuous treatments, these influence function-based estimators are not readily applicable without making strong parametric assumptions. in this work, utilizing a kernel-smoothing approach, we propose an estimator suitable for settings with continuous treatments inspired by the influence function-based estimator of tchetgen tchetgen and shpitser (2012). our proposed approach employs cross-fitting, relaxing the smoothness requirements on the nuisance functions, and allowing them to be estimated at slower rates than the target parameter. additionally, similar to influence function-based estimators, our proposed estimator is multiply robust and asymptotically normal, making it applicable for inference in settings where a parametric model cannot be assumed.",,2021-05-19,2024-02-03,"['numair sani', 'yizhen xu', 'amiremad ghassami', 'ilya shpitser']"
2105.12092,trajectory modeling via random utility inverse reinforcement learning,cs.ai cs.lg stat.ml,"we consider the problem of modeling trajectories of drivers in a road network from the perspective of inverse reinforcement learning. cars are detected by sensors placed on sparsely distributed points on the street network of a city. as rational agents, drivers are trying to maximize some reward function unknown to an external observer. we apply the concept of random utility from econometrics to model the unknown reward function as a function of observed and unobserved features. in contrast to current inverse reinforcement learning approaches, we do not assume that agents act according to a stochastic policy; rather, we assume that agents act according to a deterministic optimal policy and show that randomness in data arises because the exact rewards are not fully observed by an external observer. we introduce the concept of extended state to cope with unobserved features and develop a markov decision process formulation of drivers decisions. we present theoretical results which guarantee the existence of solutions and show that maximum entropy inverse reinforcement learning is a particular case of our approach. finally, we illustrate bayesian inference on model parameters through a case study with real trajectory data from a large city in brazil.",10.1016/j.ins.2024.120128,2021-05-25,2023-01-10,"['anselmo r. pitombeira-neto', 'helano p. santos', 'ticiana l. coelho da silva', 'josé antonio f. de macedo']"
2106.03907,deep proxy causal learning and its application to confounded bandit   policy evaluation,cs.lg stat.ml,"proxy causal learning (pcl) is a method for estimating the causal effect of treatments on outcomes in the presence of unobserved confounding, using proxies (structured side information) for the confounder. this is achieved via two-stage regression: in the first stage, we model relations among the treatment and proxies; in the second stage, we use this model to learn the effect of treatment on the outcome, given the context provided by the proxies. pcl guarantees recovery of the true causal effect, subject to identifiability conditions. we propose a novel method for pcl, the deep feature proxy variable method (dfpv), to address the case where the proxies, treatments, and outcomes are high-dimensional and have nonlinear complex relationships, as represented by deep neural network features. we show that dfpv outperforms recent state-of-the-art pcl methods on challenging synthetic benchmarks, including settings involving high dimensional image data. furthermore, we show that pcl can be applied to off-policy evaluation for the confounded bandit problem, in which dfpv also exhibits competitive performance.",,2021-06-07,2024-02-19,"['liyuan xu', 'heishiro kanagawa', 'arthur gretton']"
2106.04096,linear convergence of entropy-regularized natural policy gradient with   linear function approximation,cs.lg math.oc stat.ml,"natural policy gradient (npg) methods with entropy regularization achieve impressive empirical success in reinforcement learning problems with large state-action spaces. however, their convergence properties and the impact of entropy regularization remain elusive in the function approximation regime. in this paper, we establish finite-time convergence analyses of entropy-regularized npg with linear function approximation under softmax parameterization. in particular, we prove that entropy-regularized npg with averaging satisfies the \emph{persistence of excitation} condition, and achieves a fast convergence rate of $\tilde{o}(1/t)$ up to a function approximation error in regularized markov decision processes. this convergence result does not require any a priori assumptions on the policies. furthermore, under mild regularity conditions on the concentrability coefficient and basis vectors, we prove that entropy-regularized npg exhibits \emph{linear convergence} up to a function approximation error.",,2021-06-08,2024-02-08,"['semih cayci', 'niao he', 'r. srikant']"
2106.07106,alignment and comparison of directed networks via transition couplings   of random walks,cs.lg stat.ml,"we describe and study a transport based procedure called netotc (network optimal transition coupling) for the comparison and alignment of two networks. the networks of interest may be directed or undirected, weighted or unweighted, and may have distinct vertex sets of different sizes. given two networks and a cost function relating their vertices, netotc finds a transition coupling of their associated random walks having minimum expected cost. the minimizing cost quantifies the difference between the networks, while the optimal transport plan itself provides alignments of both the vertices and the edges of the two networks. coupling of the full random walks, rather than their marginal distributions, ensures that netotc captures local and global information about the networks, and preserves edges. netotc has no free parameters, and does not rely on randomization. we investigate a number of theoretical properties of netotc and present experiments establishing its empirical performance.",,2021-06-13,2024-02-05,"['bongsoo yi', ""kevin o'connor"", 'kevin mcgoff', 'andrew b. nobel']"
2106.11935,provably efficient representation selection in low-rank markov decision   processes: from online to offline rl,cs.lg math.oc stat.ml,"the success of deep reinforcement learning (drl) lies in its ability to learn a representation that is well-suited for the exploration and exploitation task. to understand how the choice of representation can improve the efficiency of reinforcement learning (rl), we study representation selection for a class of low-rank markov decision processes (mdps) where the transition kernel can be represented in a bilinear form. we propose an efficient algorithm, called relex, for representation learning in both online and offline rl. specifically, we show that the online version of relex, called relex-ucb, always performs no worse than the state-of-the-art algorithm without representation selection, and achieves a strictly better constant regret if the representation function class has a ""coverage"" property over the entire state-action space. for the offline counterpart, relex-lcb, we show that the algorithm can find the optimal policy if the representation class can cover the state-action space and achieves gap-dependent sample complexity. this is the first result with constant sample complexity for representation learning in offline rl.",,2021-06-22,2024-02-14,"['weitong zhang', 'jiafan he', 'dongruo zhou', 'amy zhang', 'quanquan gu']"
2106.14077,the role of contextual information in best arm identification,cs.lg econ.em math.st stat.me stat.ml stat.th,"we study the best-arm identification problem with fixed confidence when contextual (covariate) information is available in stochastic bandits. although we can use contextual information in each round, we are interested in the marginalized mean reward over the contextual distribution. our goal is to identify the best arm with a minimal number of samplings under a given value of the error rate. we show the instance-specific sample complexity lower bounds for the problem. then, we propose a context-aware version of the ""track-and-stop"" strategy, wherein the proportion of the arm draws tracks the set of optimal allocations and prove that the expected number of arm draws matches the lower bound asymptotically. we demonstrate that contextual information can be used to improve the efficiency of the identification of the best marginalized mean reward compared with the results of garivier & kaufmann (2016). we experimentally confirm that context information contributes to faster best-arm identification.",,2021-06-26,2024-02-26,"['masahiro kato', 'kaito ariu']"
2106.15436,topo-geometric analysis of variability in point clouds using persistence   landscapes,stat.me,"topological data analysis provides a set of tools to uncover low-dimensional structure in noisy point clouds. prominent amongst the tools is persistence homology, which summarizes birth-death times of homological features using data objects known as persistence diagrams. to better aid statistical analysis, a functional representation of the diagrams, known as persistence landscapes, enable use of functional data analysis and machine learning tools. topological and geometric variabilities inherent in point clouds are confounded in both persistence diagrams and landscapes, and it is important to distinguish topological signal from noise to draw reliable conclusions on the structure of the point clouds when using persistence homology. we develop a framework for decomposing variability in persistence diagrams into topological signal and topological noise through alignment of persistence landscapes using an elastic riemannian metric. aligned landscapes (amplitude) isolate the topological signal. reparameterizations used for landscape alignment (phase) are linked to a resolution parameter used to generate persistence diagrams, and capture topological noise in the form of geometric, global scaling and sampling variabilities. we illustrate the importance of decoupling topological signal and topological noise in persistence diagrams (landscapes) using several simulated examples. we also demonstrate that our approach provides novel insights in two real data studies.",,2021-06-29,2024-02-01,"['james matuk', 'sebastian kurtek', 'karthik bharath']"
2106.15493,generalized orthogonal procrustes problem under arbitrary adversaries,cs.it math.it math.oc stat.ml,"the generalized orthogonal procrustes problem (gopp) plays a fundamental role in several scientific disciplines including statistics, imaging science and computer vision. despite its tremendous practical importance, it is generally an np-hard problem to find the least squares estimator. we study the semidefinite relaxation (sdr) and an iterative method named generalized power method (gpm) to find the least squares estimator, and investigate the performance under a signal-plus-noise model. we show that the sdr recovers the least squares estimator exactly and moreover the generalized power method with a proper initialization converges linearly to the global minimizer to the sdr, provided that the signal-to-noise ratio is large. the main technique follows from showing the nonlinear mapping involved in the gpm is essentially a local contraction mapping and then applying the well-known banach fixed-point theorem finishes the proof. in addition, we analyze the low-rank factorization algorithm and show the corresponding optimization landscape is free of spurious local minimizers under nearly identical conditions that enables the success of sdr approach. the highlight of our work is that the theoretical guarantees are purely algebraic and do not assume any statistical priors of the additive adversaries, and thus it applies to various interesting settings.",,2021-06-29,2024-01-13,['shuyang ling']
2107.02780,"causal inference with corrupted data: measurement error, missing values,   discretization, and differential privacy",econ.em cs.lg math.st stat.ml stat.th,"the us census bureau will deliberately corrupt data sets derived from the 2020 us census, enhancing the privacy of respondents while potentially reducing the precision of economic analysis. to investigate whether this trade-off is inevitable, we formulate a semiparametric model of causal inference with high dimensional corrupted data. we propose a procedure for data cleaning, estimation, and inference with data cleaning-adjusted confidence intervals. we prove consistency and gaussian approximation by finite sample arguments, with a rate of $n^{ 1/2}$ for semiparametric estimands that degrades gracefully for nonparametric estimands. our key assumption is that the true covariates are approximately low rank, which we interpret as approximate repeated measurements and empirically validate. our analysis provides nonasymptotic theoretical contributions to matrix completion, statistical learning, and semiparametric statistics. calibrated simulations verify the coverage of our data cleaning adjusted confidence intervals and demonstrate the relevance of our results for census-derived data.",,2021-07-06,2024-02-12,"['anish agarwal', 'rahul singh']"
2107.08020,online graph topology learning from matrix-valued time series,stat.ml cs.lg,"this paper is concerned with the statistical analysis of matrix-valued time series. these are data collected over a network of sensors (typically a set of spatial locations) along time, where a vector of features is observed per time instant per sensor. thus each sensor is characterized by a vectorial time series. we would like to identify the dependency structure among these sensors and represent it by a graph. when there is only one feature per sensor, the vector auto-regressive models have been widely adapted to infer the structure of granger causality. the resulting graph is referred to as causal graph. our first contribution is then extending var models to matrix-variate models to serve the purpose of graph learning. secondly, we propose two online procedures respectively in low and high dimensions, which can update quickly the estimates of coefficients when new samples arrive. in particular in high dimensional regime, a novel lasso-type is introduced and we develop its homotopy algorithms for the online learning. we also provide an adaptive tuning procedure for the regularization parameter. lastly, we consider that, the application of ar models onto data usually requires detrending the raw data, however, this step is forbidden in online context. therefore, we augment the proposed ar models by incorporating trend as extra parameter, and then adapt the online algorithms to the augmented data models, which allow us to simultaneously learn the graph and trend from streaming samples. in this work, we consider primarily the periodic trend. numerical experiments using both synthetic and real data are performed, whose results support the effectiveness of the proposed methods.",,2021-07-16,2024-02-01,"['yiye jiang', 'jérémie bigot', 'sofian maabout']"
2107.11869,adaptive estimation and uniform confidence bands for nonparametric   structural functions and elasticities,econ.em stat.me stat.ml,"we introduce two data-driven procedures for optimal estimation and inference in nonparametric models using instrumental variables. the first is a data-driven choice of sieve dimension for a popular class of sieve two-stage least squares estimators. when implemented with this choice, estimators of both the structural function $h_0$ and its derivatives (such as elasticities) converge at the fastest possible (i.e., minimax) rates in sup-norm. the second is for constructing uniform confidence bands (ucbs) for $h_0$ and its derivatives. our ucbs guarantee coverage over a generic class of data-generating processes and contract at the minimax rate, possibly up to a logarithmic factor. as such, our ucbs are asymptotically more efficient than ucbs based on the usual approach of undersmoothing. as an application, we estimate the elasticity of the intensive margin of firm exports in a monopolistic competition model of international trade. simulations illustrate the good performance of our procedures in empirically calibrated designs. our results provide evidence against common parameterizations of the distribution of unobserved firm heterogeneity.",,2021-07-25,2024-01-07,"['xiaohong chen', 'timothy christensen', 'sid kankanala']"
2107.12365,inference for heteroskedastic pca with missing data,math.st cs.it cs.lg math.it stat.me stat.ml stat.th,"this paper studies how to construct confidence regions for principal component analysis (pca) in high dimension, a problem that has been vastly under-explored. while computing measures of uncertainty for nonlinear/nonconvex estimators is in general difficult in high dimension, the challenge is further compounded by the prevalent presence of missing data and heteroskedastic noise. we propose a novel approach to performing valid inference on the principal subspace under a spiked covariance model with missing data, on the basis of an estimator called heteropca (zhang et al., 2022). we develop non-asymptotic distributional guarantees for heteropca, and demonstrate how these can be invoked to compute both confidence regions for the principal subspace and entrywise confidence intervals for the spiked covariance matrix. our inference procedures are fully data-driven and adaptive to heteroskedastic random noise, without requiring prior knowledge about the noise levels.",,2021-07-26,2024-02-28,"['yuling yan', 'yuxin chen', 'jianqing fan']"
2108.00473,derivative-free alternating projection algorithms for general   nonconvex-concave minimax problems,math.oc cs.lg stat.ml,"in this paper, we study zeroth-order algorithms for nonconvex-concave minimax problems, which have attracted widely attention in machine learning, signal processing and many other fields in recent years. we propose a zeroth-order alternating randomized gradient projection (zo-agp) algorithm for smooth nonconvex-concave minimax problems, and its iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{o}(\varepsilon^{-4})$, and the number of function value estimation is bounded by $\mathcal{o}(d_{x}+d_{y})$ per iteration. moreover, we propose a zeroth-order block alternating randomized proximal gradient algorithm (zo-bapg) for solving block-wise nonsmooth nonconvex-concave minimax optimization problems, and the iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{o}(\varepsilon^{-4})$ and the number of function value estimation per iteration is bounded by $\mathcal{o}(k d_{x}+d_{y})$. to the best of our knowledge, this is the first time that zeroth-order algorithms with iteration complexity gurantee are developed for solving both general smooth and block-wise nonsmooth nonconvex-concave minimax problems. numerical results on data poisoning attack problem and distributed nonconvex sparse principal component analysis problem validate the efficiency of the proposed algorithms.",,2021-08-01,2024-01-25,"['zi xu', 'ziqi wang', 'jingjing shen', 'yuhong dai']"
2108.13329,on the effects of biased quantum random numbers on the initialization of   artificial neural networks,quant-ph stat.ml,"recent advances in practical quantum computing have led to a variety of cloud-based quantum computing platforms that allow researchers to evaluate their algorithms on noisy intermediate-scale quantum (nisq) devices. a common property of quantum computers is that they can exhibit instances of true randomness as opposed to pseudo-randomness obtained from classical systems. investigating the effects of such true quantum randomness in the context of machine learning is appealing, and recent results vaguely suggest that benefits can indeed be achieved from the use of quantum random numbers. to shed some more light on this topic, we empirically study the effects of hardware-biased quantum random numbers on the initialization of artificial neural network weights in numerical experiments. we find no statistically significant difference in comparison with unbiased quantum random numbers as well as biased and unbiased random numbers from a classical pseudo-random number generator. the quantum random numbers for our experiments are obtained from real quantum hardware.",10.1007/s10994-023-06490-y,2021-08-30,2023-12-19,"['raoul heese', 'moritz wolter', 'sascha mücke', 'lukas franken', 'nico piatkowski']"
2109.00783,computer vision self-supervised learning methods on time series,cs.lg cs.ai stat.ml,"self-supervised learning (ssl) has had great success in both computer vision. most of the current mainstream computer vision ssl frameworks are based on siamese network architecture. these approaches often rely on cleverly crafted loss functions and training setups to avoid feature collapse. in this study, we evaluate if those computer-vision ssl frameworks are also effective on a different modality (\textit{i.e.,} time series). the effectiveness is experimented and evaluated on the ucr and uea archives, and we show that the computer vision ssl frameworks can be effective even for time series. in addition, we propose a new method that improves on the recently proposed vicreg method. our method improves on a \textit{covariance} term proposed in vicreg, and in addition we augment the head of the architecture by an iterative normalization layer that accelerates the convergence of the model.",,2021-09-02,2024-01-26,"['daesoo lee', 'erlend aune']"
2109.03445,convergence of batch asynchronous stochastic approximation with   applications to reinforcement learning,stat.ml cs.ai cs.lg cs.sy eess.sy math.pr,"ever since its introduction in the classic paper of robbins and monro in 1951, stochastic approximation (sa) has become a standard tool for finding a solution of an equation of the form $f(\theta) = 0$, when only noisy measurements of $f(\cdot)$ are available. in most situations, \textit{every component} of the putative solution $\theta_t$ is updated at each step $t$. in some applications such as $q$-learning, a key technique in reinforcement learning (rl), \textit{only one component} of $\theta_t$ is updated at each $t$. this is known as \textbf{asynchronous} sa. the topic of study in the present paper is to study \textbf{block asynchronous sa (basa)}, in which, at each step $t$, \textit{some but not necessarily all} components of $\theta_t$ are updated. the theory presented here embraces both conventional (synchronous) sa as well as asynchronous sa, and all in-between possibilities. we also prove bounds on the \textit{rate} of convergence of $\theta_t$ to the solutions.   as a prelude to the new results, we also briefly survey some results on the convergence of the stochastic gradient method, proved in a companion paper by the present authors.",,2021-09-08,2024-02-20,"['rajeeva l. karandikar', 'm. vidyasagar']"
2109.09264,computationally efficient high-dimensional bayesian optimization via   variable selection,cs.lg stat.ml,"bayesian optimization (bo) is a method for globally optimizing black-box functions. while bo has been successfully applied to many scenarios, developing effective bo algorithms that scale to functions with high-dimensional domains is still a challenge. optimizing such functions by vanilla bo is extremely time-consuming. alternative strategies for high-dimensional bo that are based on the idea of embedding the high-dimensional space to the one with low dimension are sensitive to the choice of the embedding dimension, which needs to be pre-specified. we develop a new computationally efficient high-dimensional bo method that exploits variable selection. our method is able to automatically learn axis-aligned sub-spaces, i.e. spaces containing selected variables, without the demand of any pre-specified hyperparameters. we theoretically analyze the computational complexity of our algorithm and derive the regret bound. we empirically show the efficacy of our method on several synthetic and real problems.",,2021-09-19,2024-02-12,"['yihang shen', 'carl kingsford']"
2109.10399,subseasonalclimateusa: a dataset for subseasonal forecasting and   benchmarking,physics.ao-ph cs.lg stat.ml,"subseasonal forecasting of the weather two to six weeks in advance is critical for resource allocation and advance disaster notice but poses many challenges for the forecasting community. at this forecast horizon, physics-based dynamical models have limited skill, and the targets for prediction depend in a complex manner on both local weather variables and global climate variables. recently, machine learning methods have shown promise in advancing the state of the art but only at the cost of complex data curation, integrating expert knowledge with aggregation across multiple relevant data sources, file formats, and temporal and spatial resolutions. to streamline this process and accelerate future development, we introduce subseasonalclimateusa, a curated dataset for training and benchmarking subseasonal forecasting models in the united states. we use this dataset to benchmark a diverse suite of models, including operational dynamical models, classical meteorological baselines, and ten state-of-the-art machine learning and deep learning-based methods from the literature. overall, our benchmarks suggest simple and effective ways to extend the accuracy of current operational models. subseasonalclimateusa is regularly updated and accessible via the https://github.com/microsoft/subseasonal_data/ python package.",,2021-09-21,2024-01-16,"['soukayna mouatadid', 'paulo orenstein', 'genevieve flaspohler', 'miruna oprescu', 'judah cohen', 'franklyn wang', 'sean knight', 'maria geogdzhayeva', 'sam levang', 'ernest fraenkel', 'lester mackey']"
2109.11172,clustering performance analysis using a new correlation-based cluster   validity index,stat.ml cs.cv cs.lg,"there are various cluster validity indices used for evaluating clustering results. one of the main objectives of using these indices is to seek the optimal unknown number of clusters. some indices work well for clusters with different densities, sizes, and shapes. yet, one shared weakness of those validity indices is that they often provide only one optimal number of clusters. that number is unknown in real-world problems, and there might be more than one possible option. we develop a new cluster validity index based on a correlation between an actual distance between a pair of data points and a centroid distance of clusters that the two points occupy. our proposed index constantly yields several local peaks and overcomes the previously stated weakness. several experiments in different scenarios, including uci real-world data sets, have been conducted to compare the proposed validity index with several well-known ones. an r package related to this new index called ncvalid is available at https://github.com/nwiroonsri/ncvalid.",10.1016/j.patcog.2023.109910,2021-09-23,2022-07-25,['nathakhun wiroonsri']
2109.13004,optimising for interpretability: convolutional dynamic alignment   networks,stat.ml cs.cv cs.lg,"we introduce a new family of neural network models called convolutional dynamic alignment networks (coda nets), which are performant classifiers with a high degree of inherent interpretability. their core building blocks are dynamic alignment units (daus), which are optimised to transform their inputs with dynamically computed weight vectors that align with task-relevant patterns. as a result, coda nets model the classification prediction through a series of input-dependent linear transformations, allowing for linear decomposition of the output into individual input contributions. given the alignment of the daus, the resulting contribution maps align with discriminative input patterns. these model-inherent decompositions are of high visual quality and outperform existing attribution methods under quantitative metrics. further, coda nets constitute performant classifiers, achieving on par results to resnet and vgg models on e.g. cifar-10 and tinyimagenet. lastly, coda nets can be combined with conventional neural network models to yield powerful classifiers that more easily scale to complex datasets such as imagenet whilst exhibiting an increased interpretable depth, i.e., the output can be explained well in terms of contributions from intermediate layers within the network.",10.1109/tpami.2022.3226041,2021-09-27,2024-01-15,"['moritz böhle', 'mario fritz', 'bernt schiele']"
2110.01440,arithmetic average density fusion -- part i: some statistic and   information-theoretic results,math.st cs.it math.it stat.th,"finite mixture such as the gaussian mixture is a flexible and powerful probabilistic modeling tool for representing the multimodal distribution widely involved in many estimation and learning problems. the core of it is representing the target distribution by the arithmetic average (aa) of a finite number of sub-distributions which constitute a mixture. while the mixture has been widely used for single sensor filter design, it is only recent that the aa fusion demonstrates compelling performance for multi-sensor filter design. in this paper, some statistic and information-theoretic results are given on the covariance consistency, mean square error, mode-preservation capacity, and the information divergence of the aa fusion approach. in particular, based on the concept of conservative fusion, the relationship of the aa fusion with the existing conservative fusion approaches such as covariance union and covariance intersection is exposed. a suboptimal weighting approach has been proposed, which jointly with the best mixture-fit property of the aa fusion leads to a max-min optimization problem. linear gaussian models are considered for algorithm illustration and simulation comparison, resulting in the first-ever aa fusion-based multi-sensor kalman filter.",10.1016/j.inffus.2023.102199,2021-10-01,2023-11-13,"['tiancheng li', 'yan song', 'enbin song', 'hongqi fan']"
2110.04829,adaptive joint distribution learning,stat.ml cs.lg cs.na math.na,"we develop a new framework for embedding joint probability distributions in tensor product reproducing kernel hilbert spaces (rkhs). our framework accommodates a low-dimensional, normalized and positive model of a radon-nikodym derivative, which we estimate from sample sizes of up to several million data points, alleviating the inherent limitations of rkhs modeling. well-defined normalized and positive conditional distributions are natural by-products to our approach. the embedding is fast to compute and accommodates learning problems ranging from prediction to classification. our theoretical findings are supplemented by favorable numerical results.",,2021-10-10,2024-01-10,"['damir filipovic', 'michael multerer', 'paul schneider']"
2110.05475,bayesian hidden markov models for latent variable labeling assignments   in conflict research: application to the role ceasefires play in conflict   dynamics,stat.ap stat.me,"a crucial challenge for solving problems in conflict research is in leveraging the semi-supervised nature of the data that arise. observed response data such as counts of battle deaths over time indicate latent processes of interest such as intensity and duration of conflicts, but defining and labeling instances of these unobserved processes requires nuance and imprecision. the availability of such labels, however, would make it possible to study the effect of intervention-related predictors - such as ceasefires - directly on conflict dynamics (e.g., latent intensity) rather than through an intermediate proxy like observed counts of battle deaths. motivated by this problem and the new availability of the eth-prio civil conflict ceasefires data set, we propose a bayesian autoregressive (ar) hidden markov model (hmm) framework as a sufficiently flexible machine learning approach for semi-supervised regime labeling with uncertainty quantification. we motivate our approach by illustrating the way it can be used to study the role that ceasefires play in shaping conflict dynamics. this ceasefires data set is the first systematic and globally comprehensive data on ceasefires, and our work is the first to analyze this new data and to explore the effect of ceasefires on conflict dynamics in a comprehensive and cross-country manner.",,2021-10-07,2024-01-22,"['jonathan p williams', 'gudmund h hermansen', 'håvard strand', 'govinda clayton', 'håvard mokleiv nygård']"
2110.06257,causal discovery from conditionally stationary time series,cs.lg stat.ml,"causal discovery, i.e., inferring underlying causal relationships from observational data, has been shown to be highly challenging for ai systems. in time series modeling context, traditional causal discovery methods mainly consider constrained scenarios with fully observed variables and/or data from stationary time-series. we develop a causal discovery approach to handle a wide class of non-stationary time-series that are conditionally stationary, where the non-stationary behaviour is modeled as stationarity conditioned on a set of (possibly hidden) state variables. named state-dependent causal inference (sdci), our approach is able to recover the underlying causal dependencies, provably with fully-observed states and empirically with hidden states. the latter is confirmed by experiments on synthetic linear system and nonlinear particle interaction data, where sdci achieves superior performance over baseline causal discovery methods. improved results over non-causal rnns on modeling nba player movements demonstrate the potential of our method and motivate the use of causality-driven methods for forecasting.",,2021-10-12,2024-02-23,"['carles balsells-rodas', 'ruibo tu', 'hedvig kjellstrom', 'yingzhen li']"
2110.08111,an active learning approach for improving the performance of equilibrium   based chemical simulations,stat.ml cs.lg,"in this paper, we propose a novel sequential data-driven method for dealing with equilibrium based chemical simulations, which can be seen as a specific machine learning approach called active learning. the underlying idea of our approach is to consider the function to estimate as a sample of a gaussian process which allows us to compute the global uncertainty on the function estimation. thanks to this estimation and with almost no parameter to tune, the proposed method sequentially chooses the most relevant input data at which the function to estimate has to be evaluated to build a surrogate model. hence, the number of evaluations of the function to estimate is dramatically limited. our active learning method is validated through numerical experiments and applied to a complex chemical system commonly used in geoscience.",10.1007/s10596-022-10130-0,2021-10-15,2021-12-17,"['mary savino', 'céline lévy-leduc', 'marc leconte', 'benoit cochepin']"
2110.12510,post-regularization confidence bands for ordinary differential equations,stat.me stat.ml,"ordinary differential equation (ode) is an important tool to study the dynamics of a system of biological and physical processes. a central question in ode modeling is to infer the significance of individual regulatory effect of one signal variable on another. however, building confidence band for ode with unknown regulatory relations is challenging, and it remains largely an open question. in this article, we construct post-regularization confidence band for individual regulatory function in ode with unknown functionals and noisy data observations. our proposal is the first of its kind, and is built on two novel ingredients. the first is a new localized kernel learning approach that combines reproducing kernel learning with local taylor approximation, and the second is a new de-biasing method that tackles infinite-dimensional functionals and additional measurement errors. we show that the constructed confidence band has the desired asymptotic coverage probability, and the recovered regulatory network approaches the truth with probability tending to one. we establish the theoretical properties when the number of variables in the system can be either smaller or larger than the number of sampling time points, and we study the regime-switching phenomenon. we demonstrate the efficacy of the proposed method through both simulations and illustrations with two data applications.",,2021-10-24,2024-02-04,"['xiaowu dai', 'lexin li']"
2110.15083,nearest neighbor empirical processes,math.st stat.ml stat.th,"in the regression framework, the empirical measure based on the responses resulting from the nearest neighbors, among the covariates, to a given point $x$ is introduced and studied as a central statistical quantity. first, the associated empirical process is shown to satisfy a uniform central limit theorem under a local bracketing entropy condition on the underlying class of functions reflecting the localizing nature of the nearest neighbor algorithm. second a uniform non-asymptotic bound is established under a well-known condition, often referred to as vapnik-chervonenkis, on the uniform entropy numbers. the covariance of the gaussian limit obtained in the uniform central limit theorem is simply equal to the conditional covariance operator given the covariate value. this suggests the possibility of using standard formulas to estimate the variance by using only the nearest neighbors instead of the full data. this is illustrated on two problems: the estimation of the conditional cumulative distribution function and local linear regression.",,2021-10-27,2024-01-30,['françois portier']
2111.04746,realizable learning is all you need,cs.lg stat.ml,"the equivalence of realizable and agnostic learnability is a fundamental phenomenon in learning theory. with variants ranging from classical settings like pac learning and regression to recent trends such as adversarially robust learning, it's surprising that we still lack a unified theory; traditional proofs of the equivalence tend to be disparate, and rely on strong model-specific assumptions like uniform convergence and sample compression.   in this work, we give the first model-independent framework explaining the equivalence of realizable and agnostic learnability: a three-line blackbox reduction that simplifies, unifies, and extends our understanding across a wide variety of settings. this includes models with no known characterization of learnability such as learning with arbitrary distributional assumptions and more general loss functions, as well as a host of other popular settings such as robust learning, partial learning, fair learning, and the statistical query model.   more generally, we argue that the equivalence of realizable and agnostic learning is actually a special case of a broader phenomenon we call property generalization: any desirable property of a learning algorithm (e.g. noise tolerance, privacy, stability) that can be satisfied over finite hypothesis classes extends (possibly in some variation) to any learnable hypothesis class.",10.46298/theoretics.24.2,2021-11-08,2024-02-02,"['max hopkins', 'daniel m. kane', 'shachar lovett', 'gaurav mahajan']"
2111.09790,mcce: monte carlo sampling of realistic counterfactual explanations,stat.ml cs.lg,"we introduce mcce: monte carlo sampling of valid and realistic counterfactual explanations for tabular data, a novel counterfactual explanation method that generates on-manifold, actionable and valid counterfactuals by modeling the joint distribution of the mutable features given the immutable features and the decision. unlike other on-manifold methods that tend to rely on variational autoencoders and have strict prediction model and data requirements, mcce handles any type of prediction model and categorical features with more than two levels. mcce first models the joint distribution of the features and the decision with an autoregressive generative model where the conditionals are estimated using decision trees. then, it samples a large set of observations from this model, and finally, it removes the samples that do not obey certain criteria. we compare mcce with a range of state-of-the-art on-manifold counterfactual methods using four well-known data sets and show that mcce outperforms these methods on all common performance metrics and speed. in particular, including the decision in the modeling process improves the efficiency of the method substantially.",,2021-11-18,2024-01-24,"['annabelle redelmeier', 'martin jullum', 'kjersti aas', 'anders løland']"
2111.10275,composite goodness-of-fit tests with kernels,stat.ml cs.lg stat.me,"model misspecification can create significant challenges for the implementation of probabilistic models, and this has led to development of a range of robust methods which directly account for this issue. however, whether these more involved methods are required will depend on whether the model is really misspecified, and there is a lack of generally applicable methods to answer this question. in this paper, we propose one such method. more precisely, we propose kernel-based hypothesis tests for the challenging composite testing problem, where we are interested in whether the data comes from any distribution in some parametric family. our tests make use of minimum distance estimators based on the maximum mean discrepancy and the kernel stein discrepancy. they are widely applicable, including whenever the density of the parametric model is known up to normalisation constant, or if the model takes the form of a simulator. as our main result, we show that we are able to estimate the parameter and conduct our test on the same data (without data splitting), while maintaining a correct test level. our approach is illustrated on a range of problems, including testing for goodness-of-fit of an unnormalised non-parametric density model, and an intractable generative model of a biological cellular network.",,2021-11-19,2024-02-27,"['oscar key', 'arthur gretton', 'françois-xavier briol', 'tamara fernandez']"
2111.13646,dimension reduction with prior information for knowledge discovery,stat.ml cs.lg,"this paper addresses the problem of mapping high-dimensional data to a low-dimensional space, in the presence of other known features. this problem is ubiquitous in science and engineering as there are often controllable/measurable features in most applications. to solve this problem, this paper proposes a broad class of methods, which is referred to as conditional multidimensional scaling (mds). an algorithm for optimizing the objective function of conditional mds is also developed. the convergence of this algorithm is proven under mild assumptions. conditional mds is illustrated with kinship terms, facial expressions, textile fabrics, car-brand perception, and cylinder machining examples. these examples demonstrate the advantages of conditional mds over conventional dimension reduction in improving the estimation quality of the reduced-dimension space and simplifying visualization and knowledge discovery tasks. computer codes for this work are available in the open-source cml r package.",10.1109/tpami.2023.3346212,2021-11-26,2023-12-29,['anh tuan bui']
2112.00365,probability-generating function kernels for spherical data,stat.ml cs.lg,"probability-generating function (pgf) kernels are introduced, which constitute a class of kernels supported on the unit hypersphere, for the purposes of spherical data analysis. pgf kernels generalize rbf kernels in the context of spherical data. the properties of pgf kernels are studied. a semi-parametric learning algorithm is introduced to enable the use of pgf kernels with spherical data.",,2021-12-01,2024-02-01,"['theodore papamarkou', 'alexey lindo']"
2112.03220,cross-validation for change-point regression: pitfalls and solutions,stat.me math.st stat.co stat.ml stat.th,"cross-validation is the standard approach for tuning parameter selection in many non-parametric regression problems. however its use is less common in change-point regression, perhaps as its prediction error-based criterion may appear to permit small spurious changes and hence be less well-suited to estimation of the number and location of change-points. we show that in fact the problems of cross-validation with squared error loss are more severe and can lead to systematic under- or over-estimation of the number of change-points, and highly suboptimal estimation of the mean function in simple settings where changes are easily detectable. we propose two simple approaches to remedy these issues, the first involving the use of absolute error rather than squared error loss, and the second involving modifying the holdout sets used. for the latter, we provide conditions that permit consistent estimation of the number of change-points for a general change-point estimation procedure. we show these conditions are satisfied for least squares estimation using new results on its performance when supplied with the incorrect number of change-points. numerical experiments show that our new approaches are competitive with common change-point methods using classical tuning parameter choices when error distributions are well-specified, but can substantially outperform these in misspecified models. an implementation of our methodology is available in the r package crossvalidationcp on cran.",,2021-12-06,2024-02-12,"['florian pein', 'rajen d. shah']"
2112.07145,linear discriminant analysis with high-dimensional mixed variables,stat.me stat.ml,"datasets containing both categorical and continuous variables are frequently encountered in many areas, and with the rapid development of modern measurement technologies, the dimensions of these variables can be very high. despite the recent progress made in modelling high-dimensional data for continuous variables, there is a scarcity of methods that can deal with a mixed set of variables. to fill this gap, this paper develops a novel approach for classifying high-dimensional observations with mixed variables. our framework builds on a location model, in which the distributions of the continuous variables conditional on categorical ones are assumed gaussian. we overcome the challenge of having to split data into exponentially many cells, or combinations of the categorical variables, by kernel smoothing, and provide new perspectives for its bandwidth choice to ensure an analogue of bochner's lemma, which is different to the usual bias-variance tradeoff. we show that the two sets of parameters in our model can be separately estimated and provide penalized likelihood for their estimation. results on the estimation accuracy and the misclassification rates are established, and the competitive performance of the proposed classifier is illustrated by extensive simulation and real data studies.",,2021-12-13,2024-01-02,"['binyan jiang', 'chenlei leng', 'cheng wang', 'zhongqing yang', 'xinyang yu']"
2112.09820,"gpex, a framework for interpreting artificial neural networks",cs.lg stat.ml,"the analogy between gaussian processes (gps) and deep artificial neural networks (anns) has received a lot of interest, and has shown promise to unbox the blackbox of deep anns. existing theoretical works put strict assumptions on the ann (e.g. requiring all intermediate layers to be wide, or using specific activation functions). accommodating those theoretical assumptions is hard in recent deep architectures, and those theoretical conditions need refinement as new deep architectures emerge. in this paper we derive an evidence lower-bound that encourages the gp's posterior to match the ann's output without any requirement on the ann. using our method we find out that on 5 datasets, only a subset of those theoretical assumptions are sufficient. indeed, in our experiments we used a normal resnet-18 or feed-forward backbone with a single wide layer in the end. one limitation of training gps is the lack of scalability with respect to the number of inducing points. we use novel computational techniques that allow us to train gps with hundreds of thousands of inducing points and with gpu acceleration. as shown in our experiments, doing so has been essential to get a close match between the gps and the anns on 5 datasets. we implement our method as a publicly available tool called gpex: https://github.com/amirakbarnejad/gpex. on 5 datasets (4 image datasets, and 1 biological dataset) and anns with 2 types of functionality (classifier or attention-mechanism) we were able to find gps whose outputs closely match those of the corresponding anns. after matching the gps to the anns, we used the gps' kernel functions to explain the anns' decisions. we provide more than 200 explanations (around 30 explanations in the paper and the rest in the supplementary) which are highly interpretable by humans and show the ability of the obtained gps to unbox the anns' decisions.",,2021-12-17,2024-01-10,"['amir akbarnejad', 'gilbert bigras', 'nilanjan ray']"
2112.10955,joint learning of linear time-invariant dynamical systems,stat.ml cs.lg cs.sy eess.sy math.ds,"linear time-invariant systems are very popular models in system theory and applications. a fundamental problem in system identification that remains rather unaddressed in extant literature is to leverage commonalities amongst related linear systems to estimate their transition matrices more accurately. to address this problem, the current paper investigates methods for jointly estimating the transition matrices of multiple systems. it is assumed that the transition matrices are unknown linear functions of some unknown shared basis matrices. we establish finite-time estimation error rates that fully reflect the roles of trajectory lengths, dimension, and number of systems under consideration. the presented results are fairly general and show the significant gains that can be achieved by pooling data across systems in comparison to learning each system individually. further, they are shown to be robust against model misspecifications. to obtain the results, we develop novel techniques that are of interest for addressing similar joint-learning problems. they include tightly bounding estimation errors in terms of the eigen-structures of transition matrices, establishing sharp high probability bounds for singular values of dependent random matrices, and capturing effects of misspecified transition matrices as the systems evolve over time.",,2021-12-20,2024-01-02,"['aditya modi', 'mohamad kazem shirani faradonbeh', 'ambuj tewari', 'george michailidis']"
2112.14946,a causal inference framework for spatial confounding,stat.me,"recently, addressing spatial confounding has become a major topic in spatial statistics. however, the literature has provided conflicting definitions, and many proposed definitions do not address the issue of confounding as it is understood in causal inference. we define spatial confounding as the existence of an unmeasured causal confounder with a spatial structure. we present a causal inference framework for nonparametric identification of the causal effect of a continuous exposure on an outcome in the presence of spatial confounding. we propose double machine learning (dml), a procedure in which flexible models are used to regress both the exposure and outcome variables on confounders to arrive at a causal estimator with favorable robustness properties and convergence rates, and we prove that this approach is consistent and asymptotically normal under spatial dependence. as far as we are aware, this is the first approach to spatial confounding that does not rely on restrictive parametric assumptions (such as linearity, effect homogeneity, or gaussianity) for both identification and estimation. we demonstrate the advantages of the dml approach analytically and in simulations. we apply our methods and reasoning to a study of the effect of fine particulate matter exposure during pregnancy on birthweight in california.",,2021-12-30,2024-01-22,"['brian gilbert', 'abhirup datta', 'joan a. casey', 'elizabeth l. ogburn']"
2201.00409,global convergence of optimized adaptive importance samplers,stat.co stat.me stat.ml,"we analyze the optimized adaptive importance sampler (oais) for performing monte carlo integration with general proposals. we leverage a classical result which shows that the bias and the mean-squared error (mse) of the importance sampling scales with the $\chi^2$-divergence between the target and the proposal and develop a scheme which performs global optimization of $\chi^2$-divergence. while it is known that this quantity is convex for exponential family proposals, the case of the general proposals has been an open problem. we close this gap by utilizing the nonasymptotic bounds for stochastic gradient langevin dynamics (sgld) for the global optimization of $\chi^2$-divergence and derive nonasymptotic bounds for the mse by leveraging recent results from non-convex optimization literature. the resulting ais schemes have explicit theoretical guarantees that are uniform-in-time.",,2022-01-02,2024-01-28,['ömer deniz akyildiz']
2201.01793,spectral clustering with variance information for group structure   estimation in panel data,stat.me stat.ml,"consider a panel data setting where repeated observations on individuals are available. often it is reasonable to assume that there exist groups of individuals that share similar effects of observed characteristics, but the grouping is typically unknown in advance. we first conduct a local analysis which reveals that the variances of the individual coefficient estimates contain useful information for the estimation of group structure. we then propose a method to estimate unobserved groupings for general panel data models that explicitly account for the variance information. our proposed method remains computationally feasible with a large number of individuals and/or repeated measurements on each individual. the developed ideas can also be applied even when individual-level data are not available and only parameter estimates together with some quantification of estimation uncertainty are given to the researcher. a thorough simulation study demonstrates superior performance of our method than existing methods and we apply the method to two empirical applications.",,2022-01-05,2024-02-08,"['lu yu', 'jiaying gu', 'stanislav volgushev']"
2201.01942,efficiently disentangle causal representations,cs.lg stat.ml,"this paper proposes an efficient approach to learning disentangled representations with causal mechanisms based on the difference of conditional probabilities in original and new distributions. we approximate the difference with models' generalization abilities so that it fits in the standard machine learning framework and can be efficiently computed. in contrast to the state-of-the-art approach, which relies on the learner's adaptation speed to new distribution, the proposed approach only requires evaluating the model's generalization ability. we provide a theoretical explanation for the advantage of the proposed method, and our experiments show that the proposed technique is 1.9--11.0$\times$ more sample efficient and 9.4--32.4 times quicker than the previous method on various tasks. the source code is available at \url{https://github.com/yuanpeng16/edcr}.",,2022-01-06,2024-01-01,"['yuanpeng li', 'joel hestness', 'mohamed elhoseiny', 'liang zhao', 'kenneth church']"
2201.01954,federated optimization of smooth loss functions,cs.lg math.oc math.st stat.ml stat.th,"in this work, we study empirical risk minimization (erm) within a federated learning framework, where a central server minimizes an erm objective function using training data that is stored across $m$ clients. in this setting, the federated averaging (fedave) algorithm is the staple for determining $\epsilon$-approximate solutions to the erm problem. similar to standard optimization algorithms, the convergence analysis of fedave only relies on smoothness of the loss function in the optimization parameter. however, loss functions are often very smooth in the training data too. to exploit this additional smoothness, we propose the federated low rank gradient descent (fedlrgd) algorithm. since smoothness in data induces an approximate low rank structure on the loss function, our method first performs a few rounds of communication between the server and clients to learn weights that the server can use to approximate clients' gradients. then, our method solves the erm problem at the server using inexact gradient descent. to show that fedlrgd can have superior performance to fedave, we present a notion of federated oracle complexity as a counterpart to canonical oracle complexity. under some assumptions on the loss function, e.g., strong convexity in parameter, $\eta$-h\""older smoothness in data, etc., we prove that the federated oracle complexity of fedlrgd scales like $\phi m(p/\epsilon)^{\theta(d/\eta)}$ and that of fedave scales like $\phi m(p/\epsilon)^{3/4}$ (neglecting sub-dominant factors), where $\phi\gg 1$ is a ""communication-to-computation ratio,"" $p$ is the parameter dimension, and $d$ is the data dimension. then, we show that when $d$ is small and the loss function is sufficiently smooth in the data, fedlrgd beats fedave in federated oracle complexity. finally, in the course of analyzing fedlrgd, we also establish a result on low rank approximation of latent variable models.",10.1109/tit.2023.3317168,2022-01-06,2024-01-03,"['ali jadbabaie', 'anuran makur', 'devavrat shah']"
2201.05149,the curse of overparametrization in adversarial training: precise   analysis of robust generalization for random features regression,cs.lg math.st stat.ml stat.th,"successful deep learning models often involve training neural network architectures that contain more parameters than the number of training samples. such overparametrized models have been extensively studied in recent years, and the virtues of overparametrization have been established from both the statistical perspective, via the double-descent phenomenon, and the computational perspective via the structural properties of the optimization landscape.   despite the remarkable success of deep learning architectures in the overparametrized regime, it is also well known that these models are highly vulnerable to small adversarial perturbations in their inputs. even when adversarially trained, their performance on perturbed inputs (robust generalization) is considerably worse than their best attainable performance on benign inputs (standard generalization). it is thus imperative to understand how overparametrization fundamentally affects robustness.   in this paper, we will provide a precise characterization of the role of overparametrization on robustness by focusing on random features regression models (two-layer neural networks with random first layer weights). we consider a regime where the sample size, the input dimension and the number of parameters grow in proportion to each other, and derive an asymptotically exact formula for the robust generalization error when the model is adversarially trained. our developed theory reveals the nontrivial effect of overparametrization on robustness and indicates that for adversarially trained random features models, high overparametrization can hurt robust generalization.",,2022-01-13,2024-02-01,"['hamed hassani', 'adel javanmard']"
2201.06606,drift vs shift: decoupling trends and changepoint analysis,stat.me,"we introduce a new approach for decoupling trends (drift) and changepoints (shifts) in time series. our locally adaptive model-based approach for robustly decoupling combines bayesian trend filtering and machine learning based regularization. an over-parameterized bayesian dynamic linear model (dlm) is first applied to characterize drift. then a weighted penalized likelihood estimator is paired with the estimated dlm posterior distribution to identify shifts. we show how bayesian dlms specified with so-called shrinkage priors can provide smooth estimates of underlying trends in the presence of complex noise components. however, their inability to shrink exactly to zero inhibits direct changepoint detection. in contrast, penalized likelihood methods are highly effective in locating changepoints. however, they require data with simple patterns in both signal and noise. the proposed decoupling approach combines the strengths of both, i.e. the flexibility of bayesian dlms with the hard thresholding property of penalized likelihood estimators, to provide changepoint analysis in complex, modern settings. the proposed framework is outlier robust and can identify a variety of changes, including in mean and slope. it is also easily extended for analysis of parameter shifts in time-varying parameter models like dynamic regressions. we illustrate the flexibility and contrast the performance and robustness of our approach with several alternative methods across a wide range of simulations and application examples.",,2022-01-17,2024-01-06,"['haoxuan wu', 'toryn l. j. schafer', 'sean ryan', 'david s. matteson']"
2201.07794,a non-expert's introduction to data ethics for mathematicians,math.ho cs.cy cs.lg physics.soc-ph stat.ml,"i give a short introduction to data ethics. i begin with some background information and societal context for data ethics. i then discuss data ethics in mathematical-science education and indicate some available course material. i briefly highlight a few efforts -- at my home institution and elsewhere -- on data ethics, society, and social good. i then discuss open data in research, research replicability and some other ethical issues in research, and the tension between privacy and open data and code, and a few controversial studies and reactions to studies. i then discuss ethical principles, institutional review boards, and a few other considerations in the scientific use of human data. finally, i briefly survey a variety of research and lay articles that are relevant to data ethics and data privacy. i conclude with a brief summary.   my focal audience is mathematicians, but i hope that this chapter will also be useful to others. i am not an expert about data ethics, and this chapter provides only a starting point on this wide-ranging topic. i encourage you to examine the resources that i discuss and to reflect carefully on data ethics, its role in mathematics education, and the societal implications of data and data analysis. as data and technology continue to evolve, i hope that such careful reflection will continue throughout your life.",,2022-01-18,2023-12-31,['mason a. porter']
2201.08343,using machine learning to test causal hypotheses in conjoint analysis,stat.me stat.ml,"conjoint analysis is a popular experimental design used to measure multidimensional preferences. researchers examine how varying a factor of interest, while controlling for other relevant factors, influences decision-making. currently, there exist two methodological approaches to analyzing data from a conjoint experiment. the first focuses on estimating the average marginal effects of each factor while averaging over the other factors. although this allows for straightforward design-based estimation, the results critically depend on the distribution of other factors and how interaction effects are aggregated. an alternative model-based approach can compute various quantities of interest, but requires researchers to correctly specify the model, a challenging task for conjoint analysis with many factors and possible interactions. in addition, a commonly used logistic regression has poor statistical properties even with a moderate number of factors when incorporating interactions. we propose a new hypothesis testing approach based on the conditional randomization test to answer the most fundamental question of conjoint analysis: does a factor of interest matter in any way given the other factors? our methodology is solely based on the randomization of factors, and hence is free from assumptions. yet, it allows researchers to use any test statistic, including those based on complex machine learning algorithms. as a result, we are able to combine the strengths of the existing design-based and model-based approaches. we illustrate the proposed methodology through conjoint analysis of immigration preferences and political candidate evaluation. we also extend the proposed approach to test for regularity assumptions commonly used in conjoint analysis. an open-source software package is available for implementing the proposed methodology.",10.1017/pan.2023.41,2022-01-20,2022-08-17,"['dae woong ham', 'kosuke imai', 'lucas janson']"
2201.10936,figaro: generating symbolic music with fine-grained artistic control,cs.sd cs.lg eess.as stat.ml,"generating music with deep neural networks has been an area of active research in recent years. while the quality of generated samples has been steadily increasing, most methods are only able to exert minimal control over the generated sequence, if any. we propose the self-supervised description-to-sequence task, which allows for fine-grained controllable generation on a global level. we do so by extracting high-level features about the target sequence and learning the conditional distribution of sequences given the corresponding high-level description in a sequence-to-sequence modelling setup. we train figaro (fine-grained music generation via attention-based, robust control) by applying description-to-sequence modelling to symbolic music. by combining learned high level features with domain knowledge, which acts as a strong inductive bias, the model achieves state-of-the-art results in controllable symbolic music generation and generalizes well beyond the training distribution.",,2022-01-26,2024-02-22,"['dimitri von rütte', 'luca biggio', 'yannic kilcher', 'thomas hofmann']"
2201.11358,fairness implications of encoding protected categorical attributes,cs.lg cs.cy cs.ds stat.ml,"past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g.\ support vector machines, gradient boosting decision trees or linear models. thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. this work compares the accuracy and fairness implications of the two most well-known encoding methods: \emph{one-hot encoding} and \emph{target encoding}. we distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. the first type, \textit{irreducible bias}, is due to direct group category discrimination, and the second type, \textit{reducible bias}, is due to the large variance in statistically underrepresented groups. we investigate the interaction between categorical encodings and target encoding regularization methods that reduce unfairness. furthermore, we consider the problem of intersectional unfairness that may arise when machine learning best practices improve performance measures by encoding several categorical attributes into a high-cardinality feature.",10.1145/3600211.3604657,2022-01-27,2023-05-05,"['carlos mougan', 'jose m. alvarez', 'salvatore ruggieri', 'steffen staab']"
2201.13128,deletion robust submodular maximization over matroids,cs.ds cs.lg stat.ml,"maximizing a monotone submodular function is a fundamental task in machine learning. in this paper, we study the deletion robust version of the problem under the classic matroids constraint. here the goal is to extract a small size summary of the dataset that contains a high value independent set even after an adversary deleted some elements. we present constant-factor approximation algorithms, whose space complexity depends on the rank $k$ of the matroid and the number $d$ of deleted elements. in the centralized setting we present a $(3.582+o(\varepsilon))$-approximation algorithm with summary size $o(k + \frac{d \log k}{\varepsilon^2})$. in the streaming setting we provide a $(5.582+o(\varepsilon))$-approximation algorithm with summary size and memory $o(k + \frac{d \log k}{\varepsilon^2})$. we complement our theoretical results with an in-depth experimental analysis showing the effectiveness of our algorithms on real-world datasets.",,2022-01-31,,"['paul dütting', 'federico fusco', 'silvio lattanzi', 'ashkan norouzi-fard', 'morteza zadimoghaddam']"
2202.00625,black-box bayesian inference for economic agent-based models,econ.em cs.ma stat.ml,"simulation models, in particular agent-based models, are gaining popularity in economics. the considerable flexibility they offer, as well as their capacity to reproduce a variety of empirically observed behaviours of complex systems, give them broad appeal, and the increasing availability of cheap computing power has made their use feasible. yet a widespread adoption in real-world modelling and decision-making scenarios has been hindered by the difficulty of performing parameter estimation for such models. in general, simulation models lack a tractable likelihood function, which precludes a straightforward application of standard statistical inference techniques. several recent works have sought to address this problem through the application of likelihood-free inference techniques, in which parameter estimates are determined by performing some form of comparison between the observed data and simulation output. however, these approaches are (a) founded on restrictive assumptions, and/or (b) typically require many hundreds of thousands of simulations. these qualities make them unsuitable for large-scale simulations in economics and can cast doubt on the validity of these inference methods in such scenarios. in this paper, we investigate the efficacy of two classes of black-box approximate bayesian inference methods that have recently drawn significant attention within the probabilistic machine learning community: neural posterior estimation and neural density ratio estimation. we present benchmarking experiments in which we demonstrate that neural network based black-box methods provide state of the art parameter inference for economic simulation models, and crucially are compatible with generic multivariate time-series data. in addition, we suggest appropriate assessment criteria for future benchmarking of approximate bayesian inference procedures for economic simulation models.",10.1016/j.jedc.2024.104827,2022-02-01,,"['joel dyer', 'patrick cannon', 'j. doyne farmer', 'sebastian schmon']"
2202.00769,distributional reinforcement learning by sinkhorn divergence,cs.lg stat.ml,"the empirical success of distributional reinforcement learning~(rl) highly depends on the distribution representation and the choice of distribution divergence. in this paper, we propose \textit{sinkhorn distributional rl~(sinkhorndrl)} that learns unrestricted statistics from return distributions and leverages sinkhorn divergence to minimize the difference between current and target bellman return distributions. theoretically, we prove the contraction properties of sinkhorndrl, consistent with the interpolation nature of sinkhorn divergence between wasserstein distance and maximum mean discrepancy~(mmd). we also establish the equivalence between sinkhorn divergence and a regularized mmd with a regularized moment matching behavior, contributing to explaining the superiority of sinkhorndrl. empirically, we show that sinkhorndrl is consistently better or comparable to existing algorithms on the atari games suite.",,2022-02-01,2024-02-02,"['ke sun', 'yingnan zhao', 'wulong liu', 'bei jiang', 'linglong kong']"
2202.01456,fast and explainable clustering based on sorting,cs.lg cs.ds stat.co stat.ml,"we introduce a fast and explainable clustering method called classix. it consists of two phases, namely a greedy aggregation phase of the sorted data into groups of nearby data points, followed by the merging of groups into clusters. the algorithm is controlled by two scalar parameters, namely a distance parameter for the aggregation and another parameter controlling the minimal cluster size. extensive experiments are conducted to give a comprehensive evaluation of the clustering performance on synthetic and real-world datasets, with various cluster shapes and low to high feature dimensionality. our experiments demonstrate that classix competes with state-of-the-art clustering algorithms. the algorithm has linear space complexity and achieves near linear time complexity on a wide range of problems. its inherent simplicity allows for the generation of intuitive explanations of the computed clusters.",,2022-02-03,2024-02-15,"['xinye chen', 'stefan güttel']"
2202.05560,controlling multiple errors simultaneously with a pac-bayes bound,stat.ml cs.lg math.st stat.th,"current pac-bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. however, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis classifications. we provide the first pac-bayes bound capable of providing such rich information by bounding the kullback-leibler divergence between the empirical and true probabilities of a set of m error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. we transform our bound into a differentiable training objective. our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing pac-bayes bounds can only bound a particular pre-decided weighting of the error types. in contrast our bound implicitly controls all uncountably many weightings simultaneously.",,2022-02-11,2024-02-22,"['reuben adams', 'john shawe-taylor', 'benjamin guedj']"
2202.05612,high-dimensional inference and fdr control for simulated markov random   fields,stat.ml cs.lg math.st stat.th,"identifying important features linked to a response variable is a fundamental task in various scientific domains. this article explores statistical inference for simulated markov random fields in high-dimensional settings. we introduce a methodology based on markov chain monte carlo maximum likelihood estimation (mcmc-mle) with elastic-net regularization. under mild conditions on the mcmc method, our penalized mcmc-mle method achieves $\ell_{1}$-consistency. we propose a decorrelated score test, establishing both its asymptotic normality and that of a one-step estimator, along with the associated confidence interval. furthermore, we construct two false discovery rate control procedures via the asymptotic behaviors for both p-values and e-values. comprehensive numerical simulations confirm the theoretical validity of the proposed methods.",,2022-02-11,2024-01-19,"['haoyu wei', 'xiaoyu lei', 'yixin han', 'huiming zhang']"
2202.05650,bernstein flows for flexible posteriors in variational bayes,stat.ml cs.lg,"variational inference (vi) is a technique to approximate difficult to compute posteriors by optimization. in contrast to mcmc, vi scales to many observations. in the case of complex posteriors, however, state-of-the-art vi approaches often yield unsatisfactory posterior approximations. this paper presents bernstein flow variational inference (bf-vi), a robust and easy-to-use method, flexible enough to approximate complex multivariate posteriors. bf-vi combines ideas from normalizing flows and bernstein polynomial-based transformation models. in benchmark experiments, we compare bf-vi solutions with exact posteriors, mcmc solutions, and state-of-the-art vi methods including normalizing flow based vi. we show for low-dimensional models that bf-vi accurately approximates the true posterior; in higher-dimensional models, bf-vi outperforms other vi methods. further, we develop with bf-vi a bayesian model for the semi-structured melanoma challenge data, combining a cnn model part for image data with an interpretable model part for tabular data, and demonstrate for the first time how the use of vi in semi-structured models.",,2022-02-11,2024-02-23,"['oliver dürr', 'stephan hörling', 'daniel dold', 'ivonne kovylov', 'beate sick']"
2202.05656,interprettime: a new approach for the systematic evaluation of   neural-network interpretability in time series classification,cs.lg cs.ai stat.ml,"we present a novel approach to evaluate the performance of interpretability methods for time series classification, and propose a new strategy to assess the similarity between domain experts and machine data interpretation. the novel approach leverages a new family of synthetic datasets and introduces new interpretability evaluation metrics. the approach addresses several common issues encountered in the literature, and clearly depicts how well an interpretability method is capturing neural network's data usage, providing a systematic interpretability evaluation framework. the new methodology highlights the superiority of shapley value sampling and integrated gradients for interpretability in time-series classification tasks.",,2022-02-11,,"['hugues turbé', 'mina bjelogrlic', 'christian lovis', 'gianmarco mengaldo']"
2202.07365,a statistical learning view of simple kriging,stat.ml cs.lg,"in the big data era, with the ubiquity of geolocation sensors in particular, massive datasets exhibiting a possibly complex spatial dependence structure are becoming increasingly available. in this context, the standard probabilistic theory of statistical learning does not apply directly and guarantees of the generalization capacity of predictive rules learned from such data are left to establish. we analyze here the simple kriging task from a statistical learning perspective, i.e. by carrying out a nonparametric finite-sample predictive analysis. given $d\geq 1$ values taken by a realization of a square integrable random field $x=\{x_s\}_{s\in s}$, $s\subset \mathbb{r}^2$, with unknown covariance structure, at sites $s_1,\; \ldots,\; s_d$ in $s$, the goal is to predict the unknown values it takes at any other location $s\in s$ with minimum quadratic risk. the prediction rule being derived from a training spatial dataset: a single realization $x'$ of $x$, independent from those to be predicted, observed at $n\geq 1$ locations $\sigma_1,\; \ldots,\; \sigma_n$ in $s$. despite the connection of this minimization problem with kernel ridge regression, establishing the generalization capacity of empirical risk minimizers is far from straightforward, due to the non independent and identically distributed nature of the training data $x'_{\sigma_1},\; \ldots,\; x'_{\sigma_n}$ involved in the learning procedure. in this article, non-asymptotic bounds of order $o_{\mathbb{p}}(1/\sqrt{n})$ are proved for the excess risk of a plug-in predictive rule mimicking the true minimizer in the case of isotropic stationary gaussian processes, observed at locations forming a regular grid in the learning stage. these theoretical results are illustrated by various numerical experiments, on simulated data and on real-world datasets.",,2022-02-15,2024-02-02,"['emilia siviero', 'emilie chautru', 'stephan clémençon']"
2202.09497,gradient estimation with discrete stein operators,stat.ml cs.lg,"gradient estimation -- approximating the gradient of an expectation with respect to the parameters of a distribution -- is central to the solution of many machine learning problems. however, when the distribution is discrete, most common gradient estimators suffer from excessive variance. to improve the quality of gradient estimation, we introduce a variance reduction technique based on stein operators for discrete distributions. we then use this technique to build flexible control variates for the reinforce leave-one-out estimator. our control variates can be adapted online to minimize variance and do not require extra evaluations of the target function. in benchmark generative modeling tasks such as training binary variational autoencoders, our gradient estimator achieves substantially lower variance than state-of-the-art estimators with the same number of function evaluations.",,2022-02-18,2024-02-24,"['jiaxin shi', 'yuhao zhou', 'jessica hwang', 'michalis k. titsias', 'lester mackey']"
2202.09674,generalized optimistic methods for convex-concave saddle point problems,math.oc cs.lg stat.ml,"the optimistic gradient method has seen increasing popularity for solving convex-concave saddle point problems. to analyze its iteration complexity, a recent work [arxiv:1906.01115] proposed an interesting perspective that interprets this method as an approximation to the proximal point method. in this paper, we follow this approach and distill the underlying idea of optimism to propose a generalized optimistic method, which includes the optimistic gradient method as a special case. our general framework can handle constrained saddle point problems with composite objective functions and can work with arbitrary norms using bregman distances. moreover, we develop a backtracking line search scheme to select the step sizes without knowledge of the smoothness coefficients. we instantiate our method with first-, second- and higher-order oracles and give best-known global iteration complexity bounds. for our first-order method, we show that the averaged iterates converge at a rate of $o(1/n)$ when the objective function is convex-concave, and it achieves linear convergence when the objective is strongly-convex-strongly-concave. for our second- and higher-order methods, under the additional assumption that the distance-generating function has lipschitz gradient, we prove a complexity bound of $o(1/\epsilon^\frac{2}{p+1})$ in the convex-concave setting and a complexity bound of $o((l_pd^\frac{p-1}{2}/\mu)^\frac{2}{p+1}+\log\log\frac{1}{\epsilon})$ in the strongly-convex-strongly-concave setting, where $l_p$ ($p\geq 2$) is the lipschitz constant of the $p$-th-order derivative, $\mu$ is the strong convexity parameter, and $d$ is the initial bregman distance to the saddle point. moreover, our line search scheme provably only requires a constant number of calls to a subproblem solver per iteration on average, making our first- and second-order methods particularly amenable to implementation.",,2022-02-19,2024-01-10,"['ruichen jiang', 'aryan mokhtari']"
2202.09724,bayes-optimal classifiers under group fairness,stat.ml cs.lg,"machine learning algorithms are becoming integrated into more and more high-stakes decision-making processes, such as in social welfare issues. due to the need of mitigating the potentially disparate impacts from algorithmic predictions, many approaches have been proposed in the emerging area of fair machine learning. however, the fundamental problem of characterizing bayes-optimal classifiers under various group fairness constraints has only been investigated in some special cases. based on the classical neyman-pearson argument (neyman and pearson, 1933; shao, 2003) for optimal hypothesis testing, this paper provides a unified framework for deriving bayes-optimal classifiers under group fairness. this enables us to propose a group-based thresholding method we call fairbayes, that can directly control disparity, and achieve an essentially optimal fairness-accuracy tradeoff. these advantages are supported by thorough experiments.",,2022-02-19,2024-02-06,"['xianli zeng', 'edgar dobriban', 'guang cheng']"
2202.12797,learning dynamic mechanisms in unknown environments: a reinforcement   learning approach,cs.lg cs.gt math.oc stat.ml,"dynamic mechanism design studies how mechanism designers should allocate resources among agents in a time-varying environment. we consider the problem where the agents interact with the mechanism designer according to an unknown markov decision process (mdp), where agent rewards and the mechanism designer's state evolve according to an episodic mdp with unknown reward functions and transition kernels. we focus on the online setting with linear function approximation and propose novel learning algorithms to recover the dynamic vickrey-clarke-grove (vcg) mechanism over multiple rounds of interaction. a key contribution of our approach is incorporating reward-free online reinforcement learning (rl) to aid exploration over a rich policy space to estimate prices in the dynamic vcg mechanism. we show that the regret of our proposed method is upper bounded by $\tilde{\mathcal{o}}(t^{2/3})$ and further devise a lower bound to show that our algorithm is efficient, incurring the same $\tilde{\mathcal{o}}(t^{2 / 3})$ regret as the lower bound, where $t$ is the total number of rounds. our work establishes the regret guarantee for online rl in solving dynamic mechanism design problems without prior knowledge of the underlying model.",,2022-02-25,2024-02-25,"['shuang qiu', 'boxiang lyu', 'qinglin meng', 'zhaoran wang', 'zhuoran yang', 'michael i. jordan']"
2202.12887,fault-tolerant neural networks from biological error correction codes,cs.lg cs.ne q-bio.nc stat.ml,"it has been an open question in deep learning if fault-tolerant computation is possible: can arbitrarily reliable computation be achieved using only unreliable neurons? in the grid cells of the mammalian cortex, analog error correction codes have been observed to protect states against neural spiking noise, but their role in information processing is unclear. here, we use these biological error correction codes to develop a universal fault-tolerant neural network that achieves reliable computation if the faultiness of each neuron lies below a sharp threshold; remarkably, we find that noisy biological neurons fall below this threshold. the discovery of a phase transition from faulty to fault-tolerant neural computation suggests a mechanism for reliable computation in the cortex and opens a path towards understanding noisy analog systems relevant to artificial intelligence and neuromorphic computing.",,2022-02-25,2024-02-09,"['alexander zlokapa', 'andrew k. tan', 'john m. martyn', 'ila r. fiete', 'max tegmark', 'isaac l. chuang']"
2202.13059,theoretical error analysis of entropy approximation for gaussian mixture,stat.ml cs.lg,"gaussian mixture distributions are commonly employed to represent general probability distributions. despite the importance of using gaussian mixtures for uncertainty estimation, the entropy of a gaussian mixture cannot be analytically calculated. notably, gal and ghahramani [2016] proposed the approximate entropy that is the sum of the entropies of unimodal gaussian distributions. this approximation is easy to analytically calculate regardless of dimension, but there lack theoretical guarantees. in this paper, we theoretically analyze the approximation error between the true entropy and the approximate one to reveal when this approximation works effectively. this error is controlled by how far apart each gaussian component of the gaussian mixture. to measure such separation, we introduce the ratios of the distances between the means to the sum of the variances of each gaussian component of the gaussian mixture, and we reveal that the error converges to zero as the ratios tend to infinity. this convergence situation is more likely to occur in higher dimensional spaces. therefore, our results provide a guarantee that this approximation works well in higher dimension problems, particularly in scenarios such as neural networks that involve a large number of weights.",,2022-02-25,2024-02-06,"['takashi furuya', 'hiroyuki kusumoto', 'koichi taniguchi', 'naoya kanno', 'kazuma suetake']"
2202.13426,bayesian active learning for discrete latent variable models,cs.lg q-bio.nc stat.ml,"active learning seeks to reduce the amount of data required to fit the parameters of a model, thus forming an important class of techniques in modern machine learning. however, past work on active learning has largely overlooked latent variable models, which play a vital role in neuroscience, psychology, and a variety of other engineering and scientific disciplines. here we address this gap by proposing a novel framework for maximum-mutual-information input selection for discrete latent variable regression models. we first apply our method to a class of models known as ""mixtures of linear regressions"" (mlr). while it is well known that active learning confers no advantage for linear-gaussian regression models, we use fisher information to show analytically that active learning can nevertheless achieve large gains for mixtures of such models, and we validate this improvement using both simulations and real-world data. we then consider a powerful class of temporally structured latent variable models given by a hidden markov model (hmm) with generalized linear model (glm) observations, which has recently been used to identify discrete states from animal decision-making data. we show that our method substantially reduces the amount of data needed to fit glm-hmm, and outperforms a variety of approximate methods based on variational and amortized inference. infomax learning for latent variable models thus offers a powerful for characterizing temporally structured latent states, with a wide variety of applications in neuroscience and beyond.",10.1162/neco_a_01646,2022-02-27,2023-06-02,"['aditi jha', 'zoe c. ashwood', 'jonathan w. pillow']"
2203.00144,the concordance index decomposition: a measure for a deeper   understanding of survival prediction models,cs.lg stat.me stat.ml,"the concordance index (c-index) is a commonly used metric in survival analysis for evaluating the performance of a prediction model. in this paper, we propose a decomposition of the c-index into a weighted harmonic mean of two quantities: one for ranking observed events versus other observed events, and the other for ranking observed events versus censored cases. this decomposition enables a finer-grained analysis of the relative strengths and weaknesses between different survival prediction methods. the usefulness of this decomposition is demonstrated through benchmark comparisons against classical models and state-of-the-art methods, together with the new variational generative neural-network-based method (surved) proposed in this paper. the performance of the models is assessed using four publicly available datasets with varying levels of censoring. using the c-index decomposition and synthetic censoring, the analysis shows that deep learning models utilize the observed events more effectively than other models. this allows them to keep a stable c-index in different censoring levels. in contrast to such deep learning methods, classical machine learning models deteriorate when the censoring level decreases due to their inability to improve on ranking the events versus other events.",10.1016/j.artmed.2024.102781,2022-02-28,2024-01-20,"['abdallah alabdallah', 'mattias ohlsson', 'sepideh pashami', 'thorsteinn rögnvaldsson']"
2203.00444,parameter-free mirror descent,cs.lg math.oc stat.ml,"we develop a modified online mirror descent framework that is suitable for building adaptive and parameter-free algorithms in unbounded domains. we leverage this technique to develop the first unconstrained online linear optimization algorithm achieving an optimal dynamic regret bound, and we further demonstrate that natural strategies based on follow-the-regularized-leader are unable to achieve similar results. we also apply our mirror descent framework to build new parameter-free implicit updates, as well as a simplified and improved unconstrained scale-free algorithm.",,2022-02-26,2024-02-08,"['andrew jacobsen', 'ashok cutkosky']"
2203.01360,neural galerkin schemes with active learning for high-dimensional   evolution equations,math.na cs.lg cs.na stat.ml,"deep neural networks have been shown to provide accurate function approximations in high dimensions. however, fitting network parameters requires informative training data that are often challenging to collect in science and engineering applications. this work proposes neural galerkin schemes based on deep learning that generate training data with active learning for numerically solving high-dimensional partial differential equations. neural galerkin schemes build on the dirac-frenkel variational principle to train networks by minimizing the residual sequentially over time, which enables adaptively collecting new training data in a self-informed manner that is guided by the dynamics described by the partial differential equations. this is in contrast to other machine learning methods that aim to fit network parameters globally in time without taking into account training data acquisition. our finding is that the active form of gathering training data of the proposed neural galerkin schemes is key for numerically realizing the expressive power of networks in high dimensions. numerical experiments demonstrate that neural galerkin schemes have the potential to enable simulating phenomena and processes with many variables for which traditional and other deep-learning-based solvers fail, especially when features of the solutions evolve locally such as in high-dimensional wave propagation problems and interacting particle systems described by fokker-planck and kinetic equations.",10.1016/j.jcp.2023.112588,2022-03-02,2024-02-29,"['joan bruna', 'benjamin peherstorfer', 'eric vanden-eijnden']"
2203.07346,sparse random hypergraphs: non-backtracking spectra and community   detection,math.pr math.co stat.ml,"we consider the community detection problem in a sparse $q$-uniform hypergraph $g$, assuming that $g$ is generated according to the hypergraph stochastic block model (hsbm). we prove that a spectral method based on the non-backtracking operator for hypergraphs works with high probability down to the generalized kesten-stigum detection threshold conjectured by angelini et al. (2015). we characterize the spectrum of the non-backtracking operator for the sparse hsbm and provide an efficient dimension reduction procedure using the ihara-bass formula for hypergraphs. as a result, community detection for the sparse hsbm on $n$ vertices can be reduced to an eigenvector problem of a $2n\times 2n$ non-normal matrix constructed from the adjacency matrix and the degree matrix of the hypergraph. to the best of our knowledge, this is the first provable and efficient spectral algorithm that achieves the conjectured threshold for hsbms with $r$ blocks generated according to a general symmetric probability tensor.",,2022-03-14,2024-01-26,"['ludovic stephan', 'yizhe zhu']"
2203.09438,an explainable stacked ensemble model for static route-free estimation   of time of arrival,cs.lg stat.ml,"to compare alternative taxi schedules and to compute them, as well as to provide insights into an upcoming taxi trip to drivers and passengers, the duration of a trip or its estimated time of arrival (eta) is predicted. to reach a high prediction precision, machine learning models for eta are state of the art. one yet unexploited option to further increase prediction precision is to combine multiple eta models into an ensemble. while an increase of prediction precision is likely, the main drawback is that the predictions made by such an ensemble become less transparent due to the sophisticated ensemble architecture. one option to remedy this drawback is to apply explainable artificial intelligence (xai). the contribution of this paper is three-fold. first, we combine multiple machine learning models from our previous work for eta into a two-level ensemble model - a stacked ensemble model - which on its own is novel; therefore, we can outperform previous state-of-the-art static route-free eta approaches. second, we apply existing xai methods to explain the first- and second-level models of the ensemble. third, we propose three joining methods for combining the first-level explanations with the second-level ones. those joining methods enable us to explain stacked ensembles for regression tasks. an experimental evaluation shows that the eta models correctly learned the importance of those input features driving the prediction.",,2022-03-17,2024-01-11,"['sören schleibaum', 'jörg p. müller', 'monika sester']"
2203.10118,bayesian structural learning with parametric marginals for count data:   an application to microbiota systems,stat.me,"high dimensional and heterogeneous count data are collected in various applied fields. in this paper, we look closely at high-resolution sequencing data on the microbiome, which have enabled researchers to study the genomes of entire microbial communities. revealing the underlying interactions between these communities is of vital importance to learn how microbes influence human health. to perform structural learning from multivariate count data such as these, we develop a novel gaussian copula graphical model with two key elements. firstly, we employ parametric regression to characterize the marginal distributions. this step is crucial for accommodating the impact of external covariates. neglecting this adjustment could potentially introduce distortions in the inference of the underlying network of dependences. secondly, we advance a bayesian structure learning framework, based on a computationally efficient search algorithm that is suited to high dimensionality. the approach returns simultaneous inference of the marginal effects and of the dependence structure, including graph uncertainty estimates. a simulation study and a real data analysis of microbiome data highlight the applicability of the proposed approach at inferring networks from multivariate count data in general, and its relevance to microbiome analyses in particular. the proposed method is implemented in the r package bdgraph.",,2022-03-18,2024-01-11,"['veronica vinciotti', 'pariya behrouzi', 'reza mohammadi']"
2203.10215,convergence error analysis of reflected gradient langevin dynamics for   globally optimizing non-convex constrained problems,math.oc math.pr stat.ml,"gradient langevin dynamics and a variety of its variants have attracted increasing attention owing to their convergence towards the global optimal solution, initially in the unconstrained convex framework while recently even in convex constrained non-convex problems. in the present work, we extend those frameworks to non-convex problems on a non-convex feasible region with a global optimization algorithm built upon reflected gradient langevin dynamics and derive its convergence rates. by effectively making use of its reflection at the boundary in combination with the probabilistic representation for the poisson equation with the neumann boundary condition, we present promising convergence rates, particularly faster than the existing one for convex constrained non-convex problems.",,2022-03-18,2024-01-26,"['kanji sato', 'akiko takeda', 'reiichiro kawai', 'taiji suzuki']"
2203.12808,robustness against weak or invalid instruments: exploring nonlinear   treatment models with machine learning,stat.me math.st stat.ml stat.th,"we discuss causal inference for observational studies with possibly invalid instrumental variables. we propose a novel methodology called two-stage curvature identification (tsci) by exploring the nonlinear treatment model with machine learning. {the first-stage machine learning enables improving the instrumental variable's strength and adjusting for different forms of violating the instrumental variable assumptions.} the success of tsci requires the instrumental variable's effect on treatment to differ from its violation form. a novel bias correction step is implemented to remove bias resulting from the potentially high complexity of machine learning. our proposed \texttt{tsci} estimator is shown to be asymptotically unbiased and gaussian even if the machine learning algorithm does not consistently estimate the treatment model. furthermore, we design a data-dependent method to choose the best among several candidate violation forms. we apply tsci to study the effect of education on earnings.",,2022-03-23,2024-01-04,"['zijian guo', 'mengchu zheng', 'peter bühlmann']"
2203.13423,modeling attrition in recommender systems with departing bandits,cs.lg cs.ir stat.ml,"traditionally, when recommender systems are formalized as multi-armed bandits, the policy of the recommender system influences the rewards accrued, but not the length of interaction. however, in real-world systems, dissatisfied users may depart (and never come back). in this work, we propose a novel multi-armed bandit setup that captures such policy-dependent horizons. our setup consists of a finite set of user types, and multiple arms with bernoulli payoffs. each (user type, arm) tuple corresponds to an (unknown) reward probability. each user's type is initially unknown and can only be inferred through their response to recommendations. moreover, if a user is dissatisfied with their recommendation, they might depart the system. we first address the case where all users share the same type, demonstrating that a recent ucb-based algorithm is optimal. we then move forward to the more challenging case, where users are divided among two types. while naive approaches cannot handle this setting, we provide an efficient learning algorithm that achieves $\tilde{o}(\sqrt{t})$ regret, where $t$ is the number of users.",,2022-03-24,2024-02-15,"['omer ben-porat', 'lee cohen', 'liu leqi', 'zachary c. lipton', 'yishay mansour']"
2203.13559,nonparametric conditional local independence testing,math.st stat.ml stat.th,"conditional local independence is an asymmetric independence relation among continuous time stochastic processes. it describes whether the evolution of one process is directly influenced by another process given the histories of additional processes, and it is important for the description and learning of causal relations among processes. we develop a model-free framework for testing the hypothesis that a counting process is conditionally locally independent of another process. to this end, we introduce a new functional parameter called the local covariance measure (lcm), which quantifies deviations from the hypothesis. following the principles of double machine learning, we propose an estimator of the lcm and a test of the hypothesis using nonparametric estimators and sample splitting or cross-fitting. we call this test the (cross-fitted) local covariance test ((x)-lct), and we show that its level and power can be controlled uniformly, provided that the nonparametric estimators are consistent with modest rates. we illustrate the theory by an example based on a marginalized cox model with time-dependent covariates, and we show in simulations that when double machine learning is used in combination with cross-fitting, then the test works well without restrictive parametric assumptions.",10.1214/23-aos2323,2022-03-25,2023-02-24,"['alexander mangulad christgau', 'lasse petersen', 'niels richard hansen']"
2204.00036,a statistical decision-theoretical perspective on the two-stage approach   to parameter estimation,stat.me cs.sy eess.sy stat.ml,"one of the most important problems in system identification and statistics is how to estimate the unknown parameters of a given model. optimization methods and specialized procedures, such as empirical minimization (em) can be used in case the likelihood function can be computed. for situations where one can only simulate from a parametric model, but the likelihood is difficult or impossible to evaluate, a technique known as the two-stage (ts) approach can be applied to obtain reliable parametric estimates. unfortunately, there is currently a lack of theoretical justification for ts. in this paper, we propose a statistical decision-theoretical derivation of ts, which leads to bayesian and minimax estimators. we also show how to apply the ts approach on models for independent and identically distributed samples, by computing quantiles of the data as a first step, and using a linear function as the second stage. the proposed method is illustrated via numerical simulations.",10.1109/cdc51059.2022.9993024,2022-03-31,2022-04-15,"['braghadeesh lakshminarayanan', 'cristian r. rojas']"
2204.02909,a friendly tutorial on mean-field spin glass techniques for   non-physicists,math.st cond-mat.dis-nn math.pr stat.th,this tutorial is based on lecture notes written for a class taught in the statistics department at stanford in the winter quarter of 2017. the objective was to provide a working knowledge of some of the techniques developed over the last 40 years by theoretical physicists and mathematicians to study mean field spin glasses and their applications to high-dimenensional statistics and statistical learning.,,2022-04-06,2024-01-25,"['andrea montanari', 'subhabrata sen']"
2204.03562,sliced gradient-enhanced kriging for high-dimensional function   approximation,stat.ml math.st stat.th,"gradient-enhanced kriging (ge-kriging) is a well-established surrogate modelling technique for approximating expensive computational models. however, it tends to get impractical for high-dimensional problems due to the size of the inherent correlation matrix and the associated high-dimensional hyper-parameter tuning problem. to address these issues, a new method, called sliced ge-kriging (sge-kriging), is developed in this paper for reducing both the size of the correlation matrix and the number of hyper-parameters. we first split the training sample set into multiple slices, and invoke bayes' theorem to approximate the full likelihood function via a sliced likelihood function, in which multiple small correlation matrices are utilized to describe the correlation of the sample set rather than one large one. then, we replace the original high-dimensional hyper-parameter tuning problem with a low-dimensional counterpart by learning the relationship between the hyper-parameters and the derivative-based global sensitivity indices. the performance of sge-kriging is finally validated by means of numerical experiments with several benchmarks and a high-dimensional aerodynamic modeling problem. the results show that the sge-kriging model features an accuracy and robustness that is comparable to the standard one but comes at much less training costs. the benefits are most evident for high-dimensional problems with tens of variables.",10.1137/22m154315x,2022-04-05,2024-01-04,"['kai cheng', 'ralf zimmermann']"
2204.04476,high-dimensional asymptotics of langevin dynamics in spiked matrix   models,math.st cs.lg math.pr stat.ml stat.th,"we study langevin dynamics for recovering the planted signal in the spiked matrix model. we provide a ""path-wise"" characterization of the overlap between the output of the langevin algorithm and the planted signal. this overlap is characterized in terms of a self-consistent system of integro-differential equations, usually referred to as the crisanti-horner-sommers-cugliandolo-kurchan (chsck) equations in the spin glass literature. as a second contribution, we derive an explicit formula for the limiting overlap in terms of the signal-to-noise ratio and the injected noise in the diffusion. this uncovers a sharp phase transition -- in one regime, the limiting overlap is strictly positive, while in the other, the injected noise overcomes the signal, and the limiting overlap is zero.",10.1093/imaiai/iaad042,2022-04-09,,"['tengyuan liang', 'subhabrata sen', 'pragya sur']"
2204.05003,local convergence rates of the nonparametric least squares estimator   with applications to transfer learning,math.st stat.th,"convergence properties of empirical risk minimizers can be conveniently expressed in terms of the associated population risk. to derive bounds for the performance of the estimator under covariate shift, however, pointwise convergence rates are required. under weak assumptions on the design distribution, it is shown that least squares estimators (lse) over 1-lipschitz functions are also minimax rate optimal with respect to a weighted uniform norm, where the weighting accounts in a natural way for the non-uniformity of the design distribution. this implies that although least squares is a global criterion, the lse adapts locally to the size of the design density. we develop a new indirect proof technique that establishes the local convergence behavior based on a carefully chosen local perturbation of the lse. the obtained local rates are then applied to analyze the lse for transfer learning under covariate shift.",,2022-04-11,2023-12-29,"['johannes schmidt-hieber', 'petr zamolodtchikov']"
2204.06990,observable adjustments in single-index models for regularized   m-estimators,math.st stat.ml stat.th,"we consider observations $(x,y)$ from single index models with unknown link function, gaussian covariates and a regularized m-estimator $\hat\beta$ constructed from convex loss function and regularizer. in the regime where sample size $n$ and dimension $p$ are both increasing such that $p/n$ has a finite limit, the behavior of the empirical distribution of $\hat\beta$ and the predicted values $x\hat\beta$ has been previously characterized in a number of models: the empirical distributions are known to converge to proximal operators of the loss and penalty in a related gaussian sequence model, which captures the interplay between ratio $p/n$, loss, regularization and the data generating process. this connection between$(\hat\beta,x\hat\beta)$ and the corresponding proximal operators require solving fixed-point equations that typically involve unobservable quantities such as the prior distribution on the index or the link function.   this paper develops a different theory to describe the empirical distribution of $\hat\beta$ and $x\hat\beta$: approximations of $(\hat\beta,x\hat\beta)$ in terms of proximal operators are provided that only involve observable adjustments. these proposed observable adjustments are data-driven, e.g., do not require prior knowledge of the index or the link function. these new adjustments yield confidence intervals for individual components of the index, as well as estimators of the correlation of $\hat\beta$ with the index. the interplay between loss, regularization and the model is thus captured in a data-driven manner, without solving the fixed-point equations studied in previous works. the results apply to both strongly convex regularizers and unregularized m-estimation. simulations are provided for the square and logistic loss in single index models including logistic regression and 1-bit compressed sensing with 20\% corrupted bits.",,2022-04-14,2024-01-03,['pierre c bellec']
2204.07526,statistical-computational trade-offs in tensor pca and related problems   via communication complexity,math.st cs.it cs.lg math.it stat.ml stat.th,"tensor pca is a stylized statistical inference problem introduced by montanari and richard to study the computational difficulty of estimating an unknown parameter from higher-order moment tensors. unlike its matrix counterpart, tensor pca exhibits a statistical-computational gap, i.e., a sample size regime where the problem is information-theoretically solvable but conjectured to be computationally hard. this paper derives computational lower bounds on the run-time of memory bounded algorithms for tensor pca using communication complexity. these lower bounds specify a trade-off among the number of passes through the data sample, the sample size, and the memory required by any algorithm that successfully solves tensor pca. while the lower bounds do not rule out polynomial-time algorithms, they do imply that many commonly-used algorithms, such as gradient descent and power method, must have a higher iteration count when the sample size is not large enough. similar lower bounds are obtained for non-gaussian component analysis, a family of statistical estimation problems in which low-order moment tensors carry no information about the unknown parameter. finally, stronger lower bounds are obtained for an asymmetric variant of tensor pca and related statistical estimation problems. these results explain why many estimators for these problems use a memory state that is significantly larger than the effective dimensionality of the parameter of interest.",,2022-04-15,2024-01-20,"['rishabh dudeja', 'daniel hsu']"
2204.10031,beyond the density operator and tr(\rho a): exploiting the higher-order   statistics of random-coefficient pure states for quantum information   processing,quant-ph physics.data-an stat.ml,"two types of states are widely used in quantum mechanics, namely (deterministic-coefficient) pure states and statistical mixtures. a density operator can be associated with each of them. we here address a third type of states, that we previously introduced in a more restricted framework. these states generalize pure ones by replacing each of their deterministic ket coefficients by a random variable. we therefore call them random-coefficient pure states, or rcps. we analyze their properties and their relationships with both types of usual states. we show that rcps contain much richer information than the density operator and mean of observables that we associate with them. this occurs because the latter operator only exploits the second-order statistics of the random state coefficients, whereas their higher-order statistics contain additional information. that information can be accessed in practice with the multiple-preparation procedure that we propose for rcps, by using second-order and higher-order statistics of associated random probabilities of measurement outcomes. exploiting these higher-order statistics opens the way to a very general approach for performing advanced quantum information processing tasks. we illustrate the relevance of this approach with a generic example, dealing with the estimation of parameters of a quantum process and thus related to quantum process tomography. this parameter estimation is performed in the non-blind (i.e. supervised) or blind (i.e. unsupervised) mode. we show that this problem cannot be solved by using only the density operator \rho of an rcps and the associated mean value tr(\rho a) of the operator a that corresponds to the considered physical quantity. we succeed in solving this problem by exploiting a fourth-order statistical parameter of state coefficients, in addition to second-order statistics. numerical tests validate this result.",10.1007/s11128-023-03970-x,2022-04-21,,"['yannick deville', 'alain deville']"
2204.10969,combining doubly robust methods and machine learning for estimating   average treatment effects for observational real-world data,stat.me stat.ap stat.ml,"observational cohort studies are increasingly being used for comparative effectiveness research to assess the safety of therapeutics. recently, various doubly robust methods have been proposed for average treatment effect estimation by combining the treatment model and the outcome model via different vehicles, such as matching, weighting, and regression. the key advantage of doubly robust estimators is that they require either the treatment model or the outcome model to be correctly specified to obtain a consistent estimator of average treatment effects, and therefore lead to a more accurate and often more precise inference. however, little work has been done to understand how doubly robust estimators differ due to their unique strategies of using the treatment and outcome models and how machine learning techniques can be combined to boost their performance. here we examine multiple popular doubly robust methods and compare their performance using different treatment and outcome modeling via extensive simulations and a real-world application. we found that incorporating machine learning with doubly robust estimators such as the targeted maximum likelihood estimator gives the best overall performance. practical guidance on how to apply doubly robust estimators is provided.",,2022-04-22,2024-01-09,"['xiaoqing tan', 'shu yang', 'wenyu ye', 'douglas e. faries', 'ilya lipkovich', 'zbigniew kadziola']"
2205.00362,a simple and general duality proof for wasserstein distributionally   robust optimization,math.oc cs.lg stat.ml,"we present an elementary yet general proof of duality for wasserstein distributionally robust optimization. the duality holds for any arbitrary kantorovich transport cost, measurable loss function, and nominal probability distribution, provided that an interchangeability principle holds, which is equivalent to certain measurability conditions. to illustrate the broader applicability of our approach, we provide a rigorous treatment of duality results in distributionally robust markov decision processes and distributionally robust multistage stochastic programming. furthermore, we extend the result to other problems including infinity-wasserstein distributionally robust optimization, risk-averse optimization, and globalized distributionally robust counterpart.",,2022-04-30,2023-12-31,"['luhao zhang', 'jincheng yang', 'rui gao']"
2205.00605,cluster-based regression using variational inference and applications in   financial forecasting,q-fin.st cs.lg stat.me stat.ml,"this paper describes an approach to simultaneously identify clusters and estimate cluster-specific regression parameters from the given data. such an approach can be useful in learning the relationship between input and output when the regression parameters for estimating output are different in different regions of the input space. variational inference (vi), a machine learning approach to obtain posterior probability densities using optimization techniques, is used to identify clusters of explanatory variables and regression parameters for each cluster. from these results, one can obtain both the expected value and the full distribution of predicted output. other advantages of the proposed approach include the elegant theoretical solution and clear interpretability of results. the proposed approach is well-suited for financial forecasting where markets have different regimes (or clusters) with different patterns and correlations of market changes in each regime. in financial applications, knowledge about such clusters can provide useful insights about portfolio performance and identify the relative importance of variables in different market regimes. an illustrative example of predicting one-day s&p change is considered to illustrate the approach and compare the performance of the proposed approach with standard regression without clusters. due to the broad applicability of the problem, its elegant theoretical solution, and the computational efficiency of the proposed algorithm, the approach may be useful in a number of areas extending beyond the financial domain.",,2022-05-01,2023-12-30,"['udai nagpal', 'krishan nagpal']"
2205.04151,learning effective dynamics from data-driven stochastic systems,stat.ml cs.lg,"multiscale stochastic dynamical systems have been widely adopted to a variety of scientific and engineering problems due to their capability of depicting complex phenomena in many real world applications. this work is devoted to investigating the effective dynamics for slow-fast stochastic dynamical systems. given observation data on a short-term period satisfying some unknown slow-fast stochastic systems, we propose a novel algorithm including a neural network called auto-sde to learn invariant slow manifold. our approach captures the evolutionary nature of a series of time-dependent autoencoder neural networks with the loss constructed from a discretized stochastic differential equation. our algorithm is also validated to be accurate, stable and effective through numerical experiments under various evaluation metrics.",10.1063/5.0126667,2022-05-09,2023-12-29,"['lingyu feng', 'ting gao', 'min dai', 'jinqiao duan']"
2205.05359,exploring local explanations of nonlinear models using animated linear   projections,stat.ml cs.ai cs.lg,"the increased predictive power of machine learning models comes at the cost of increased complexity and loss of interpretability, particularly in comparison to parametric statistical models. this trade-off has led to the emergence of explainable ai (xai) which provides methods, such as local explanations (les) and local variable attributions (lvas), to shed light on how a model use predictors to arrive at a prediction. these provide a point estimate of the linear variable importance in the vicinity of a single observation. however, lvas tend not to effectively handle association between predictors. to understand how the interaction between predictors affects the variable importance estimate, we can convert lvas into linear projections and use the radial tour. this is also useful for learning how a model has made a mistake, or the effect of outliers, or the clustering of observations. the approach is illustrated with examples from categorical (penguin species, chocolate types) and quantitative (soccer/football salaries, house prices) response models. the methods are implemented in the r package cheem, available on cran.",,2022-05-11,2024-01-18,"['nicholas spyrison', 'dianne cook', 'przemyslaw biecek']"
2205.07384,incorporating prior knowledge into neural networks through an implicit   composite kernel,cs.lg cs.ai stat.ml,"it is challenging to guide neural network (nn) learning with prior knowledge. in contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a gaussian process (gp). many deep learning applications could be enhanced by modeling such known properties. for example, convolutional neural networks (cnns) are frequently used in remote sensing, which is subject to strong seasonal effects. we propose to blend the strengths of deep learning and the clear modeling capabilities of gps by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). we implement this idea by combining a deep network and an efficient mapping based on the nystrom approximation, which we call implicit composite kernel (ick). we then adopt a sample-then-optimize approach to approximate the full gp posterior distribution. we demonstrate that ick has superior performance and flexibility on both synthetic and real-world data sets. we believe that ick framework can be used to include prior information into neural networks in many applications.",,2022-05-15,2024-02-28,"['ziyang jiang', 'tongshu zheng', 'yiling liu', 'david carlson']"
2205.10200,the fairness of credit scoring models,stat.ml cs.lg q-fin.rm,"in credit markets, screening algorithms aim to discriminate between good-type and bad-type borrowers. however, when doing so, they can also discriminate between individuals sharing a protected attribute (e.g. gender, age, racial origin) and the rest of the population. this can be unintentional and originate from the training dataset or from the model itself. we show how to formally test the algorithmic fairness of scoring models and how to identify the variables responsible for any lack of fairness. we then use these variables to optimize the fairness-performance trade-off. our framework provides guidance on how algorithmic fairness can be monitored by lenders, controlled by their regulators, improved for the benefit of protected groups, while still maintaining a high level of forecasting accuracy.",,2022-05-20,2024-02-08,"['christophe hurlin', 'christophe pérignon', 'sébastien saurin']"
2205.11359,towards size-independent generalization bounds for deep operator nets,cs.lg cs.na math.na stat.ml,"in recent times machine learning methods have made significant advances in becoming a useful tool for analyzing physical systems. a particularly active area in this theme has been ""physics-informed machine learning"" which focuses on using neural nets for numerically solving differential equations. in this work, we aim to advance the theory of measuring out-of-sample error while training deeponets -- which is among the most versatile ways to solve pde systems in one-shot.   firstly, for a class of deeponets, we prove a bound on their rademacher complexity which does not explicitly scale with the width of the nets involved. secondly, we use this to show how the huber loss can be chosen so that for these deeponet classes generalization error bounds can be obtained that have no explicit dependence on the size of the nets. we note that our theoretical results apply to any pde being targeted to be solved by deeponets.",,2022-05-23,2024-01-22,"['pulkit gopalani', 'sayar karmakar', 'dibyakanti kumar', 'anirbit mukherjee']"
2205.11677,semi-supervised clustering of sparse graphs: crossing the   information-theoretic threshold,stat.ml cs.lg math.oc math.pr,"the stochastic block model is a canonical random graph model for clustering and community detection on network-structured data. decades of extensive study on the problem have established many profound results, among which the phase transition at the kesten-stigum threshold is particularly interesting both from a mathematical and an applied standpoint. it states that no estimator based on the network topology can perform substantially better than chance on sparse graphs if the model parameter is below a certain threshold. nevertheless, if we slightly extend the horizon to the ubiquitous semi-supervised setting, such a fundamental limitation will disappear completely. we prove that with an arbitrary fraction of the labels revealed, the detection problem is feasible throughout the parameter domain. moreover, we introduce two efficient algorithms, one combinatorial and one based on optimization, to integrate label information with graph structures. our work brings a new perspective to the stochastic model of networks and semidefinite program research.",,2022-05-23,2024-02-27,"['junda sheng', 'thomas strohmer']"
2205.12112,stereographic markov chain monte carlo,stat.co stat.me stat.ml,"high-dimensional distributions, especially those with heavy tails, are notoriously difficult for off-the-shelf mcmc samplers: the combination of unbounded state spaces, diminishing gradient information, and local moves results in empirically observed ``stickiness'' and poor theoretical mixing properties -- lack of geometric ergodicity. in this paper, we introduce a new class of mcmc samplers that map the original high-dimensional problem in euclidean space onto a sphere and remedy these notorious mixing problems. in particular, we develop random-walk metropolis type algorithms as well as versions of the bouncy particle sampler that are uniformly ergodic for a large class of light and heavy-tailed distributions and also empirically exhibit rapid convergence in high dimensions. in the best scenario, the proposed samplers can enjoy the ``blessings of dimensionality'' that the convergence is faster in higher dimensions.",,2022-05-24,2024-02-20,"['jun yang', 'krzysztof łatuszyński', 'gareth o. roberts']"
2205.14461,collaborative likelihood-ratio estimation over graphs,stat.ml cs.lg,"assuming we have iid observations from two unknown probability density functions (pdfs), $p$ and $q$, the likelihood-ratio estimation (lre) is an elegant approach to compare the two pdfs only by relying on the available data. in this paper, we introduce the first -to the best of our knowledge-graph-based extension of this problem, which reads as follows: suppose each node $v$ of a fixed graph has access to observations coming from two unknown node-specific pdfs, $p_v$ and $q_v$, and the goal is to estimate for each node the likelihood-ratio between both pdfs by also taking into account the information provided by the graph structure. the node-level estimation tasks are supposed to exhibit similarities conveyed by the graph, which suggests that the nodes could collaborate to solve them more efficiently. we develop this idea in a concrete non-parametric method that we call graph-based relative unconstrained least-squares importance fitting (grulsif). we derive convergence rates for our collaborative approach that highlights the role played by variables such as the number of available observations per node, the size of the graph, and how accurately the graph structure encodes the similarity between tasks. these theoretical results explicit the situations where collaborative estimation effectively leads to an improvement in performance compared to solving each problem independently. finally, in a series of experiments, we illustrate how grulsif infers the likelihood-ratios at the nodes of the graph more accurately compared to state-of-the art lre methods, which would operate independently at each node, and we also verify that the behavior of grulsif is aligned with our previous theoretical analysis.",,2022-05-28,2024-01-31,"['alejandro de la concha', 'nicolas vayatis', 'argyris kalogeratos']"
2205.14839,adversarial bandits against arbitrary strategies,cs.lg stat.ml,"we study the adversarial bandit problem against arbitrary strategies, in which $s$ is the parameter for the hardness of the problem and this parameter is not given to the agent. to handle this problem, we adopt the master-base framework using the online mirror descent method (omd). we first provide a master-base algorithm with simple omd, achieving $\tilde{o}(s^{1/2}k^{1/3}t^{2/3})$, in which $t^{2/3}$ comes from the variance of loss estimators. to mitigate the impact of the variance, we propose using adaptive learning rates for omd and achieve $\tilde{o}(\min\{\mathbb{e}[\sqrt{skt\rho_t(h^\dagger)}],s\sqrt{kt}\})$, where $\rho_t(h^\dagger)$ is a variance term for loss estimators.",,2022-05-29,2024-02-07,"['jung-hun kim', 'se-young yun']"
2205.15059,hilbert curve projection distance for distribution comparison,cs.lg stat.ml,"distribution comparison plays a central role in many machine learning tasks like data classification and generative modeling. in this study, we propose a novel metric, called hilbert curve projection (hcp) distance, to measure the distance between two probability distributions with low complexity. in particular, we first project two high-dimensional probability distributions using hilbert curve to obtain a coupling between them, and then calculate the transport distance between these two distributions in the original space, according to the coupling. we show that hcp distance is a proper metric and is well-defined for probability measures with bounded supports. furthermore, we demonstrate that the modified empirical hcp distance with the $l_p$ cost in the $d$-dimensional space converges to its population counterpart at a rate of no more than $o(n^{-1/2\max\{d,p\}})$. to suppress the curse-of-dimensionality, we also develop two variants of the hcp distance using (learnable) subspace projections. experiments on both synthetic and real-world data show that our hcp distance works as an effective surrogate of the wasserstein distance with low complexity and overcomes the drawbacks of the sliced wasserstein distance.",,2022-05-30,2024-02-06,"['tao li', 'cheng meng', 'hongteng xu', 'jun yu']"
2206.01409,hybrid parameter search and dynamic model selection for mixed-variable   bayesian optimization,cs.lg math.st stat.ml stat.th,"this paper presents a new type of hybrid model for bayesian optimization (bo) adept at managing mixed variables, encompassing both quantitative (continuous and integer) and qualitative (categorical) types. our proposed new hybrid models (named hybridm) merge the monte carlo tree search structure (mcts) for categorical variables with gaussian processes (gp) for continuous ones. hybridm leverages the upper confidence bound tree search (ucts) for mcts strategy, showcasing the tree architecture's integration into bayesian optimization. our innovations, including dynamic online kernel selection in the surrogate modeling phase and a unique ucts search strategy, position our hybrid models as an advancement in mixed-variable surrogate models. numerical experiments underscore the superiority of hybrid models, highlighting their potential in bayesian optimization.",10.1080/10618600.2024.2308216,2022-06-03,2024-01-18,"['hengrui luo', 'younghyun cho', 'james w. demmel', 'xiaoye s. li', 'yang liu']"
2206.01864,model-informed generative adversarial network (mi-gan) for learning   optimal power flow,cs.lg stat.ml,"the optimal power flow (opf) problem, as a critical component of power system operations, becomes increasingly difficult to solve due to the variability, intermittency, and unpredictability of renewable energy brought to the power system. although traditional optimization techniques, such as stochastic and robust optimization approaches, could be leveraged to address the opf problem, in the face of renewable energy uncertainty, i.e., the dynamic coefficients in the optimization model, their effectiveness in dealing with large-scale problems remains limited. as a result, deep learning techniques, such as neural networks, have recently been developed to improve computational efficiency in solving opf problems with the utilization of data. however, the feasibility and optimality of the solution may not be guaranteed, and the system dynamics cannot be properly addressed as well. in this paper, we propose an optimization model-informed generative adversarial network (mi-gan) framework to solve opf under uncertainty. the main contributions are summarized into three aspects: (1) to ensure feasibility and improve optimality of generated solutions, three important layers are proposed: feasibility filter layer, comparison layer, and gradient-guided layer; (2) in the gan-based framework, an efficient model-informed selector incorporating these three new layers is established; and (3) a new recursive iteration algorithm is also proposed to improve solution optimality and handle the system dynamics. the numerical results on ieee test systems show that the proposed method is very effective and promising.",10.1080/24725854.2023.2286507,2022-06-03,2024-01-17,"['yuxuan li', 'chaoyue zhao', 'chenang liu']"
2206.01900,estimating counterfactual treatment outcomes over time in complex   multiagent scenarios,cs.ai cs.lg cs.ma stat.me stat.ml,"evaluation of intervention in a multiagent system, e.g., when humans should intervene in autonomous driving systems and when a player should pass to teammates for a good shot, is challenging in various engineering and scientific fields. estimating the individual treatment effect (ite) using counterfactual long-term prediction is practical to evaluate such interventions. however, most of the conventional frameworks did not consider the time-varying complex structure of multiagent relationships and covariate counterfactual prediction. this may lead to erroneous assessments of ite and difficulty in interpretation. here we propose an interpretable, counterfactual recurrent network in multiagent systems to estimate the effect of the intervention. our model leverages graph variational recurrent neural networks and theory-based computation with domain knowledge for the ite estimation framework based on long-term prediction of multiagent covariates and outcomes, which can confirm the circumstances under which the intervention is effective. on simulated models of an automated vehicle and biological agents with time-varying confounders, we show that our methods achieved lower estimation errors in counterfactual covariates and the most effective treatment timing than the baselines. furthermore, using real basketball data, our methods performed realistic counterfactual predictions and evaluated the counterfactual passes in shot scenarios.",,2022-06-04,2024-02-17,"['keisuke fujii', 'koh takeuchi', 'atsushi kuribayashi', 'naoya takeishi', 'yoshinobu kawahara', 'kazuya takeda']"
2206.02164,estimating and mitigating the congestion effect of curbside pick-ups and   drop-offs: a causal inference approach,cs.lg cs.ai stat.me,"curb space is one of the busiest areas in urban road networks. especially in recent years, the rapid increase of ride-hailing trips and commercial deliveries has induced massive pick-ups/drop-offs (pudos), which occupy the limited curb space that was designed and built decades ago. these pudos could jam curbside utilization and disturb the mainline traffic flow, evidently leading to significant negative societal externalities. however, there is a lack of an analytical framework that rigorously quantifies and mitigates the congestion effect of pudos in the system view, particularly with little data support and involvement of confounding effects. to bridge this research gap, this paper develops a rigorous causal inference approach to estimate the congestion effect of pudos on general regional networks. a causal graph is set to represent the spatio-temporal relationship between pudos and traffic speed, and a double and separated machine learning (dsml) method is proposed to quantify how pudos affect traffic congestion. additionally, a re-routing formulation is developed and solved to encourage passenger walking and traffic flow re-routing to achieve system optimization. numerical experiments are conducted using real-world data in the manhattan area. on average, 100 additional units of pudos in a region could reduce the traffic speed by 3.70 and 4.54 mph on weekdays and weekends, respectively. re-routing trips with pudos on curb space could respectively reduce the system-wide total travel time by 2.44% and 2.12% in midtown and central park on weekdays. sensitivity analysis is also conducted to demonstrate the effectiveness and robustness of the proposed framework.",10.1287/trsc.2022.0195,2022-06-05,2024-01-02,"['xiaohui liu', 'sean qian', 'hock-hai teo', 'wei ma']"
2206.02286,augloss: a robust augmentation-based fine tuning methodology,cs.lg cs.cv stat.ml,"deep learning (dl) models achieve great successes in many domains. however, dl models increasingly face safety and robustness concerns, including noisy labeling in the training stage and feature distribution shifts in the testing stage. previous works made significant progress in addressing these problems, but the focus has largely been on developing solutions for only one problem at a time. for example, recent work has argued for the use of tunable robust loss functions to mitigate label noise, and data augmentation (e.g., augmix) to combat distribution shifts. as a step towards addressing both problems simultaneously, we introduce augloss, a simple but effective methodology that achieves robustness against both train-time noisy labeling and test-time feature distribution shifts by unifying data augmentation and robust loss functions. we conduct comprehensive experiments in varied settings of real-world dataset corruption to showcase the gains achieved by augloss compared to previous state-of-the-art methods. lastly, we hope this work will open new directions for designing more robust and reliable dl models under real-world corruptions.",,2022-06-05,2024-01-28,"['kyle otstot', 'andrew yang', 'john kevin cava', 'lalitha sankar']"
2206.03019,the survival bandit problem,cs.lg stat.ml,"we introduce and study a new variant of the multi-armed bandit problem (mab), called the survival bandit problem (s-mab). while in both problems, the objective is to maximize the so-called cumulative reward, in this new variant, the procedure is interrupted if the cumulative reward falls below a preset threshold. this simple yet unexplored extension of the mab follows from many practical applications. for example, when testing two medicines against each other on voluntary patients, people's health are at stake, and it is necessary to be able to interrupt experiments if serious side effects occur or if the disease syndromes are not dissipated by the treatment. from a theoretical perspective, the s-mab is the first variant of the mab where the procedure may or may not be interrupted. we start by formalizing the s-mab and we define its objective as the minimization of the so-called survival regret, which naturally generalizes the regret of the mab. then, we show that the objective of the s-mab is considerably more difficult than the mab, in the sense that contrary to the mab, no policy can achieve a reasonably small (i.e., sublinear) survival regret. instead, we minimize the survival regret in the sense of pareto, i.e., we seek a policy whose cumulative reward cannot be improved for some problem instance without being sacrificed for another one. for that purpose, we identify two key components in the survival regret: the regret given no ruin (which corresponds to the regret in the mab), and the probability that the procedure is interrupted, called the probability of ruin. we derive a lower bound on the probability of ruin, as well as policies whose probability of ruin matches the lower bound. finally, based on a doubling trick on those policies, we derive a policy which minimizes the survival regret in the sense of pareto, giving an answer to an open problem by perotto et al. (colt 2019).",,2022-06-07,2024-01-06,"['charles riou', 'junya honda', 'masashi sugiyama']"
2206.03183,risk measures and upper probabilities: coherence and stratification,cs.lg math.st stat.th,"machine learning typically presupposes classical probability theory which implies that aggregation is built upon expectation. there are now multiple reasons to motivate looking at richer alternatives to classical probability theory as a mathematical foundation for machine learning. we systematically examine a powerful and rich class of alternative aggregation functionals, known variously as spectral risk measures, choquet integrals or lorentz norms. we present a range of characterization results, and demonstrate what makes this spectral family so special. in doing so we arrive at a natural stratification of all coherent risk measures in terms of the upper probabilities that they induce by exploiting results from the theory of rearrangement invariant banach spaces. we empirically demonstrate how this new approach to uncertainty helps tackling practical machine learning problems.",,2022-06-07,2024-01-29,"['christian fröhlich', 'robert c. williamson']"
2206.04044,model-based reinforcement learning for offline zero-sum markov games,cs.lg cs.gt cs.it math.it math.st stat.ml stat.th,"this paper makes progress towards learning nash equilibria in two-player zero-sum markov games from offline data. specifically, consider a $\gamma$-discounted infinite-horizon markov game with $s$ states, where the max-player has $a$ actions and the min-player has $b$ actions. we propose a pessimistic model-based algorithm with bernstein-style lower confidence bounds -- called vi-lcb-game -- that provably finds an $\varepsilon$-approximate nash equilibrium with a sample complexity no larger than $\frac{c_{\mathsf{clipped}}^{\star}s(a+b)}{(1-\gamma)^{3}\varepsilon^{2}}$ (up to some log factor). here, $c_{\mathsf{clipped}}^{\star}$ is some unilateral clipped concentrability coefficient that reflects the coverage and distribution shift of the available data (vis-\`a-vis the target data), and the target accuracy $\varepsilon$ can be any value within $\big(0,\frac{1}{1-\gamma}\big]$. our sample complexity bound strengthens prior art by a factor of $\min\{a,b\}$, achieving minimax optimality for the entire $\varepsilon$-range. an appealing feature of our result lies in algorithmic simplicity, which reveals the unnecessity of variance reduction and sample splitting in achieving sample optimality.",,2022-06-08,2024-02-26,"['yuling yan', 'gen li', 'yuxin chen', 'jianqing fan']"
2206.04277,on hypothesis transfer learning of functional linear models,stat.ml cs.lg,"we study the transfer learning (tl) for the functional linear regression (flr) under the reproducing kernel hilbert space (rkhs) framework, observing the tl techniques in existing high-dimensional linear regression is not compatible with the truncation-based flr methods as functional data are intrinsically infinite-dimensional and generated by smooth underlying processes. we measure the similarity across tasks using rkhs distance, allowing the type of information being transferred tied to the properties of the imposed rkhs. building on the hypothesis offset transfer learning paradigm, two algorithms are proposed: one conducts the transfer when positive sources are known, while the other leverages aggregation techniques to achieve robust transfer without prior information about the sources. we establish lower bounds for this learning problem and show the proposed algorithms enjoy a matching asymptotic upper bound. these analyses provide statistical insights into factors that contribute to the dynamics of the transfer. we also extend the results to functional generalized linear models. the effectiveness of the proposed algorithms is demonstrated on extensive synthetic data as well as a financial data application.",,2022-06-09,2024-02-22,"['haotian lin', 'matthew reimherr']"
2206.05581,federated offline reinforcement learning,stat.ml cs.lg stat.me,"evidence-based or data-driven dynamic treatment regimes are essential for personalized medicine, which can benefit from offline reinforcement learning (rl). although massive healthcare data are available across medical institutions, they are prohibited from sharing due to privacy constraints. besides, heterogeneity exists in different sites. as a result, federated offline rl algorithms are necessary and promising to deal with the problems. in this paper, we propose a multi-site markov decision process model that allows for both homogeneous and heterogeneous effects across sites. the proposed model makes the analysis of the site-level features possible. we design the first federated policy optimization algorithm for offline rl with sample complexity. the proposed algorithm is communication-efficient, which requires only a single round of communication interaction by exchanging summary statistics. we give a theoretical guarantee for the proposed algorithm, where the suboptimality for the learned policies is comparable to the rate as if data is not distributed. extensive simulations demonstrate the effectiveness of the proposed algorithm. the method is applied to a sepsis dataset in multiple sites to illustrate its use in clinical settings.",,2022-06-11,2024-01-27,"['doudou zhou', 'yufeng zhang', 'aaron sonabend-w', 'zhaoran wang', 'junwei lu', 'tianxi cai']"
2206.06854,on the explainable properties of 1-lipschitz neural networks: an optimal   transport perspective,cs.ai cs.cr cs.cv cs.lg stat.ml,"input gradients have a pivotal role in a variety of applications, including adversarial attack algorithms for evaluating model robustness, explainable ai techniques for generating saliency maps, and counterfactual explanations.however, saliency maps generated by traditional neural networks are often noisy and provide limited insights. in this paper, we demonstrate that, on the contrary, the saliency maps of 1-lipschitz neural networks, learned with the dual loss of an optimal transportation problem, exhibit desirable xai properties:they are highly concentrated on the essential parts of the image with low noise, significantly outperforming state-of-the-art explanation approaches across various models and metrics. we also prove that these maps align unprecedentedly well with human explanations on imagenet.to explain the particularly beneficial properties of the saliency map for such models, we prove this gradient encodes both the direction of the transportation plan and the direction towards the nearest adversarial attack. following the gradient down to the decision boundary is no longer considered an adversarial attack, but rather a counterfactual explanation that explicitly transports the input from one class to another. thus, learning with such a loss jointly optimizes the classification objective and the alignment of the gradient, i.e. the saliency map, to the transportation plan direction.these networks were previously known to be certifiably robust by design, and we demonstrate that they scale well for large problems and models, and are tailored for explainability using a fast and straightforward method.",,2022-06-14,2024-02-02,"['mathieu serrurier', 'franck mamalet', 'thomas fel', 'louis béthune', 'thibaut boissin']"
2206.07751,on the identifiability of nonlinear ica: sparsity and beyond,cs.lg cs.ai stat.ml,"nonlinear independent component analysis (ica) aims to recover the underlying independent latent sources from their observable nonlinear mixtures. how to make the nonlinear ica model identifiable up to certain trivial indeterminacies is a long-standing problem in unsupervised learning. recent breakthroughs reformulate the standard independence assumption of sources as conditional independence given some auxiliary variables (e.g., class labels and/or domain/time indexes) as weak supervision or inductive bias. however, nonlinear ica with unconditional priors cannot benefit from such developments. we explore an alternative path and consider only assumptions on the mixing process, such as structural sparsity. we show that under specific instantiations of such constraints, the independent latent sources can be identified from their nonlinear mixtures up to a permutation and a component-wise transformation, thus achieving nontrivial identifiability of nonlinear ica without auxiliary variables. we provide estimation methods and validate the theoretical results experimentally. the results on image data suggest that our conditions may hold in a number of practical data generating processes.",,2022-06-15,2024-02-25,"['yujia zheng', 'ignavier ng', 'kun zhang']"
2206.08541,ensemble distributional forecasting for insurance loss reserving,stat.me q-fin.rm stat.ap,"loss reserving generally focuses on identifying a single model that can generate superior predictive performance. however, different loss reserving models specialise in capturing different aspects of loss data. this is recognised in practice in the sense that results from different models are often considered, and sometimes combined. for instance, actuaries may take a weighted average of the prediction outcomes from various loss reserving models, often based on subjective assessments.   in this paper, we propose a systematic framework to objectively combine (i.e. ensemble) multiple _stochastic_ loss reserving models such that the strengths offered by different models can be utilised effectively. our framework contains two main innovations compared to existing literature and practice. firstly, our criteria model combination considers the full distributional properties of the ensemble and not just the central estimate - which is of particular importance in the reserving context. secondly, our framework is that it is tailored for the features inherent to reserving data. these include, for instance, accident, development, calendar, and claim maturity effects. crucially, the relative importance and scarcity of data across accident periods renders the problem distinct from the traditional ensembling techniques in statistical learning.   our framework is illustrated with a complex synthetic dataset. in the results, the optimised ensemble outperforms both (i) traditional model selection strategies, and (ii) an equally weighted ensemble. in particular, the improvement occurs not only with central estimates but also relevant quantiles, such as the 75th percentile of reserves (typically of interest to both insurers and regulators).",,2022-06-17,2024-02-19,"['benjamin avanzi', 'yanfeng li', 'bernard wong', 'alan xian']"
2206.08756,"tensor-on-tensor regression: riemannian optimization,   over-parameterization, statistical-computational gap, and their interplay",math.st cs.lg math.oc stat.me stat.ml stat.th,"we study the tensor-on-tensor regression, where the goal is to connect tensor responses to tensor covariates with a low tucker rank parameter tensor/matrix without the prior knowledge of its intrinsic rank. we propose the riemannian gradient descent (rgd) and riemannian gauss-newton (rgn) methods and cope with the challenge of unknown rank by studying the effect of rank over-parameterization. we provide the first convergence guarantee for the general tensor-on-tensor regression by showing that rgd and rgn respectively converge linearly and quadratically to a statistically optimal estimate in both rank correctly-parameterized and over-parameterized settings. our theory reveals an intriguing phenomenon: riemannian optimization methods naturally adapt to over-parameterization without modifications to their implementation. we also prove the statistical-computational gap in scalar-on-tensor regression by a direct low-degree polynomial argument. our theory demonstrates a ""blessing of statistical-computational gap"" phenomenon: in a wide range of scenarios in tensor-on-tensor regression for tensors of order three or higher, the computationally required sample size matches what is needed by moderate rank over-parameterization when considering computationally feasible estimators, while there are no such benefits in the matrix settings. this shows moderate rank over-parameterization is essentially ""cost-free"" in terms of sample size in tensor-on-tensor regression of order three or higher. finally, we conduct simulation studies to show the advantages of our proposed methods and to corroborate our theoretical findings.",,2022-06-17,2024-01-15,"['yuetian luo', 'anru r. zhang']"
2206.09107,tree-guided rare feature selection and logic aggregation with electronic   health records data,cs.lg stat.ap stat.me stat.ml,"statistical learning with a large number of rare binary features is commonly encountered in analyzing electronic health records (ehr) data, especially in the modeling of disease onset with prior medical diagnoses and procedures. dealing with the resulting highly sparse and large-scale binary feature matrix is notoriously challenging as conventional methods may suffer from a lack of power in testing and inconsistency in model fitting while machine learning methods may suffer from the inability of producing interpretable results or clinically-meaningful risk factors. to improve ehr-based modeling and utilize the natural hierarchical structure of disease classification, we propose a tree-guided feature selection and logic aggregation approach for large-scale regression with rare binary features, in which dimension reduction is achieved through not only a sparsity pursuit but also an aggregation promoter with the logic operator of ``or''. we convert the combinatorial problem into a convex linearly-constrained regularized estimation, which enables scalable computation with theoretical guarantees. in a suicide risk study with ehr data, our approach is able to select and aggregate prior mental health diagnoses as guided by the diagnosis hierarchy of the international classification of diseases. by balancing the rarity and specificity of the ehr diagnosis records, our strategy improves both prediction and model interpretation. we identify important higher-level categories and subcategories of mental health conditions and simultaneously determine the level of specificity needed for each of them in predicting suicide risk.",,2022-06-17,2024-02-26,"['jianmin chen', 'robert h. aseltine', 'fei wang', 'kun chen']"
2206.09754,guided structure learning of dags for count data,stat.me,"in this paper, we tackle structure learning of directed acyclic graphs (dags), with the idea of exploiting available prior knowledge of the domain at hand to guide the search of the best structure. in particular, we assume to know the topological ordering of variables in addition to the given data. we study a new algorithm for learning the structure of dags, proving its theoretical consistence in the limit of infinite observations. furthermore, we experimentally compare the proposed algorithm to a number of popular competitors, in order to study its behavior in finite samples.",,2022-06-20,2024-01-18,"['thi kim hue nguyen', 'monica chiogna', 'davide risso', 'erika banzato']"
2206.10078,the manifold scattering transform for high-dimensional point cloud data,cs.lg cs.na eess.sp math.na stat.ml,"the manifold scattering transform is a deep feature extractor for data defined on a riemannian manifold. it is one of the first examples of extending convolutional neural network-like operators to general manifolds. the initial work on this model focused primarily on its theoretical stability and invariance properties but did not provide methods for its numerical implementation except in the case of two-dimensional surfaces with predefined meshes. in this work, we present practical schemes, based on the theory of diffusion maps, for implementing the manifold scattering transform to datasets arising in naturalistic systems, such as single cell genetics, where the data is a high-dimensional point cloud modeled as lying on a low-dimensional manifold. we show that our methods are effective for signal classification and manifold classification tasks.",,2022-06-20,2024-01-21,"['joyce chew', 'holly r. steach', 'siddharth viswanath', 'hau-tieng wu', 'matthew hirn', 'deanna needell', 'smita krishnaswamy', 'michael perlmutter']"
2206.10477,survival kernets: scalable and interpretable deep kernel survival   analysis with an accuracy guarantee,cs.lg stat.ml,"kernel survival analysis models estimate individual survival distributions with the help of a kernel function, which measures the similarity between any two data points. such a kernel function can be learned using deep kernel survival models. in this paper, we present a new deep kernel survival model called a survival kernet, which scales to large datasets in a manner that is amenable to model interpretation and also theoretical analysis. specifically, the training data are partitioned into clusters based on a recently developed training set compression scheme for classification and regression called kernel netting that we extend to the survival analysis setting. at test time, each data point is represented as a weighted combination of these clusters, and each such cluster can be visualized. for a special case of survival kernets, we establish a finite-sample error bound on predicted survival distributions that is, up to a log factor, optimal. whereas scalability at test time is achieved using the aforementioned kernel netting compression strategy, scalability during training is achieved by a warm-start procedure based on tree ensembles such as xgboost and a heuristic approach to accelerating neural architecture search. on four standard survival analysis datasets of varying sizes (up to roughly 3 million data points), we show that survival kernets are highly competitive compared to various baselines tested in terms of time-dependent concordance index. our code is available at: https://github.com/georgehc/survival-kernets",,2022-06-21,2024-02-19,['george h. chen']
2206.11492,gradual domain adaptation via normalizing flows,stat.ml cs.lg,"standard domain adaptation methods do not work well when a large gap exists between the source and target domains. gradual domain adaptation is one of the approaches used to address the problem. it involves leveraging the intermediate domain, which gradually shifts from the source domain to the target domain. in previous work, it is assumed that the number of intermediate domains is large and the distance between adjacent domains is small; hence, the gradual domain adaptation algorithm, involving self-training with unlabeled datasets, is applicable. in practice, however, gradual self-training will fail because the number of intermediate domains is limited and the distance between adjacent domains is large. we propose the use of normalizing flows to deal with this problem while maintaining the framework of unsupervised domain adaptation. the proposed method learns a transformation from the distribution of the target domain to the gaussian mixture distribution via the source domain. we evaluate our proposed method by experiments using real-world datasets and confirm that it mitigates the above-explained problem and improves the classification performance.",,2022-06-23,2024-01-23,"['shogo sagawa', 'hideitsu hino']"
2206.12532,"causal scoring: a framework for effect estimation, effect ordering, and   effect classification",stat.ml cs.lg,"this paper introduces causal scoring as a novel approach to frame causal estimation in the context of decision making. causal scoring entails the estimation of scores that support decision making by providing insights into causal effects. we present three valuable causal interpretations of these scores: effect estimation (ee), effect ordering (eo), and effect classification (ec). in the ee interpretation, the causal score represents the effect itself. the eo interpretation implies that the score can serve as a proxy for the magnitude of the effect, enabling the sorting of individuals based on their causal effects. the ec interpretation enables the classification of individuals into high- and low-effect categories using a predefined threshold. we demonstrate the value of these alternative causal interpretations (eo and ec) through two key results. first, we show that aligning the statistical modeling with the desired causal interpretation improves the accuracy of causal estimation. second, we establish that more flexible causal interpretations are plausible in a wider range of settings and propose conditions to assess their validity. we showcase the practical utility of causal scoring through diverse scenarios, including situations involving unobserved confounding due to self-selection, lack of data on the primary outcome of interest, or lack of data on how individuals behave when intervened. these examples illustrate how causal scoring facilitates reasoning about flexible causal interpretations of statistical estimates in various contexts. they encompass confounded estimates, effect estimates on surrogate outcomes, and even predictions about non-causal quantities as potential causal scores.",,2022-06-24,2024-02-16,"['carlos fernández-loría', 'jorge loría']"
2206.14674,signature methods in machine learning,stat.ml cs.lg cs.na math.ca math.na math.st stat.me stat.th,"signature-based techniques give mathematical insight into the interactions between complex streams of evolving data. these insights can be quite naturally translated into numerical approaches to understanding streamed data, and perhaps because of their mathematical precision, have proved useful in analysing streamed data in situations where the data is irregular, and not stationary, and the dimension of the data and the sample sizes are both moderate. understanding streamed multi-modal data is exponential: a word in $n$ letters from an alphabet of size $d$ can be any one of $d^n$ messages. signatures remove the exponential amount of noise that arises from sampling irregularity, but an exponential amount of information still remain. this survey aims to stay in the domain where that exponential scaling can be managed directly. scalability issues are an important challenge in many problems but would require another survey article and further ideas. this survey describes a range of contexts where the data sets are small enough to remove the possibility of massive machine learning, and the existence of small sets of context free and principled features can be used effectively. the mathematical nature of the tools can make their use intimidating to non-mathematicians. the examples presented in this article are intended to bridge this communication gap and provide tractable working examples drawn from the machine learning context. notebooks are available online for several of these examples. this survey builds on the earlier paper of ilya chevryev and andrey kormilitzin which had broadly similar aims at an earlier point in the development of this machinery. this article illustrates how the theoretical insights offered by signatures are simply realised in the analysis of application data in a way that is largely agnostic to the data type.",,2022-06-29,2024-01-26,"['terry lyons', 'andrew d. mcleod']"
2207.00109,ranking in generalized linear bandits,stat.ml cs.ir cs.lg math.oc,"we study the ranking problem in generalized linear bandits. at each time, the learning agent selects an ordered list of items and observes stochastic outcomes. in recommendation systems, displaying an ordered list of the most attractive items is not always optimal as both position and item dependencies result in a complex reward function. a very naive example is the lack of diversity when all the most attractive items are from the same category. we model the position and item dependencies in the ordered list and design ucb and thompson sampling type algorithms for this problem. our work generalizes existing studies in several directions, including position dependencies where position discount is a particular case, and connecting the ranking problem to graph theory.",,2022-06-30,2024-01-01,"['amitis shidani', 'george deligiannidis', 'arnaud doucet']"
2207.00391,a theoretical analysis of the learning dynamics under class imbalance,stat.ml cs.lg,"data imbalance is a common problem in machine learning that can have a critical effect on the performance of a model. various solutions exist but their impact on the convergence of the learning dynamics is not understood. here, we elucidate the significant negative impact of data imbalance on learning, showing that the learning curves for minority and majority classes follow sub-optimal trajectories when training with a gradient-based optimizer. this slowdown is related to the imbalance ratio and can be traced back to a competition between the optimization of different classes. our main contribution is the analysis of the convergence of full-batch (gd) and stochastic gradient descent (sgd), and of variants that renormalize the contribution of each per-class gradient. we find that gd is not guaranteed to decrease the loss for each class but that this problem can be addressed by performing a per-class normalization of the gradient. with sgd, class imbalance has an additional effect on the direction of the gradients: the minority class suffers from a higher directional noise, which reduces the effectiveness of the per-class gradient normalization. our findings not only allow us to understand the potential and limitations of strategies involving the per-class gradients, but also the reason for the effectiveness of previously used solutions for class imbalance such as oversampling.",,2022-07-01,2024-02-19,"['emanuele francazi', 'marco baity-jesi', 'aurelien lucchi']"
2207.04771,functional generalized empirical likelihood estimation for conditional   moment restrictions,cs.lg math.st stat.ml stat.th,"important problems in causal inference, economics, and, more generally, robust machine learning can be expressed as conditional moment restrictions, but estimation becomes challenging as it requires solving a continuum of unconditional moment restrictions. previous works addressed this problem by extending the generalized method of moments (gmm) to continuum moment restrictions. in contrast, generalized empirical likelihood (gel) provides a more general framework and has been shown to enjoy favorable small-sample properties compared to gmm-based estimators. to benefit from recent developments in machine learning, we provide a functional reformulation of gel in which arbitrary models can be leveraged. motivated by a dual formulation of the resulting infinite dimensional optimization problem, we devise a practical method and explore its asymptotic properties. finally, we provide kernel- and neural network-based implementations of the estimator, which achieve state-of-the-art empirical performance on two conditional moment restriction problems.",,2022-07-11,2024-02-16,"['heiner kremer', 'jia-jie zhu', 'krikamol muandet', 'bernhard schölkopf']"
2207.06342,efficient error and variance estimation for randomized matrix   computations,math.na cs.na stat.ml,"randomized matrix algorithms have become workhorse tools in scientific computing and machine learning. to use these algorithms safely in applications, they should be coupled with posterior error estimates to assess the quality of the output. to meet this need, this paper proposes two diagnostics: a leave-one-out error estimator for randomized low-rank approximations and a jackknife resampling method to estimate the variance of the output of a randomized matrix computation. both of these diagnostics are rapid to compute for randomized low-rank approximation algorithms such as the randomized svd and randomized nystr\""om approximation, and they provide useful information that can be used to assess the quality of the computed output and guide algorithmic parameter choices.",10.1137/23m1558537,2022-07-13,2024-02-23,"['ethan n. epperly', 'joel a. tropp']"
2207.06944,differentially private graph learning via sensitivity-bounded   personalized pagerank,cs.cr cs.lg cs.si stat.ml,"personalized pagerank (ppr) is a fundamental tool in unsupervised learning of graph representations such as node ranking, labeling, and graph embedding. however, while data privacy is one of the most important recent concerns, existing ppr algorithms are not designed to protect user privacy. ppr is highly sensitive to the input graph edges: the difference of only one edge may cause a big change in the ppr vector, potentially leaking private user data.   in this work, we propose an algorithm which outputs an approximate ppr and has provably bounded sensitivity to input edges. in addition, we prove that our algorithm achieves similar accuracy to non-private algorithms when the input graph has large degrees. our sensitivity-bounded ppr directly implies private algorithms for several tools of graph learning, such as, differentially private (dp) ppr ranking, dp node classification, and dp node embedding. to complement our theoretical analysis, we also empirically verify the practical performances of our algorithms.",,2022-07-14,2024-02-14,"['alessandro epasto', 'vahab mirrokni', 'bryan perozzi', 'anton tsitsulin', 'peilin zhong']"
2207.07624,feed-forward latent domain adaptation,cs.lg stat.ml,"we study a new highly-practical problem setting that enables resource-constrained edge devices to adapt a pre-trained model to their local data distributions. recognizing that device's data are likely to come from multiple latent domains that include a mixture of unlabelled domain-relevant and domain-irrelevant examples, we focus on the comparatively under-studied problem of latent domain adaptation. considering limitations of edge devices, we aim to only use a pre-trained model and adapt it in a feed-forward way, without using back-propagation and without access to the source data. modelling these realistic constraints bring us to the novel and practically important problem setting of feed-forward latent domain adaptation. our solution is to meta-learn a network capable of embedding the mixed-relevance target dataset and dynamically adapting inference for target examples using cross-attention. the resulting framework leads to consistent improvements over strong erm baselines. we also show that our framework sometimes even improves on the upper bound of domain-supervised adaptation, where only domain-relevant instances are provided for adaptation. this suggests that human annotated domain labels may not always be optimal, and raises the possibility of doing better through automated instance selection.",,2022-07-15,2024-01-31,"['ondrej bohdal', 'da li', 'shell xu hu', 'timothy hospedales']"
2207.10199,provably tuning the elasticnet across instances,cs.lg stat.ml,"an important unresolved challenge in the theory of regularization is to set the regularization coefficients of popular techniques like the elasticnet with general provable guarantees. we consider the problem of tuning the regularization parameters of ridge regression, lasso, and the elasticnet across multiple problem instances, a setting that encompasses both cross-validation and multi-task hyperparameter optimization. we obtain a novel structural result for the elasticnet which characterizes the loss as a function of the tuning parameters as a piecewise-rational function with algebraic boundaries. we use this to bound the structural complexity of the regularized loss functions and show generalization guarantees for tuning the elasticnet regression coefficients in the statistical setting. we also consider the more challenging online learning setting, where we show vanishing average expected regret relative to the optimal parameter pair. we further extend our results to tuning classification algorithms obtained by thresholding regression fits regularized by ridge, lasso, or elasticnet. our results are the first general learning-theoretic guarantees for this important class of problems that avoid strong assumptions on the data distribution. furthermore, our guarantees hold for both validation and popular information criterion objectives.",,2022-07-20,2024-01-15,"['maria-florina balcan', 'mikhail khodak', 'dravyansh sharma', 'ameet talwalkar']"
2207.12051,flowsheet synthesis through hierarchical reinforcement learning and   graph neural networks,cs.lg math.oc stat.ml,"process synthesis experiences a disruptive transformation accelerated by digitization and artificial intelligence. we propose a reinforcement learning algorithm for chemical process design based on a state-of-the-art actor-critic logic. our proposed algorithm represents chemical processes as graphs and uses graph convolutional neural networks to learn from process graphs. in particular, the graph neural networks are implemented within the agent architecture to process the states and make decisions. moreover, we implement a hierarchical and hybrid decision-making process to generate flowsheets, where unit operations are placed iteratively as discrete decisions and corresponding design variables are selected as continuous decisions. we demonstrate the potential of our method to design economically viable flowsheets in an illustrative case study comprising equilibrium reactions, azeotropic separation, and recycles. the results show quick learning in discrete, continuous, and hybrid action spaces. due to the flexible architecture of the proposed reinforcement learning agent, the method is predestined to include large action-state spaces and an interface to process simulators in future research.",10.1002/aic.17938,2022-07-25,,"['laura stops', 'roel leenhouts', 'qinghe gao', 'artur m. schweidtmann']"
2207.12638,variance estimation in graphs with the fused lasso,math.st cs.lg stat.ml stat.th,"we study the problem of variance estimation in general graph-structured problems. first, we develop a linear time estimator for the homoscedastic case that can consistently estimate the variance in general graphs. we show that our estimator attains minimax rates for the chain and 2d grid graphs when the mean signal has total variation with canonical scaling. furthermore, we provide general upper bounds on the mean squared error performance of the fused lasso estimator in general graphs under a moment condition and a bound on the tail behavior of the errors. these upper bounds allow us to generalize for broader classes of distributions, such as sub-exponential, many existing results on the fused lasso that are only known to hold with the assumption that errors are sub-gaussian random variables. exploiting our upper bounds, we then study a simple total variation regularization estimator for estimating the signal of variances in the heteroscedastic case. we also provide lower bounds showing that our heteroscedastic variance estimator attains minimax rates for estimating signals of bounded variation in grid graphs, and $k$-nearest neighbor graphs, and the estimator is consistent for estimating the variances in any connected graph.",,2022-07-25,2024-02-18,['oscar hernan madrid padilla']
2207.13071,missing values handling for machine learning portfolios,stat.me q-fin.gn stat.ap,"we characterize the structure and origins of missingness for 159 cross-sectional return predictors and study missing value handling for portfolios constructed using machine learning. simply imputing with cross-sectional means performs well compared to rigorous expectation-maximization methods. this stems from three facts about predictor data: (1) missingness occurs in large blocks organized by time, (2) cross-sectional correlations are small, and (3) missingness tends to occur in blocks organized by the underlying data source. as a result, observed data provide little information about missing data. sophisticated imputations introduce estimation noise that can lead to underperformance if machine learning is not carefully applied.",,2022-07-20,2024-01-11,"['andrew y. chen', 'jack mccoy']"
2208.00124,physical parameter calibration,stat.me math.st stat.th,"computer simulation models are widely used to study complex physical systems. a related fundamental topic is the inverse problem, also called calibration, which aims at learning about the values of parameters in the model based on observations. in most real applications, the parameters have specific physical meanings, and we call them physical parameters. to recognize the true underlying physical system, we need to effectively estimate such parameters. however, existing calibration methods cannot do this well due to the model identifiability problem. this paper proposes a semi-parametric model, called the discrepancy decomposition model, to describe the discrepancy between the physical system and the computer model. the proposed model possesses a clear interpretation, and more importantly, it is identifiable under mild conditions. under this model, we present estimators of the physical parameters and the discrepancy, and then establish their asymptotic properties. numerical examples show that the proposed method can better estimate the physical parameters than existing methods.",,2022-07-29,2024-01-02,"['yang li', 'shifeng xiong']"
2208.03246,non-asymptotic analysis of ensemble kalman updates: effective dimension   and localization,stat.ml cs.na math.na math.st stat.me stat.th,"many modern algorithms for inverse problems and data assimilation rely on ensemble kalman updates to blend prior predictions with observed data. ensemble kalman methods often perform well with a small ensemble size, which is essential in applications where generating each particle is costly. this paper develops a non-asymptotic analysis of ensemble kalman updates that rigorously explains why a small ensemble size suffices if the prior covariance has moderate effective dimension due to fast spectrum decay or approximate sparsity. we present our theory in a unified framework, comparing several implementations of ensemble kalman updates that use perturbed observations, square root filtering, and localization. as part of our analysis, we develop new dimension-free covariance estimation bounds for approximately sparse matrices that may be of independent interest.",10.1093/imaiai/iaad043,2022-08-05,2023-10-05,"['omar al ghattas', 'daniel sanz-alonso']"
2208.03915,dynamic maintenance of kernel density estimation data structure: from   practice to theory,cs.lg stat.ml,"kernel density estimation (kde) stands out as a challenging task in machine learning. the problem is defined in the following way: given a kernel function $f(x,y)$ and a set of points $\{x_1, x_2, \cdots, x_n \} \subset \mathbb{r}^d$, we would like to compute $\frac{1}{n}\sum_{i=1}^{n} f(x_i,y)$ for any query point $y \in \mathbb{r}^d$. recently, there has been a growing trend of using data structures for efficient kde. however, the proposed kde data structures focus on static settings. the robustness of kde data structures over dynamic changing data distributions is not addressed. in this work, we focus on the dynamic maintenance of kde data structures with robustness to adversarial queries. especially, we provide a theoretical framework of kde data structures. in our framework, the kde data structures only require subquadratic spaces. moreover, our data structure supports the dynamic update of the dataset in sublinear time. furthermore, we can perform adaptive queries with the potential adversary in sublinear time.",,2022-08-08,2024-02-13,"['jiehao liang', 'zhao song', 'zhaozhuo xu', 'junze yin', 'danyang zhuo']"
2208.04284,on rademacher complexity-based generalization bounds for deep learning,stat.ml cs.lg,"we show that the rademacher complexity-based approach can generate non-vacuous generalisation bounds on convolutional neural networks (cnns) for classifying a small number of classes of images. the development of new talagrand's contraction lemmas for high-dimensional mappings between function spaces and cnns for general lipschitz activation functions is a key technical contribution. our results show that the rademacher complexity does not depend on the network length for cnns with some special types of activation functions such as relu, leaky relu, parametric rectifier linear unit, sigmoid, and tanh.",,2022-08-08,2024-02-09,['lan v. truong']
2208.04508,training overparametrized neural networks in sublinear time,cs.lg cs.ds stat.ml,"the success of deep learning comes at a tremendous computational and energy cost, and the scalability of training massively overparametrized neural networks is becoming a real barrier to the progress of artificial intelligence (ai). despite the popularity and low cost-per-iteration of traditional backpropagation via gradient decent, stochastic gradient descent (sgd) has prohibitive convergence rate in non-convex settings, both in theory and practice.   to mitigate this cost, recent works have proposed to employ alternative (newton-type) training methods with much faster convergence rate, albeit with higher cost-per-iteration. for a typical neural network with $m=\mathrm{poly}(n)$ parameters and input batch of $n$ datapoints in $\mathbb{r}^d$, the previous work of [brand, peng, song, and weinstein, itcs'2021] requires $\sim mnd + n^3$ time per iteration. in this paper, we present a novel training method that requires only $m^{1-\alpha} n d + n^3$ amortized time in the same overparametrized regime, where $\alpha \in (0.01,1)$ is some fixed constant. this method relies on a new and alternative view of neural networks, as a set of binary search trees, where each iteration corresponds to modifying a small subset of the nodes in the tree. we believe this view would have further applications in the design and analysis of deep neural networks (dnns).",,2022-08-08,2024-02-07,"['yichuan deng', 'hang hu', 'zhao song', 'omri weinstein', 'danyang zhuo']"
2208.05767,distributionally robust model-based offline reinforcement learning with   near-optimal sample complexity,cs.lg stat.ml,"this paper concerns the central issues of model robustness and sample efficiency in offline reinforcement learning (rl), which aims to learn to perform decision making from history data without active exploration. due to uncertainties and variabilities of the environment, it is critical to learn a robust policy -- with as few samples as possible -- that performs well even when the deployed environment deviates from the nominal one used to collect the history dataset. we consider a distributionally robust formulation of offline rl, focusing on tabular robust markov decision processes with an uncertainty set specified by the kullback-leibler divergence in both finite-horizon and infinite-horizon settings. to combat with sample scarcity, a model-based algorithm that combines distributionally robust value iteration with the principle of pessimism in the face of uncertainty is proposed, by penalizing the robust value estimates with a carefully designed data-driven penalty term. under a mild and tailored assumption of the history dataset that measures distribution shift without requiring full coverage of the state-action space, we establish the finite-sample complexity of the proposed algorithms. we further develop an information-theoretic lower bound, which suggests that learning rmdps is at least as hard as the standard mdps when the uncertainty level is sufficient small, and corroborates the tightness of our upper bound up to polynomial factors of the (effective) horizon length for a range of uncertainty levels. to the best our knowledge, this provides the first provably near-optimal robust offline rl algorithm that learns under model uncertainty and partial coverage.",,2022-08-11,2023-12-28,"['laixi shi', 'yuejie chi']"
2208.06727,reliable emulation of complex functionals by active learning with error   control,physics.chem-ph stat.ap,"a statistical emulator can be used as a surrogate of complex physics-based calculations to drastically reduce the computational cost. its successful implementation hinges on an accurate representation of the nonlinear response surface with a high-dimensional input space. conventional ""space-filling"" designs, including random sampling and latin hypercube sampling, become inefficient as the dimensionality of the input variables increases, and the predictive accuracy of the emulator can degrade substantially for a test input distant from the training input set. to address this fundamental challenge, we develop a reliable emulator for predicting complex functionals by active learning with error control (alec). the algorithm is applicable to infinite-dimensional mapping with high-fidelity predictions and a controlled predictive error. the computational efficiency has been demonstrated by emulating the classical density functional theory (cdft) calculations, a statistical-mechanical method widely used in modeling the equilibrium properties of complex molecular systems. we show that alec is much more accurate than conventional emulators based on the gaussian processes with ""space-filling"" designs and alternative active learning methods. besides, it is computationally more efficient than direct cdft calculations. alec can be a reliable building block for emulating expensive functionals owing to its minimal computational cost, controllable predictive error, and fully automatic features.",10.1063/5.0121805,2022-08-13,2024-01-30,"['xinyi fang', 'mengyang gu', 'jianzhong wu']"
2208.07573,higher-order accurate two-sample network inference and network hashing,stat.me math.st stat.ml stat.th,"two-sample hypothesis testing for network comparison presents many significant challenges, including: leveraging repeated network observations and known node registration, but without requiring them to operate; relaxing strong structural assumptions; achieving finite-sample higher-order accuracy; handling different network sizes and sparsity levels; fast computation and memory parsimony; controlling false discovery rate (fdr) in multiple testing; and theoretical understandings, particularly regarding finite-sample accuracy and minimax optimality. in this paper, we develop a comprehensive toolbox, featuring a novel main method and its variants, all accompanied by strong theoretical guarantees, to address these challenges. our method outperforms existing tools in speed and accuracy, and it is proved power-optimal. our algorithms are user-friendly and versatile in handling various data structures (single or repeated network observations; known or unknown node registration). we also develop an innovative framework for offline hashing and fast querying as a very useful tool for large network databases. we showcase the effectiveness of our method through comprehensive simulations and applications to two real-world datasets, which revealed intriguing new structures.",,2022-08-16,2024-02-02,"['meijia shao', 'dong xia', 'yuan zhang', 'qiong wu', 'shuo chen']"
2208.09894,byzantines can also learn from history: fall of centered clipping in   federated learning,cs.lg cs.ai cs.cr cs.dc stat.ml,"the increasing popularity of the federated learning (fl) framework due to its success in a wide range of collaborative learning tasks also induces certain security concerns. among many vulnerabilities, the risk of byzantine attacks is of particular concern, which refers to the possibility of malicious clients participating in the learning process. hence, a crucial objective in fl is to neutralize the potential impact of byzantine attacks and to ensure that the final model is trustable. it has been observed that the higher the variance among the clients' models/updates, the more space there is for byzantine attacks to be hidden. as a consequence, by utilizing momentum, and thus, reducing the variance, it is possible to weaken the strength of known byzantine attacks. the centered clipping (cc) framework has further shown that the momentum term from the previous iteration, besides reducing the variance, can be used as a reference point to neutralize byzantine attacks better. in this work, we first expose vulnerabilities of the cc framework, and introduce a novel attack strategy that can circumvent the defences of cc and other robust aggregators and reduce their test accuracy up to %33 on best-case scenarios in image classification tasks. then, we propose a new robust and fast defence mechanism that is effective against the proposed and other existing byzantine attacks.",10.1109/tifs.2023.3345171,2022-08-21,2024-01-01,"['kerem ozfatura', 'emre ozfatura', 'alptekin kupcu', 'deniz gunduz']"
2208.10962,prediction of good reaction coordinates and future evolution of md   trajectories using regularized sparse autoencoders: a novel deep learning   approach,physics.chem-ph cs.lg q-bio.qm stat.me stat.ml,"identifying reaction coordinates(rcs) is an active area of research, given the crucial role rcs play in determining the progress of a chemical reaction. the choice of the reaction coordinate is often based on heuristic knowledge. however, an essential criterion for the choice is that the coordinate should capture both the reactant and product states unequivocally. also, the coordinate should be the slowest one so that all the other degrees of freedom can easily equilibrate along the reaction coordinate. also, the coordinate should be the slowest one so that all the other degrees of freedom can easily equilibrate along the reaction coordinate. we used a regularised sparse autoencoder, an energy-based model, to discover a crucial set of reaction coordinates. along with discovering reaction coordinates, our model also predicts the evolution of a molecular dynamics(md) trajectory. we showcased that including sparsity enforcing regularisation helps in choosing a small but important set of reaction coordinates. we used two model systems to demonstrate our approach: alanine dipeptide system and proflavine and dna system, which exhibited intercalation of proflavine into dna minor groove in an aqueous environment. we model md trajectory as a multivariate time series, and our latent variable model performs the task of multi-step time series prediction. this idea is inspired by the popular sparse coding approach - to represent each input sample as a linear combination of few elements taken from a set of representative patterns.",,2022-08-22,2024-01-13,['abhijit gupta']
2208.11665,statistical exploration of the manifold hypothesis,stat.me cs.lg stat.ml,"the manifold hypothesis is a widely accepted tenet of machine learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. this phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern ai technologies. we show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -- the latent metric model -- via elementary concepts such as latent variables, correlation and stationarity. this establishes a general statistical explanation for why the manifold hypothesis seems to hold in so many situations. informed by the latent metric model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism. these procedures operate under minimal assumptions and make use of well known, scaleable graph-analytic algorithms.",,2022-08-24,2024-02-09,"['nick whiteley', 'annie gray', 'patrick rubin-delanchy']"
2208.13219,visualizing high-dimensional loss landscapes with hessian directions,cs.lg cs.cv stat.co stat.ml,"analyzing geometric properties of high-dimensional loss functions, such as local curvature and the existence of other optima around a certain point in loss space, can help provide a better understanding of the interplay between neural network structure, implementation attributes, and learning performance. in this work, we combine concepts from high-dimensional probability and differential geometry to study how curvature properties in lower-dimensional loss representations depend on those in the original loss space. we show that saddle points in the original space are rarely correctly identified as such in expected lower-dimensional representations if random projections are used. the principal curvature in the expected lower-dimensional representation is proportional to the mean curvature in the original loss space. hence, the mean curvature in the original loss space determines if saddle points appear, on average, as either minima, maxima, or almost flat regions. we use the connection between expected curvature in random projections and mean curvature in the original space (i.e., the normalized hessian trace) to compute hutchinson-type trace estimates without calculating hessian-vector products as in the original hutchinson method. because random projections are not suitable to correctly identify saddle information, we propose to study projections along dominant hessian directions that are associated with the largest and smallest principal curvatures. we connect our findings to the ongoing debate on loss landscape flatness and generalizability. finally, for different common image classifiers and a function approximator, we show and compare random and hessian projections of loss landscapes with up to about $7\times 10^6$ parameters.",10.1088/1742-5468/ad13fc,2022-08-28,2023-12-01,"['lucas böttcher', 'gregory wheeler']"
2208.13643,a variance-reduced stochastic gradient tracking algorithm for   decentralized optimization with orthogonality constraints,math.oc cs.lg cs.sy eess.sy stat.ml,"decentralized optimization with orthogonality constraints is found widely in scientific computing and data science. since the orthogonality constraints are nonconvex, it is quite challenging to design efficient algorithms. existing approaches leverage the geometric tools from riemannian optimization to solve this problem at the cost of high sample and communication complexities. to relieve this difficulty, based on two novel techniques that can waive the orthogonality constraints, we propose a variance-reduced stochastic gradient tracking (vrsgt) algorithm with the convergence rate of $o(1 / k)$ to a stationary point. to the best of our knowledge, vrsgt is the first algorithm for decentralized optimization with orthogonality constraints that reduces both sampling and communication complexities simultaneously. in the numerical experiments, vrsgt has a promising performance in a real-world autonomous driving application.",10.3934/jimo.2023018,2022-08-29,,"['lei wang', 'xin liu']"
2208.14919,arma cell: a modular and effective approach for neural autoregressive   modeling,cs.lg cs.ne stat.ml,"the autoregressive moving average (arma) model is a classical, and arguably one of the most studied approaches to model time series data. it has compelling theoretical properties and is widely used among practitioners. more recent deep learning approaches popularize recurrent neural networks (rnns) and, in particular, long short-term memory (lstm) cells that have become one of the best performing and most common building blocks in neural time series modeling. while advantageous for time series data or sequences with long-term effects, complex rnn cells are not always a must and can sometimes even be inferior to simpler recurrent approaches. in this work, we introduce the arma cell, a simpler, modular, and effective approach for time series modeling in neural networks. this cell can be used in any neural network architecture where recurrent structures are present and naturally handles multivariate time series using vector autoregression. we also introduce the convarma cell as a natural successor for spatially-correlated time series. our experiments show that the proposed methodology is competitive with popular alternatives in terms of performance while being more robust and compelling due to its simplicity",,2022-08-31,2024-01-11,"['philipp schiele', 'christoph berninger', 'david rügamer']"
2209.01172,an interpretable and efficient infinite-order vector autoregressive   model for high-dimensional time series,stat.me stat.ml,"as a special infinite-order vector autoregressive (var) model, the vector autoregressive moving average (varma) model can capture much richer temporal patterns than the widely used finite-order var model. however, its practicality has long been hindered by its non-identifiability, computational intractability, and difficulty of interpretation, especially for high-dimensional time series. this paper proposes a novel sparse infinite-order var model for high-dimensional time series, which avoids all above drawbacks while inheriting essential temporal patterns of the varma model. as another attractive feature, the temporal and cross-sectional structures of the varma-type dynamics captured by this model can be interpreted separately, since they are characterized by different sets of parameters. this separation naturally motivates the sparsity assumption on the parameters determining the cross-sectional dependence. as a result, greater statistical efficiency and interpretability can be achieved with little loss of temporal information. we introduce two $\ell_1$-regularized estimation methods for the proposed model, which can be efficiently implemented via block coordinate descent algorithms, and derive the corresponding nonasymptotic error bounds. a consistent model order selection method based on the bayesian information criteria is also developed. the merit of the proposed approach is supported by simulation studies and a real-world macroeconomic data analysis.",,2022-09-02,2024-02-24,['yao zheng']
2209.02935,normalised clustering accuracy: an asymmetric external cluster validity   measure,cs.lg stat.ml,"there is no, nor will there ever be, single best clustering algorithm, but we would still like to be able to distinguish between methods which work well on certain task types and those that systematically underperform. clustering algorithms are traditionally evaluated using either internal or external validity measures. internal measures quantify different aspects of the obtained partitions, e.g., the average degree of cluster compactness or point separability. yet, their validity is questionable, because the clusterings they promote can sometimes be meaningless. external measures, on the other hand, compare the algorithms' outputs to the fixed ground truth groupings that are provided by experts. in this paper, we argue that the commonly-used classical partition similarity scores, such as the normalised mutual information, fowlkes--mallows, or adjusted rand index, miss some desirable properties. in particular, they do not identify worst-case scenarios correctly nor are they easily interpretable. as a consequence, it can be difficult to evaluate clustering algorithms on diverse benchmark datasets. to remedy these issues, we propose and analyse a new measure: a version of the optimal set-matching accuracy, which is normalised, monotonic with respect to some similarity relation, scale invariant, and corrected for the imbalancedness of cluster sizes (but neither symmetric nor adjusted for chance).",,2022-09-07,2024-01-13,['marek gagolewski']
2209.03597,a penalized criterion for selecting the number of clusters for k-medians,math.st stat.th,"clustering is a usual unsupervised machine learning technique for grouping the data points into groups based upon similar features. we focus here on unsupervised clustering for contaminated data, i.e in the case where k-medians should be preferred to k-means because of its robustness. more precisely, we concentrate on a common question in clustering: how to chose the number of clusters? the answer proposed here is to consider the choice of the optimal number of clusters as the minimization of a risk function via penalization. in this paper, we obtain a suitable penalty shape for our criterion and derive an associated oracle-type inequality. finally, the performance of this approach with different types of k-medians algorithms is compared on a simulation study with other popular techniques. all studied algorithms are available in the r package kmedians on cran.",,2022-09-08,2024-02-27,"['antoine godichon-baggioni', 'sobihan surendran']"
2209.04329,heterogeneous treatment effect bounds under sample selection with an   application to the effects of social media on political polarization,econ.em stat.ml,"we propose a method for estimation and inference for bounds for heterogeneous causal effect parameters in general sample selection models where the treatment can affect whether an outcome is observed and no exclusion restrictions are available. the method provides conditional effect bounds as functions of policy relevant pre-treatment variables. it allows for conducting valid statistical inference on the unidentified conditional effects. we use a flexible debiased/double machine learning approach that can accommodate non-linear functional forms and high-dimensional confounders. easily verifiable high-level conditions for estimation, misspecification robust confidence intervals, and uniform confidence bands are provided as well. we re-analyze data from a large scale field experiment on facebook on counter-attitudinal news subscription with attrition. our method yields substantially tighter effect bounds compared to conventional methods and suggests depolarization effects for younger users.",,2022-09-09,2024-01-20,['phillip heiler']
2209.05550,mathematical framework for online social media auditing,cs.lg cs.si stat.co stat.ml,"social media platforms (smps) leverage algorithmic filtering (af) as a means of selecting the content that constitutes a user's feed with the aim of maximizing their rewards. selectively choosing the contents to be shown on the user's feed may yield a certain extent of influence, either minor or major, on the user's decision-making, compared to what it would have been under a natural/fair content selection. as we have witnessed over the past decade, algorithmic filtering can cause detrimental side effects, ranging from biasing individual decisions to shaping those of society as a whole, for example, diverting users' attention from whether to get the covid-19 vaccine or inducing the public to choose a presidential candidate. the government's constant attempts to regulate the adverse effects of af are often complicated, due to bureaucracy, legal affairs, and financial considerations. on the other hand smps seek to monitor their own algorithmic activities to avoid being fined for exceeding the allowable threshold. in this paper, we mathematically formalize this framework and utilize it to construct a data-driven statistical auditing procedure to regulate af from deflecting users' beliefs over time, along with sample complexity guarantees. this state-of-the-art algorithm can be used either by authorities acting as external regulators or by smps for self-auditing.",,2022-09-12,2024-02-20,"['wasim huleihel', 'yehonathan refael']"
2209.06950,lossy image compression with conditional diffusion models,eess.iv cs.cv cs.lg stat.ml,"this paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. the approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. in contrast to vae-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. our approach thus introduces an additional ``content'' latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. the remaining ``texture'' variables characterizing the diffusion process are synthesized at decoding time. we show that the model's performance can be tuned toward perceptual metrics of interest. our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported fid scores than the gan-based model, while also yielding competitive performance with vae-based models in several distortion metrics. furthermore, training the diffusion with $\mathcal{x}$-parameterization enables high-quality reconstructions in only a handful of decoding steps, greatly affecting the model's practicality. our code is available at: \url{https://github.com/buggyyang/cdc_compression}",,2022-09-14,2024-01-02,"['ruihan yang', 'stephan mandt']"
2209.07481,variational representations of annealing paths: bregman information   under monotonic embedding,cs.lg cs.it math.it math.st stat.ml stat.th,"markov chain monte carlo methods for sampling from complex distributions and estimating normalization constants often simulate samples from a sequence of intermediate distributions along an annealing path, which bridges between a tractable initial distribution and a target density of interest. prior works have constructed annealing paths using quasi-arithmetic means, and interpreted the resulting intermediate densities as minimizing an expected divergence to the endpoints. to analyze these variational representations of annealing paths, we extend known results showing that the arithmetic mean over arguments minimizes the expected bregman divergence to a single representative point. in particular, we obtain an analogous result for quasi-arithmetic means, when the inputs to the bregman divergence are transformed under a monotonic embedding function. our analysis highlights the interplay between quasi-arithmetic means, parametric families, and divergence functionals using the rho-tau representational bregman divergence framework, and associates common divergence functionals with intermediate densities along an annealing path.",10.1007/s41884-023-00129-6,2022-09-15,2024-02-06,"['rob brekelmans', 'frank nielsen']"
2209.08411,dynaconf: dynamic forecasting of non-stationary time series,cs.lg stat.ml,"deep learning has shown impressive results in a variety of time series forecasting tasks, where modeling the conditional distribution of the future given the past is the essence. however, when this conditional distribution is non-stationary, it poses challenges for these models to learn consistently and to predict accurately. in this work, we propose a new method to model non-stationary conditional distributions over time by clearly decoupling stationary conditional distribution modeling from non-stationary dynamics modeling. our method is based on a bayesian dynamic model that can adapt to conditional distribution changes and a deep conditional distribution model that handles multivariate time series using a factorized output space. our experimental results on synthetic and real-world datasets show that our model can adapt to non-stationary time series better than state-of-the-art deep learning solutions.",,2022-09-17,2024-02-24,"['siqi liu', 'andreas lehrmann']"
2209.11924,interventional causal representation learning,stat.ml cs.lg,"causal representation learning seeks to extract high-level latent factors from low-level sensory data. most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. however, interventional data is prevalent across applications. can interventional data facilitate causal representation learning? we explore this question in this paper. the key observation is that interventional data often carries geometric signatures of the latent factors' support (i.e. what values each latent can possibly take). for example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents' support and their ancestors'. leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect $do$ interventions. moreover, we can achieve block affine identification, namely the estimated latent factors are only entangled with a few other latents if we have access to data from imperfect interventions. these results highlight the unique power of interventional data in causal representation learning; they can enable provable identification of latent factors without any assumptions about their distributions or dependency structure.",,2022-09-24,2024-02-22,"['kartik ahuja', 'divyat mahajan', 'yixin wang', 'yoshua bengio']"
2209.14846,modeling and learning on high-dimensional matrix-variate sequences,stat.me,"we propose a new matrix factor model, named radfam, which is strictly derived based on the general rank decomposition and assumes a structure of a high-dimensional vector factor model for each basis vector. radfam contributes a novel class of low-rank latent structure that makes tradeoff between signal intensity and dimension reduction from the perspective of tensor subspace. based on the intrinsic separable covariance structure of radfam, for a collection of matrix-valued observations, we derive a new class of pca variants for estimating loading matrices, and sequentially the latent factor matrices. the peak signal-to-noise ratio of radfam is proved to be superior in the category of pca-type estimations. we also establish the asymptotic theory including the consistency, convergence rates, and asymptotic distributions for components in the signal part. numerically, we demonstrate the performance of radfam in applications such as matrix reconstruction, supervised learning, and clustering, on uncorrelated and correlated data, respectively.",,2022-09-29,2024-02-12,"['xu zhang', 'catherine c. liu', 'jianhua guo', 'k. c. yuen', 'a. h. welsh']"
2210.00822,combinatorial and algebraic perspectives on the marginal independence   structure of bayesian networks,stat.me math.ag math.co math.st stat.ml stat.th,"we consider the problem of estimating the marginal independence structure of a bayesian network from observational data, learning an undirected graph we call the unconditional dependence graph. we show that unconditional dependence graphs of bayesian networks correspond to the graphs having equal independence and intersection numbers. using this observation, a gr\""obner basis for a toric ideal associated to unconditional dependence graphs of bayesian networks is given and then extended by additional binomial relations to connect the space of all such graphs. an mcmc method, called grues (gr\""obner-based unconditional equivalence search), is implemented based on the resulting moves and applied to synthetic gaussian data. grues recovers the true marginal independence structure via a penalized maximum likelihood or map estimate at a higher rate than simple independence tests while also yielding an estimate of the posterior, for which the $20\%$ hpd credible sets include the true structure at a high rate for data-generating graphs with density at least $0.5$.",,2022-10-03,2024-01-31,"['danai deligeorgaki', 'alex markham', 'pratik misra', 'liam solus']"
2210.03728,dynamic latent separation for deep learning,cs.lg cs.ai stat.ml,"a core problem in machine learning is to learn expressive latent variables for model prediction on complex data that involves multiple sub-components in a flexible and interpretable fashion. here, we develop an approach that improves expressiveness, provides partial interpretation, and is not restricted to specific applications. the key idea is to dynamically distance data samples in the latent space and thus enhance the output diversity. our dynamic latent separation method, inspired by atomic physics, relies on the jointly learned structures of each data sample, which also reveal the importance of each sub-component for distinguishing data samples. this approach, atom modeling, requires no supervision of the latent space and allows us to learn extra partially interpretable representations besides the original goal of a model. we empirically demonstrate that the algorithm also enhances the performance of small to larger-scale models in various classification and generation problems.",,2022-10-07,2024-02-11,"['yi-lin tuan', 'zih-yun chiu', 'william yang wang']"
2210.05021,"the good, the bad and the ugly sides of data augmentation: an implicit   spectral regularization perspective",cs.lg stat.ml,"data augmentation (da) is a powerful workhorse for bolstering performance in modern machine learning. specific augmentations like translations and scaling in computer vision are traditionally believed to improve generalization by generating new (artificial) data from the same distribution. however, this traditional viewpoint does not explain the success of prevalent augmentations in modern machine learning (e.g. randomized masking, cutout, mixup), that greatly alter the training data distribution. in this work, we develop a new theoretical framework to characterize the impact of a general class of da on underparameterized and overparameterized linear model generalization. our framework reveals that da induces implicit spectral regularization through a combination of two distinct effects: a) manipulating the relative proportion of eigenvalues of the data covariance matrix in a training-data-dependent manner, and b) uniformly boosting the entire spectrum of the data covariance matrix through ridge regression. these effects, when applied to popular augmentations, give rise to a wide variety of phenomena, including discrepancies in generalization between over-parameterized and under-parameterized regimes and differences between regression and classification tasks. our framework highlights the nuanced and sometimes surprising impacts of da on generalization, and serves as a testbed for novel augmentation design.",,2022-10-10,2024-02-27,"['chi-heng lin', 'chiraag kaushik', 'eva l. dyer', 'vidya muthukumar']"
2210.06934,on the potential benefits of entropic regularization for smoothing   wasserstein estimators,stat.ml stat.ap stat.me,"this paper is focused on the study of entropic regularization in optimal transport as a smoothing method for wasserstein estimators, through the prism of the classical tradeoff between approximation and estimation errors in statistics. wasserstein estimators are defined as solutions of variational problems whose objective function involves the use of an optimal transport cost between probability measures. such estimators can be regularized by replacing the optimal transport cost by its regularized version using an entropy penalty on the transport plan. the use of such a regularization has a potentially significant smoothing effect on the resulting estimators. in this work, we investigate its potential benefits on the approximation and estimation properties of regularized wasserstein estimators. our main contribution is to discuss how entropic regularization may reach, at a lower computational cost, statistical performances that are comparable to those of un-regularized wasserstein estimators in statistical learning problems involving distributional data analysis. to this end, we present new theoretical results on the convergence of regularized wasserstein estimators. we also study their numerical performances using simulated and real data in the supervised learning problem of proportions estimation in mixture models using optimal transport.",,2022-10-13,2024-01-30,"['jérémie bigot', 'paul freulon', 'boris p. hejblum', 'arthur leclaire']"
2210.07893,numerically stable sparse gaussian processes via minimum separation   using cover trees,stat.ml cs.lg,"gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, bayesian optimization, or in latent gaussian models. within a system, the gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. in this work, we study the numerical stability of scalable sparse approximations based on inducing points. to do so, we first review numerical stability, and illustrate typical situations in which gaussian process models can be unstable. building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. for low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. this is done via a modification of the cover tree data structure, which is of independent interest. we additionally propose an alternative sparse approximation for regression with a gaussian likelihood which trades off a small amount of performance to further improve stability. we provide illustrative examples showing the relationship between stability of calculations and predictive performance of inducing point methods on spatial tasks.",,2022-10-14,2024-01-16,"['alexander terenin', 'david r. burt', 'artem artemev', 'seth flaxman', 'mark van der wilk', 'carl edward rasmussen', 'hong ge']"
2210.09745,transfer learning with affine model transformation,stat.ml cs.lg,"supervised transfer learning has received considerable attention due to its potential to boost the predictive power of machine learning in scenarios where data are scarce. generally, a given set of source models and a dataset from a target domain are used to adapt the pre-trained models to a target domain by statistically learning domain shift and domain-specific factors. while such procedurally and intuitively plausible methods have achieved great success in a wide range of real-world applications, the lack of a theoretical basis hinders further methodological development. this paper presents a general class of transfer learning regression called affine model transfer, following the principle of expected-square loss minimization. it is shown that the affine model transfer broadly encompasses various existing methods, including the most common procedure based on neural feature extractors. furthermore, the current paper clarifies theoretical properties of the affine model transfer such as generalization error and excess risk. through several case studies, we demonstrate the practical benefits of modeling and estimating inter-domain commonality and domain-specific factors separately with the affine-type transfer models.",,2022-10-18,2024-01-19,"['shunya minami', 'kenji fukumizu', 'yoshihiro hayashi', 'ryo yoshida']"
2210.09921,finite-time analysis of single-timescale actor-critic,cs.lg math.oc stat.ml,"actor-critic methods have achieved significant success in many challenging applications. however, its finite-time convergence is still poorly understood in the most practical single-timescale form. existing works on analyzing single-timescale actor-critic have been limited to i.i.d. sampling or tabular setting for simplicity. we investigate the more practical online single-timescale actor-critic algorithm on continuous state space, where the critic assumes linear function approximation and updates with a single markovian sample per actor step. previous analysis has been unable to establish the convergence for such a challenging scenario. we demonstrate that the online single-timescale actor-critic method provably finds an $\epsilon$-approximate stationary point with $\widetilde{\mathcal{o}}(\epsilon^{-2})$ sample complexity under standard assumptions, which can be further improved to $\mathcal{o}(\epsilon^{-2})$ under the i.i.d. sampling. our novel framework systematically evaluates and controls the error propagation between the actor and critic. it offers a promising approach for analyzing other single-timescale reinforcement learning algorithms as well.",,2022-10-18,2024-01-26,"['xuyang chen', 'lin zhao']"
2210.09929,differentially private diffusion models,stat.ml cs.cr cs.lg,"while modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. generative models trained with differential privacy (dp) on sensitive data can sidestep this challenge, providing access to synthetic data instead. we build on the recent success of diffusion models (dms) and introduce differentially private diffusion models (dpdms), which enforce privacy using differentially private stochastic gradient descent (dp-sgd). we investigate the dm parameterization and the sampling algorithm, which turn out to be crucial ingredients in dpdms, and propose noise multiplicity, a powerful modification of dp-sgd tailored to the training of dms. we validate our novel dpdms on image generation benchmarks and achieve state-of-the-art performance in all experiments. moreover, on standard benchmarks, classifiers trained on dpdm-generated synthetic data perform on par with task-specific dp-sgd-trained classifiers, which has not been demonstrated before for dp generative models. project page and code: https://nv-tlabs.github.io/dpdm.",,2022-10-18,2023-12-30,"['tim dockhorn', 'tianshi cao', 'arash vahdat', 'karsten kreis']"
2210.09974,theoretical guarantees for permutation-equivariant quantum neural   networks,quant-ph cs.lg stat.ml,"despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. for instance, models based on quantum neural networks (qnns) can suffer from excessive local minima and barren plateaus in their training landscapes. recently, the nascent field of geometric quantum machine learning (gqml) has emerged as a potential solution to some of those issues. the key insight of gqml is that one should design architectures, such as equivariant qnns, encoding the symmetries of the problem at hand. here, we focus on problems with permutation symmetry (i.e., the group of symmetry $s_n$), and show how to build $s_n$-equivariant qnns. we provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and generalize well from small amounts of data. to verify our results, we perform numerical simulations for a graph state classification task. our work provides the first theoretical guarantees for equivariant qnns, thus indicating the extreme power and potential of gqml.",10.1038/s41534-024-00804-1,2022-10-18,2024-02-13,"['louis schatzki', 'martin larocca', 'quynh t. nguyen', 'frederic sauvage', 'm. cerezo']"
2210.10171,doubly-robust and heteroscedasticity-aware sample trimming for causal   inference,stat.me,"a popular method for variance reduction in observational causal inference is propensity-based trimming, the practice of removing units with extreme propensities from the sample. this practice has theoretical grounding when the data are homoscedastic and the propensity model is parametric (yang and ding, 2018; crump et al. 2009), but in modern settings where heteroscedastic data are analyzed with non-parametric models, existing theory fails to support current practice. in this work, we address this challenge by developing new methods and theory for sample trimming. our contributions are three-fold: first, we describe novel procedures for selecting which units to trim. our procedures differ from previous work in that we trim not only units with small propensities, but also units with extreme conditional variances. second, we give new theoretical guarantees for inference after trimming. in particular, we show how to perform inference on the trimmed subpopulation without requiring that our regressions converge at parametric rates. instead, we make only fourth-root rate assumptions like those in the double machine learning literature. this result applies to conventional propensity-based trimming as well and thus may be of independent interest. finally, we propose a bootstrap-based method for constructing simultaneously valid confidence intervals for multiple trimmed sub-populations, which are valuable for navigating the trade-off between sample size and variance reduction inherent in trimming. we validate our methods in simulation, on the 2007-2008 national health and nutrition examination survey, and on a semi-synthetic medicare dataset and find promising results in all settings.",,2022-10-18,2024-01-29,"['samir khan', 'johan ugander']"
2210.11049,how does a deep learning model architecture impact its privacy? a   comprehensive study of privacy attacks on cnns and transformers,cs.cr cs.ai cs.lg stat.ml,"as a booming research area in the past decade, deep learning technologies have been driven by big data collected and processed on an unprecedented scale. however, privacy concerns arise due to the potential leakage of sensitive information from the training data. recent research has revealed that deep learning models are vulnerable to various privacy attacks, including membership inference attacks, attribute inference attacks, and gradient inversion attacks. notably, the efficacy of these attacks varies from model to model. in this paper, we answer a fundamental question: does model architecture affect model privacy? by investigating representative model architectures from convolutional neural networks (cnns) to transformers, we demonstrate that transformers generally exhibit higher vulnerability to privacy attacks than cnns. additionally, we identify the micro design of activation layers, stem layers, and ln layers, as major factors contributing to the resilience of cnns against privacy attacks, while the presence of attention modules is another main factor that exacerbates the privacy vulnerability of transformers. our discovery reveals valuable insights for deep learning models to defend against privacy attacks and inspires the research community to develop privacy-friendly model architectures.",,2022-10-20,2024-02-02,"['guangsheng zhang', 'bo liu', 'huan tian', 'tianqing zhu', 'ming ding', 'wanlei zhou']"
2210.11327,improving data quality with training dynamics of gradient boosting   decision trees,cs.lg stat.ml,"real world datasets contain incorrectly labeled instances that hamper the performance of the model and, in particular, the ability to generalize out of distribution. also, each example might have different contribution towards learning. this motivates studies to better understanding of the role of data instances with respect to their contribution in good metrics in models. in this paper we propose a method based on metrics computed from training dynamics of gradient boosting decision trees (gbdts) to assess the behavior of each training example. we focus on datasets containing mostly tabular or structured data, for which the use of decision trees ensembles are still the state-of-the-art in terms of performance. our methods achieved the best results overall when compared with confident learning, direct heuristics and a robust boosting algorithm. we show results on detecting noisy labels in order clean datasets, improving models' metrics in synthetic and real public datasets, as well as on a industry case in which we deployed a model based on the proposed solution.",,2022-10-20,2024-02-22,"['moacir antonelli ponti', 'lucas de angelis oliveira', 'mathias esteban', 'valentina garcia', 'juan martín román', 'luis argerich']"
2210.12061,validation of composite systems by discrepancy propagation,cs.lg stat.ml,"assessing the validity of a real-world system with respect to given quality criteria is a common yet costly task in industrial applications due to the vast number of required real-world tests. validating such systems by means of simulation offers a promising and less expensive alternative, but requires an assessment of the simulation accuracy and therefore end-to-end measurements. additionally, covariate shifts between simulations and actual usage can cause difficulties for estimating the reliability of such systems. in this work, we present a validation method that propagates bounds on distributional discrepancy measures through a composite system, thereby allowing us to derive an upper bound on the failure probability of the real system from potentially inaccurate simulations. each propagation step entails an optimization problem, where -- for measures such as maximum mean discrepancy (mmd) -- we develop tight convex relaxations based on semidefinite programs. we demonstrate that our propagation method yields valid and useful bounds for composite systems exhibiting a variety of realistic effects. in particular, we show that the proposed method can successfully account for data shifts within the experimental design as well as model inaccuracies within the simulation.",,2022-10-21,2024-01-03,"['david reeb', 'kanil patel', 'karim barsim', 'martin schiegg', 'sebastian gerwinn']"
2210.13386,contraction of locally differentially private mechanisms,cs.it cs.cr math.it math.st stat.ml stat.th,"we investigate the contraction properties of locally differentially private mechanisms. more specifically, we derive tight upper bounds on the divergence between $pk$ and $qk$ output distributions of an $\epsilon$-ldp mechanism $k$ in terms of a divergence between the corresponding input distributions $p$ and $q$, respectively. our first main technical result presents a sharp upper bound on the $\chi^2$-divergence $\chi^2(pk}\|qk)$ in terms of $\chi^2(p\|q)$ and $\varepsilon$. we also show that the same result holds for a large family of divergences, including kl-divergence and squared hellinger distance. the second main technical result gives an upper bound on $\chi^2(pk\|qk)$ in terms of total variation distance $\mathsf{tv}(p, q)$ and $\epsilon$. we then utilize these bounds to establish locally private versions of the van trees inequality, le cam's, assouad's, and the mutual information methods, which are powerful tools for bounding minimax estimation risks. these results are shown to lead to better privacy analyses than the state-of-the-arts in several statistical problems such as entropy and discrete distribution estimation, non-parametric density estimation, and hypothesis testing.",,2022-10-24,2024-02-06,"['shahab asoodeh', 'huanyu zhang']"
2210.13785,analysis of estimating the bayes rule for gaussian mixture models with a   specified missing-data mechanism,stat.ml cs.lg,"semi-supervised learning (ssl) approaches have been successfully applied in a wide range of engineering and scientific fields. this paper investigates the generative model framework with a missingness mechanism for unclassified observations, as introduced by ahfock and mclachlan(2020). we show that in a partially classified sample, a classifier using bayes rule of allocation with a missing-data mechanism can surpass a fully supervised classifier in a two-class normal homoscedastic model, especially with moderate to low overlap and proportion of missing class labels, or with large overlap but few missing labels. it also outperforms a classifier with no missing-data mechanism regardless of the overlap region or the proportion of missing class labels. our exploration of two- and three-component normal mixture models with unequal covariances through simulations further corroborates our findings. finally, we illustrate the use of the proposed classifier with a missing-data mechanism on interneuronal and skin lesion datasets.",,2022-10-25,2023-12-29,['ziyang lyu']
2210.13954,i prefer not to say: protecting user consent in models with optional   personal data,cs.lg cs.ai cs.cy stat.ml,"we examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. some users consent to their data being used whereas others object and keep their data undisclosed. in this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users' privacy. this observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. to address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. this excludes implicit information contained in the decision to share data or not. we offer the first solution to this problem by proposing the notion of protected user consent (puc), which we prove to be loss-optimal under our protection requirement. we observe that privacy and performance are not fundamentally at odds with each other and that it is possible for a decision maker to benefit from additional data while respecting users' consent. to learn puc-compliant models, we devise a model-agnostic data augmentation strategy with finite sample convergence guarantees. finally, we analyze the implications of puc on challenging real datasets, tasks, and models.",,2022-10-25,2024-02-02,"['tobias leemann', 'martin pawelczyk', 'christian thomas eberle', 'gjergji kasneci']"
2210.14051,bridging distributional and risk-sensitive reinforcement learning with   provable regret bounds,cs.lg cs.ai stat.ml,"we study the regret guarantee for risk-sensitive reinforcement learning (rsrl) via distributional reinforcement learning (drl) methods. in particular, we consider finite episodic markov decision processes whose objective is the entropic risk measure (entrm) of return. by leveraging a key property of the entrm, the independence property, we establish the risk-sensitive distributional dynamic programming framework. we then propose two novel drl algorithms that implement optimism through two different schemes, including a model-free one and a model-based one.   we prove that they both attain $\tilde{\mathcal{o}}(\frac{\exp(|\beta| h)-1}{|\beta|}h\sqrt{s^2ak})$ regret upper bound, where $s$, $a$, $k$, and $h$ represent the number of states, actions, episodes, and the time horizon, respectively. it matches rsvi2 proposed in \cite{fei2021exponential}, with novel distributional analysis. to the best of our knowledge, this is the first regret analysis that bridges drl and rsrl in terms of sample complexity.   acknowledging the computational inefficiency associated with the model-free drl algorithm, we propose an alternative drl algorithm with distribution representation. this approach not only maintains the established regret bounds but also significantly amplifies computational efficiency.   we also prove a tighter minimax lower bound of $\omega(\frac{\exp(\beta h/6)-1}{\beta h}h\sqrt{sat})$ for the $\beta>0$ case, which recovers the tight lower bound $\omega(h\sqrt{sat})$ in the risk-neutral setting.",,2022-10-25,2024-01-25,"['hao liang', 'zhi-quan luo']"
2210.14080,learning individual treatment effects under heterogeneous interference   in networks,cs.lg cs.ai cs.si stat.me,"estimates of individual treatment effects from networked observational data are attracting increasing attention these days. one major challenge in network scenarios is the violation of the stable unit treatment value assumption (sutva), which assumes that the treatment assignment of a unit does not influence others' outcomes. in network data, due to interference, the outcome of a unit is influenced not only by its treatment (i.e., direct effects) but also by others' treatments (i.e., spillover effects). furthermore, the influences from other units are always heterogeneous (e.g., friends with similar interests affect a person differently than friends with different interests). in this paper, we focus on the problem of estimating individual treatment effects (both direct and spillover effects) under heterogeneous interference. to address this issue, we propose a novel dual weighting regression (dwr) algorithm by simultaneously learning attention weights that capture the heterogeneous interference and sample weights to eliminate the complex confounding bias in networks. we formulate the entire learning process as a bi-level optimization problem. in theory, we present generalization error bounds for individual treatment effect estimation. extensive experiments on four benchmark datasets demonstrate that the proposed dwr algorithm outperforms state-of-the-art methods for estimating individual treatment effects under heterogeneous interference.",,2022-10-25,2024-01-25,"['ziyu zhao', 'yuqi bai', 'kun kuang', 'ruoxuan xiong', 'fei wu']"
2210.14484,imputation of missing values in multi-view data,stat.ml cs.lg stat.me,"data for which a set of objects is described by multiple distinct feature sets (called views) is known as multi-view data. when missing values occur in multi-view data, all features in a view are likely to be missing simultaneously. this leads to very large quantities of missing data which, especially when combined with high-dimensionality, makes the application of conditional imputation methods computationally infeasible. we introduce a new imputation method based on the existing stacked penalized logistic regression (staplr) algorithm for multi-view learning. it performs imputation in a dimension-reduced space to address computational challenges inherent to the multi-view context. we compare the performance of the new imputation method with several existing imputation algorithms in simulated data sets. the results show that the new imputation method leads to competitive results at a much lower computational cost, and makes the use of advanced imputation algorithms such as missforest and predictive mean matching possible in settings where they would otherwise be computationally infeasible.",,2022-10-26,2024-02-29,"['wouter van loon', 'marjolein fokkema', 'frank de vos', 'marisa koini', 'reinhold schmidt', 'mark de rooij']"
2210.16311,simultaneous off-the-grid learning of mixtures issued from a continuous   dictionary,stat.ml cs.lg math.pr math.st stat.th,"in this paper we observe a set, possibly a continuum, of signals corrupted by noise. each signal is a finite mixture of an unknown number of features belonging to a continuous dictionary. the continuous dictionary is parametrized by a real non-linear parameter. we shall assume that the signals share an underlying structure by assuming that each signal has its active features included in a finite and sparse set. we formulate regularized optimization problem to estimate simultaneously the linear coefficients in the mixtures and the non-linear parameters of the features. the optimization problem is composed of a data fidelity term and a $(\ell_1,l^p)$-penalty. we call its solution the group-nonlinear-lasso and provide high probability bounds on the prediction error using certificate functions. following recent works on the geometry of off-the-grid methods, we show that such functions can be constructed provided the parameters of the active features are pairwise separated by a constant with respect to a riemannian metric.when the number of signals is finite and the noise is assumed gaussian, we give refinements of our results for $p=1$ and $p=2$ using tail bounds on suprema of gaussian and $\chi^2$ random processes. when $p=2$, our prediction error reaches the rates obtained by the group-lasso estimator in the multi-task linear regression model. furthermore, for $p=2$ these prediction rates are faster than for $p=1$ when all signals share most of the non-linear parameters.",,2022-10-27,2024-02-23,"['cristina butucea', 'jean-françois delmas', 'anne dutfoy', 'clément hardy']"
2211.00450,"birth-death dynamics for sampling: global convergence, approximations   and their asymptotics",math.ap math.pr stat.ml,"motivated by the challenge of sampling gibbs measures with nonconvex potentials, we study a continuum birth-death dynamics. we improve results in previous works [51,57] and provide weaker hypotheses under which the probability density of the birth-death governed by kullback-leibler divergence or by $\chi^2$ divergence converge exponentially fast to the gibbs equilibrium measure, with a universal rate that is independent of the potential barrier. to build a practical numerical sampler based on the pure birth-death dynamics, we consider an interacting particle system, which is inspired by the gradient flow structure and the classical fokker-planck equation and relies on kernel-based approximations of the measure. using the technique of $\gamma$-convergence of gradient flows, we show that on the torus, smooth and bounded positive solutions of the kernelized dynamics converge on finite time intervals, to the pure birth-death dynamics as the kernel bandwidth shrinks to zero. moreover we provide quantitative estimates on the bias of minimizers of the energy corresponding to the kernelized dynamics. finally we prove the long-time asymptotic results on the convergence of the asymptotic states of the kernelized dynamics towards the gibbs measure.",10.1088/1361-6544/acf988,2022-11-01,2023-08-14,"['yulong lu', 'dejan slepčev', 'lihan wang']"
2211.01345,generative machine learning methods for multivariate ensemble   post-processing,physics.ao-ph cs.lg stat.me,"ensemble weather forecasts based on multiple runs of numerical weather prediction models typically show systematic errors and require post-processing to obtain reliable forecasts. accurately modeling multivariate dependencies is crucial in many practical applications, and various approaches to multivariate post-processing have been proposed where ensemble predictions are first post-processed separately in each margin and multivariate dependencies are then restored via copulas. these two-step methods share common key limitations, in particular the difficulty to include additional predictors in modeling the dependencies. we propose a novel multivariate post-processing method based on generative machine learning to address these challenges. in this new class of nonparametric data-driven distributional regression models, samples from the multivariate forecast distribution are directly obtained as output of a generative neural network. the generative model is trained by optimizing a proper scoring rule which measures the discrepancy between the generated and observed data, conditional on exogenous input variables. our method does not require parametric assumptions on univariate distributions or multivariate dependencies and allows for incorporating arbitrary predictors. in two case studies on multivariate temperature and wind speed forecasting at weather stations over germany, our generative model shows significant improvements over state-of-the-art methods and particularly improves the representation of spatial dependencies.",10.1214/23-aoas1784,2022-09-26,2024-02-01,"['jieyu chen', 'tim janke', 'florian steinke', 'sebastian lerch']"
2211.02920,gmgm: a fast multi-axis gaussian graphical model,stat.ml cs.lg,"this paper introduces the gaussian multi-graphical model, a model to construct sparse graph representations of matrix- and tensor-variate data. we generalize prior work in this area by simultaneously learning this representation across several tensors that share axes, which is necessary to allow the analysis of multimodal datasets such as those encountered in multi-omics. our algorithm uses only a single eigendecomposition per axis, achieving an order of magnitude speedup over prior work in the ungeneralized case. this allows the use of our methodology on large multi-modal datasets such as single-cell multi-omics data, which was challenging with previous approaches. we validate our model on synthetic data and five real-world datasets.",,2022-11-05,2024-02-27,"['bailey andrew', 'david westhead', 'luisa cutillo']"
2211.03595,from denoising diffusions to denoising markov models,stat.ml cs.lg,denoising diffusions are state-of-the-art generative models exhibiting remarkable empirical performance. they work by diffusing the data distribution into a gaussian distribution and then learning to reverse this noising process to obtain synthetic datapoints. the denoising diffusion relies on approximations of the logarithmic derivatives of the noised data densities using score matching. such models can also be used to perform approximate posterior simulation when one can only sample from the prior and likelihood. we propose a unifying framework generalising this approach to a wide class of spaces and leading to an original extension of score matching. we illustrate the resulting models on various applications.,,2022-11-07,2024-02-18,"['joe benton', 'yuyang shi', 'valentin de bortoli', 'george deligiannidis', 'arnaud doucet']"
2211.03846,federated causal discovery from interventions,cs.lg cs.ma stat.me,"causal discovery serves a pivotal role in mitigating model uncertainty through recovering the underlying causal mechanisms among variables. in many practical domains, such as healthcare, access to the data gathered by individual entities is limited, primarily for privacy and regulatory constraints. however, the majority of existing causal discovery methods require the data to be available in a centralized location. in response, researchers have introduced federated causal discovery. while previous federated methods consider distributed observational data, the integration of interventional data remains largely unexplored. we propose fedcdi, a federated framework for inferring causal structures from distributed data containing interventional samples. in line with the federated learning framework, fedcdi improves privacy by exchanging belief updates rather than raw samples. additionally, it introduces a novel intervention-aware method for aggregating individual updates. we analyze scenarios with shared or disjoint intervened covariates, and mitigate the adverse effects of interventional data heterogeneity. the performance and scalability of fedcdi is rigorously tested across a variety of synthetic and real-world graphs.",,2022-11-07,2024-02-11,"['amin abyaneh', 'nino scherrer', 'patrick schwab', 'stefan bauer', 'bernhard schölkopf', 'arash mehrjou']"
2211.04659,when is momentum extragradient optimal? a polynomial-based analysis,cs.lg math.oc stat.ml,"the extragradient method has gained popularity due to its robust convergence properties for differentiable games. unlike single-objective optimization, game dynamics involve complex interactions reflected by the eigenvalues of the game vector field's jacobian scattered across the complex plane. this complexity can cause the simple gradient method to diverge, even for bilinear games, while the extragradient method achieves convergence. building on the recently proven accelerated convergence of the momentum extragradient method for bilinear games \citep{azizian2020accelerating}, we use a polynomial-based analysis to identify three distinct scenarios where this method exhibits further accelerated convergence. these scenarios encompass situations where the eigenvalues reside on the (positive) real line, lie on the real line alongside complex conjugates, or exist solely as complex conjugates. furthermore, we derive the hyperparameters for each scenario that achieve the fastest convergence rate.",,2022-11-08,2024-02-10,"['junhyung lyle kim', 'gauthier gidel', 'anastasios kyrillidis', 'fabian pedregosa']"
2211.05408,controlling moments with kernel stein discrepancies,stat.ml cs.lg stat.co,"kernel stein discrepancies (ksds) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. notable applications include the diagnosis of approximate mcmc samplers and goodness-of-fit tests for unnormalized statistical models. the present work analyzes the convergence control properties of ksds. we first show that standard ksds used for weak convergence control fail to control moment convergence. to address this limitation, we next provide sufficient conditions under which alternative diffusion ksds control both moment and weak convergence. as an immediate consequence we develop, for each $q > 0$, the first ksds known to exactly characterize $q$-wasserstein convergence.",,2022-11-10,2024-01-11,"['heishiro kanagawa', 'alessandro barp', 'arthur gretton', 'lester mackey']"
2211.06755,the chipower transformation: a valid alternative to logratio   transformations in compositional data analysis,stat.me,"the approach to analysing compositional data has been dominated by the use of logratio transformations, to ensure exact subcompositional coherence and, in some situations, exact isometry as well. a problem with this approach is that data zeros, found in most applications, have to be replaced to allow the logarithmic transformation. an alternative new approach, called the `chipower' transformation, which allows data zeros, is to combine the standardization inherent in the chi-square distance in correspondence analysis, with the essential elements of the box-cox power transformation. the chipower transformation is justified because it} defines between-sample distances that tend to logratio distances for strictly positive data as the power parameter tends to zero, and are then equivalent to transforming to logratios. for data with zeros, a value of the power can be identified that brings the chipower transformation as close as possible to a logratio transformation, without having to substitute the zeros. especially in the area of high-dimensional data, this alternative approach can present such a high level of coherence and isometry as to be a valid approach to the analysis of compositional data. furthermore, in a supervised learning context, if the compositional variables serve as predictors of a response in a modelling framework, for example generalized linear models, then the power can be used as a tuning parameter in optimizing the accuracy of prediction through cross-validation. the chipower-transformed variables have a straightforward interpretation, since they are each identified with single compositional parts, not ratios.",,2022-11-12,2024-02-28,['michael greenacre']
2211.07092,offline estimation of controlled markov chains: minimaxity and sample   complexity,stat.ml cs.lg math.st stat.th,"in this work, we study a natural nonparametric estimator of the transition probability matrices of a finite controlled markov chain. we consider an offline setting with a fixed dataset, collected using a so-called logging policy. we develop sample complexity bounds for the estimator and establish conditions for minimaxity. our statistical bounds depend on the logging policy through its mixing properties. we show that achieving a particular statistical risk bound involves a subtle and interesting trade-off between the strength of the mixing properties and the number of samples. we demonstrate the validity of our results under various examples, such as ergodic markov chains, weakly ergodic inhomogeneous markov chains, and controlled markov chains with non-stationary markov, episodic, and greedy controls. lastly, we use these sample complexity bounds to establish concomitant ones for offline evaluation of stationary markov control policies.",,2022-11-13,2024-01-26,"['imon banerjee', 'harsha honnappa', 'vinayak rao']"
2211.07245,assessing uncertainty in similarity scoring: performance & fairness in   face recognition,cs.cv cs.ai cs.lg stat.ml,"the roc curve is the major tool for assessing not only the performance but also the fairness properties of a similarity scoring function. in order to draw reliable conclusions based on empirical roc analysis, accurately evaluating the uncertainty level related to statistical versions of the roc curves of interest is absolutely necessary, especially for applications with considerable societal impact such as face recognition. in this article, we prove asymptotic guarantees for empirical roc curves of similarity functions as well as for by-product metrics useful to assess fairness. we also explain that, because the false acceptance/rejection rates are of the form of u-statistics in the case of similarity scoring, the naive bootstrap approach may jeopardize the assessment procedure. a dedicated recentering technique must be used instead. beyond the theoretical analysis carried out, various experiments using real face image datasets provide strong empirical evidence of the practical relevance of the methods promoted here, when applied to several roc-based measures such as popular fairness metrics.",,2022-11-14,2024-02-20,"['jean-rémy conti', 'stéphan clémençon']"
2211.07866,efficient estimation for longitudinal networks via adaptive merging,stat.ml cs.lg,"longitudinal network consists of a sequence of temporal edges among multiple nodes, where the temporal edges are observed in real time. it has become ubiquitous with the rise of online social platform and e-commerce, but largely under-investigated in literature. in this paper, we propose an efficient estimation framework for longitudinal network, leveraging strengths of adaptive network merging, tensor decomposition and point process. it merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. a projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. a thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the estimation error and also provides guideline for network merging under various scenarios. we further demonstrate the advantage of the proposed method through extensive numerical experiments on synthetic datasets and a militarized interstate dispute dataset.",,2022-11-14,2024-01-04,"['haoran zhang', 'junhui wang']"
2211.08262,a mixed-categorical correlation kernel for gaussian process,math.oc cs.lg stat.ml,"recently, there has been a growing interest for mixed-categorical meta-models based on gaussian process (gp) surrogates. in this setting, several existing approaches use different strategies either by using continuous kernels (e.g., continuous relaxation and gower distance based gp) or by using a direct estimation of the correlation matrix. in this paper, we present a kernel-based approach that extends continuous exponential kernels to handle mixed-categorical variables. the proposed kernel leads to a new gp surrogate that generalizes both the continuous relaxation and the gower distance based gp models. we demonstrate, on both analytical and engineering problems, that our proposed gp model gives a higher likelihood and a smaller residual error than the other kernel-based state-of-the-art models. our method is available in the open-source software smt.",10.1016/j.neucom.2023.126472,2022-11-15,2024-01-23,"['p. saves', 'y. diouane', 'n. bartoli', 't. lefebvre', 'j. morlier']"
2211.08405,multimodal generative models for bankruptcy prediction using textual   data,q-fin.rm cs.lg stat.ml,"textual data from financial filings, e.g., the management's discussion & analysis (mda) section in form 10-k, has been used to improve the prediction accuracy of bankruptcy models. in practice, however, we cannot obtain the mda section for all public companies, which limits the use of mda data in traditional bankruptcy models, as they need complete data to make predictions. the two main reasons for the lack of mda are: (i) not all companies are obliged to submit the mda and (ii) technical problems arise when crawling and scrapping the mda section. to solve this limitation, this research introduces the conditional multimodal discriminative (cmmd) model that learns multimodal representations that embed information from accounting, market, and textual data modalities. the cmmd model needs a sample with all data modalities for model training. at test time, the cmmd model only needs access to accounting and market modalities to generate multimodal representations, which are further used to make bankruptcy predictions and to generate words from the missing mda modality. with this novel methodology, it is realistic to use textual data in bankruptcy prediction models, since accounting and market data are available for all companies, unlike textual data. the empirical results of this research show that if financial regulators, or investors, were to use traditional models using mda data, they would only be able to make predictions for 60% of the companies. furthermore, the classification performance of our proposed methodology is superior to that of a large number of traditional classifier models, taking into account all the companies in our sample.",,2022-10-26,2024-02-24,"['rogelio a. mancisidor', 'kjersti aas']"
2211.08594,orthogonal polynomials approximation algorithm (opaa):a functional   analytic approach to estimating probability densities,cs.lg math.fa stat.ml,"we present the new orthogonal polynomials approximation algorithm (opaa), a parallelizable algorithm that estimates probability distributions using functional analytic approach: first, it finds a smooth functional estimate of the probability distribution, whether it is normalized or not; second, the algorithm provides an estimate of the normalizing weight; and third, the algorithm proposes a new computation scheme to compute such estimates.   a core component of opaa is a special transform of the square root of the joint distribution into a special functional space of our construct. through this transform, the evidence is equated with the $l^2$ norm of the transformed function, squared. hence, the evidence can be estimated by the sum of squares of the transform coefficients. computations can be parallelized and completed in one pass.   opaa can be applied broadly to the estimation of probability density functions. in bayesian problems, it can be applied to estimating the normalizing weight of the posterior, which is also known as the evidence, serving as an alternative to existing optimization-based methods.",,2022-11-15,2024-01-20,['lilian w. bialokozowicz']
2211.09221,the non-overlapping statistical approximation to overlapping group lasso,stat.ml cs.lg,"group lasso is a commonly used regularization method in statistical learning in which parameters are eliminated from the model according to predefined groups. however, when the groups overlap, optimizing the group lasso penalized objective can be time-consuming on large-scale problems because of the non-separability induced by the overlapping groups. this bottleneck has seriously limited the application of overlapping group lasso regularization in many modern problems, such as gene pathway selection and graphical model estimation. in this paper, we propose a separable penalty as an approximation of the overlapping group lasso penalty. thanks to the separability, the computation of regularization based on our penalty is substantially faster than that of the overlapping group lasso, especially for large-scale and high-dimensional problems. we show that the penalty is the tightest separable relaxation of the overlapping group lasso norm within the family of $\ell_{q_1}/\ell_{q_2}$ norms. moreover, we show that the estimator based on the proposed separable penalty is statistically equivalent to the one based on the overlapping group lasso penalty with respect to their error bounds and the rate-optimal performance under the squared loss. we demonstrate the faster computational time and statistical equivalence of our method compared with the overlapping group lasso in simulation examples and a classification problem of cancer tumors based on gene expression and multiple gene pathways.",,2022-11-16,2024-02-20,"['mingyu qi', 'tianxi li']"
2211.10747,exploring validation metrics for offline model-based optimisation with   diffusion models,stat.ml cs.lg,"in model-based optimisation (mbo) we are interested in using machine learning to design candidates that maximise some measure of reward with respect to a black box function called the (ground truth) oracle, which is expensive to compute since it involves executing a real world process. in offline mbo we wish to do so without assuming access to such an oracle during training or validation, with makes evaluation non-straightforward. while an approximation to the ground oracle can be trained and used in place of it during model validation to measure the mean reward over generated candidates, the evaluation is approximate and vulnerable to adversarial examples. measuring the mean reward of generated candidates over this approximation is one such `validation metric', whereas we are interested in a more fundamental question which is finding which validation metrics correlate the most with the ground truth. this involves proposing validation metrics and quantifying them over many datasets for which the ground truth is known, for instance simulated environments. this is encapsulated under our proposed evaluation framework which is also designed to measure extrapolation, which is the ultimate goal behind leveraging generative models for mbo. while our evaluation framework is model agnostic we specifically evaluate denoising diffusion models due to their state-of-the-art performance, as well as derive interesting insights such as ranking the most effective validation metrics as well as discussing important hyperparameters.",,2022-11-19,2024-01-13,"['christopher beckham', 'alexandre piche', 'david vazquez', 'christopher pal']"
2211.10805,on the pointwise behavior of recursive partitioning and its implications   for heterogeneous causal effect estimation,stat.ml cs.lg math.st stat.th,"decision tree learning is increasingly being used for pointwise inference. important applications include causal heterogenous treatment effects and dynamic policy decisions, as well as conditional quantile regression and design of experiments, where tree estimation and inference is conducted at specific values of the covariates. in this paper, we call into question the use of decision trees (trained by adaptive recursive partitioning) for such purposes by demonstrating that they can fail to achieve polynomial rates of convergence in uniform norm with non-vanishing probability, even with pruning. instead, the convergence may be arbitrarily slow or, in some important special cases, such as honest regression trees, fail completely. we show that random forests can remedy the situation, turning poor performing trees into nearly optimal procedures, at the cost of losing interpretability and introducing two additional tuning parameters. the two hallmarks of random forests, subsampling and the random feature selection mechanism, are seen to each distinctively contribute to achieving nearly optimal performance for the model class considered.",,2022-11-19,2024-02-06,"['matias d. cattaneo', 'jason m. klusowski', 'peter m. tian']"
2211.10968,statistical optimality of divide and conquer kernel-based functional   linear regression,stat.ml cs.lg,"previous analysis of regularized functional linear regression in a reproducing kernel hilbert space (rkhs) typically requires the target function to be contained in this kernel space. this paper studies the convergence performance of divide-and-conquer estimators in the scenario that the target function does not necessarily reside in the underlying rkhs. as a decomposition-based scalable approach, the divide-and-conquer estimators of functional linear regression can substantially reduce the algorithmic complexities in time and memory. we develop an integral operator approach to establish sharp finite sample upper bounds for prediction with divide-and-conquer estimators under various regularity conditions of explanatory variables and target function. we also prove the asymptotic optimality of the derived rates by building the mini-max lower bounds. finally, we consider the convergence of noiseless estimators and show that the rates can be arbitrarily fast under mild conditions.",,2022-11-20,2024-02-18,"['jiading liu', 'lei shi']"
2211.11278,optimal extended neighbourhood rule $k$ nearest neighbours ensemble,stat.ml cs.lg,"the traditional k nearest neighbor (knn) approach uses a distance formula within a spherical region to determine the k closest training observations to a test sample point. however, this approach may not work well when test point is located outside this region. moreover, aggregating many base knn learners can result in poor ensemble performance due to high classification errors. to address these issues, a new optimal extended neighborhood rule based ensemble method is proposed in this paper. this rule determines neighbors in k steps starting from the closest sample point to the unseen observation and selecting subsequent nearest data points until the required number of observations is reached. each base model is constructed on a bootstrap sample with a random subset of features, and optimal models are selected based on out-of-bag performance after building a sufficient number of models. the proposed ensemble is compared with state-of-the-art methods on 17 benchmark datasets using accuracy, cohen's kappa, and brier score (bs). the performance of the proposed method is also assessed by adding contrived features in the original data.",,2022-11-21,2024-02-15,"['amjad ali', 'zardad khan', 'dost muhammad khan', 'saeed aldahmani']"
2211.11700,high-dimensional undirected graphical models for arbitrary mixed data,stat.ml cs.lg stat.me,"graphical models are an important tool in exploring relationships between variables in complex, multivariate data. methods for learning such graphical models are well developed in the case where all variables are either continuous or discrete, including in high-dimensions. however, in many applications data span variables of different types (e.g. continuous, count, binary, ordinal, etc.), whose principled joint analysis is nontrivial. latent gaussian copula models, in which all variables are modeled as transformations of underlying jointly gaussian variables, represent a useful approach. recent advances have shown how the binary-continuous case can be tackled, but the general mixed variable type regime remains challenging. in this work, we make the simple yet useful observation that classical ideas concerning polychoric and polyserial correlations can be leveraged in a latent gaussian copula framework. building on this observation we propose flexible and scalable methodology for data with variables of entirely general mixed type. we study the key properties of the approaches theoretically and empirically, via extensive simulations as well an illustrative application to data from the uk biobank concerning covid-19 risk factors.",,2022-11-21,2024-02-14,"['konstantin göbler', 'anne miloschewski', 'mathias drton', 'sach mukherjee']"
2211.12121,least squares approximations in linear statistical inverse learning   problems,math.st cs.na math.na stat.th,"statistical inverse learning aims at recovering an unknown function $f$ from randomly scattered and possibly noisy point evaluations of another function $g$, connected to $f$ via an ill-posed mathematical model. in this paper we blend statistical inverse learning theory with the classical regularization strategy of applying finite-dimensional projections. our key finding is that coupling the number of random point evaluations with the choice of projection dimension, one can derive probabilistic convergence rates for the reconstruction error of the maximum likelihood (ml) estimator. convergence rates in expectation are derived with a ml estimator complemented with a norm-based cut-off operation. moreover, we prove that the obtained rates are minimax optimal.",,2022-11-22,2024-01-19,['tapio helin']
2211.12343,diffusion model based posterior sampling for noisy linear inverse   problems,cs.lg cs.cv cs.it math.it stat.ml,"we consider the ubiquitous linear inverse problems with additive gaussian noise and propose an unsupervised sampling approach called diffusion model based posterior sampling (dmps) to reconstruct the unknown signal from noisy linear measurements. specifically, using one diffusion model (dm) as an implicit prior, the fundamental difficulty in performing posterior sampling is that the noise-perturbed likelihood score, i.e., gradient of an annealed likelihood function, is intractable. to circumvent this problem, we introduce a simple yet effective closed-form approximation using an uninformative prior assumption. extensive experiments are conducted on a variety of noisy linear inverse problems such as noisy super-resolution, denoising, deblurring, and colorization. in all tasks, the proposed dmps demonstrates highly competitive or even better performances on various tasks while being 3 times faster than the state-of-the-art competitor diffusion posterior sampling (dps).",,2022-11-19,2024-01-24,"['xiangming meng', 'yoshiyuki kabashima']"
2211.12612,transfer learning for contextual multi-armed bandits,stat.ml cs.lg math.st stat.th,"motivated by a range of applications, we study in this paper the problem of transfer learning for nonparametric contextual multi-armed bandits under the covariate shift model, where we have data collected on source bandits before the start of the target bandit learning. the minimax rate of convergence for the cumulative regret is established and a novel transfer learning algorithm that attains the minimax regret is proposed. the results quantify the contribution of the data from the source domains for learning in the target domain in the context of nonparametric contextual multi-armed bandits.   in view of the general impossibility of adaptation to unknown smoothness, we develop a data-driven algorithm that achieves near-optimal statistical guarantees (up to a logarithmic factor) while automatically adapting to the unknown parameters over a large collection of parameter spaces under an additional self-similarity assumption. a simulation study is carried out to illustrate the benefits of utilizing the data from the auxiliary source domains for learning in the target domain.",,2022-11-22,2024-01-24,"['changxiao cai', 't. tony cai', 'hongzhe li']"
2211.12620,promises and pitfalls of threshold-based auto-labeling,cs.lg cs.ai stat.ml,"creating large-scale high-quality labeled datasets is a major bottleneck in supervised machine learning workflows. threshold-based auto-labeling (tbal), where validation data obtained from humans is used to find a confidence threshold above which the data is machine-labeled, reduces reliance on manual annotation. tbal is emerging as a widely-used solution in practice. given the long shelf-life and diverse usage of the resulting datasets, understanding when the data obtained by such auto-labeling systems can be relied on is crucial. this is the first work to analyze tbal systems and derive sample complexity bounds on the amount of human-labeled validation data required for guaranteeing the quality of machine-labeled data. our results provide two crucial insights. first, reasonable chunks of unlabeled data can be automatically and accurately labeled by seemingly bad models. second, a hidden downside of tbal systems is potentially prohibitive validation data usage. together, these insights describe the promise and pitfalls of using such systems. we validate our theoretical guarantees with extensive experiments on synthetic and real datasets.",,2022-11-22,2024-02-21,"['harit vishwakarma', 'heguang lin', 'frederic sala', 'ramya korlakai vinayak']"
2211.12653,consistency of oblique decision tree and its boosting and random forest,math.st stat.th,"classification and regression tree (cart), random forest (rf) and gradient boosting tree (gbt) are probably the most popular set of statistical learning methods. however, their statistical consistency can only be proved under very restrictive assumptions on the underlying regression function. as an extension to standard cart, the oblique decision tree (odt), which uses linear combinations of predictors as partitioning variables, has received much attention. odt tends to perform numerically better than cart and requires fewer partitions. in this paper, we show that odt is consistent for very general regression functions as long as they are continuous. then, we prove the consistency of the odt-based random forest (odrf), whether fully grown or not. finally, we propose an ensemble of gbt for regression by borrowing the technique of orthogonal matching pursuit and study its consistency under very mild conditions on the tree structure. after refining existing computer packages according to the established theory, extensive experiments on real data sets show that both our ensemble boosting trees and odrf have noticeable overall improvements over rf and other forests.",,2022-11-22,2024-01-11,"['haoran zhan', 'yu liu', 'yingcun xia']"
2211.14115,inverse feasibility in over-the-air federated learning,stat.ml cs.lg,"we introduce the concept of inverse feasibility for linear forward models as a tool to enhance ota fl algorithms. inverse feasibility is defined as an upper bound on the condition number of the forward operator as a function of its parameters. we analyze an existing ota fl model using this definition, identify areas for improvement, and propose a new ota fl model. numerical experiments illustrate the main implications of the theoretical results. the proposed framework, which is based on inverse problem theory, can potentially complement existing notions of security and privacy by providing additional desirable characteristics to networks.",,2022-11-25,2024-02-19,"['tomasz piotrowski', 'rafail ismayilov', 'matthias frey', 'renato l. g. cavalcante']"
2211.14221,learning large causal structures from inverse covariance matrix via   sparse matrix decomposition,cs.lg math.oc stat.me,"learning causal structures from observational data is a fundamental problem facing important computational challenges when the number of variables is large. in the context of linear structural equation models (sems), this paper focuses on learning causal structures from the inverse covariance matrix. the proposed method, called icid for independence-preserving decomposition from inverse covariance matrix, is based on continuous optimization of a matrix decomposition model that preserves the nonzero patterns of the inverse covariance matrix. through theoretical and empirical evidences, we show that icid efficiently identifies the sought directed acyclic graph (dag) assuming the knowledge of noise variances. moreover, icid is shown empirically to be robust under bounded misspecification of noise variances in the case where the noise variances are non-equal. the proposed method enjoys a low complexity, as reflected by its time efficiency in the experiments, and also enables a novel regularization scheme that yields highly accurate solutions on the simulated fmri data (smith et al., 2011) in comparison with state-of-the-art algorithms.",,2022-11-25,2024-02-19,"['shuyu dong', 'kento uemura', 'akito fujii', 'shuang chang', 'yusuke koyanagi', 'koji maruhashi', 'michèle sebag']"
2211.14297,doubly robust nearest neighbors in factor models,stat.ml cs.lg,"we introduce and analyze an improved variant of nearest neighbors (nn) for estimation with missing data in latent factor models. we consider a matrix completion problem with missing data, where the $(i, t)$-th entry, when observed, is given by its mean $f(u_i, v_t)$ plus mean-zero noise for an unknown function $f$ and latent factors $u_i$ and $v_t$. prior nn strategies, like unit-unit nn, for estimating the mean $f(u_i, v_t)$ relies on existence of other rows $j$ with $u_j \approx u_i$. similarly, time-time nn strategy relies on existence of columns $t'$ with $v_{t'} \approx v_t$. these strategies provide poor performance respectively when similar rows or similar columns are not available. our estimate is doubly robust to this deficit in two ways: (1) as long as there exist either good row or good column neighbors, our estimate provides a consistent estimate. (2) furthermore, if both good row and good column neighbors exist, it provides a (near-)quadratic improvement in the non-asymptotic error and admits a significantly narrower asymptotic confidence interval when compared to both unit-unit or time-time nn.",,2022-11-25,2024-01-29,"['raaz dwivedi', 'katherine tian', 'sabina tomkins', 'predrag klasnja', 'susan murphy', 'devavrat shah']"
2211.14555,distribution free prediction sets for node classification,stat.ml cs.lg,"graph neural networks (gnns) are able to achieve high classification accuracy on many important real world datasets, but provide no rigorous notion of predictive uncertainty. quantifying the confidence of gnn models is difficult due to the dependence between datapoints induced by the graph structure. we leverage recent advances in conformal prediction to construct prediction sets for node classification in inductive learning scenarios. we do this by taking an existing approach for conformal classification that relies on \textit{exchangeable} data and modifying it by appropriately weighting the conformal scores to reflect the network structure. we show through experiments on standard benchmark datasets using popular gnn models that our approach provides tighter and better calibrated prediction sets than a naive application of conformal prediction.",,2022-11-26,2024-01-08,['jase clarkson']
2211.15943,fully stochastic trust-region sequential quadratic programming for   equality-constrained optimization problems,math.oc stat.co stat.ml,"we propose a trust-region stochastic sequential quadratic programming algorithm (tr-stosqp) to solve nonlinear optimization problems with stochastic objectives and deterministic equality constraints. we consider a fully stochastic setting, where at each step a single sample is generated to estimate the objective gradient. the algorithm adaptively selects the trust-region radius and, compared to the existing line-search stosqp schemes, allows us to utilize indefinite hessian matrices (i.e., hessians without modification) in sqp subproblems. as a trust-region method for constrained optimization, our algorithm must address an infeasibility issue -- the linearized equality constraints and trust-region constraints may lead to infeasible sqp subproblems. in this regard, we propose an adaptive relaxation technique to compute the trial step, consisting of a normal step and a tangential step. to control the lengths of these two steps while ensuring a scale-invariant property, we adaptively decompose the trust-region radius into two segments, based on the proportions of the rescaled feasibility and optimality residuals to the rescaled full kkt residual. the normal step has a closed form, while the tangential step is obtained by solving a trust-region subproblem, to which a solution ensuring the cauchy reduction is sufficient for our study. we establish a global almost sure convergence guarantee for tr-stosqp, and illustrate its empirical performance on both a subset of problems in the cutest test set and constrained logistic regression problems using data from the libsvm collection.",,2022-11-29,2024-01-28,"['yuchen fang', 'sen na', 'michael w. mahoney', 'mladen kolar']"
2211.16298,double robust bayesian inference on average treatment effects,econ.em stat.me stat.ml,"we propose a double robust bayesian inference procedure on the average treatment effect (ate) under unconfoundedness. our robust bayesian approach involves two important modifications: first, we adjust the prior distributions of the conditional mean function; second, we correct the posterior distribution of the resulting ate. both adjustments make use of pilot estimators motivated by the semiparametric influence function for ate estimation. we prove asymptotic equivalence of our bayesian procedure and efficient frequentist ate estimators by establishing a new semiparametric bernstein-von mises theorem under double robustness; i.e., the lack of smoothness of conditional mean functions can be compensated by high regularity of the propensity score and vice versa. consequently, the resulting bayesian credible sets form confidence intervals with asymptotically exact coverage probability. in simulations, our double robust bayesian procedure leads to significant bias reduction of point estimation over conventional bayesian methods and more accurate coverage of confidence intervals compared to existing frequentist methods. we illustrate our method in an application to the national supported work demonstration.",,2022-11-29,2024-02-21,"['christoph breunig', 'ruixuan liu', 'zhengfei yu']"
2212.00133,generative adversarial learning of sinkhorn algorithm initializations,cs.lg math.oc stat.ml,"the sinkhorn algorithm is the state-of-the-art to approximate solutions of entropic optimal transport (ot) distances between discrete probability distributions. we show that meticulously training a neural network to learn initializations to the algorithm via the entropic ot dual problem can significantly speed up convergence, while maintaining desirable properties of the sinkhorn algorithm, such as differentiability and parallelizability. we train our predictive network in an adversarial fashion using a second, generating network and a self-supervised bootstrapping loss. the predictive network is universal in the sense that it is able to generalize to any pair of distributions of fixed dimension and cost at inference, and we prove that we can make the generating network universal in the sense that it is capable of producing any pair of distributions during training. furthermore, we show that our network can even be used as a standalone ot solver to approximate regularized transport distances to a few percent error, which makes it the first meta neural ot solver.",,2022-11-30,2024-02-01,"['jonathan geuter', 'vaios laschos']"
2212.00219,are you using test log-likelihood correctly?,stat.ml cs.lg stat.ot,"test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. we present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. specifically, our examples show that (i) approximate bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.",,2022-11-30,2024-01-18,"['sameer k. deshpande', 'soumya ghosh', 'tin d. nguyen', 'tamara broderick']"
2212.03722,optimal transport map estimation in general function spaces,math.st stat.ml stat.th,"we study the problem of estimating a function $t$ given independent samples from a distribution $p$ and from the pushforward distribution $t_\sharp p$. this setting is motivated by applications in the sciences, where $t$ represents the evolution of a physical system over time, and in machine learning, where, for example, $t$ may represent a transformation learned by a deep neural network trained for a generative modeling task. to ensure identifiability, we assume that $t = \nabla \varphi_0$ is the gradient of a convex function, in which case $t$ is known as an \emph{optimal transport map}. prior work has studied the estimation of $t$ under the assumption that it lies in a h\""older class, but general theory is lacking. we present a unified methodology for obtaining rates of estimation of optimal transport maps in general function spaces. our assumptions are significantly weaker than those appearing in the literature: we require only that the source measure $p$ satisfy a poincar\'e inequality and that the optimal map be the gradient of a smooth convex function that lies in a space whose metric entropy can be controlled. as a special case, we recover known estimation rates for h\""older transport maps, but also obtain nearly sharp results in many settings not covered by prior work. for example, we provide the first statistical rates of estimation when $p$ is the normal distribution and the transport map is given by an infinite-width shallow neural network.",,2022-12-07,2024-01-02,"['vincent divol', 'jonathan niles-weed', 'aram-alexandre pooladian']"
2212.04091,strong identifiability and parameter learning in regression with   heterogeneous response,math.st stat.ml stat.th,"mixtures of regression are a powerful class of models for regression learning with respect to a highly uncertain and heterogeneous response variable of interest. in addition to being a rich predictive model for the response given some covariates, the parameters in this model class provide useful information about the heterogeneity in the data population, which is represented by the conditional distributions for the response given the covariates associated with a number of distinct but latent subpopulations. in this paper, we investigate conditions of strong identifiability, rates of convergence for conditional density and parameter estimation, and the bayesian posterior contraction behavior arising in finite mixture of regression models, under exact-fitted and over-fitted settings and when the number of components is unknown. this theory is applicable to common choices of link functions and families of conditional distributions employed by practitioners. we provide simulation studies and data illustrations, which shed some light on the parameter learning behavior found in several popular regression mixture models reported in the literature.",,2022-12-08,2024-01-28,"['dat do', 'linh do', 'xuanlong nguyen']"
2212.04382,structure of classifier boundaries: case study for a naive bayes   classifier,stat.ml cs.lg,"whether based on models, training data or a combination, classifiers place (possibly complex) input data into one of a relatively small number of output categories. in this paper, we study the structure of the boundary--those points for which a neighbor is classified differently--in the context of an input space that is a graph, so that there is a concept of neighboring inputs, the scientific setting is a model-based naive bayes classifier for dna reads produced by next generation sequencers. we show that the boundary is both large and complicated in structure. we create a new measure of uncertainty, called neighbor similarity, that compares the result for a point to the distribution of results for its neighbors. this measure not only tracks two inherent uncertainty measures for the bayes classifier, but also can be implemented, at a computational cost, for classifiers without inherent measures of uncertainty.",,2022-12-08,2024-02-09,"['alan f. karr', 'zac bowen', 'adam a. porter']"
2212.04458,general-purpose in-context learning by meta-learning transformers,cs.lg cs.ai cs.ne stat.ml,"modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. one particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. such a model takes in training data, and produces test-set predictions across a wide range of problems, without any explicit definition of an inference model, training loss, or optimization algorithm. in this paper we show that transformers and other black-box models can be meta-trained to act as general-purpose in-context learners. we characterize transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-optimization. we further show that the capabilities of meta-trained algorithms are bottlenecked by the accessible state size (memory) determining the next prediction, unlike standard models which are thought to be bottlenecked by parameter count. finally, we propose practical interventions such as biasing the training distribution that improve the meta-training and meta-generalization of general-purpose in-context learning algorithms.",,2022-12-08,2024-01-09,"['louis kirsch', 'james harrison', 'jascha sohl-dickstein', 'luke metz']"
2212.05949,corruption-robust algorithms with uncertainty weighting for nonlinear   contextual bandits and markov decision processes,stat.ml cs.lg,"despite the significant interest and progress in reinforcement learning (rl) problems with adversarial corruption, current works are either confined to the linear setting or lead to an undesired $\tilde{o}(\sqrt{t}\zeta)$ regret bound, where $t$ is the number of rounds and $\zeta$ is the total amount of corruption. in this paper, we consider the contextual bandit with general function approximation and propose a computationally efficient algorithm to achieve a regret of $\tilde{o}(\sqrt{t}+\zeta)$. the proposed algorithm relies on the recently developed uncertainty-weighted least-squares regression from linear contextual bandit and a new weighted estimator of uncertainty for the general function class. in contrast to the existing analysis that heavily relies on the linear structure, we develop a novel technique to control the sum of weighted uncertainty, thus establishing the final regret bounds. we then generalize our algorithm to the episodic mdp setting and first achieve an additive dependence on the corruption level $\zeta$ in the scenario of general function approximation. notably, our algorithms achieve regret bounds either nearly match the performance lower bound or improve the existing methods for all the corruption levels and in both known and unknown $\zeta$ cases.",,2022-12-12,2024-02-10,"['chenlu ye', 'wei xiong', 'quanquan gu', 'tong zhang']"
2212.06251,autoregressive bandits,cs.lg stat.ml,"autoregressive processes naturally arise in a large variety of real-world scenarios, including stock markets, sales forecasting, weather prediction, advertising, and pricing. when facing a sequential decision-making problem in such a context, the temporal dependence between consecutive observations should be properly accounted for guaranteeing convergence to the optimal policy. in this work, we propose a novel online learning setting, namely, autoregressive bandits (arbs), in which the observed reward is governed by an autoregressive process of order $k$, whose parameters depend on the chosen action. we show that, under mild assumptions on the reward process, the optimal policy can be conveniently computed. then, we devise a new optimistic regret minimization algorithm, namely, autoregressive upper confidence bound (ar-ucb), that suffers sublinear regret of order $\widetilde{\mathcal{o}} \left( \frac{(k+1)^{3/2}\sqrt{nt}}{(1-\gamma)^2}\right)$, where $t$ is the optimization horizon, $n$ is the number of actions, and $\gamma < 1$ is a stability index of the process. finally, we empirically validate our algorithm, illustrating its advantages w.r.t. bandit baselines and its robustness to misspecification of key parameters.",,2022-12-12,2024-02-19,"['francesco bacchiocchi', 'gianmarco genalti', 'davide maran', 'marco mussi', 'marcello restelli', 'nicola gatti', 'alberto maria metelli']"
2212.06693,transfer learning with large-scale quantile regression,math.st stat.co stat.me stat.th,"quantile regression is increasingly encountered in modern big data applications due to its robustness and flexibility. we consider the scenario of learning the conditional quantiles of a specific target population when the available data may go beyond the target and be supplemented from other sources that possibly share similarities with the target. a crucial question is how to properly distinguish and utilize useful information from other sources to improve the quantile estimation and inference at the target. we develop transfer learning methods for high-dimensional quantile regression by detecting informative sources whose models are similar to the target and utilizing them to improve the target model. we show that under reasonable conditions, the detection of the informative sources based on sample splitting is consistent. compared to the naive estimator with only the target data, the transfer learning estimator achieves a much lower error rate as a function of the sample sizes, the signal-to-noise ratios, and the similarity measures among the target and the source models. extensive simulation studies demonstrate the superiority of our proposed approach. we apply our methods to tackle the problem of detecting hard-landing risk for flight safety and show the benefits and insights gained from transfer learning of three different types of airplanes: boeing 737, airbus a320, and airbus a380.",,2022-12-13,2024-02-25,"['jun jin', 'jun yan', 'robert h. aseltine', 'kun chen']"
2212.07052,on lasso for high dimensional predictive regression,econ.em stat.ml,"this paper examines lasso, a widely-used $l_{1}$-penalized regression method, in high dimensional linear predictive regressions, particularly when the number of potential predictors exceeds the sample size and numerous unit root regressors are present. the consistency of lasso is contingent upon two key components: the deviation bound of the cross product of the regressors and the error term, and the restricted eigenvalue of the gram matrix. we present new probabilistic bounds for these components, suggesting that lasso's rates of convergence are different from those typically observed in cross-sectional cases. when applied to a mixture of stationary, nonstationary, and cointegrated predictors, lasso maintains its asymptotic guarantee if predictors are scale-standardized. leveraging machine learning and macroeconomic domain expertise, lasso demonstrates strong performance in forecasting the unemployment rate, as evidenced by its application to the fred-md database.",,2022-12-14,2024-01-16,"['ziwei mei', 'zhentao shi']"
2212.08123,bayesian posterior approximation with stochastic ensembles,cs.lg cs.cv stat.ml,"we introduce ensembles of stochastic neural networks to approximate the bayesian posterior, combining stochastic methods such as dropout with deep ensembles. the stochastic ensembles are formulated as families of distributions and trained to approximate the bayesian posterior with variational inference. we implement stochastic ensembles based on monte carlo dropout, dropconnect and a novel non-parametric version of dropout and evaluate them on a toy problem and cifar image classification. for both tasks, we test the quality of the posteriors directly against hamiltonian monte carlo simulations. our results show that stochastic ensembles provide more accurate posterior estimates than other popular baselines for bayesian inference.",10.1109/cvpr52729.2023.01317,2022-12-15,2024-01-03,"['oleksandr balabanov', 'bernhard mehlig', 'hampus linander']"
2212.08642,"estimating higher-order mixed memberships via the $\ell_{2,\infty}$   tensor perturbation bound",math.st math.oc stat.me stat.ml stat.th,"higher-order multiway data is ubiquitous in machine learning and statistics and often exhibits community-like structures, where each component (node) along each different mode has a community membership associated with it. in this paper we propose the tensor mixed-membership blockmodel, a generalization of the tensor blockmodel positing that memberships need not be discrete, but instead are convex combinations of latent communities. we establish the identifiability of our model and propose a computationally efficient estimation procedure based on the higher-order orthogonal iteration algorithm (hooi) for tensor svd composed with a simplex corner-finding algorithm. we then demonstrate the consistency of our estimation procedure by providing a per-node error bound, which showcases the effect of higher-order structures on estimation accuracy. to prove our consistency result, we develop the $\ell_{2,\infty}$ tensor perturbation bound for hooi under independent, heteroskedastic, subgaussian noise that may be of independent interest. our analysis uses a novel leave-one-out construction for the iterates, and our bounds depend only on spectral properties of the underlying low-rank tensor under nearly optimal signal-to-noise ratio conditions such that tensor svd is computationally feasible. finally, we apply our methodology to real and simulated data, demonstrating some effects not identifiable from the model with discrete community memberships.",,2022-12-16,2024-02-01,"['joshua agterberg', 'anru zhang']"
2212.08949,managing temporal resolution in continuous value estimation: a   fundamental trade-off,cs.lg cs.sy eess.sy stat.ml,"a default assumption in reinforcement learning (rl) and optimal control is that observations arrive at discrete time points on a fixed clock cycle. yet, many applications involve continuous-time systems where the time discretization, in principle, can be managed. the impact of time discretization on rl methods has not been fully characterized in existing theory, but a more detailed analysis of its effect could reveal opportunities for improving data-efficiency. we address this gap by analyzing monte-carlo policy evaluation for lqr systems and uncover a fundamental trade-off between approximation and statistical error in value estimation. importantly, these two errors behave differently to time discretization, leading to an optimal choice of temporal resolution for a given data budget. these findings show that managing the temporal resolution can provably improve policy evaluation efficiency in lqr systems with finite data. empirically, we demonstrate the trade-off in numerical simulations of lqr instances and standard rl benchmarks for non-linear continuous control.",,2022-12-17,2024-01-16,"['zichen zhang', 'johannes kirschner', 'junxi zhang', 'francesco zanini', 'alex ayoub', 'masood dehghan', 'dale schuurmans']"
2212.10789,multi-modal molecule structure-text model for text-based retrieval and   editing,cs.lg cs.cl q-bio.qm stat.ml,"there is increasing adoption of artificial intelligence in drug discovery. however, existing studies use machine learning to mainly utilize the chemical structures of molecules but ignore the vast textual knowledge available in chemistry. incorporating textual knowledge enables us to realize new drug design objectives, adapt to text-based instructions and predict complex biological activities. here we present a multi-modal molecule structure-text model, moleculestm, by jointly learning molecules' chemical structures and textual descriptions via a contrastive learning strategy. to train moleculestm, we construct a large multi-modal dataset, namely, pubchemstm, with over 280,000 chemical structure-text pairs. to demonstrate the effectiveness and utility of moleculestm, we design two challenging zero-shot tasks based on text instructions, including structure-text retrieval and molecule editing. moleculestm has two main properties: open vocabulary and compositionality via natural language. in experiments, moleculestm obtains the state-of-the-art generalization ability to novel biochemical concepts across various benchmarks.",,2022-12-21,2024-01-29,"['shengchao liu', 'weili nie', 'chengpeng wang', 'jiarui lu', 'zhuoran qiao', 'ling liu', 'jian tang', 'chaowei xiao', 'anima anandkumar']"
2212.11880,parameter inference based on gaussian processes informed by nonlinear   partial differential equations,math.na cs.na stat.me stat.ml,"partial differential equations (pdes) are widely used for the description of physical and engineering phenomena. some key parameters involved in pdes, which represent certain physical properties with important scientific interpretations, are difficult or even impossible to measure directly. estimating these parameters from noisy and sparse experimental data of related physical quantities is an important task. many methods for pde parameter inference involve a large number of evaluations for numerical solutions to pde through algorithms such as the finite element method, which can be time-consuming, especially for nonlinear pdes. in this paper, we propose a novel method for the inference of unknown parameters in pdes, called the pde-informed gaussian process (pigp) based parameter inference method. through modeling the pde solution as a gaussian process (gp), we derive the manifold constraints induced by the (linear) pde structure such that, under the constraints, the gp satisfies the pde. for nonlinear pdes, we propose an augmentation method that transforms the nonlinear pde into an equivalent pde system linear in all derivatives, which our pigp-based method can handle. the proposed method can be applied to a broad spectrum of nonlinear pdes. the pigp-based method can be applied to multi-dimensional pde systems and pde systems with unobserved components. like conventional bayesian approaches, the method can provide uncertainty quantification for both the unknown parameters and the pde solution. the pigp-based method also completely bypasses the numerical solver for pdes. the proposed method is demonstrated through several application examples from different areas.",,2022-12-22,2024-02-01,"['zhaohui li', 'shihao yang', 'jeff wu']"
2212.13069,homophily modulates double descent generalization in graph convolution   networks,cs.lg cond-mat.dis-nn stat.ml,"graph neural networks (gnns) excel in modeling relational data such as biological, social, and transportation networks, but the underpinnings of their success are not well understood. traditional complexity measures from statistical learning theory fail to account for observed phenomena like the double descent or the impact of relational semantics on generalization error. motivated by experimental observations of ``transductive'' double descent in key networks and datasets, we use analytical tools from statistical physics and random matrix theory to precisely characterize generalization in simple graph convolution networks on the contextual stochastic block model. our results illuminate the nuances of learning on homophilic versus heterophilic data and predict double descent whose existence in gnns has been questioned by recent work. we show how risk is shaped by the interplay between the graph noise, feature noise, and the number of training labels. our findings apply beyond stylized models, capturing qualitative trends in real-world gnns and datasets. as a case in point, we use our analytic insights to improve performance of state-of-the-art graph convolution networks on heterophilic datasets.",,2022-12-26,2024-01-23,"['cheng shi', 'liming pan', 'hong hu', 'ivan dokmanić']"
2212.13597,optimal regularization for a data source,math.oc math.mg math.st stat.ml stat.th,"in optimization-based approaches to inverse problems and to statistical estimation, it is common to augment criteria that enforce data fidelity with a regularizer that promotes desired structural properties in the solution. the choice of a suitable regularizer is typically driven by a combination of prior domain information and computational considerations. convex regularizers are attractive computationally but they are limited in the types of structure they can promote. on the other hand, nonconvex regularizers are more flexible in the forms of structure they can promote and they have showcased strong empirical performance in some applications, but they come with the computational challenge of solving the associated optimization problems. in this paper, we seek a systematic understanding of the power and the limitations of convex regularization by investigating the following questions: given a distribution, what is the optimal regularizer for data drawn from the distribution? what properties of a data source govern whether the optimal regularizer is convex? we address these questions for the class of regularizers specified by functionals that are continuous, positively homogeneous, and positive away from the origin. we say that a regularizer is optimal for a data distribution if the gibbs density with energy given by the regularizer maximizes the population likelihood (or equivalently, minimizes cross-entropy loss) over all regularizer-induced gibbs densities. as the regularizers we consider are in one-to-one correspondence with star bodies, we leverage dual brunn-minkowski theory to show that a radial function derived from a data distribution is akin to a ``computational sufficient statistic'' as it is the key quantity for identifying optimal regularizers and for assessing the amenability of a data source to convex regularization.",,2022-12-27,2024-02-05,"['oscar leong', ""eliza o'reilly"", 'yong sheng soh', 'venkat chandrasekaran']"
2212.14424,normalizing flow neural networks by jko scheme,stat.ml cs.lg,"normalizing flow is a class of deep generative models for efficient sampling and likelihood estimation, which achieves attractive performance, particularly in high dimensions. the flow is often implemented using a sequence of invertible residual blocks. existing works adopt special network architectures and regularization of flow trajectories. in this paper, we develop a neural ode flow network called jko-iflow, inspired by the jordan-kinderleherer-otto (jko) scheme, which unfolds the discrete-time dynamic of the wasserstein gradient flow. the proposed method stacks residual blocks one after another, allowing efficient block-wise training of the residual blocks, avoiding sampling sde trajectories and score matching or variational learning, thus reducing the memory load and difficulty in end-to-end training. we also develop adaptive time reparameterization of the flow network with a progressive refinement of the induced trajectory in probability space to improve the model accuracy further. experiments with synthetic and real data show that the proposed jko-iflow network achieves competitive performance compared with existing flow and diffusion models at a significantly reduced computational and memory cost.",,2022-12-29,2024-02-15,"['chen xu', 'xiuyuan cheng', 'yao xie']"
2301.01642,ci-gnn: a granger causality-inspired graph neural network for   interpretable brain network-based psychiatric diagnosis,stat.ml cs.lg q-bio.nc,"there is a recent trend to leverage the power of graph neural networks (gnns) for brain-network based psychiatric diagnosis, which,in turn, also motivates an urgent need for psychiatrists to fully understand the decision behavior of the used gnns. however, most of the existing gnn explainers are either post-hoc in which another interpretive model needs to be created to explain a well-trained gnn, or do not consider the causal relationship between the extracted explanation and the decision, such that the explanation itself contains spurious correlations and suffers from weak faithfulness. in this work, we propose a granger causality-inspired graph neural network (ci-gnn), a built-in interpretable model that is able to identify the most influential subgraph (i.e., functional connectivity within brain regions) that is causally related to the decision (e.g., major depressive disorder patients or healthy controls), without the training of an auxillary interpretive network. ci-gnn learns disentangled subgraph-level representations {\alpha} and \b{eta} that encode, respectively, the causal and noncausal aspects of original graph under a graph variational autoencoder framework, regularized by a conditional mutual information (cmi) constraint. we theoretically justify the validity of the cmi regulation in capturing the causal relationship. we also empirically evaluate the performance of ci-gnn against three baseline gnns and four state-of-the-art gnn explainers on synthetic data and three large-scale brain disease datasets. we observe that ci-gnn achieves the best performance in a wide range of metrics and provides more reliable and concise explanations which have clinical evidence.the source code and implementation details of ci-gnn are freely available at github repository (https://github.com/zkz-brain/ci-gnn/).",,2023-01-04,2024-01-28,"['kaizhong zheng', 'shujian yu', 'badong chen']"
2301.02739,rank-transformed subsampling: inference for multiple data splitting and   exchangeable p-values,stat.me math.st stat.th,"many testing problems are readily amenable to randomised tests such as those employing data splitting. however despite their usefulness in principle, randomised tests have obvious drawbacks. firstly, two analyses of the same dataset may lead to different results. secondly, the test typically loses power because it does not fully utilise the entire sample. as a remedy to these drawbacks, we study how to combine the test statistics or p-values resulting from multiple random realisations such as through random data splits. we develop rank-transformed subsampling as a general method for delivering large sample inference about the combined statistic or p-value under mild assumptions. we apply our methodology to a wide range of problems, including testing unimodality in high-dimensional data, testing goodness-of-fit of parametric quantile regression models, testing no direct effect in a sequentially randomised trial and calibrating cross-fit double machine learning confidence intervals. in contrast to existing p-value aggregation schemes that can be highly conservative, our method enjoys type-i error control that asymptotically approaches the nominal level. moreover, compared to using the ordinary subsampling, we show that our rank transform can remove the first-order bias in approximating the null under alternatives and greatly improve power.",,2023-01-06,2024-01-22,"['f. richard guo', 'rajen d. shah']"
2301.03180,subset verification and search algorithms for causal dags,cs.lg cs.ds stat.ml,"learning causal relationships between variables is a fundamental task in causal inference and directed acyclic graphs (dags) are a popular choice to represent the causal relationships. as one can recover a causal graph only up to its markov equivalence class from observations, interventions are often used for the recovery task. interventions are costly in general and it is important to design algorithms that minimize the number of interventions performed. in this work, we study the problem of identifying the smallest set of interventions required to learn the causal relationships between a subset of edges (target edges). under the assumptions of faithfulness, causal sufficiency, and ideal interventions, we study this problem in two settings: when the underlying ground truth causal graph is known (subset verification) and when it is unknown (subset search). for the subset verification problem, we provide an efficient algorithm to compute a minimum sized interventional set; we further extend these results to bounded size non-atomic interventions and node-dependent interventional costs. for the subset search problem, in the worst case, we show that no algorithm (even with adaptivity or randomization) can achieve an approximation ratio that is asymptotically better than the vertex cover of the target edges when compared with the subset verification number. this result is surprising as there exists a logarithmic approximation algorithm for the search problem when we wish to recover the whole causal graph. to obtain our results, we prove several interesting structural properties of interventional causal graphs that we believe have applications beyond the subset verification/search problems studied here.",,2023-01-09,2024-02-13,"['davin choo', 'kirankumar shiragur']"
2301.03749,markovian sliced wasserstein distances: beyond independent projections,stat.ml cs.lg,"sliced wasserstein (sw) distance suffers from redundant projections due to independent uniform random projecting directions. to partially overcome the issue, max k sliced wasserstein (max-k-sw) distance ($k\geq 1$), seeks the best discriminative orthogonal projecting directions. despite being able to reduce the number of projections, the metricity of max-k-sw cannot be guaranteed in practice due to the non-optimality of the optimization. moreover, the orthogonality constraint is also computationally expensive and might not be effective. to address the problem, we introduce a new family of sw distances, named markovian sliced wasserstein (msw) distance, which imposes a first-order markov structure on projecting directions. we discuss various members of msw by specifying the markov structure including the prior distribution, the transition distribution, and the burning and thinning technique. moreover, we investigate the theoretical properties of msw including topological properties (metricity, weak convergence, and connection to other distances), statistical properties (sample complexity, and monte carlo estimation error), and computational properties (computational complexity and memory complexity). finally, we compare msw distances with previous sw variants in various applications such as gradient flows, color transfer, and deep generative modeling to demonstrate the favorable performance of msw.",,2023-01-09,2023-12-31,"['khai nguyen', 'tongzheng ren', 'nhat ho']"
2301.03962,a unified theory of diversity in ensemble learning,cs.lg cs.ai stat.ml,"we present a theory of ensemble diversity, explaining the nature of diversity for a wide range of supervised learning scenarios. this challenge has been referred to as the holy grail of ensemble learning, an open research issue for over 30 years. our framework reveals that diversity is in fact a hidden dimension in the bias-variance decomposition of the ensemble loss. we prove a family of exact bias-variance-diversity decompositions, for a wide range of losses in both regression and classification, e.g., squared, cross-entropy, and poisson losses. for losses where an additive bias-variance decomposition is not available (e.g., 0/1 loss) we present an alternative approach: quantifying the effects of diversity, which turn out to be dependent on the label distribution. overall, we argue that diversity is a measure of model fit, in precisely the same sense as bias and variance, but accounting for statistical dependencies between ensemble members. thus, we should not be maximising diversity as so many works aim to do -- instead, we have a bias/variance/diversity trade-off to manage.",,2023-01-10,2024-02-07,"['danny wood', 'tingting mu', 'andrew webb', 'henry reeve', 'mikel luján', 'gavin brown']"
2301.06297,inference via robust optimal transportation: theory and methods,math.st stat.ml stat.th,"optimal transportation theory and the related $p$-wasserstein distance ($w_p$, $p\geq 1$) are widely-applied in statistics and machine learning. in spite of their popularity, inference based on these tools has some issues. for instance, it is sensitive to outliers and it may not be even defined when the underlying model has infinite moments. to cope with these problems, first we consider a robust version of the primal transportation problem and show that it defines the {robust wasserstein distance}, $w^{(\lambda)}$, depending on a tuning parameter $\lambda > 0$. second, we illustrate the link between $w_1$ and $w^{(\lambda)}$ and study its key measure theoretic aspects. third, we derive some concentration inequalities for $w^{(\lambda)}$. fourth, we use $w^{(\lambda)}$ to define minimum distance estimators, we provide their statistical guarantees and we illustrate how to apply the derived concentration inequalities for a data driven selection of $\lambda$. fifth, we provide the {dual} form of the robust optimal transportation problem and we apply it to machine learning problems (generative adversarial networks and domain adaptation). numerical exercises provide evidence of the benefits yielded by our novel methods.",,2023-01-16,2024-02-29,"['yiming ma', 'hang liu', 'davide la vecchia', 'metthieu lerasle']"
2301.06535,"case-base neural networks: survival analysis with time-varying,   higher-order interactions",stat.ml cs.lg,"in the context of survival analysis, data-driven neural network-based methods have been developed to model complex covariate effects. while these methods may provide better predictive performance than regression-based approaches, not all can model time-varying interactions and complex baseline hazards. to address this, we propose case-base neural networks (cbnns) as a new approach that combines the case-base sampling framework with flexible neural network architectures. using a novel sampling scheme and data augmentation to naturally account for censoring, we construct a feed-forward neural network that includes time as an input. cbnns predict the probability of an event occurring at a given moment to estimate the full hazard function. we compare the performance of cbnns to regression and neural network-based survival methods in a simulation and three case studies using two time-dependent metrics. first, we examine performance on a simulation involving a complex baseline hazard and time-varying interactions to assess all methods, with cbnn outperforming competitors. then, we apply all methods to three real data applications, with cbnns outperforming the competing models in two studies and showing similar performance in the third. our results highlight the benefit of combining case-base sampling with deep learning to provide a simple and flexible framework for data-driven modeling of single event survival outcomes that estimates time-varying effects and a complex baseline hazard by design. an r package is available at https://github.com/jesse-islam/cbnn.",,2023-01-16,2024-01-09,"['jesse islam', 'maxime turgeon', 'robert sladek', 'sahir bhatnagar']"
2301.06907,deep conditional measure quantization,stat.ml cs.ai cs.lg math.pr,"quantization of a probability measure means representing it with a finite set of dirac masses that approximates the input distribution well enough (in some metric space of probability measures). various methods exists to do so, but the situation of quantizing a conditional law has been less explored. we propose a method, called dcmq, involving a huber-energy kernel-based approach coupled with a deep neural network architecture. the method is tested on several examples and obtains promising results.",10.1007/978-3-031-53036-4_24,2023-01-17,2023-03-24,['gabriel turinici']
2301.07530,optimistically tempered online learning,cs.lg math.oc stat.ml,"optimistic online learning algorithms have been developed to exploit expert advices, assumed optimistically to be always useful. however, it is legitimate to question the relevance of such advices \emph{w.r.t.} the learning information provided by gradient-based online algorithms. in this work, we challenge the confidence assumption on the expert and develop the \emph{optimistically tempered} (ot) online learning framework as well as ot adaptations of online algorithms. our algorithms come with sound theoretical guarantees in the form of dynamic regret bounds, and we eventually provide experimental validation of the usefulness of the ot approach.",,2023-01-18,2024-02-14,"['maxime haddouche', 'olivier wintenberger', 'benjamin guedj']"
2301.08158,semiparametric inference using fractional posteriors,math.st stat.ml stat.th,"we establish a general bernstein--von mises theorem for approximately linear semiparametric functionals of fractional posterior distributions based on nonparametric priors. this is illustrated in a number of nonparametric settings and for different classes of prior distributions, including gaussian process priors. we show that fractional posterior credible sets can provide reliable semiparametric uncertainty quantification, but have inflated size. to remedy this, we further propose a \textit{shifted-and-rescaled} fractional posterior set that is an efficient confidence set having optimal size under regularity conditions. as part of our proofs, we also refine existing contraction rate results for fractional posteriors by sharpening the dependence of the rate on the fractional exponent.",,2023-01-19,2024-02-06,"[""alice l'huillier"", 'luke travis', 'ismaël castillo', 'kolyan ray']"
2301.08544,multi-armed bandits and quantum channel oracles,quant-ph stat.ml,"multi-armed bandits are one of the theoretical pillars of reinforcement learning. recently, the investigation of quantum algorithms for multi-armed bandit problems was started, and it was found that a quadratic speed-up (in query complexity) is possible when the arms and the randomness of the rewards of the arms can be queried in superposition. here we introduce further bandit models where we only have limited access to the randomness of the rewards, but we can still query the arms in superposition. we show that then the query complexity is the same as for classical algorithms. this generalizes the prior result that no speed-up is possible for unstructured search when the oracle has positive failure probability.",,2023-01-20,2024-02-26,"['simon buchholz', 'jonas m. kübler', 'bernhard schölkopf']"
2301.08994,how to measure evidence and its strength: bayes factors or relative   belief ratios?,math.st stat.me stat.ml stat.th,"both the bayes factor and the relative belief ratio satisfy the principle of evidence and so can be seen to be valid measures of statistical evidence. certainly bayes factors are regularly employed. the question then is: which of these measures of evidence is more appropriate? it is argued here that there are questions concerning the validity of a current commonly used definition of the bayes factor based on a mixture prior and, when all is considered, the relative belief ratio has better properties as a measure of evidence. it is further shown that, when a natural restriction on the mixture prior is imposed, the bayes factor equals the relative belief ratio obtained without using the mixture prior. even with this restriction, this still leaves open the question of how the strength of evidence is to be measured. it is argued here that the current practice of using the size of the bayes factor to measure strength is not correct and a solution to this issue is presented. several general criticisms of these measures of evidence are also discussed and addressed.",,2023-01-21,2024-02-26,"['luai al-labadi', 'ayman alzaatreh', 'michael evans']"
2301.09397,ddml: double/debiased machine learning in stata,econ.em stat.ml,"we introduce the package ddml for double/debiased machine learning (ddml) in stata. estimators of causal parameters for five different econometric models are supported, allowing for flexible estimation of causal effects of endogenous variables in settings with unknown functional forms and/or many exogenous variables. ddml is compatible with many existing supervised machine learning programs in stata. we recommend using ddml in combination with stacking estimation which combines multiple machine learners into a final predictor. we provide monte carlo evidence to support our recommendation.",,2023-01-23,2024-01-06,"['achim ahrens', 'christian b. hansen', 'mark e. schaffer', 'thomas wiemann']"
2301.09505,rethinking the expressive power of gnns via graph biconnectivity,cs.lg stat.ml,"designing expressive graph neural networks (gnns) is a central topic in learning graph-structured data. while numerous approaches have been proposed to improve gnns in terms of the weisfeiler-lehman (wl) test, generally there is still a lack of deep understanding of what additional power they can systematically and provably gain. in this paper, we take a fundamentally different perspective to study the expressive power of gnns beyond the wl test. specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. as biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular gnns can learn it easily as well. however, after a thorough review of prior gnn architectures, we surprisingly find that most of them are not expressive for any of these metrics. the only exception is the esan framework, for which we give a theoretical justification of its power. we proceed to introduce a principled and more efficient approach, called the generalized distance weisfeiler-lehman (gd-wl), which is provably expressive for all biconnectivity metrics. practically, we show gd-wl can be implemented by a transformer-like architecture that preserves expressiveness and enjoys full parallelizability. a set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior gnn architectures.",,2023-01-23,2024-02-10,"['bohang zhang', 'shengjie luo', 'liwei wang', 'di he']"
2301.11270,principled reinforcement learning with human feedback from pairwise or   $k$-wise comparisons,cs.lg cs.ai cs.hc math.st stat.ml stat.th,"we provide a theoretical framework for reinforcement learning with human feedback (rlhf). our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (mle) converges under both the bradley-terry-luce (btl) model and the plackett-luce (pl) model. however, we show that when training a policy based on the learned reward model, mle fails while a pessimistic mle provides policies with improved performance under certain coverage assumptions. additionally, we demonstrate that under the pl model, the true mle and an alternative mle that splits the $k$-wise comparison into pairwise comparisons both converge. moreover, the true mle is asymptotically more efficient. our results validate the empirical success of existing rlhf algorithms in instructgpt and provide new insights for algorithm design. furthermore, our results unify the problem of rlhf and max-entropy inverse reinforcement learning (irl), and provide the first sample complexity bound for max-entropy irl.",,2023-01-26,2024-02-07,"['banghua zhu', 'jiantao jiao', 'michael i. jordan']"
2301.11584,robust variance-regularized risk minimization with concomitant scaling,stat.ml cs.lg,"under losses which are potentially heavy-tailed, we consider the task of minimizing sums of the loss mean and standard deviation, without trying to accurately estimate the variance. by modifying a technique for variance-free robust mean estimation to fit our problem setting, we derive a simple learning procedure which can be easily combined with standard gradient-based solvers to be used in traditional machine learning workflows. empirically, we verify that our proposed approach, despite its simplicity, performs as well or better than even the best-performing candidates derived from alternative criteria such as cvar or dro risks on a variety of datasets.",,2023-01-27,2024-02-08,['matthew j. holland']
2301.12334,don't play favorites: minority guidance for diffusion models,cs.lg cs.ai cs.cv stat.ml,"we explore the problem of generating minority samples using diffusion models. the minority samples are instances that lie on low-density regions of a data manifold. generating a sufficient number of such minority instances is important, since they often contain some unique attributes of the data. however, the conventional generation process of the diffusion models mostly yields majority samples (that lie on high-density regions of the manifold) due to their high likelihoods, making themselves ineffective and time-consuming for the minority generating task. in this work, we present a novel framework that can make the generation process of the diffusion models focus on the minority samples. we first highlight that tweedie's denoising formula yields favorable results for majority samples. the observation motivates us to introduce a metric that describes the uniqueness of a given sample. to address the inherent preference of the diffusion models w.r.t. the majority samples, we further develop minority guidance, a sampling technique that can guide the generation process toward regions with desired likelihood levels. experiments on benchmark real datasets demonstrate that our minority guidance can greatly improve the capability of generating high-quality minority samples over existing generative samplers. we showcase that the performance benefit of our framework persists even in demanding real-world scenarios such as medical imaging, further underscoring the practical significance of our work. code is available at https://github.com/soobin-um/minority-guidance.",,2023-01-28,2024-02-26,"['soobin um', 'suhyeon lee', 'jong chul ye']"
2301.12767,"compression, generalization and learning",cs.lg cs.ai math.st stat.ml stat.th,"a compression function is a map that slims down an observational set into a subset of reduced size, while preserving its informational content. in multiple applications, the condition that one new observation makes the compressed set change is interpreted that this observation brings in extra information and, in learning theory, this corresponds to misclassification, or misprediction. in this paper, we lay the foundations of a new theory that allows one to keep control on the probability of change of compression (which maps into the statistical ""risk"" in learning applications). under suitable conditions, the cardinality of the compressed set is shown to be a consistent estimator of the probability of change of compression (without any upper limit on the size of the compressed set); moreover, unprecedentedly tight finite-sample bounds to evaluate the probability of change of compression are obtained under a generally applicable condition of preference. all results are usable in a fully agnostic setup, i.e., without requiring any a priori knowledge on the probability distribution of the observations. not only these results offer a valid support to develop trust in observation-driven methodologies, they also play a fundamental role in learning techniques as a tool for hyper-parameter tuning.",,2023-01-30,2024-01-08,"['marco c. campi', 'simone garatti']"
2301.13139,a novel framework for policy mirror descent with general   parameterization and linear convergence,stat.ml cs.lg math.oc math.st stat.th,"modern policy optimization methods in reinforcement learning, such as trpo and ppo, owe their success to the use of parameterized policies. however, while theoretical guarantees have been established for this class of algorithms, especially in the tabular setting, the use of general parameterization schemes remains mostly unjustified. in this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parameterizations. the policy class induced by our scheme recovers known classes, e.g., softmax, and generates new ones depending on the choice of mirror map. using our framework, we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization. to demonstrate the ability of our framework to accommodate general parameterization schemes, we provide its sample complexity when using shallow neural networks, show that it represents an improvement upon the previous best results, and empirically validate the effectiveness of our theoretical claims on classic control tasks.",,2023-01-30,2024-02-13,"['carlo alfano', 'rui yuan', 'patrick rebeschini']"
2301.13289,on the statistical benefits of temporal difference learning,cs.lg stat.ml,"given a dataset on actions and resulting long-term rewards, a direct estimation approach fits value functions that minimize prediction error on the training data. temporal difference learning (td) methods instead fit value functions by minimizing the degree of temporal inconsistency between estimates made at successive time-steps. focusing on finite state markov chains, we provide a crisp asymptotic theory of the statistical advantages of this approach. first, we show that an intuitive inverse trajectory pooling coefficient completely characterizes the percent reduction in mean-squared error of value estimates. depending on problem structure, the reduction could be enormous or nonexistent. next, we prove that there can be dramatic improvements in estimates of the difference in value-to-go for two states: td's errors are bounded in terms of a novel measure - the problem's trajectory crossing time - which can be much smaller than the problem's time horizon.",,2023-01-30,2024-02-14,"['david cheikhi', 'daniel russo']"
2302.00293,"a survey of methods, challenges and perspectives in causality",cs.lg stat.me,"deep learning models have shown success in a large variety of tasks by extracting correlation patterns from high-dimensional data but still struggle when generalizing out of their initial distribution. as causal engines aim to learn mechanisms independent from a data distribution, combining deep learning with causality can have a great impact on the two fields. in this paper, we further motivate this assumption. we perform an extensive overview of the theories and methods for causality from different perspectives, with an emphasis on deep learning and the challenges met by the two domains. we show early attempts to bring the fields together and the possible perspectives for the future. we finish by providing a large variety of applications for techniques from causality.",,2023-02-01,2023-12-31,"['gaël gendron', 'michael witbrock', 'gillian dobbie']"
2302.00316,accelerated first-order optimization under nonlinear constraints,math.oc cs.lg eess.sp stat.ml,"we exploit analogies between first-order algorithms for constrained optimization and non-smooth dynamical systems to design a new class of accelerated first-order algorithms for constrained optimization. unlike frank-wolfe or projected gradients, these algorithms avoid optimization over the entire feasible set at each iteration. we prove convergence to stationary points even in a nonconvex setting and we derive accelerated rates for the convex setting both in continuous time, as well as in discrete time. an important property of these algorithms is that constraints are expressed in terms of velocities instead of positions, which naturally leads to sparse, local and convex approximations of the feasible set (even if the feasible set is nonconvex). thus, the complexity tends to grow mildly in the number of decision variables and in the number of constraints, which makes the algorithms suitable for machine learning applications. we apply our algorithms to a compressed sensing and a sparse regression problem, showing that we can treat nonconvex $\ell^p$ constraints ($p<1$) efficiently, while recovering state-of-the-art performance for $p=1$.",,2023-02-01,2024-01-02,"['michael muehlebach', 'michael i. jordan']"
2302.00695,versatile energy-based probabilistic models for high energy physics,cs.lg hep-ex hep-ph stat.ml,"as a classical generative modeling approach, energy-based models have the natural advantage of flexibility in the form of the energy function. recently, energy-based models have achieved great success in modeling high-dimensional data in computer vision and natural language processing. in line with these advancements, we build a multi-purpose energy-based probabilistic model for high energy physics events at the large hadron collider. this framework builds on a powerful generative model and describes higher-order inter-particle interactions. it suits different encoding architectures and builds on implicit generation. as for applicative aspects, it can serve as a powerful parameterized event generator for physics simulation, a generic anomalous signal detector free from spurious correlations, and an augmented event classifier for particle identification.",,2023-02-01,2024-01-18,"['taoli cheng', 'aaron courville']"
2302.00704,pathologies of predictive diversity in deep ensembles,cs.lg stat.ml,"classic results establish that encouraging predictive diversity improves performance in ensembles of low-capacity models, e.g. through bagging or boosting. here we demonstrate that these intuitions do not apply to high-capacity neural network ensembles (deep ensembles), and in fact the opposite is often true. in a large scale study of nearly 600 neural network classification ensembles, we examine a variety of interventions that trade off component model performance for predictive diversity. while such interventions can improve the performance of small neural network ensembles (in line with standard intuitions), they harm the performance of the large neural network ensembles most often used in practice. surprisingly, we also find that discouraging predictive diversity is often benign in large-network ensembles, fully inverting standard intuitions. even when diversity-promoting interventions do not sacrifice component model performance (e.g. using heterogeneous architectures and training paradigms), we observe an opportunity cost associated with pursuing increased predictive diversity. examining over 1000 ensembles, we observe that the performance benefits of diverse architectures/training procedures are easily dwarfed by the benefits of simply using higher-capacity models, despite the fact that such higher capacity models often yield significantly less predictive diversity. overall, our findings demonstrate that standard intuitions around predictive diversity, originally developed for low-capacity ensembles, do not directly apply to modern high-capacity deep ensembles. this work clarifies fundamental challenges to the goal of improving deep ensembles by making them more diverse, while suggesting an alternative path: simply forming ensembles from ever more powerful (and less diverse) component models.",,2023-02-01,2024-01-09,"['taiga abe', 'e. kelly buchanan', 'geoff pleiss', 'john p. cunningham']"
2302.00834,sharp lower bounds on interpolation by deep relu neural networks at   irregularly spaced data,cs.lg cs.ne stat.ml,"we study the interpolation power of deep relu neural networks. specifically, we consider the question of how efficiently, in terms of the number of parameters, deep relu networks can interpolate values at $n$ datapoints in the unit ball which are separated by a distance $\delta$. we show that $\omega(n)$ parameters are required in the regime where $\delta$ is exponentially small in $n$, which gives the sharp result in this regime since $o(n)$ parameters are always sufficient. this also shows that the bit-extraction technique used to prove lower bounds on the vc dimension cannot be applied to irregularly spaced datapoints. finally, as an application we give a lower bound on the approximation rates that deep relu neural networks can achieve for sobolev spaces at the embedding endpoint.",,2023-02-01,2024-02-23,['jonathan w. siegel']
2302.00878,the contextual lasso: sparse linear models via deep neural networks,stat.ml cs.lg stat.me,"sparse linear models are one of several core tools for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. with this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. this dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. the fitting process learns this function nonparametrically via a deep neural network. to attain sparse coefficients, we train the network with a novel lasso regularizer in the form of a projection layer that maps the network's output onto the space of $\ell_1$-constrained linear models. an extensive suite of experiments on real and synthetic data suggests that the learned models, which remain highly transparent, can be sparser than the regular lasso without sacrificing the predictive power of a standard deep neural network.",,2023-02-02,2024-01-02,"['ryan thompson', 'amir dezfouli', 'robert kohn']"
2302.00982,stochastic optimal transport in banach spaces for regularized estimation   of multivariate quantiles,math.pr stat.ml,"we introduce a new stochastic algorithm for solving entropic optimal transport (eot) between two absolutely continuous probability measures $\mu$ and $\nu$. our work is motivated by the specific setting of monge-kantorovich quantiles where the source measure $\mu$ is either the uniform distribution on the unit hypercube or the spherical uniform distribution. using the knowledge of the source measure, we propose to parametrize a kantorovich dual potential by its fourier coefficients. in this way, each iteration of our stochastic algorithm reduces to two fourier transforms that enables us to make use of the fast fourier transform (fft) in order to implement a fast numerical method to solve eot. we study the almost sure convergence of our stochastic algorithm that takes its values in an infinite-dimensional banach space. then, using numerical experiments, we illustrate the performances of our approach on the computation of regularized monge-kantorovich quantiles. in particular, we investigate the potential benefits of entropic regularization for the smooth estimation of multivariate quantiles using data sampled from the target measure $\nu$.",,2023-02-02,2024-02-19,"['bernard bercu', 'jérémie bigot', 'gauthier thurin']"
2302.01757,rs-del: edit distance robustness certificates for sequence classifiers   via randomized deletion,cs.cr cs.lg stat.ml,"randomized smoothing is a leading approach for constructing classifiers that are certifiably robust against adversarial examples. existing work on randomized smoothing has focused on classifiers with continuous inputs, such as images, where $\ell_p$-norm bounded adversaries are commonly studied. however, there has been limited work for classifiers with discrete or variable-size inputs, such as for source code, which require different threat models and smoothing mechanisms. in this work, we adapt randomized smoothing for discrete sequence classifiers to provide certified robustness against edit distance-bounded adversaries. our proposed smoothing mechanism randomized deletion (rs-del) applies random deletion edits, which are (perhaps surprisingly) sufficient to confer robustness against adversarial deletion, insertion and substitution edits. our proof of certification deviates from the established neyman-pearson approach, which is intractable in our setting, and is instead organized around longest common subsequences. we present a case study on malware detection--a binary classification problem on byte sequences where classifier evasion is a well-established threat model. when applied to the popular malconv malware detection model, our smoothing mechanism rs-del achieves a certified accuracy of 91% at an edit distance radius of 128 bytes.",,2023-01-30,2024-01-24,"['zhuoqun huang', 'neil g. marchant', 'keane lucas', 'lujo bauer', 'olga ohrimenko', 'benjamin i. p. rubinstein']"
2302.02718,a log-linear non-parametric online changepoint detection algorithm based   on functional pruning,stat.me stat.co stat.ml,"online changepoint detection aims to detect anomalies and changes in real-time in high-frequency data streams, sometimes with limited available computational resources. this is an important task that is rooted in many real-world applications, including and not limited to cybersecurity, medicine and astrophysics. while fast and efficient online algorithms have been recently introduced, these rely on parametric assumptions which are often violated in practical applications. motivated by data streams from the telecommunications sector, we build a flexible nonparametric approach to detect a change in the distribution of a sequence. our procedure, np-focus, builds a sequential likelihood ratio test for a change in a set of points of the empirical cumulative density function of our data. this is achieved by keeping track of the number of observations above or below those points. thanks to functional pruning ideas, np-focus has a computational cost that is log-linear in the number of observations and is suitable for high-frequency data streams. in terms of detection power, np-focus is seen to outperform current nonparametric online changepoint techniques in a variety of settings. we demonstrate the utility of the procedure on both simulated and real data.",10.1109/tsp.2023.3343550,2023-02-06,2024-01-11,"['gaetano romano', 'idris a eckley', 'paul fearnhead']"
2302.03068,evaluating self-supervised learning via risk decomposition,cs.lg cs.ai stat.ml,"self-supervised learning (ssl) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. yet ssl is typically evaluated using a single metric: linear probing on imagenet. this does not provide much insight into why or when a model is better, now how to improve it. to address this, we propose an ssl risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. we provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 ssl vision models evaluated on imagenet. our analysis gives valuable insights for designing and using ssl models. for example, it highlights the main sources of error and shows how to improve ssl in specific settings (full- vs few-shot) by trading off error components. all results and pretrained models are at https://github.com/yanndubs/ssl-risk-decomposition.",,2023-02-06,2024-01-08,"['yann dubois', 'tatsunori hashimoto', 'percy liang']"
2302.03421,a unified recipe for deriving (time-uniform) pac-bayes bounds,stat.ml cs.it cs.lg math.it math.st stat.th,"we present a unified framework for deriving pac-bayesian generalization bounds. unlike most previous literature on this topic, our bounds are anytime-valid (i.e., time-uniform), meaning that they hold at all stopping times, not only for a fixed sample size. our approach combines four tools in the following order: (a) nonnegative supermartingales or reverse submartingales, (b) the method of mixtures, (c) the donsker-varadhan formula (or other convex duality principles), and (d) ville's inequality. our main result is a pac-bayes theorem which holds for a wide class of discrete stochastic processes. we show how this result implies time-uniform versions of well-known classical pac-bayes bounds, such as those of seeger, mcallester, maurer, and catoni, in addition to many recent bounds. we also present several novel bounds. our framework also enables us to relax traditional assumptions; in particular, we consider nonstationary loss functions and non-i.i.d. data. in sum, we unify the derivation of past bounds and ease the search for future bounds: one may simply check if our supermartingale or submartingale conditions are met and, if so, be guaranteed a (time-uniform) pac-bayes bound.",,2023-02-07,2024-01-03,"['ben chugg', 'hongjian wang', 'aaditya ramdas']"
2302.03660,flow matching on general geometries,cs.lg cs.ai stat.ml,"we propose riemannian flow matching (rfm), a simple yet powerful framework for training continuous normalizing flows on manifolds. existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. riemannian flow matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. the key ingredient behind rfm is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing euclidean case. to extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. our method achieves state-of-the-art performance on many real-world non-euclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.",,2023-02-07,2024-02-26,"['ricky t. q. chen', 'yaron lipman']"
2302.03693,concept algebra for (score-based) text-controlled generative models,cs.cl cs.lg stat.ml,"this paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. a key property of such models is that they can compose disparate concepts in a `disentangled' manner. this suggests these models have internal representations that encode concepts in a `disentangled' manner. here, we focus on the idea that concepts are encoded as subspaces of some representation space. we formalize what this means, show there's a natural choice for the representation, and develop a simple method for identifying the part of the representation corresponding to a given concept. in particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. we demonstrate the idea with examples using stable diffusion. code in https://github.com/zihao12/concept-algebra-code",,2023-02-07,2024-02-07,"['zihao wang', 'lin gui', 'jeffrey negrea', 'victor veitch']"
2302.04313,geometry-complete diffusion for 3d molecule generation and optimization,cs.lg cs.ai q-bio.bm q-bio.qm stat.ml,"denoising diffusion probabilistic models (ddpms) have recently taken the field of generative modeling by storm, pioneering new state-of-the-art results in disciplines such as computer vision and computational biology for diverse tasks ranging from text-guided image generation to structure-guided protein design. along this latter line of research, methods have recently been proposed for generating 3d molecules using equivariant graph neural networks (gnns) within a ddpm framework. however, such methods are unable to learn important geometric and physical properties of 3d molecules during molecular graph generation, as they adopt molecule-agnostic and non-geometric gnns as their 3d graph denoising networks, which negatively impacts their ability to effectively scale to datasets of large 3d molecules. in this work, we address these gaps by introducing the geometry-complete diffusion model (gcdm) for 3d molecule generation, which outperforms existing 3d molecular diffusion models by significant margins across conditional and unconditional settings for the qm9 dataset as well as for the larger geom-drugs dataset. importantly, we demonstrate that the geometry-complete denoising process gcdm learns for 3d molecule generation allows the model to generate realistic and stable large molecules at the scale of geom-drugs, whereas previous methods fail to do so with the features they learn. additionally, we show that extensions of gcdm can not only effectively design 3d molecules for specific protein pockets but also that gcdm's geometric features can effectively be repurposed to directly optimize the geometry and chemical composition of existing 3d molecules for specific molecular properties, demonstrating new, real-world versatility of molecular diffusion models. our source code and data are freely available at https://github.com/bioinfomachinelearning/bio-diffusion.",,2023-02-08,2024-02-04,"['alex morehead', 'jianlin cheng']"
2302.04658,the sample complexity of approximate rejection sampling with   applications to smoothed online learning,stat.ml cs.lg,"suppose we are given access to $n$ independent samples from distribution $\mu$ and we wish to output one of them with the goal of making the output distributed as close as possible to a target distribution $\nu$. in this work we show that the optimal total variation distance as a function of $n$ is given by $\tilde\theta(\frac{d}{f'(n)})$ over the class of all pairs $\nu,\mu$ with a bounded $f$-divergence $d_f(\nu\|\mu)\leq d$. previously, this question was studied only for the case when the radon-nikodym derivative of $\nu$ with respect to $\mu$ is uniformly bounded. we then consider an application in the seemingly very different field of smoothed online learning, where we show that recent results on the minimax regret and the regret of oracle-efficient algorithms still hold even under relaxed constraints on the adversary (to have bounded $f$-divergence, as opposed to bounded radon-nikodym derivative). finally, we also study efficacy of importance sampling for mean estimates uniform over a function class and compare importance sampling with rejection sampling.",,2023-02-09,2024-02-23,"['adam block', 'yury polyanskiy']"
2302.04763,on sampling with approximate transport maps,stat.ml cs.lg,"transport maps can ease the sampling of distributions with non-trivial geometries by transforming them into distributions that are easier to handle. the potential of this approach has risen with the development of normalizing flows (nf) which are maps parameterized with deep neural networks trained to push a reference distribution towards a target. nf-enhanced samplers recently proposed blend (markov chain) monte carlo methods with either (i) proposal draws from the flow or (ii) a flow-based reparametrization. in both cases, the quality of the learned transport conditions performance. the present work clarifies for the first time the relative strengths and weaknesses of these two approaches. our study concludes that multimodal targets can be reliably handled with flow-based proposals up to moderately high dimensions. in contrast, methods relying on reparametrization struggle with multimodality but are more robust otherwise in high-dimensional settings and under poor training. to further illustrate the influence of target-proposal adequacy, we also derive a new quantitative bound for the mixing time of the independent metropolis-hastings sampler.",,2023-02-09,2024-02-18,"['louis grenioux', 'alain durmus', 'éric moulines', 'marylou gabrié']"
2302.04925,information theoretic lower bounds for information theoretic upper   bounds,cs.lg stat.ml,"we examine the relationship between the mutual information between the output model and the empirical sample and the generalization of the algorithm in the context of stochastic convex optimization. despite increasing interest in information-theoretic generalization bounds, it is uncertain if these bounds can provide insight into the exceptional performance of various learning algorithms. our study of stochastic convex optimization reveals that, for true risk minimization, dimension-dependent mutual information is necessary. this indicates that existing information-theoretic generalization bounds fall short in capturing the generalization capabilities of algorithms like sgd and regularized erm, which have dimension-independent sample complexity.",,2023-02-09,2024-01-14,['roi livni']
2302.05793,distributional gflownets with quantile flows,cs.lg cs.ai stat.co stat.ml,"generative flow networks (gflownets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. despite being inspired from reinforcement learning, the current gflownet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. in this work, we adopt a distributional paradigm for gflownets, turning each flow function into a distribution, thus providing more informative learning signals during training. by parameterizing each edge flow through their quantile functions, our proposed \textit{quantile matching} gflownet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even in settings with deterministic rewards.",,2023-02-11,2024-02-17,"['dinghuai zhang', 'ling pan', 'ricky t. q. chen', 'aaron courville', 'yoshua bengio']"
2302.05978,bayesian methods in tensor analysis,stat.me,"tensors, also known as multidimensional arrays, are useful data structures in machine learning and statistics. in recent years, bayesian methods have emerged as a popular direction for analyzing tensor-valued data since they provide a convenient way to introduce sparsity into the model and conduct uncertainty quantification. in this article, we provide an overview of frequentist and bayesian methods for solving tensor completion and regression problems, with a focus on bayesian methods. we review common bayesian tensor approaches including model formulation, prior assignment, posterior computation, and theoretical properties. we also discuss potential future directions in this field.",10.4310/23-sii802,2023-02-12,2023-06-05,"['yiyao shi', 'weining shen']"
2302.06025,statistical complexity and optimal algorithms for non-linear ridge   bandits,stat.ml cs.it cs.lg math.it math.st stat.th,"we consider the sequential decision-making problem where the mean outcome is a non-linear function of the chosen action. compared with the linear model, two curious phenomena arise in non-linear models: first, in addition to the ""learning phase"" with a standard parametric rate for estimation or regret, there is an ""burn-in period"" with a fixed cost determined by the non-linear function; second, achieving the smallest burn-in cost requires new exploration algorithms. for a special family of non-linear functions named ridge functions in the literature, we derive upper and lower bounds on the optimal burn-in cost, and in addition, on the entire learning trajectory during the burn-in period via differential equations. in particular, a two-stage algorithm that first finds a good initial action and then treats the problem as locally linear is statistically optimal. in contrast, several classical algorithms, such as ucb and algorithms relying on regression oracles, are provably suboptimal.",,2023-02-12,2024-01-09,"['nived rajaraman', 'yanjun han', 'jiantao jiao', 'kannan ramchandran']"
2302.06595,when can we track significant preference shifts in dueling bandits?,cs.lg stat.ml,"the $k$-armed dueling bandits problem, where the feedback is in the form of noisy pairwise preferences, has been widely studied due its applications in information retrieval, recommendation systems, etc. motivated by concerns that user preferences/tastes can evolve over time, we consider the problem of dueling bandits with distribution shifts. specifically, we study the recent notion of significant shifts (suk and kpotufe, 2022), and ask whether one can design an adaptive algorithm for the dueling problem with $o(\sqrt{k\tilde{l}t})$ dynamic regret, where $\tilde{l}$ is the (unknown) number of significant shifts in preferences. we show that the answer to this question depends on the properties of underlying preference distributions.   firstly, we give an impossibility result that rules out any algorithm with $o(\sqrt{k\tilde{l}t})$ dynamic regret under the well-studied condorcet and sst classes of preference distributions. secondly, we show that $\text{sst} \cap \text{sti}$ is the largest amongst popular classes of preference distributions where it is possible to design such an algorithm. overall, our results provides an almost complete resolution of the above question for the hierarchy of distribution classes.",,2023-02-13,2024-01-24,"['joe suk', 'arpit agarwal']"
2302.06935,reference prior for bayesian estimation of seismic fragility curves,stat.ap stat.me,"one of the central quantities of probabilistic seismic risk assessment studies is the fragility curve, which represents the probability of failure of a mechanical structure conditional on a scalar measure derived from the seismic ground motion. estimating such curves is a difficult task because, for many structures of interest, few data are available and the data are only binary; i.e., they indicate the state of the structure, failure or non-failure. this framework concerns complex equipments such as electrical devices encountered in industrial installations. in order to address this challenging framework a wide range of the methods in the literature rely on a parametric log-normal model. bayesian approaches allow for efficient learning of the model parameters. however, the choice of the prior distribution has a non-negligible influence on the posterior distribution and, therefore, on any resulting estimate. we propose a thorough study of this parametric bayesian estimation problem when the data are limited and binary. using the reference prior theory as a support, we suggest an objective approach for the prior choice. this approach leads to the jeffreys prior which is explicitly derived for this problem for the first time. the posterior distribution is proven to be proper (i.e., it integrates to unity) with the jeffreys prior and improper with some classical priors from the literature. the posterior distribution with the jeffreys prior is also shown to vanish at the boundaries of the parameters domain, so sampling the posterior distribution of the parameters does not produce anomalously small or large values. therefore, this does not produce degenerate fragility curves such as unit-step functions and the jeffreys prior leads to robust credibility intervals. the numerical results obtained on two different case studies, including an industrial case, illustrate the theoretical predictions.",,2023-02-14,2024-01-31,"['antoine van biesbroeck', 'clement gauchy', 'cyril feau', 'josselin garnier']"
2302.07200,neurosymbolic ai for reasoning over knowledge graphs: a survey,cs.ai cs.lo stat.ml,"neurosymbolic ai is an increasingly active area of research that combines symbolic reasoning methods with deep learning to leverage their complementary benefits. as knowledge graphs are becoming a popular way to represent heterogeneous and multi-relational data, methods for reasoning on graph structures have attempted to follow this neurosymbolic paradigm. traditionally, such approaches have utilized either rule-based inference or generated representative numerical embeddings from which patterns could be extracted. however, several recent studies have attempted to bridge this dichotomy to generate models that facilitate interpretability, maintain competitive performance, and integrate expert knowledge. therefore, we survey methods that perform neurosymbolic reasoning tasks on knowledge graphs and propose a novel taxonomy by which we can classify them. specifically, we propose three major categories: (1) logically-informed embedding approaches, (2) embedding approaches with logical constraints, and (3) rule learning approaches. alongside the taxonomy, we provide a tabular overview of the approaches and links to their source code, if available, for more direct comparison. finally, we discuss the unique characteristics and limitations of these methods, then propose several prospective directions toward which this field of research could evolve.",,2023-02-14,2024-02-06,"['lauren nicole delong', 'ramon fernández mir', 'jacques d. fleuriot']"
2302.07930,interpretable deep learning methods for multiview learning,cs.lg stat.me stat.ml,"technological advances have enabled the generation of unique and complementary types of data or views (e.g. genomics, proteomics, metabolomics) and opened up a new era in multiview learning research with the potential to lead to new biomedical discoveries. we propose ideepviewlearn (interpretable deep learning method for multiview learning) for learning nonlinear relationships in data from multiple views while achieving feature selection. ideepviewlearn combines deep learning flexibility with the statistical benefits of data and knowledge-driven feature selection, giving interpretable results. deep neural networks are used to learn view-independent low-dimensional embedding through an optimization problem that minimizes the difference between observed and reconstructed data, while imposing a regularization penalty on the reconstructed data. the normalized laplacian of a graph is used to model bilateral relationships between variables in each view, therefore, encouraging selection of related variables. ideepviewlearn is tested on simulated and two real-world data, including breast cancer-related gene expression and methylation data. ideepviewlearn had competitive classification results and identified genes and cpg sites that differentiated between individuals who died from breast cancer and those who did not. the results of our real data application and simulations with small to moderate sample sizes suggest that ideepviewlearn may be a useful method for small-sample-size problems compared to other deep learning methods for multiview learning.",10.1186/s12859-024-05679-9,2023-02-15,2024-02-15,"['hengkang wang', 'han lu', 'ju sun', 'sandra e safo']"
2302.08298,unleashing the potential of acquisition functions in high-dimensional   bayesian optimization,cs.lg stat.ml,"bayesian optimization (bo) is widely used to optimize expensive-to-evaluate black-box functions.bo first builds a surrogate model to represent the objective function and assesses its uncertainty. it then decides where to sample by maximizing an acquisition function (af) based on the surrogate model. however, when dealing with high-dimensional problems, finding the global maximum of the af becomes increasingly challenging. in such cases, the initialization of the af maximizer plays a pivotal role, as an inadequate setup can severely hinder the effectiveness of the af.   this paper investigates a largely understudied problem concerning the impact of af maximizer initialization on exploiting afs' capability. our large-scale empirical study shows that the widely used random initialization strategy often fails to harness the potential of an af. in light of this, we propose a better initialization approach by employing multiple heuristic optimizers to leverage the historical data of black-box optimization to generate initial points for the af maximize. we evaluate our approach with a range of heavily studied synthetic functions and real-world applications. experimental results show that our techniques, while simple, can significantly enhance the standard bo and outperform state-of-the-art methods by a large margin in most test cases.",,2023-02-16,2024-01-23,"['jiayu zhao', 'renyu yang', 'shenghao qiu', 'zheng wang']"
2302.08766,a lower bound and a near-optimal algorithm for bilevel empirical risk   minimization,stat.ml cs.lg math.oc,"bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. in many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. in this context, we propose a bilevel extension of the celebrated sarah algorithm. we demonstrate that the algorithm requires $\mathcal{o}((n+m)^{\frac12}\varepsilon^{-1})$ oracle calls to achieve $\varepsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. this lower bound is attained by our algorithm, making it optimal in terms of sample complexity.",,2023-02-17,2024-02-20,"['mathieu dagréou', 'thomas moreau', 'samuel vaiter', 'pierre ablin']"
2302.09357,stochastic online instrumental variable regression: regrets for   endogeneity and bandit feedback,cs.lg stat.ml,"endogeneity, i.e. the dependence of noise and covariates, is a common phenomenon in real data due to omitted variables, strategic behaviours, measurement errors etc. in contrast, the existing analyses of stochastic online linear regression with unbounded noise and linear bandits depend heavily on exogeneity, i.e. the independence of noise and covariates. motivated by this gap, we study the over- and just-identified instrumental variable (iv) regression, specifically two-stage least squares, for stochastic online learning, and propose to use an online variant of two-stage least squares, namely o2sls. we show that o2sls achieves $\mathcal o(d_{x}d_{z}\log^2 t)$ identification and $\widetilde{\mathcal o}(\gamma \sqrt{d_{z} t})$ oracle regret after $t$ interactions, where $d_{x}$ and $d_{z}$ are the dimensions of covariates and ivs, and $\gamma$ is the bias due to endogeneity. for $\gamma=0$, i.e. under exogeneity, o2sls exhibits $\mathcal o(d_{x}^2 \log^2 t)$ oracle regret, which is of the same order as that of the stochastic online ridge. then, we leverage o2sls as an oracle to design oful-iv, a stochastic linear bandit algorithm to tackle endogeneity. oful-iv yields $\widetilde{\mathcal o}(\sqrt{d_{x}d_{z}t})$ regret that matches the regret lower bound under exogeneity. for different datasets with endogeneity, we experimentally show efficiencies of o2sls and oful-iv.",,2023-02-18,2024-02-25,"['riccardo della vecchia', 'debabrota basu']"
2302.09656,credal bayesian deep learning,cs.lg stat.ml,"uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. although bayesian neural networks (bnns) allow for uncertainty in the predictions to be assessed, different sources of uncertainty are indistinguishable. we present credal bayesian deep learning (cbdl). heuristically, cbdl allows to train an (uncountably) infinite ensemble of bnns, using only finitely many elements. this is possible thanks to prior and likelihood finitely generated credal sets (fgcss), a concept from the imprecise probability literature. intuitively, convex combinations of a finite collection of prior-likelihood pairs are able to represent infinitely many such pairs. after training, cbdl outputs a set of posteriors on the parameters of the neural network. at inference time, such posterior set is used to derive a set of predictive distributions that is in turn utilized to distinguish between aleatoric and epistemic uncertainties, and to quantify them. the predictive set also produces either (i) a collection of outputs enjoying desirable probabilistic guarantees, or (ii) the single output that is deemed the best, that is, the one having the highest predictive lower probability -- another imprecise-probabilistic concept. cbdl is more robust than single bnns to prior and likelihood misspecification, and to distribution shift. we show that cbdl is better at quantifying and disentangling different types of uncertainties than single bnns, ensemble of bnns, and bayesian model averaging. in addition, we apply cbdl to two case studies to demonstrate its downstream tasks capabilities: one, for motion prediction in autonomous driving scenarios, and two, to model blood glucose and insulin dynamics for artificial pancreas control. we show that cbdl performs better when compared to an ensemble of bnns baseline.",,2023-02-19,2024-02-22,"['michele caprio', 'souradeep dutta', 'kuk jin jang', 'vivian lin', 'radoslav ivanov', 'oleg sokolsky', 'insup lee']"
2302.10295,correlation clustering with active learning of pairwise similarities,cs.lg stat.ml,"correlation clustering is a well-known unsupervised learning setting that deals with positive and negative pairwise similarities. in this paper, we study the case where the pairwise similarities are not given in advance and must be queried in a cost-efficient way. thereby, we develop a generic active learning framework for this task that benefits from several advantages, e.g., flexibility in the type of feedback that a user/annotator can provide, adaptation to any correlation clustering algorithm and query strategy, and robustness to noise. in addition, we propose and analyze a number of novel query strategies suited to this setting. we demonstrate the effectiveness of our framework and the proposed query strategies via several experimental studies.",,2023-02-20,2024-02-12,"['linus aronsson', 'morteza haghir chehreghani']"
2302.10364,gaussian processes at the helm(holtz): a more fluid model for ocean   currents,stat.me cs.lg physics.ao-ph stat.ap stat.ml,"given sparse observations of buoy velocities, oceanographers are interested in reconstructing ocean currents away from the buoys and identifying divergences in a current vector field. as a first and modular step, we focus on the time-stationary case - for instance, by restricting to short time periods. since we expect current velocity to be a continuous but highly non-linear function of spatial location, gaussian processes (gps) offer an attractive model. but we show that applying a gp with a standard stationary kernel directly to buoy data can struggle at both current reconstruction and divergence identification, due to some physically unrealistic prior assumptions. to better reflect known physical properties of currents, we propose to instead put a standard stationary kernel on the divergence and curl-free components of a vector field obtained through a helmholtz decomposition. we show that, because this decomposition relates to the original vector field just via mixed partial derivatives, we can still perform inference given the original data with only a small constant multiple of additional computational expense. we illustrate the benefits of our method with theory and experiments on synthetic and real ocean data.",,2023-02-20,2023-06-20,"['renato berlinghieri', 'brian l. trippe', 'david r. burt', 'ryan giordano', 'kaushik srinivasan', 'tamay özgökmen', 'junfei xia', 'tamara broderick']"
2302.10684,contraction and convergence rates for discretized kinetic langevin   dynamics,math.na cs.na stat.co,"we provide a framework to analyze the convergence of discretized kinetic langevin dynamics for $m$-$\nabla$lipschitz, $m$-convex potentials. our approach gives convergence rates of $\mathcal{o}(m/m)$, with explicit stepsize restrictions, which are of the same order as the stability threshold for gaussian targets and are valid for a large interval of the friction parameter. we apply this methodology to various integration schemes which are popular in the molecular dynamics and machine learning communities. finally, we introduce the property ""$\gamma$-limit convergent"" (glc) to characterize underdamped langevin schemes that converge to overdamped dynamics in the high-friction limit and which have stepsize restrictions that are independent of the friction parameter; we show that this property is not generic by exhibiting methods from both the class and its complement. we further provide asymptotic bias estimates for the baoab scheme, which remain accurate in the high-friction limit by comparison to a modified stochastic dynamics which preserves the invariant measure.",,2023-02-21,2024-01-23,"['benedict leimkuhler', 'daniel paulin', 'peter a. whalley']"
2302.11944,counterfactual situation testing: uncovering discrimination under   fairness given the difference,stat.ml cs.cy cs.lg,"we present counterfactual situation testing (cst), a causal data mining framework for detecting discrimination in classifiers. cst aims to answer in an actionable and meaningful way the intuitive question ""what would have been the model outcome had the individual, or complainant, been of a different protected status?"" it extends the legally-grounded situation testing of thanh et al. (2011) by operationalizing the notion of fairness given the difference using counterfactual reasoning. for any complainant, we find and compare similar protected and non-protected instances in the dataset used by the classifier to construct a control and test group, where a difference between the decision outcomes of the two groups implies potential individual discrimination. unlike situation testing, which builds both groups around the complainant, we build the test group on the complainant's counterfactual generated using causal knowledge. the counterfactual is intended to reflect how the protected attribute when changed affects the seemingly neutral attributes used by the classifier, which is taken for granted in many frameworks for discrimination. under cst, we compare similar individuals within each group but dissimilar individuals across both groups due to the possible difference between the complainant and its counterfactual. evaluating our framework on two classification scenarios, we show that it uncovers a greater number of cases than situation testing, even when the classifier satisfies the counterfactual fairness condition of kusner et al. (2017).",10.1145/3617694.3623222,2023-02-23,2023-10-16,"['jose m. alvarez', 'salvatore ruggieri']"
2302.12024,comparative study of coupling and autoregressive flows through robust   statistical tests,stat.ml cs.lg hep-ex hep-ph,"normalizing flows have emerged as a powerful brand of generative models, as they not only allow for efficient sampling of complicated target distributions, but also deliver density estimation by construction. we propose here an in-depth comparison of coupling and autoregressive flows, both of the affine and rational quadratic spline type, considering four different architectures: real-valued non-volume preserving (realnvp), masked autoregressive flow (maf), coupling rational quadratic spline (c-rqs), and autoregressive rational quadratic spline (a-rqs). we focus on a set of multimodal target distributions of increasing dimensionality ranging from 4 to 400. the performances are compared by means of different test-statistics for two-sample tests, built from known distance measures: the sliced wasserstein distance, the dimension-averaged one-dimensional kolmogorov-smirnov test, and the frobenius norm of the difference between correlation matrices. furthermore, we include estimations of the variance of both the metrics and the trained models. our results indicate that the a-rqs algorithm stands out both in terms of accuracy and training speed. nonetheless, all the algorithms are generally able, without too much fine-tuning, to learn complicated distributions with limited training data and in a reasonable time, of the order of hours on a tesla a40 gpu. the only exception is the c-rqs, which takes significantly longer to train, does not always provide good accuracy, and becomes unstable for large dimensionalities. all algorithms have been implemented using tensorflow2 and tensorflow probability and made available on \href{https://github.com/nf4hep/normalizingflowshd}{github}.",,2023-02-23,2024-01-16,"['andrea coccaro', 'marco letizia', 'humberto reyes-gonzalez', 'riccardo torre']"
2302.12074,active learning for structural reliability analysis with multiple limit   state functions through variance-enhanced pc-kriging surrogate models,cs.lg cs.ai stat.ap stat.co,"existing active strategies for training surrogate models yield accurate structural reliability estimates by aiming at design space regions in the vicinity of a specified limit state function. in many practical engineering applications, various damage conditions, e.g. repair, failure, should be probabilistically characterized, thus demanding the estimation of multiple performance functions. in this work, we investigate the capability of active learning approaches for efficiently selecting training samples under a limited computational budget while still preserving the accuracy associated with multiple surrogated limit states. specifically, pc-kriging-based surrogate models are actively trained considering a variance correction derived from leave-one-out cross-validation error information, whereas the sequential learning scheme relies on u-function-derived metrics. the proposed active learning approaches are tested in a highly nonlinear structural reliability setting, whereas in a more practical application, failure and repair events are stochastically predicted in the aftermath of a ship collision against an offshore wind substructure. the results show that a balanced computational budget administration can be effectively achieved by successively targeting the specified multiple limit state functions within a unified active learning scheme.",,2023-02-23,,"['j. moran a.', 'p. g. morato', 'p. rigo']"
2302.12148,streaming data recovery via bayesian tensor train decomposition,cs.lg math.st stat.ml stat.th,"in this paper, we study a bayesian tensor train (tt) decomposition method to recover streaming data by approximating the latent structure in high-order streaming data. drawing on the streaming variational bayes method, we introduce the tt format into bayesian tensor decomposition methods for streaming data, and formulate posteriors of tt cores. thanks to the bayesian framework of the tt format, the proposed algorithm (sptt) excels in recovering streaming data with high-order, incomplete, and noisy properties. the experiments in synthetic and real-world datasets show the accuracy of our method compared to state-of-the-art bayesian tensor decomposition methods for streaming data.",,2023-02-23,2024-02-28,"['yunyu huang', 'yani feng', 'qifeng liao']"
2302.12565,variational linearized laplace approximation for bayesian deep learning,stat.ml cs.lg,"the linearized laplace approximation (lla) has been recently used to perform uncertainty estimation on the predictions of pre-trained deep neural networks (dnns). however, its widespread application is hindered by significant computational costs, particularly in scenarios with a large number of training points or dnn parameters. consequently, additional approximations of lla, such as kronecker-factored or diagonal approximate ggn matrices, are utilized, potentially compromising the model's performance. to address these challenges, we propose a new method for approximating lla using a variational sparse gaussian process (gp). our method is based on the dual rkhs formulation of gps and retains as the predictive mean the output of the original dnn. furthermore, it allows for efficient stochastic optimization, which results in sub-linear training time in the size of the training dataset. specifically, its training cost is independent of the number of training points. we compare our proposed method against accelerated lla (ella), which relies on the nystr\""om approximation, as well as other lla variants employing the sample-then-optimize principle. experimental results, both on regression and classification datasets, show that our method outperforms these already existing efficient variants of lla, both in terms of the quality of the predictive distribution and in terms of total computational time.",,2023-02-24,2024-02-02,"['luis a. ortega', 'simón rodríguez santana', 'daniel hernández-lobato']"
2302.14753,learning hidden markov models using conditional samples,cs.lg cs.ai stat.ml,"this paper is concerned with the computational complexity of learning the hidden markov model (hmm). although hmms are some of the most widely used tools in sequential and time series modeling, they are cryptographically hard to learn in the standard setting where one has access to i.i.d. samples of observation sequences. in this paper, we depart from this setup and consider an interactive access model, in which the algorithm can query for samples from the conditional distributions of the hmms. we show that interactive access to the hmm enables computationally efficient learning algorithms, thereby bypassing cryptographic hardness. specifically, we obtain efficient algorithms for learning hmms in two settings:   (a) an easier setting where we have query access to the exact conditional probabilities. here our algorithm runs in polynomial time and makes polynomially many queries to approximate any hmm in total variation distance.   (b) a harder setting where we can only obtain samples from the conditional distributions. here the performance of the algorithm depends on a new parameter, called the fidelity of the hmm. we show that this captures cryptographically hard instances and previously known positive results.   we also show that these results extend to a broader class of distributions with latent low rank structure. our algorithms can be viewed as generalizations and robustifications of angluin's $l^*$ algorithm for learning deterministic finite automata from membership queries.",,2023-02-28,2024-02-24,"['sham m. kakade', 'akshay krishnamurthy', 'gaurav mahajan', 'cyril zhang']"
2303.01031,identifiability and consistent estimation for gaussian chain graph   models,stat.me,"the chain graph model admits both undirected and directed edges in one graph, where symmetric conditional dependencies are encoded via undirected edges and asymmetric causal relations are encoded via directed edges. though frequently encountered in practice, the chain graph model has been largely under investigated in literature, possibly due to the lack of identifiability conditions between undirected and directed edges. in this paper, we first establish a set of novel identifiability conditions for the gaussian chain graph model, exploiting a low rank plus sparse decomposition of the precision matrix. further, an efficient learning algorithm is built upon the identifiability conditions to fully recover the chain graph structure. theoretical analysis on the proposed method is conducted, assuring its asymptotic consistency in recovering the exact chain graph structure. the advantage of the proposed method is also supported by numerical experiments on both simulated examples and a real application on the standard & poor 500 index data.",,2023-03-02,2024-01-26,"['ruixuan zhao', 'haoran zhang', 'junhui wang']"
2303.01422,design-based conformal prediction,stat.me stat.ml,"conformal prediction is an assumption-lean approach to generating distribution-free prediction intervals or sets, for nearly arbitrary predictive models, with guaranteed finite-sample coverage. conformal methods are an active research topic in statistics and machine learning, but only recently have they been extended to non-exchangeable data. in this paper, we invite survey methodologists to begin using and contributing to conformal methods. we introduce how conformal prediction can be applied to data from several common complex sample survey designs, under a framework of design-based inference for a finite population, and we point out gaps where survey methodologists could fruitfully apply their expertise. our simulations empirically bear out the theoretical guarantees of finite-sample coverage, and our real-data example demonstrates how conformal prediction can be applied to complex sample survey data in practice.",,2023-03-02,2023-07-27,['jerzy wieczorek']
2303.02444,calibrating transformers via sparse gaussian processes,cs.lg stat.ml,"transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. extending transformer's success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. to address this, we propose sparse gaussian process attention (sgpa), which performs bayesian inference directly in the output space of multi-head attention blocks (mhas) in transformer to calibrate its uncertainty. it replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse gaussian processes (sgp) techniques to approximate the posterior processes of mha outputs. empirically, on a suite of prediction tasks on text, images and graphs, sgpa-based transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.",,2023-03-04,2024-01-23,"['wenlong chen', 'yingzhen li']"
2303.02561,camel: curvature-augmented manifold embedding and learning,cs.lg stat.ml,"a novel method, named curvature-augmented manifold embedding and learning (camel), is proposed for high dimensional data classification, dimension reduction, and visualization. camel utilizes a topology metric defined on the riemannian manifold, and a unique riemannian metric for both distance and curvature to enhance its expressibility. the method also employs a smooth partition of unity operator on the riemannian manifold to convert localized orthogonal projection to global embedding, which captures both the overall topological structure and local similarity simultaneously. the local orthogonal vectors provide a physical interpretation of the significant characteristics of clusters. therefore, camel not only provides a low-dimensional embedding but also interprets the physics behind this embedding. camel has been evaluated on various benchmark datasets and has shown to outperform state-of-the-art methods, especially for high-dimensional datasets. the method's distinct benefits are its high expressibility, interpretability, and scalability. the paper provides a detailed discussion on riemannian distance and curvature metrics, physical interpretability, hyperparameter effect, manifold stability, and computational efficiency for a holistic understanding of camel. finally, the paper presents the limitations and future work of camel along with key conclusions.",,2023-03-04,2024-01-16,"['nan xu', 'yongming liu']"
2303.02566,mfai: a scalable bayesian matrix factorization approach to leveraging   auxiliary information,stat.ml cs.lg stat.co,"in various practical situations, matrix factorization methods suffer from poor data quality, such as high data sparsity and low signal-to-noise ratio (snr). here, we consider a matrix factorization problem by utilizing auxiliary information, which is massively available in real-world applications, to overcome the challenges caused by poor data quality. unlike existing methods that mainly rely on simple linear models to combine auxiliary information with the main data matrix, we propose to integrate gradient boosted trees in the probabilistic matrix factorization framework to effectively leverage auxiliary information (mfai). thus, mfai naturally inherits several salient features of gradient boosted trees, such as the capability of flexibly modeling nonlinear relationships and robustness to irrelevant features and missing values in auxiliary information. the parameters in mfai can be automatically determined under the empirical bayes framework, making it adaptive to the utilization of auxiliary information and immune to overfitting. moreover, mfai is computationally efficient and scalable to large datasets by exploiting variational inference. we demonstrate the advantages of mfai through comprehensive numerical results from simulation studies and real data analyses. our approach is implemented in the r package mfair available at https://github.com/yanglabhkust/mfair.",,2023-03-04,2024-02-12,"['zhiwei wang', 'fa zhang', 'cong zheng', 'xianghong hu', 'mingxuan cai', 'can yang']"
2303.02901,$\alpha$-divergence improves the entropy production estimation via   machine learning,cond-mat.stat-mech cs.lg stat.ml,"recent years have seen a surge of interest in the algorithmic estimation of stochastic entropy production (ep) from trajectory data via machine learning. a crucial element of such algorithms is the identification of a loss function whose minimization guarantees the accurate ep estimation. in this study, we show that there exists a host of loss functions, namely those implementing a variational representation of the $\alpha$-divergence, which can be used for the ep estimation. by fixing $\alpha$ to a value between $-1$ and $0$, the $\alpha$-neep (neural estimator for entropy production) exhibits a much more robust performance against strong nonequilibrium driving or slow dynamics, which adversely affects the existing method based on the kullback-leibler divergence ($\alpha = 0$). in particular, the choice of $\alpha = -0.5$ tends to yield the optimal results. to corroborate our findings, we present an exactly solvable simplification of the ep estimation problem, whose loss function landscape and stochastic properties give deeper intuition into the robustness of the $\alpha$-neep.",,2023-03-06,2024-01-19,"['euijoon kwon', 'yongjoo baek']"
2303.03374,to stay or not to stay in the pre-train basin: insights on ensembling in   transfer learning,cs.lg stat.ml,"transfer learning and ensembling are two popular techniques for improving the performance and robustness of neural networks. due to the high cost of pre-training, ensembles of models fine-tuned from a single pre-trained checkpoint are often used in practice. such models end up in the same basin of the loss landscape, which we call the pre-train basin, and thus have limited diversity. in this work, we show that ensembles trained from a single pre-trained checkpoint may be improved by better exploring the pre-train basin, however, leaving the basin results in losing the benefits of transfer learning and in degradation of the ensemble quality. based on the analysis of existing exploration methods, we propose a more effective modification of the snapshot ensembles (sse) for transfer learning setup, starsse, which results in stronger ensembles and uniform model soups.",,2023-03-06,2024-01-15,"['ildus sadrtdinov', 'dmitrii pozdeev', 'dmitry vetrov', 'ekaterina lobacheva']"
2303.03502,analyzing risk factors for post-acute recovery in older adults with   alzheimer's disease and related dementia: a new semi-parametric model for   large-scale medicare claims,stat.me,"nearly 300,000 older adults experience a hip fracture every year, the majority of which occur following a fall. unfortunately, recovery after fall-related trauma such as hip fracture is poor, where older adults diagnosed with alzheimer's disease and related dementia (adrd) spend a particularly long time in hospitals or rehabilitation facilities during the post-operative recuperation period. because older adults value functional recovery and spending time at home versus facilities as key outcomes after hospitalization, identifying factors that influence days spent at home after hospitalization is imperative. while several individual-level factors have been identified, the characteristics of the treating hospital have recently been identified as contributors. however, few methodological rigorous approaches are available to help overcome potential sources of bias such as hospital-level unmeasured confounders, informative hospital size, and loss to follow-up due to death. this article develops a useful tool equipped with unsupervised learning to simultaneously handle statistical complexities that are often encountered in health services research, especially when using large administrative claims databases. the proposed estimator has a closed form, thus only requiring light computation load in a large-scale study. we further develop its asymptotic properties that can be used to make statistical inference in practice. extensive simulation studies demonstrate superiority of the proposed estimator compared to existing estimators.",10.1002/sim.9982,2023-03-06,2024-02-01,"['biyi shen', 'haoyu ren', 'michelle shardell', 'jason falvey', 'chixiang chen']"
2303.04040,uncertainty quantification of spatiotemporal travel demand with   probabilistic graph neural networks,cs.lg stat.ap stat.ml,"recent studies have significantly improved the prediction accuracy of travel demand using graph neural networks. however, these studies largely ignored uncertainty that inevitably exists in travel demand prediction. to fill this gap, this study proposes a framework of probabilistic graph neural networks (prob-gnn) to quantify the spatiotemporal uncertainty of travel demand. this prob-gnn framework is substantiated by deterministic and probabilistic assumptions, and empirically applied to the task of predicting the transit and ridesharing demand in chicago. we found that the probabilistic assumptions (e.g. distribution tail, support) have a greater impact on uncertainty prediction than the deterministic ones (e.g. deep modules, depth). among the family of prob-gnns, the gnns with truncated gaussian and laplace distributions achieve the highest performance in transit and ridesharing data. even under significant domain shifts, prob-gnns can predict the ridership uncertainty in a stable manner, when the models are trained on pre-covid data and tested across multiple periods during and after the covid-19 pandemic. prob-gnns also reveal the spatiotemporal pattern of uncertainty, which is concentrated on the afternoon peak hours and the areas with large travel volumes. overall, our findings highlight the importance of incorporating randomness into deep learning for spatiotemporal ridership prediction. future research should continue to investigate versatile probabilistic assumptions to capture behavioral randomness, and further develop methods to quantify uncertainty to build resilient cities.",,2023-03-07,2024-02-22,"['qingyi wang', 'shenhao wang', 'dingyi zhuang', 'haris koutsopoulos', 'jinhua zhao']"
2303.05369,data-dependent generalization bounds via variable-size compressibility,stat.ml cs.it cs.lg math.it,"in this paper, we establish novel data-dependent upper bounds on the generalization error through the lens of a ""variable-size compressibility"" framework that we introduce newly here. in this framework, the generalization error of an algorithm is linked to a variable-size 'compression rate' of its input data. this is shown to yield bounds that depend on the empirical measure of the given input data at hand, rather than its unknown distribution. our new generalization bounds that we establish are tail bounds, tail bounds on the expectation, and in-expectations bounds. moreover, it is shown that our framework also allows to derive general bounds on any function of the input data and output hypothesis random variables. in particular, these general bounds are shown to subsume and possibly improve over several existing pac-bayes and data-dependent intrinsic dimension-based bounds that are recovered as special cases, thus unveiling a unifying character of our approach. for instance, a new data-dependent intrinsic dimension-based bound is established, which connects the generalization error to the optimization trajectories and reveals various interesting connections with the rate-distortion dimension of a process, the r\'enyi information dimension of a process, and the metric mean dimension.",,2023-03-09,2024-01-30,"['milad sefidgaran', 'abdellatif zaidi']"
2303.05445,flooding with absorption: an efficient protocol for heterogeneous   bandits over complex networks,cs.lg cs.dc cs.ni stat.ml,"multi-armed bandits are extensively used to model sequential decision-making, making them ubiquitous in many real-life applications such as online recommender systems and wireless networking. we consider a multi-agent setting where each agent solves their own bandit instance endowed with a different set of arms. their goal is to minimize their group regret while collaborating via some communication protocol over a given network. previous literature on this problem only considered arm heterogeneity and networked agents separately. in this work, we introduce a setting that encompasses both features. for this novel setting, we first provide a rigorous regret analysis for a standard flooding protocol combined with the classic ucb policy. then, to mitigate the issue of high communication costs incurred by flooding in complex networks, we propose a new protocol called flooding with absorption (fwa). we provide a theoretical analysis of the resulting regret bound and discuss the advantages of using fwa over flooding. lastly, we experimentally verify on various scenarios, including dynamic networks, that fwa leads to significantly lower communication costs despite minimal regret performance loss compared to other network protocols.",10.4230/lipics.opodis.2023.20,2023-03-09,2024-02-25,"['junghyun lee', 'laura schmid', 'se-young yun']"
2303.05754,decomposed diffusion sampler for accelerating large-scale inverse   problems,cs.lg cs.ai cs.cv stat.ml,"krylov subspace, which is generated by multiplying a given vector by the matrix of a linear transformation and its successive powers, has been extensively studied in classical optimization literature to design algorithms that converge quickly for large linear inverse problems. for example, the conjugate gradient method (cg), one of the most popular krylov subspace methods, is based on the idea of minimizing the residual error in the krylov subspace. however, with the recent advancement of high-performance diffusion solvers for inverse problems, it is not clear how classical wisdom can be synergistically combined with modern diffusion models. in this study, we propose a novel and efficient diffusion sampling strategy that synergistically combines the diffusion sampling and krylov subspace methods. specifically, we prove that if the tangent space at a denoised sample by tweedie's formula forms a krylov subspace, then the cg initialized with the denoised data ensures the data consistency update to remain in the tangent space. this negates the need to compute the manifold-constrained gradient (mcg), leading to a more efficient diffusion sampling method. our method is applicable regardless of the parametrization and setting (i.e., ve, vp). notably, we achieve state-of-the-art reconstruction quality on challenging real-world medical inverse imaging problems, including multi-coil mri reconstruction and 3d ct reconstruction. moreover, our proposed method achieves more than 80 times faster inference time than the previous state-of-the-art method. code is available at https://github.com/hj-harry/dds",,2023-03-10,2024-02-19,"['hyungjin chung', 'suhyeon lee', 'jong chul ye']"
2303.05910,product jacobi-theta boltzmann machines with score matching,stat.ml cs.lg,"the estimation of probability density functions is a non trivial task that over the last years has been tackled with machine learning techniques. successful applications can be obtained using models inspired by the boltzmann machine (bm) architecture. in this manuscript, the product jacobi-theta boltzmann machine (pjtbm) is introduced as a restricted version of the riemann-theta boltzmann machine (rtbm) with diagonal hidden sector connection matrix. we show that score matching, based on the fisher divergence, can be used to fit probability densities with the pjtbm more efficiently than with the original rtbm.",,2023-03-10,2024-01-12,"['andrea pasquale', 'daniel krefl', 'stefano carrazza', 'frank nielsen']"
2303.06815,"on model compression for neural networks: framework, algorithm, and   convergence guarantee",cs.lg stat.ml,"model compression is a crucial part of deploying neural networks (nns), especially when the memory and storage of computing devices are limited in many applications. this paper focuses on two model compression techniques: low-rank approximation and weight pruning in neural networks, which are very popular nowadays. however, training nn with low-rank approximation and weight pruning always suffers significant accuracy loss and convergence issues. in this paper, a holistic framework is proposed for model compression from a novel perspective of nonconvex optimization by designing an appropriate objective function. then, we introduce nn-bcd, a block coordinate descent (bcd) algorithm to solve the nonconvex optimization. one advantage of our algorithm is that an efficient iteration scheme can be derived with closed-form, which is gradient-free. therefore, our algorithm will not suffer from vanishing/exploding gradient problems. furthermore, with the kurdyka-{\l}ojasiewicz (k{\l}) property of our objective function, we show that our algorithm globally converges to a critical point at the rate of o(1/k), where k denotes the number of iterations. lastly, extensive experiments with tensor train decomposition and weight pruning demonstrate the efficiency and superior performance of the proposed framework. our code implementation is available at https://github.com/chenyangli-97/nn-bcd",,2023-03-12,2024-01-04,"['chenyang li', 'jihoon chung', 'biao cai', 'haimin wang', 'xianlian zhou', 'bo shen']"
2303.07154,differential good arm identification,cs.lg stat.ml,"this paper targets a variant of the stochastic multi-armed bandit problem called good arm identification (gai). gai is a pure-exploration bandit problem with the goal to output as many good arms using as few samples as possible, where a good arm is defined as an arm whose expected reward is greater than a given threshold. in this work, we propose dgai - a differentiable good arm identification algorithm to improve the sample complexity of the state-of-the-art hdoc algorithm in a data-driven fashion. we also showed that the dgai can further boost the performance of a general multi-arm bandit (mab) problem given a threshold as a prior knowledge to the arm set. extensive experiments confirm that our algorithm outperform the baseline algorithms significantly in both synthetic and real world datasets for both gai and mab tasks.",,2023-03-13,2024-02-15,"['yun-da tsai', 'tzu-hsien tsai', 'shou-de lin']"
2303.07287,tight non-asymptotic inference via sub-gaussian intrinsic moment norm,stat.ml cs.lg econ.em,"in non-asymptotic learning, variance-type parameters of sub-gaussian distributions are of paramount importance. however, directly estimating these parameters using the empirical moment generating function (mgf) is infeasible. to address this, we suggest using the sub-gaussian intrinsic moment norm [buldygin and kozachenko (2000), theorem 1.3] achieved by maximizing a sequence of normalized moments. significantly, the suggested norm can not only reconstruct the exponential moment bounds of mgfs but also provide tighter sub-gaussian concentration inequalities. in practice, we provide an intuitive method for assessing whether data with a finite sample size is sub-gaussian, utilizing the sub-gaussian plot. the intrinsic moment norm can be robustly estimated via a simple plug-in approach. our theoretical findings are also applicable to reinforcement learning, including the multi-armed bandit scenario.",,2023-03-13,2024-01-19,"['huiming zhang', 'haoyu wei', 'guang cheng']"
2303.07706,on the utility of equal batch sizes for inference in stochastic gradient   descent,stat.co stat.me,"stochastic gradient descent (sgd) is an estimation tool for large data employed in machine learning and statistics. due to the markovian nature of the sgd process, inference is a challenging problem. an underlying asymptotic normality of the averaged sgd (asgd) estimator allows for the construction of a batch-means estimator of the asymptotic covariance matrix. instead of the usual increasing batch-size strategy employed in asgd, we propose a memory efficient equal batch-size strategy and show that under mild conditions, the estimator is consistent. a key feature of the proposed batching technique is that it allows for bias-correction of the variance, at no cost to memory. since joint inference for high dimensional problems may be undesirable, we present marginal-friendly simultaneous confidence intervals, and show through an example how covariance estimators of asgd can be employed in improved predictions.",,2023-03-14,2024-01-23,"['rahul singh', 'abhinek shukla', 'dootika vats']"
2303.08431,policy gradient converges to the globally optimal policy for nearly   linear-quadratic regulators,cs.lg math.oc stat.ml,"nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. as a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. in particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. assuming that the nonlinear component comprises kernels with small lipschitz coefficients, we characterize the optimization landscape of the cost function. although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. additionally, we propose an initialization mechanism to leverage these properties. building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.",,2023-03-15,2024-02-16,"['yinbin han', 'meisam razaviyayn', 'renyuan xu']"
2303.10019,multivariate probabilistic crps learning with an application to   day-ahead electricity prices,stat.ml cs.lg econ.em q-fin.cp stat.ap,"this paper presents a new method for combining (or aggregating or ensembling) multivariate probabilistic forecasts, considering dependencies between quantiles and marginals through a smoothing procedure that allows for online learning. we discuss two smoothing methods: dimensionality reduction using basis matrices and penalized smoothing. the new online learning algorithm generalizes the standard crps learning framework into multivariate dimensions. it is based on bernstein online aggregation (boa) and yields optimal asymptotic learning properties. the procedure uses horizontal aggregation, i.e., aggregation across quantiles. we provide an in-depth discussion on possible extensions of the algorithm and several nested cases related to the existing literature on online forecast combination. we apply the proposed methodology to forecasting day-ahead electricity prices, which are 24-dimensional distributional forecasts. the proposed method yields significant improvements over uniform combination in terms of continuous ranked probability score (crps). we discuss the temporal evolution of the weights and hyperparameters and present the results of reduced versions of the preferred model. a fast c++ implementation of the proposed algorithm is provided in the open-source r-package profoc on cran.",,2023-03-17,2024-02-06,"['jonathan berrisch', 'florian ziel']"
2303.10165,optimal horizon-free reward-free exploration for linear mixture mdps,cs.lg math.oc stat.ml,"we study reward-free reinforcement learning (rl) with linear function approximation, where the agent works in two phases: (1) in the exploration phase, the agent interacts with the environment but cannot access the reward; and (2) in the planning phase, the agent is given a reward function and is expected to find a near-optimal policy based on samples collected in the exploration phase. the sample complexities of existing reward-free algorithms have a polynomial dependence on the planning horizon, which makes them intractable for long planning horizon rl problems. in this paper, we propose a new reward-free algorithm for learning linear mixture markov decision processes (mdps), where the transition probability can be parameterized as a linear combination of known feature mappings. at the core of our algorithm is uncertainty-weighted value-targeted regression with exploration-driven pseudo-reward and a high-order moment estimator for the aleatoric and epistemic uncertainties. when the total reward is bounded by $1$, we show that our algorithm only needs to explore $\tilde o( d^2\varepsilon^{-2})$ episodes to find an $\varepsilon$-optimal policy, where $d$ is the dimension of the feature mapping. the sample complexity of our algorithm only has a polylogarithmic dependence on the planning horizon and therefore is ""horizon-free"". in addition, we provide an $\omega(d^2\varepsilon^{-2})$ sample complexity lower bound, which matches the sample complexity of our algorithm up to logarithmic factors, suggesting that our algorithm is optimal.",,2023-03-17,2024-02-14,"['junkai zhang', 'weitong zhang', 'quanquan gu']"
2303.10167,generalized partitioned local depth,stat.ml cs.lg cs.si physics.soc-ph,"in this paper we provide a generalization of the concept of cohesion as introduced recently by berenhaut, moore and melvin [proceedings of the national academy of sciences, 119 (4) (2022)]. the formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included. the work sheds light on the foundations of partitioned local depth, and extends the original ideas to enable probabilistic consideration of uncertain, variable and potentially conflicting information.",10.1007/s42519-023-00356-1,2023-03-17,2023-11-14,"['kenneth s. berenhaut', 'john d. foley', 'liangdongsheng lyu']"
2303.10931,approaching an unknown communication system by latent space exploration   and causal inference,stat.ml cs.lg cs.sd eess.as,"this paper proposes a methodology for discovering meaningful properties in data by exploring the latent space of unsupervised deep generative models. we combine manipulation of individual latent variables to extreme values with methods inspired by causal inference into an approach we call causal disentanglement with extreme values (cdev) and show that this method yields insights for model interpretability. with this, we can test for what properties of unknown data the model encodes as meaningful, using it to glean insight into the communication system of sperm whales (physeter macrocephalus), one of the most intriguing and understudied animal communication systems. the network architecture used has been shown to learn meaningful representations of speech; here, it is used as a learning mechanism to decipher the properties of another vocal communication system in which case we have no ground truth. the proposed methodology suggests that sperm whales encode information using the number of clicks in a sequence, the regularity of their timing, and audio properties such as the spectral mean and the acoustic regularity of the sequences. some of these findings are consistent with existing hypotheses, while others are proposed for the first time. we also argue that our models uncover rules that govern the structure of units in the communication system and apply them while generating innovative data not shown during training. this paper suggests that an interpretation of the outputs of deep neural networks with causal inference methodology can be a viable strategy for approaching data about which little is known and presents another case of how deep learning can limit the hypothesis space. finally, the proposed approach can be extended to other architectures and datasets.",,2023-03-20,2024-02-06,"['gašper beguš', 'andrej leban', 'shane gero']"
2303.11835,lipschitz-bounded 1d convolutional neural networks using the cayley   transform and the controllability gramian,cs.lg cs.sy eess.sy stat.ml,"we establish a layer-wise parameterization for 1d convolutional neural networks (cnns) with built-in end-to-end robustness guarantees. in doing so, we use the lipschitz constant of the input-output mapping characterized by a cnn as a robustness measure. we base our parameterization on the cayley transform that parameterizes orthogonal matrices and the controllability gramian of the state space representation of the convolutional layers. the proposed parameterization by design fulfills linear matrix inequalities that are sufficient for lipschitz continuity of the cnn, which further enables unconstrained training of lipschitz-bounded 1d cnns. finally, we train lipschitz-bounded 1d cnns for the classification of heart arrythmia data and show their improved robustness.",,2023-03-20,2024-01-25,"['patricia pauli', 'ruigang wang', 'ian r. manchester', 'frank allgöwer']"
2303.12407,non-asymptotic analysis of langevin-type monte carlo algorithms,math.st math.pr stat.ml stat.th,we study langevin-type algorithms for sampling from gibbs distributions such that the potentials are dissipative and their weak gradients have finite moduli of continuity not necessarily convergent to zero. our main result is a non-asymptotic upper bound of the 2-wasserstein distance between a gibbs distribution and the law of general langevin-type algorithms based on the liptser--shiryaev theory and poincar\'{e} inequalities. we apply this bound to show that the langevin monte carlo algorithm can approximate gibbs distributions with arbitrary accuracy if the potentials are dissipative and their gradients are uniformly continuous. we also propose langevin-type algorithms with spherical smoothing for distributions whose potentials are not convex or continuously differentiable.,,2023-03-22,2024-02-28,['shogo nakakita']
2303.13850,towards learning and explaining indirect causal effects in neural   networks,cs.lg cs.ai stat.me,"recently, there has been a growing interest in learning and explaining causal effects within neural network (nn) models. by virtue of nn architectures, previous approaches consider only direct and total causal effects assuming independence among input variables. we view an nn as a structural causal model (scm) and extend our focus to include indirect causal effects by introducing feedforward connections among input neurons. we propose an ante-hoc method that captures and maintains direct, indirect, and total causal effects during nn model training. we also propose an algorithm for quantifying learned causal effects in an nn model and efficient approximation strategies for quantifying causal effects in high-dimensional data. extensive experiments conducted on synthetic and real-world datasets demonstrate that the causal effects learned by our ante-hoc method better approximate the ground truth effects compared to existing methods.",,2023-03-24,2024-01-08,"['abbavaram gowtham reddy', 'saketh bachu', 'harsharaj pathak', 'benin l godfrey', 'vineeth n. balasubramanian', 'varshaneya v', 'satya narayanan kar']"
2303.14226,synthetic combinations: a causal inference framework for combinatorial   interventions,stat.me cs.lg econ.em stat.ml,"consider a setting where there are $n$ heterogeneous units and $p$ interventions. our goal is to learn unit-specific potential outcomes for any combination of these $p$ interventions, i.e., $n \times 2^p$ causal parameters. choosing a combination of interventions is a problem that naturally arises in a variety of applications such as factorial design experiments, recommendation engines, combination therapies in medicine, conjoint analysis, etc. running $n \times 2^p$ experiments to estimate the various parameters is likely expensive and/or infeasible as $n$ and $p$ grow. further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome under that combination. to address these challenges, we propose a novel latent factor model that imposes structure across units (i.e., the matrix of potential outcomes is approximately rank $r$), and combinations of interventions (i.e., the coefficients in the fourier expansion of the potential outcomes is approximately $s$ sparse). we establish identification for all $n \times 2^p$ parameters despite unobserved confounding. we propose an estimation procedure, synthetic combinations, and establish it is finite-sample consistent and asymptotically normal under precise conditions on the observation pattern. our results imply consistent estimation given $\text{poly}(r) \times \left( n + s^2p\right)$ observations, while previous methods have sample complexity scaling as $\min(n \times s^2p, \ \ \text{poly(r)} \times (n + 2^p))$. we use synthetic combinations to propose a data-efficient experimental design. empirically, synthetic combinations outperforms competing approaches on a real-world dataset on movie recommendations. lastly, we extend our analysis to do causal inference where the intervention is a permutation over $p$ items (e.g., rankings).",,2023-03-24,2024-01-15,"['abhineet agarwal', 'anish agarwal', 'suhas vijaykumar']"
2303.15041,towards black-box parameter estimation,stat.ml cs.lg,"deep learning algorithms have recently shown to be a successful tool in estimating parameters of statistical models for which simulation is easy, but likelihood computation is challenging. but the success of these approaches depends on simulating parameters that sufficiently reproduce the observed data, and, at present, there is a lack of efficient methods to produce these simulations. we develop new black-box procedures to estimate parameters of statistical models based only on weak parameter structure assumptions. for well-structured likelihoods with frequent occurrences, such as in time series, this is achieved by pre-training a deep neural network on an extensive simulated database that covers a wide range of data sizes. for other types of complex dependencies, an iterative algorithm guides simulations to the correct parameter region in multiple rounds. these approaches can successfully estimate and quantify the uncertainty of parameters from non-gaussian models with complex spatial and temporal dependencies. the success of our methods is a first step towards a fully flexible automatic black-box estimation framework.",,2023-03-27,2024-02-19,"['amanda lenzi', 'haavard rue']"
2303.15579,adjusted wasserstein distributionally robust estimator in statistical   learning,stat.ml cs.lg,"we propose an adjusted wasserstein distributionally robust estimator -- based on a nonlinear transformation of the wasserstein distributionally robust (wdro) estimator in statistical learning. the classic wdro estimator is asymptotically biased, while our adjusted wdro estimator is asymptotically unbiased, resulting in a smaller asymptotic mean squared error. meanwhile, the proposed adjusted wdro has an out-of-sample performance guarantee. further, under certain conditions, our proposed adjustment technique provides a general principle to de-bias asymptotically biased estimators. specifically, we will investigate how the adjusted wdro estimator is developed in the generalized linear model, including logistic regression, linear regression, and poisson regression. numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.",,2023-03-27,2024-01-17,"['yiling xie', 'xiaoming huo']"
2303.16372,on the query complexity of training data reconstruction in private   learning,cs.lg cs.cr stat.ml,"we analyze the number of queries that a whitebox adversary needs to make to a private learner in order to reconstruct its training data. for $(\epsilon, \delta)$ dp learners with training data drawn from any arbitrary compact metric space, we provide the \emph{first known lower bounds on the adversary's query complexity} as a function of the learner's privacy parameters. \emph{our results are minimax optimal for every $\epsilon \geq 0, \delta \in [0, 1]$, covering both $\epsilon$-dp and $(0, \delta)$ dp as corollaries}. beyond this, we obtain query complexity lower bounds for $(\alpha, \epsilon)$ r\'enyi dp learners that are valid for any $\alpha > 1, \epsilon \geq 0$. finally, we analyze data reconstruction attacks on locally compact metric spaces via the framework of metric dp, a generalization of dp that accounts for the underlying metric structure of the data. in this setting, we provide the first known analysis of data reconstruction in unbounded, high dimensional spaces and obtain query complexity lower bounds that are nearly tight modulo logarithmic factors.",,2023-03-28,2024-01-11,"['prateeti mukherjee', 'satya lokam']"
2303.17043,federated learning for heterogeneous bandits with unobserved contexts,cs.lg stat.ml,"we study the problem of federated stochastic multi-arm contextual bandits with unknown contexts, in which m agents are faced with different bandits and collaborate to learn. the communication model consists of a central server and the agents share their estimates with the central server periodically to learn to choose optimal actions in order to minimize the total regret. we assume that the exact contexts are not observable and the agents observe only a distribution of the contexts. such a situation arises, for instance, when the context itself is a noisy measurement or based on a prediction mechanism. our goal is to develop a distributed and federated algorithm that facilitates collaborative learning among the agents to select a sequence of optimal actions so as to maximize the cumulative reward. by performing a feature vector transformation, we propose an elimination-based algorithm and prove the regret bound for linearly parametrized reward functions. finally, we validated the performance of our algorithm and compared it with another baseline approach using numerical simulations on synthetic data and on the real-world movielens dataset.",,2023-03-29,2024-01-29,"['jiabin lin', 'shana moothedath']"
2303.17045,training neural networks is np-hard in fixed dimension,cs.cc cs.ds cs.lg cs.ne stat.ml,"we study the parameterized complexity of training two-layer neural networks with respect to the dimension of the input data and the number of hidden neurons, considering relu and linear threshold activation functions. albeit the computational complexity of these problems has been studied numerous times in recent years, several questions are still open. we answer questions by arora et al. [iclr '18] and khalife and basu [ipco '22] showing that both problems are np-hard for two dimensions, which excludes any polynomial-time algorithm for constant dimension. we also answer a question by froese et al. [jair '22] proving w[1]-hardness for four relus (or two linear threshold neurons) with zero training error. finally, in the relu case, we show fixed-parameter tractability for the combined parameter number of dimensions and number of relus if the network is assumed to compute a convex map. our results settle the complexity status regarding these parameters almost completely.",,2023-03-29,2024-01-18,"['vincent froese', 'christoph hertrich']"
2304.00199,applications of no-collision transportation maps in manifold learning,cs.lg cs.cv stat.ml,"in this work, we investigate applications of no-collision transportation maps introduced in [nurbekyan et. al., 2020] in manifold learning for image data. recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. indeed, comparing intensities at fixed locations often does not reveal the data structure. no-collision maps and distances developed in [nurbekyan et. al., 2020] are sensitive to geometric features similar to optimal transportation (ot) maps but much cheaper to compute due to the absence of optimization. in this work, we prove that no-collision distances provide an isometry between translations (respectively dilations) of a single probability measure and the translation (respectively dilation) vectors equipped with a euclidean distance. furthermore, we prove that no-collision transportation maps, as well as ot and linearized ot maps, do not in general provide an isometry for rotations. the numerical experiments confirm our theoretical findings and show that no-collision distances achieve similar or better performance on several manifold learning tasks compared to other ot and euclidean-based methods at a fraction of a computational cost.",,2023-03-31,2024-02-16,"['elisa negrini', 'levon nurbekyan']"
2304.01561,optimal rates of approximation by shallow relu$^k$ neural networks and   applications to nonparametric regression,stat.ml cs.lg,"we study the approximation capacity of some variation spaces corresponding to shallow relu$^k$ neural networks. it is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. for functions with less smoothness, the approximation rates in terms of the variation norm are established. using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow relu$^k$ neural networks. it is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (cnns). as applications, we study convergence rates for nonparametric regression using three relu neural network models: shallow neural network, over-parameterized neural network, and cnn. in particular, we show that shallow neural networks can achieve the minimax optimal rates for learning h\""older functions, which complements recent results for deep neural networks. it is also proven that over-parameterized (deep or shallow) neural networks can achieve nearly optimal rates for nonparametric regression.",,2023-04-04,2024-01-08,"['yunfei yang', 'ding-xuan zhou']"
2304.02388,machine learning of public sentiments toward wind energy in norway,stat.ap,"across europe negative public opinion has and may continue to limit the deployment of renewable energy infrastructure required for the transition to net-zero energy systems. understanding public sentiment and its spatio-temporal variations is as such important for decision-making and socially accepted energy systems. in this study, we apply a sentiment classification model based on a machine learning framework for natural language processing, norbert, on data collected from twitter between 2006 and 2022 to analyse the case of wind power opposition in norway. from the 68828 tweets with geospatial information, we show how discussions about wind power intensified in 2018/2019 together with a trend of more negative tweets up until 2020, both on a regional level and for norway as a whole. furthermore, we find weak geographical clustering in our data, indicating that discussions are country wide and not dominated by specific regional events or developments. twitter data allows for detailed insight into the temporal nature of public sentiments and extending this research to additional case studies of technologies, countries and sources of data (e.g. newspapers, other social media) may prove important to complement traditional survey research and the understanding of public sentiment.",10.1002/we.2902,2023-04-05,,"['oskar vågerö', 'anders bråte', 'alexandra wittemann', 'jessica yarin robinson', 'natalia sirotko-sibirskaya', 'marianne zeyringer']"
2304.02688,going further: flatness at the rescue of early stopping for adversarial   example transferability,cs.lg cs.cv stat.ml,"transferability is the property of adversarial examples to be misclassified by other models than the surrogate model for which they were crafted. previous research has shown that early stopping the training of the surrogate model substantially increases transferability. a common hypothesis to explain this is that deep neural networks (dnns) first learn robust features, which are more generic, thus a better surrogate. then, at later epochs, dnns learn non-robust features, which are more brittle, hence worst surrogate. first, we tend to refute this hypothesis, using transferability as a proxy for representation similarity. we then establish links between transferability and the exploration of the loss landscape in parameter space, focusing on sharpness, which is affected by early stopping. this leads us to evaluate surrogate models trained with seven minimizers that minimize both loss value and loss sharpness. among them, sam consistently outperforms early stopping by up to 28.8 percentage points. we discover that the strong sam regularization from large flat neighborhoods tightly links to transferability. finally, the best sharpness-aware minimizers prove competitive with other training methods and complement existing transferability techniques.",,2023-04-05,2024-02-20,"['martin gubri', 'maxime cordy', 'yves le traon']"
2304.04374,partial identification of causal effects using proxy variables,stat.me math.st stat.ml stat.th,"proximal causal inference is a recently proposed framework for evaluating causal effects in the presence of unmeasured confounding. for point identification of causal effects, it leverages a pair of so-called treatment and outcome confounding proxy variables, to identify a bridge function that matches the dependence of potential outcomes or treatment variables on the hidden factors to corresponding functions of observed proxies. unique identification of a causal effect via a bridge function crucially requires that proxies are sufficiently relevant for hidden factors, a requirement that has previously been formalized as a completeness condition. however, completeness is well-known not to be empirically testable, and although a bridge function may be well-defined, lack of completeness, sometimes manifested by availability of a single type of proxy, may severely limit prospects for identification of a bridge function and thus a causal effect; therefore, potentially restricting the application of the proximal causal framework. in this paper, we propose partial identification methods that do not require completeness and obviate the need for identification of a bridge function. that is, we establish that proxies of unobserved confounders can be leveraged to obtain bounds on the causal effect of the treatment on the outcome even if available information does not suffice to identify either a bridge function or a corresponding causal effect of interest. our bounds are non-smooth functionals of the observed data distribution. as a consequence, in the context of inference, we initially provide a smooth approximation of our bounds. subsequently, we leverage bootstrap confidence intervals on the approximated bounds. we further establish analogous partial identification results in related settings where identification hinges upon hidden mediators for which proxies are available.",,2023-04-10,2024-01-28,"['amiremad ghassami', 'ilya shpitser', 'eric tchetgen tchetgen']"
2304.04657,on the strong stability of ergodic iterations,math.pr stat.ml,"we revisit processes generated by iterated random functions driven by a stationary and ergodic sequence. such a process is called strongly stable if a random initialization exists, for which the process is stationary and ergodic, and for any other initialization, the difference between the two processes converges to zero almost surely. under some mild conditions on the corresponding recursive map, without any condition on the driving sequence, we show the strong stability of iterations. several applications are surveyed such as generalized autoregression and queuing. furthermore, new results are deduced for langevin-type iterations with dependent noise and for multitype branching processes.",,2023-04-10,2024-02-05,"['lászló györfi', 'attila lovas', 'miklós rásonyi']"
2304.05527,"black box variational inference with a deterministic objective: faster,   more accurate, and even more black box",cs.lg stat.me stat.ml,"automatic differentiation variational inference (advi) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. however, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. moreover, advi inherits the poor posterior uncertainty estimates of mean-field variational bayes (mfvb). we introduce ""deterministic advi"" (dadvi) to address these issues. dadvi replaces the intractable mfvb objective with a fixed monte carlo approximation, a technique known in the stochastic optimization literature as the ""sample average approximation"" (saa). by optimizing an approximate but deterministic objective, dadvi can use off-the-shelf second-order optimization, and, unlike standard mean-field advi, is amenable to more accurate posterior covariances via linear response (lr). in contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, dadvi and the saa can perform well with relatively few samples even in very high dimensions, though we also show that such favorable results cannot extend to variational approximations that are too expressive relative to mean-field advi. we show on a variety of real-world problems that dadvi reliably finds good solutions with default settings (unlike advi) and, together with lr covariances, is typically faster and more accurate than standard advi.",,2023-04-11,2024-01-17,"['ryan giordano', 'martin ingram', 'tamara broderick']"
2304.05658,"predicting subgroup treatment effects for a new study: motivations,   results and learnings from running a data challenge in a pharmaceutical   corporation",stat.ap,"we present the motivation, experience and learnings from a data challenge conducted at a large pharmaceutical corporation on the topic of subgroup identification. the data challenge aimed at exploring approaches to subgroup identification for future clinical trials. to mimic a realistic setting, participants had access to 4 phase iii clinical trials to derive a subgroup and predict its treatment effect on a future study not accessible to challenge participants. 30 teams registered for the challenge with around 100 participants, primarily from biostatistics organisation. we outline the motivation for running the challenge, the challenge rules and logistics. finally, we present the results of the challenge, the participant feedback as well as the learnings, and how these learnings can be translated into statistical practice.",10.1002/pst.2368,2023-04-12,,"['björn bornkamp', 'silvia zaoli', 'michela azzarito', 'ruvie martin', 'carsten philipp müller', 'conor moloney', 'giulia capestro', 'david ohlssen', 'mark baillie']"
2304.06686,okridge: scalable optimal k-sparse ridge regression,cs.lg stat.ml,"we consider an important problem in scientific discovery, namely identifying sparse governing equations for nonlinear dynamical systems. this involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. we propose a fast algorithm, okridge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an admm-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. we also propose a method to warm-start our solver, which leverages a beam search. experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing mip formulations solved by the commercial solver gurobi.",,2023-04-13,2024-01-11,"['jiachang liu', 'sam rosen', 'chudi zhong', 'cynthia rudin']"
2304.06787,"a polynomial time, pure differentially private estimator for binary   product distributions",cs.ds cs.cr cs.lg stat.ml,"we present the first $\varepsilon$-differentially private, computationally efficient algorithm that estimates the means of product distributions over $\{0,1\}^d$ accurately in total-variation distance, whilst attaining the optimal sample complexity to within polylogarithmic factors. the prior work had either solved this problem efficiently and optimally under weaker notions of privacy, or had solved it optimally while having exponential running times.",,2023-04-13,2024-01-25,['vikrant singhal']
2304.07235,what does self-attention learn from masked language modelling?,cond-mat.dis-nn cond-mat.stat-mech cs.cl stat.ml,"transformers are neural networks which revolutionised natural language processing and machine learning. they process sequences of inputs, like words, using a mechanism called self-attention, which is trained via masked language modelling (mlm). in mlm, a word is randomly masked in an input sequence, and the network is trained to predict the missing word. despite the practical success of transformers, it remains unclear what type of data distribution self-attention can learn efficiently. here, we show analytically that if one decouples the treatment of word positions and embeddings, a single layer of self-attention learns the conditionals of a generalised potts model with interactions between sites and potts colours. moreover, we show that training this neural network is exactly equivalent to solving the inverse potts problem by the so-called pseudo-likelihood method, well known in statistical physics. using this mapping, we compute the generalisation error of self-attention in a model scenario analytically using the replica method.",,2023-04-14,2024-02-07,"['riccardo rende', 'federica gerace', 'alessandro laio', 'sebastian goldt']"
2304.07347,differential geometry with extreme eigenvalues in the positive   semidefinite cone,math.dg stat.co stat.ml,"differential geometric approaches to the analysis and processing of data in the form of symmetric positive definite (spd) matrices have had notable successful applications to numerous fields including computer vision, medical imaging, and machine learning. the dominant geometric paradigm for such applications has consisted of a few riemannian geometries associated with spectral computations that are costly at high scale and in high dimensions. we present a route to a scalable geometric framework for the analysis and processing of spd-valued data based on the efficient computation of extreme generalized eigenvalues through the hilbert and thompson geometries of the semidefinite cone. we explore a particular geodesic space structure based on thompson geometry in detail and establish several properties associated with this structure. furthermore, we define a novel iterative mean of spd matrices based on this geometry and prove its existence and uniqueness for a given finite collection of points. finally, we state and prove a number of desirable properties that are satisfied by this mean.",,2023-04-14,2024-02-08,"['cyrus mostajeran', 'nathaël da costa', 'graham van goffrier', 'rodolphe sepulchre']"
2304.07401,bayesian inference on brain-computer interfaces via glass,stat.ap stat.ml,"brain-computer interfaces (bcis), particularly the p300 bci, facilitate direct communication between the brain and computers. the fundamental statistical problem in p300 bcis lies in classifying target and non-target stimuli based on electroencephalogram (eeg) signals. however, the low signal-to-noise ratio (snr) and complex spatial/temporal correlations of eeg signals present challenges in modeling and computation, especially for individuals with severe physical disabilities-bci's primary users. to address these challenges, we introduce a novel gaussian latent channel model with sparse time-varying effects (glass) under a fully bayesian framework. glass is built upon a constrained multinomial logistic regression particularly designed for the imbalanced target and non-target stimuli. the novel latent channel decomposition efficiently alleviates strong spatial correlations between eeg channels, while the soft-thresholded gaussian process (stgp) prior ensures sparse and smooth time-varying effects. we demonstrate glass substantially improves bci's performance in participants with amyotrophic lateral sclerosis (als) and identifies important eeg channels (po8, oz, po7, and pz) in parietal and occipital regions that align with existing literature. for broader accessibility, we develop an efficient gradient-based variational inference (gbvi) algorithm for posterior computation and provide a user-friendly python module available at https://github.com/bangyaozhao/glass.",,2023-04-14,2024-02-14,"['bangyao zhao', 'jane e. huggins', 'jian kang']"
2304.07472,efficient convex algorithms for universal kernel learning,stat.ml cs.lg,"the accuracy and complexity of machine learning algorithms based on kernel optimization are determined by the set of kernels over which they are able to optimize. an ideal set of kernels should: admit a linear parameterization (for tractability); be dense in the set of all kernels (for robustness); be universal (for accuracy). recently, a framework was proposed for using positive matrices to parameterize a class of positive semi-separable kernels. although this class can be shown to meet all three criteria, previous algorithms for optimization of such kernels were limited to classification and furthermore relied on computationally complex semidefinite programming (sdp) algorithms. in this paper, we pose the problem of learning semiseparable kernels as a minimax optimization problem and propose a svd-qcqp primal-dual algorithm which dramatically reduces the computational complexity as compared with previous sdp-based approaches. furthermore, we provide an efficient implementation of this algorithm for both classification and regression -- an implementation which enables us to solve problems with 100 features and up to 30,000 datums. finally, when applied to benchmark data, the algorithm demonstrates the potential for significant improvement in accuracy over typical (but non-convex) approaches such as neural nets and random forest with similar or better computation time.",,2023-04-15,2024-02-24,"['aleksandr talitckii', 'brendon k. colbert', 'matthew m. peet']"
2304.07896,out-of-variable generalization for discriminative models,cs.lg cs.ai stat.ml,"the ability of an agent to do well in new environments is a critical aspect of intelligence. in machine learning, this ability is known as $\textit{strong}$ or $\textit{out-of-distribution}$ generalization. however, merely considering differences in data distributions is inadequate for fully capturing differences between learning environments. in the present paper, we investigate $\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. this skill closely reflects the process of animate learning: we, too, explore nature by probing, observing, and measuring $\textit{subsets}$ of variables at any given time. mathematically, $\textit{out-of-variable}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. we study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. we show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. we leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors.",,2023-04-16,2024-02-08,"['siyuan guo', 'jonas wildberger', 'bernhard schölkopf']"
2304.08061,provable local learning rule by expert aggregation for a hawkes network,math.st stat.th,"we propose a simple network of hawkes processes as a cognitive model capable of learning to classify objects. our learning algorithm, named han for hawkes aggregation of neurons, is based on a local synaptic learning rule based on spiking probabilities at each output node. we were able to use local regret bounds to prove mathematically that the network is able to learn on average and even asymptotically under more restrictive assumptions.",,2023-04-17,2024-02-23,"['sophie jaffard', 'samuel vaiter', 'alexandre muzy', 'patricia reynaud-bouret']"
2304.09046,on clustering levels of a hierarchical categorical risk factor,stat.me stat.ap,"handling nominal covariates with a large number of categories is challenging for both statistical and machine learning techniques. this problem is further exacerbated when the nominal variable has a hierarchical structure. we commonly rely on methods such as the random effects approach (campo and antonio, 2023) to incorporate these covariates in a predictive model. nonetheless, in certain situations, even the random effects approach may encounter estimation problems. we propose the data-driven partitioning hierarchical risk-factors adaptive top-down (phirat) algorithm to reduce the hierarchically structured risk factor to its essence, by grouping similar categories at each level of the hierarchy. we work top-down and engineer several features to characterize the profile of the categories at a specific level in the hierarchy. in our workers' compensation case study, we characterize the risk profile of an industry via its observed damage rates and claim frequencies. in addition, we use embeddings (mikolov et al., 2013; cer et al., 2018) to encode the textual description of the economic activity of the insured company. these features are then used as input in a clustering algorithm to group similar categories. our method substantially reduces the number of categories and results in a grouping that is generalizable to out-of-sample data. moreover, we obtain a better differentiation between high-risk and low-risk companies.",,2023-04-18,2024-02-17,"['bavo d. c. campo', 'katrien antonio']"
2304.09157,neural networks for geospatial data,stat.ml cs.lg stat.me,"analysis of geospatial data has traditionally been model-based, with a mean model, customarily specified as a linear regression on the covariates, and a covariance model, encoding the spatial dependence. we relax the strong assumption of linearity and propose embedding neural networks directly within the traditional geostatistical models to accommodate non-linear mean functions while retaining all other advantages including use of gaussian processes to explicitly model the spatial covariance, enabling inference on the covariate effect through the mean and on the spatial dependence through the covariance, and offering predictions at new locations via kriging. we propose nn-gls, a new neural network estimation algorithm for the non-linear mean in gp models that explicitly accounts for the spatial covariance through generalized least squares (gls), the same loss used in the linear case. we show that nn-gls admits a representation as a special type of graph neural network (gnn). this connection facilitates use of standard neural network computational techniques for irregular geospatial data, enabling novel and scalable mini-batching, backpropagation, and kriging schemes. theoretically, we show that nn-gls will be consistent for irregularly observed spatially correlated data processes. to our knowledge this is the first asymptotic consistency result for any neural network algorithm for spatial data. we demonstrate the methodology through simulated and real datasets.",,2023-04-18,2024-01-29,"['wentao zhan', 'abhirup datta']"
2304.09221,convergence of stochastic gradient descent under a local lojasiewicz   condition for deep neural networks,cs.lg math.oc stat.ml,we study the convergence of stochastic gradient descent (sgd) for non-convex objective functions. we establish the local convergence with positive probability under the local \l{}ojasiewicz condition introduced by chatterjee in \cite{chatterjee2022convergence} and an additional local structural assumption of the loss function landscape. a key component of our proof is to ensure that the whole trajectories of sgd stay inside the local region with a positive probability. we also provide examples of neural networks with finite widths such that our assumptions hold.,,2023-04-18,2024-01-12,"['jing an', 'jianfeng lu']"
2304.09853,bridging rl theory and practice with the effective horizon,cs.lg stat.ml,"deep reinforcement learning (rl) works impressively in some environments and fails catastrophically in others. ideally, rl theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. unfortunately, current theory does not quite have this ability. we compare standard deep rl algorithms to prior sample complexity bounds by introducing a new dataset, bridge. it consists of 155 deterministic mdps from common deep rl benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. we choose to focus on deterministic environments because they share many interesting properties of stochastic environments, but are easier to analyze. using bridge, we find that prior bounds do not correlate well with when deep rl succeeds vs. fails, but discover a surprising property that does. when actions with the highest q-values under the random policy also have the highest q-values under the optimal policy (i.e. when it is optimal to be greedy on the random policy's q function), deep rl tends to succeed; when they don't, deep rl tends to fail. we generalize this property into a new complexity measure of an mdp that we call the effective horizon, which roughly corresponds to how many steps of lookahead search would be needed in that mdp in order to identify the next optimal action, when leaf nodes are evaluated with random rollouts. using bridge, we show that the effective horizon-based bounds are more closely reflective of the empirical performance of ppo and dqn than prior sample complexity bounds across four metrics. we also find that, unlike existing bounds, the effective horizon can predict the effects of using reward shaping or a pre-trained exploration policy. our code and data are available at https://github.com/cassidylaidlaw/effective-horizon",,2023-04-19,2024-01-11,"['cassidy laidlaw', 'stuart russell', 'anca dragan']"
2304.09872,depth functions for partial orders with a descriptive analysis of   machine learning algorithms,cs.lg stat.me,"we propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. despite intensive studies of depth functions in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. we introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. concretely, we analyze the distribution of different classifier performances over a sample of standard benchmark data sets. our results promisingly demonstrate that our approach differs substantially from existing benchmarking approaches and, therefore, adds a new perspective to the vivid debate on the comparison of classifiers.",,2023-04-19,2024-02-09,"['hannah blocher', 'georg schollmeyer', 'christoph jansen', 'malte nalenz']"
2304.09981,interpretable (not just posthoc-explainable) heterogeneous survivor   bias-corrected treatment effects for assignment of postdischarge   interventions to prevent readmissions,stat.me cs.lg q-bio.qm,"we used survival analysis to quantify the impact of postdischarge evaluation and management (e/m) services in preventing hospital readmission or death. our approach avoids a specific pitfall of applying machine learning to this problem, which is an inflated estimate of the effect of interventions, due to survivors bias -- where the magnitude of inflation may be conditional on heterogeneous confounders in the population. this bias arises simply because in order to receive an intervention after discharge, a person must not have been readmitted in the intervening period. after deriving an expression for this phantom effect, we controlled for this and other biases within an inherently interpretable bayesian survival framework. we identified case management services as being the most impactful for reducing readmissions overall.",,2023-04-19,2023-08-03,"['hongjing xia', 'joshua c. chang', 'sarah nowak', 'sonya mahajan', 'rohit mahajan', 'ted l. chang', 'carson c. chow']"
2304.10819,auditing and generating synthetic data with controllable trust   trade-offs,cs.lg cs.ai stat.ml,"real-world data often exhibits bias, imbalance, and privacy risks. synthetic datasets have emerged to address these issues. this paradigm relies on generative ai models to generate unbiased, privacy-preserving data while maintaining fidelity to the original data. however, assessing the trustworthiness of synthetic datasets and models is a critical challenge. we introduce a holistic auditing framework that comprehensively evaluates synthetic datasets and ai models. it focuses on preventing bias and discrimination, ensures fidelity to the source data, assesses utility, robustness, and privacy preservation. we demonstrate the framework's effectiveness by auditing various generative models across diverse use cases like education, healthcare, banking, and human resources, spanning different data modalities such as tabular, time-series, vision, and natural language. this holistic assessment is essential for compliance with regulatory safeguards. we introduce a trustworthiness index to rank synthetic datasets based on their safeguards trade-offs. furthermore, we present a trustworthiness-driven model selection and cross-validation process during training, exemplified with ""trustformers"" across various data types. this approach allows for controllable trustworthiness trade-offs in synthetic data creation. our auditing framework fosters collaboration among stakeholders, including data scientists, governance experts, internal reviewers, external certifiers, and regulators. this transparent reporting should become a standard practice to prevent bias, discrimination, and privacy violations, ensuring compliance with policies and providing accountability, safety, and performance guarantees.",,2023-04-21,2024-01-09,"['brian belgodere', 'pierre dognin', 'adam ivankay', 'igor melnyk', 'youssef mroueh', 'aleksandra mojsilovic', 'jiri navratil', 'apoorva nitsure', 'inkit padhi', 'mattia rigotti', 'jerret ross', 'yair schiff', 'radhika vedpathak', 'richard a. young']"
2304.11005,self-correcting bayesian optimization through bayesian active learning,cs.lg stat.ml,"gaussian processes are the model of choice in bayesian optimization and active learning. yet, they are highly dependent on cleverly chosen hyperparameters to reach their full potential, and little effort is devoted to finding good hyperparameters in the literature. we demonstrate the impact of selecting good hyperparameters for gps and present two acquisition functions that explicitly prioritize hyperparameter learning. statistical distance-based active learning (sal) considers the average disagreement between samples from the posterior, as measured by a statistical distance. sal outperforms the state-of-the-art in bayesian active learning on several test functions. we then introduce self-correcting bayesian optimization (scorebo), which extends sal to perform bayesian optimization and active learning simultaneously. scorebo learns the model hyperparameters at improved rates compared to vanilla bo, while outperforming the latest bayesian optimization methods on traditional benchmarks. moreover, we demonstrate the importance of self-correction on atypical bayesian optimization tasks.",,2023-04-21,2024-02-15,"['carl hvarfner', 'erik hellsten', 'frank hutter', 'luigi nardi']"
2304.11625,meaningful causal aggregation and paradoxical confounding,cs.ai stat.ml,"in aggregated variables the impact of interventions is typically ill-defined because different micro-realizations of the same macro-intervention can result in different changes of downstream macro-variables. we show that this ill-definedness of causality on aggregated variables can turn unconfounded causal relations into confounded ones and vice versa, depending on the respective micro-realization. we argue that it is practically infeasible to only use aggregated causal systems when we are free from this ill-definedness. instead, we need to accept that macro causal relations are typically defined only with reference to the micro states. on the positive side, we show that cause-effect relations can be aggregated when the macro interventions are such that the distribution of micro states is the same as in the observational distribution; we term this natural macro interventions. we also discuss generalizations of this observation.",,2023-04-23,2024-02-22,"['yuchen zhu', 'kailash budhathoki', 'jonas kuebler', 'dominik janzing']"
2304.11958,estimation of sparse linear regression coefficients under   $l$-subexponential covariates,math.st stat.ml stat.th,"we tackle estimating sparse coefficients in a linear regression when the covariates are sampled from an $l$-subexponential random vector. this vector belongs to a class of distributions that exhibit heavier tails than gaussian random vector. previous studies have established error bounds similar to those derived for gaussian random vectors. however, these methods require stronger conditions than those used for gaussian random vectors to derive the error bounds. in this study, we present an error bound identical to the one obtained for gaussian random vectors up to constant factors without imposing stronger conditions, when the covariates are drawn from an $l$-subexponential random vector. interestingly, we employ an $\ell_1$-penalized huber regression, which is known for its robustness against heavy-tailed random noises rather than covariates. we believe that this study uncovers a new aspect of the $\ell_1$-penalized huber regression method.",,2023-04-24,2024-02-06,['takeyuki sasai']
2304.12522,a new inexact proximal linear algorithm with adaptive stopping criteria   for robust phase retrieval,math.oc cs.lg eess.sp stat.co stat.ml,"this paper considers the robust phase retrieval problem, which can be cast as a nonsmooth and nonconvex optimization problem. we propose a new inexact proximal linear algorithm with the subproblem being solved inexactly. our contributions are two adaptive stopping criteria for the subproblem. the convergence behavior of the proposed methods is analyzed. through experiments on both synthetic and real datasets, we demonstrate that our methods are much more efficient than existing methods, such as the original proximal linear algorithm and the subgradient method.",,2023-04-24,2024-02-08,"['zhong zheng', 'shiqian ma', 'lingzhou xue']"
2304.12770,controlling posterior collapse by an inverse lipschitz constraint on the   decoder network,cs.lg stat.ml,"variational autoencoders (vaes) are one of the deep generative models that have experienced enormous success over the past decades. however, in practice, they suffer from a problem called posterior collapse, which occurs when the encoder coincides, or collapses, with the prior taking no information from the latent structure of the input data into consideration. in this work, we introduce an inverse lipschitz neural network into the decoder and, based on this architecture, provide a new method that can control in a simple and clear manner the degree of posterior collapse for a wide range of vae models equipped with a concrete theoretical guarantee. we also illustrate the effectiveness of our method through several numerical experiments.",,2023-04-25,2024-02-02,"['yuri kinoshita', 'kenta oono', 'kenji fukumizu', 'yuichi yoshida', 'shin-ichi maeda']"
2304.13406,onset of a conceptual outline map to get a hold on the jungle of cluster   analysis,stat.ot,"the domain of cluster analysis is a meeting point for a very rich multidisciplinary encounter, with cluster-analytic methods being studied and developed in discrete mathematics, numerical analysis, statistics, data analysis, data science, and computer science (including machine learning, data mining, and knowledge discovery), to name but a few. the other side of the coin, however, is that the domain suffers from a major accessibility problem as well as from the fact that it is rife with division across many pretty isolated islands. as a way out, the present paper offers a thorough and in-depth review of the clustering domain as a whole under the form of an outline map based on an overarching conceptual framework and a common language. with this framework we wish to contribute to structuring the clustering domain, to characterizing methods that have often been developed and studied in quite different contexts, to identifying links between methods, and to introducing a frame of reference for optimally setting up cluster analyses in data-analytic practice.",,2023-04-26,2024-01-24,"['iven van mechelen', 'christian hennig', 'henk a. l. kiers']"
2304.13586,energy-based sliced wasserstein distance,stat.ml cs.cv cs.gr cs.lg,"the sliced wasserstein (sw) distance has been widely recognized as a statistically effective and computationally efficient metric between two probability measures. a key component of the sw distance is the slicing distribution. there are two existing approaches for choosing this distribution. the first approach is using a fixed prior distribution. the second approach is optimizing for the best distribution which belongs to a parametric family of distributions and can maximize the expected distance. however, both approaches have their limitations. a fixed prior distribution is non-informative in terms of highlighting projecting directions that can discriminate two general probability measures. doing optimization for the best distribution is often expensive and unstable. moreover, designing the parametric family of the candidate distribution could be easily misspecified. to address the issues, we propose to design the slicing distribution as an energy-based distribution that is parameter-free and has the density proportional to an energy function of the projected one-dimensional wasserstein distance. we then derive a novel sliced wasserstein metric, energy-based sliced waserstein (ebsw) distance, and investigate its topological, statistical, and computational properties via importance sampling, sampling importance resampling, and markov chain methods. finally, we conduct experiments on point-cloud gradient flow, color transfer, and point-cloud reconstruction to show the favorable performance of the ebsw.",,2023-04-26,2023-12-29,"['khai nguyen', 'nhat ho']"
2304.14885,on closed-form expressions for the fisher-rao distance,math.st cs.it math.dg math.it stat.th,"the fisher-rao distance is the geodesic distance between probability distributions in a statistical manifold equipped with the fisher metric, which is a natural choice of riemannian metric on such manifolds. it has recently been applied to supervised and unsupervised problems in machine learning, in various contexts. finding closed-form expressions for the fisher-rao distance is generally a non-trivial task, and those are only available for a few families of probability distributions. in this survey, we collect examples of closed-form expressions for the fisher-rao distance of both discrete and continuous distributions, aiming to present them in a unified and accessible language. in doing so, we also: illustrate the relation between negative multinomial distributions and the hyperbolic model, include a few new examples, and write a few more in the standard form of elliptical distributions.",,2023-04-28,2024-02-27,"['henrique k. miyamoto', 'fábio c. c. meneghetti', 'julianna pinele', 'sueli i. r. costa']"
2304.14954,a class of dependent random distributions based on atom skipping,stat.me stat.ml,"we propose the plaid atoms model (pam), a novel bayesian nonparametric model for grouped data. founded on an idea of `atom skipping', pam is part of a well-established category of models that generate dependent random distributions and clusters across multiple groups. atom skipping referrs to stochastically assigning 0 weights to atoms in an infinite mixture. deploying atom skipping across groups, pam produces a dependent clustering pattern with overlapping and non-overlapping clusters across groups. as a result, interpretable posterior inference is possible such as reporting the posterior probability of a cluster being exclusive to a single group or shared among a subset of groups. we discuss the theoretical properties of the proposed and related models. minor extensions of the proposed model for multivariate or count data are presented. simulation studies and applications using real-world datasets illustrate the performance of the new models with comparison to existing models.",,2023-04-28,2023-12-30,"['dehua bi', 'yuan ji']"
2304.14989,kullback-leibler maillard sampling for multi-armed bandits with bounded   rewards,cs.lg stat.ml,"we study $k$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. it has been a challenge to design regret-efficient randomized exploration algorithms in this setting. maillard sampling~\cite{maillard13apprentissage}, an attractive alternative to thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-gaussian reward setting~\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. in this work, we propose the kullback-leibler maillard sampling (kl-ms) algorithm, a natural extension of maillard sampling for achieving kl-style gap-dependent regret bound. we show that kl-ms enjoys the asymptotic optimality when the rewards are bernoulli and has a worst-case regret bound of the form $o(\sqrt{\mu^*(1-\mu^*) k t \ln k} + k \ln t)$, where $\mu^*$ is the expected reward of the optimal arm, and $t$ is the time horizon length.",,2023-04-28,2024-02-06,"['hao qin', 'kwang-sung jun', 'chicheng zhang']"
2305.00402,sliced wasserstein estimation with control variates,stat.ml cs.cv cs.gr cs.lg,"the sliced wasserstein (sw) distances between two probability measures are defined as the expectation of the wasserstein distance between two one-dimensional projections of the two measures. the randomness comes from a projecting direction that is used to project the two input measures to one dimension. due to the intractability of the expectation, monte carlo integration is performed to estimate the value of the sw distance. despite having various variants, there has been no prior work that improves the monte carlo estimation scheme for the sw distance in terms of controlling its variance. to bridge the literature on variance reduction and the literature on the sw distance, we propose computationally efficient control variates to reduce the variance of the empirical estimation of the sw distance. the key idea is to first find gaussian approximations of projected one-dimensional measures, then we utilize the closed-form of the wasserstein-2 distance between two gaussian distributions to design the control variates. in particular, we propose using a lower bound and an upper bound of the wasserstein-2 distance between two fitted gaussians as two computationally efficient control variates. we empirically show that the proposed control variate estimators can help to reduce the variance considerably when comparing measures over images and point-clouds. finally, we demonstrate the favorable performance of the proposed control variate estimators in gradient flows to interpolate between two point-clouds and in deep generative modeling on standard image datasets, such as cifar10 and celeba.",,2023-04-30,2024-02-18,"['khai nguyen', 'nhat ho']"
2305.01849,semiparametric discovery and estimation of interaction in mixed   exposures using stochastic interventions,stat.me,"this study introduces a nonparametric definition of interaction and provides an approach to both interaction discovery and efficient estimation of this parameter. using stochastic shift interventions and ensemble machine learning, our approach identifies and quantifies interaction effects through a model-independent target parameter, estimated via targeted maximum likelihood and cross-validation. this method contrasts the expected outcomes of joint interventions with those of individual interventions. validation through simulation and application to the national institute of environmental health sciences mixtures workshop data demonstrate the efficacy of our method in detecting true interaction directions and its consistency in identifying significant impacts of furan exposure on leukocyte telomere length. our method, called supernova, advances the ability to analyze multiexposure interactions within high-dimensional data, offering significant methodological improvements to understand complex exposure dynamics in health research. we provide peer-reviewed open-source software that employs or proposed methodology in the \texttt{supernova} r package.",,2023-05-02,2024-02-28,"['david b. mccoy', 'alan e. hubbard', 'alejandro schuler', 'mark j. van der laan']"
2305.02650,a constrained ba algorithm for rate-distortion and distortion-rate   functions,cs.it cs.lg math.it stat.ml,"the blahut-arimoto (ba) algorithm has played a fundamental role in the numerical computation of rate-distortion (rd) functions. this algorithm possesses a desirable monotonic convergence property by alternatively minimizing its lagrangian with a fixed multiplier. in this paper, we propose a novel modification of the ba algorithm, wherein the multiplier is updated through a one-dimensional root-finding step using a monotonic univariate function, efficiently implemented by newton's method in each iteration. consequently, the modified algorithm directly computes the rd function for a given target distortion, without exploring the entire rd curve as in the original ba algorithm. moreover, this modification presents a versatile framework, applicable to a wide range of problems, including the computation of distortion-rate (dr) functions. theoretical analysis shows that the outputs of the modified algorithms still converge to the solutions of the rd and dr functions with rate $o(1/n)$, where $n$ is the number of iterations. additionally, these algorithms provide $\varepsilon$-approximation solutions with $o\left(\frac{mn\log n}{\varepsilon}(1+\log |\log \varepsilon|)\right)$ arithmetic operations, where $m,n$ are the sizes of source and reproduced alphabets respectively. numerical experiments demonstrate that the modified algorithms exhibit significant acceleration compared with the original ba algorithms and showcase commendable performance across classical source distributions such as discretized gaussian, laplacian and uniform sources.",,2023-05-04,2024-01-18,"['lingyi chen', 'shitong wu', 'wenhao ye', 'huihui wu', 'wenyi zhang', 'hao wu', 'bo bai']"
2305.02657,on the eigenvalue decay rates of a class of neural-network related   kernel functions defined on general domains,stat.ml cs.lg,"in this paper, we provide a strategy to determine the eigenvalue decay rate (edr) of a large class of kernel functions defined on a general domain rather than $\mathbb s^{d}$. this class of kernel functions include but are not limited to the neural tangent kernel associated with neural networks with different depths and various activation functions. after proving that the dynamics of training the wide neural networks uniformly approximated that of the neural tangent kernel regression on general domains, we can further illustrate the minimax optimality of the wide neural network provided that the underground truth function $f\in [\mathcal h_{\mathrm{ntk}}]^{s}$, an interpolation space associated with the rkhs $\mathcal{h}_{\mathrm{ntk}}$ of ntk. we also showed that the overfitted neural network can not generalize well. we believe our approach for determining the edr of kernels might be also of independent interests.",,2023-05-04,2024-01-08,"['yicheng li', 'zixiong yu', 'guhan chen', 'qian lin']"
2305.02930,piecewise normalizing flows,stat.ml cs.lg,"normalizing flows are an established approach for modelling complex probability densities through invertible transformations from a base distribution. however, the accuracy with which the target distribution can be captured by the normalizing flow is strongly influenced by the topology of the base distribution. a mismatch between the topology of the target and the base can result in a poor performance, as is typically the case for multi-modal problems. a number of different works have attempted to modify the topology of the base distribution to better match the target, either through the use of gaussian mixture models (izmailov et al., 2020; ardizzone et al., 2020; hagemann & neumayer, 2021) or learned accept/reject sampling (stimper et al., 2022). we introduce piecewise normalizing flows which divide the target distribution into clusters, with topologies that better match the standard normal base distribution, and train a series of flows to model complex multi-modal targets. we demonstrate the performance of the piecewise flows using some standard benchmarks and compare the accuracy of the flows to the approach taken in stimper et al. (2022) for modelling multi-modal distributions. we find that our approach consistently outperforms the approach in stimper et al. (2022) with a higher emulation accuracy on the standard benchmarks.",,2023-05-04,2024-02-01,"['harry bevins', 'will handley', 'thomas gessey-jones']"
2305.03594,"images of gaussian and other stochastic processes under closed,   densely-defined, unbounded linear operators",math.pr math.st stat.th,"gaussian processes (gps) are widely-used tools in spatial statistics and machine learning and the formulae for the mean function and covariance kernel of a gp $t u$ that is the image of another gp $u$ under a linear transformation $t$ acting on the sample paths of $u$ are well known, almost to the point of being folklore. however, these formulae are often used without rigorous attention to technical details, particularly when $t$ is an unbounded operator such as a differential operator, which is common in many modern applications. this note provides a self-contained proof of the claimed formulae for the case of a closed, densely-defined operator $t$ acting on the sample paths of a square-integrable (not necessarily gaussian) stochastic process. our proof technique relies upon hille's theorem for the bochner integral of a banach-valued random variable.",,2023-05-05,2024-01-11,"['tadashi matsumoto', 't. j. sullivan']"
2305.03780,boldness-recalibration for binary event predictions,stat.me stat.ml,"probability predictions are essential to inform decision making across many fields. ideally, probability predictions are (i) well calibrated, (ii) accurate, and (iii) bold, i.e., spread out enough to be informative for decision making. however, there is a fundamental tension between calibration and boldness, since calibration metrics can be high when predictions are overly cautious, i.e., non-bold. the purpose of this work is to develop a bayesian model selection-based approach to assess calibration, and a strategy for boldness-recalibration that enables practitioners to responsibly embolden predictions subject to their required level of calibration. specifically, we allow the user to pre-specify their desired posterior probability of calibration, then maximally embolden predictions subject to this constraint. we demonstrate the method with a case study on hockey home team win probabilities and then verify the performance of our procedures via simulation. we find that very slight relaxation of calibration probability (e.g., from 0.99 to 0.95) can often substantially embolden predictions when they are well calibrated and accurate (e.g., widening hockey predictions range from .26-.78 to .10-.91).",,2023-05-05,2024-01-31,"['adeline p. guthrie', 'christopher t. franck']"
2305.03938,adam-family methods for nonsmooth optimization with convergence   guarantees,math.oc cs.lg stat.ml,"in this paper, we present a comprehensive study on the convergence properties of adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. we introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. our proposed framework encompasses various popular adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.",,2023-05-06,2024-02-19,"['nachuan xiao', 'xiaoyin hu', 'xin liu', 'kim-chuan toh']"
2305.04634,neural likelihood surfaces for spatial processes with computationally   intensive or intractable likelihoods,stat.me stat.ml,"in spatial statistics, fast and accurate parameter estimation, coupled with a reliable means of uncertainty quantification, can be challenging when fitting a spatial process to real-world data because the likelihood function might be slow to evaluate or wholly intractable. in this work, we propose using convolutional neural networks to learn the likelihood function of a spatial process. through a specifically designed classification task, our neural network implicitly learns the likelihood function, even in situations where the exact likelihood is not explicitly available. once trained on the classification task, our neural network is calibrated using platt scaling which improves the accuracy of the neural likelihood surfaces. to demonstrate our approach, we compare neural likelihood surfaces and the resulting maximum likelihood estimates and approximate confidence regions with the equivalent for exact or approximate likelihood for two different spatial processes: a gaussian process and a brown-resnick process which have computationally intensive and intractable likelihoods, respectively. we conclude that our method provides fast and accurate parameter estimation with a reliable method of uncertainty quantification in situations where standard methods are either undesirably slow or inaccurate. the method is applicable to any spatial process on a grid from which fast simulations are available.",,2023-05-08,2024-02-29,"['julia walchessen', 'amanda lenzi', 'mikael kuusela']"
2305.05126,comparing foundation models using data kernels,cs.lg cs.ai stat.me,"recent advances in self-supervised learning and neural network scaling have enabled the creation of large models, known as foundation models, which can be easily adapted to a wide range of downstream tasks. the current paradigm for comparing foundation models involves evaluating them with aggregate metrics on various benchmark datasets. this method of model comparison is heavily dependent on the chosen evaluation metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. in this work, we present a methodology for directly comparing the embedding space geometry of foundation models, which facilitates model comparison without the need for an explicit evaluation metric. our methodology is grounded in random graph theory and enables valid hypothesis testing of embedding similarity on a per-datum basis. further, we demonstrate how our methodology can be extended to facilitate population level model comparison. in particular, we show how our framework can induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics. we remark on the utility of this population level model comparison as a first step towards a taxonomic science of foundation models.",,2023-05-08,2024-01-07,"['brandon duderstadt', 'hayden s. helm', 'carey e. priebe']"
2305.05400,investigating the corruption robustness of image classifiers with random   lp-norm corruptions,cs.lg cs.cv stat.ml,"robustness is a fundamental property of machine learning classifiers required to achieve safety and reliability. in the field of adversarial robustness of image classifiers, robustness is commonly defined as the stability of a model to all input changes within a p-norm distance. however, in the field of random corruption robustness, variations observed in the real world are used, while p-norm corruptions are rarely considered. this study investigates the use of random p-norm corruptions to augment the training and test data of image classifiers. we evaluate the model robustness against imperceptible random p-norm corruptions and propose a novel robustness metric. we empirically investigate whether robustness transfers across different p-norms and derive conclusions on which p-norm corruptions a model should be trained and evaluated. we find that training data augmentation with a combination of p-norm corruptions significantly improves corruption robustness, even on top of state-of-the-art data augmentation schemes.",,2023-05-09,2024-01-10,"['georg siedel', 'weijia shao', 'silvia vock', 'andrey morozov']"
2305.05465,the emergence of clusters in self-attention dynamics,cs.lg math.ap stat.ml,"viewing transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. we show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. cluster locations are determined by the initial tokens, confirming context-awareness of representations learned by transformers. using techniques from dynamical systems and partial differential equations, we show that the type of limiting object that emerges depends on the spectrum of the value matrix. additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank boolean matrix. the combination of these results mathematically confirms the empirical observation made by vaswani et al. [vsp'17] that leaders appear in a sequence of tokens when processed by transformers.",,2023-05-09,2024-02-12,"['borjan geshkovski', 'cyril letrouit', 'yury polyanskiy', 'philippe rigollet']"
2305.06927,convergence of alternating gradient descent for matrix factorization,cs.lg math.oc stat.ml,"we consider alternating gradient descent (agd) with fixed step size applied to the asymmetric matrix factorization objective. we show that, for a rank-$r$ matrix $\mathbf{a} \in \mathbb{r}^{m \times n}$, $t = c (\frac{\sigma_1(\mathbf{a})}{\sigma_r(\mathbf{a})})^2 \log(1/\epsilon)$ iterations of alternating gradient descent suffice to reach an $\epsilon$-optimal factorization $\| \mathbf{a} - \mathbf{x} \mathbf{y}^{t} \|^2 \leq \epsilon \| \mathbf{a}\|^2$ with high probability starting from an atypical random initialization. the factors have rank $d \geq r$ so that $\mathbf{x}_{t}\in\mathbb{r}^{m \times d}$ and $\mathbf{y}_{t} \in\mathbb{r}^{n \times d}$, and mild overparameterization suffices for the constant $c$ in the iteration complexity $t$ to be an absolute constant. experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves the convergence rate of gradient descent in practice. our proof is conceptually simple: a uniform polyak-\l{}ojasiewicz (pl) inequality and uniform lipschitz smoothness constant are guaranteed for a sufficient number of iterations, starting from our random initialization. our proof method should be useful for extending and simplifying convergence analyses for a broader class of nonconvex low-rank factorization problems.",,2023-05-11,2024-02-07,"['rachel ward', 'tamara g. kolda']"
2305.07572,towards convergence rates for parameter estimation in gaussian-gated   mixture of experts,stat.ml cs.lg,"originally introduced as a neural network for ensemble learning, mixture of experts (moe) has recently become a fundamental building block of highly successful modern deep neural networks for heterogeneous data analysis in several applications of machine learning and statistics. despite its popularity in practice, a satisfactory level of theoretical understanding of the moe model is far from complete. to shed new light on this problem, we provide a convergence analysis for maximum likelihood estimation (mle) in the gaussian-gated moe model. the main challenge of that analysis comes from the inclusion of covariates in the gaussian gating functions and expert networks, which leads to their intrinsic interaction via some partial differential equations with respect to their parameters. we tackle these issues by designing novel voronoi loss functions among parameters to accurately capture the heterogeneity of parameter estimation rates. our findings reveal that the mle has distinct behaviors under two complement settings of location parameters of the gaussian gating functions, namely when all these parameters are non-zero versus when at least one among them vanishes. notably, these behaviors can be characterized by the solvability of two different systems of polynomial equations. finally, we conduct a simulation study to empirically verify our theoretical results.",,2023-05-12,2024-02-09,"['huy nguyen', 'trungtin nguyen', 'khai nguyen', 'nhat ho']"
2305.08404,theoretical analysis of inductive biases in deep convolutional networks,cs.lg stat.ml,"in this paper, we provide a theoretical analysis of the inductive biases in convolutional neural networks (cnns). we start by examining the universality of cnns, i.e., the ability to approximate any continuous functions. we prove that a depth of $\mathcal{o}(\log d)$ suffices for deep cnns to achieve this universality, where $d$ in the input dimension. additionally, we establish that learning sparse functions with cnns requires only $\widetilde{\mathcal{o}}(\log^2d)$ samples, indicating that deep cnns can efficiently capture {\em long-range} sparse correlations. these results are made possible through a novel combination of the multichanneling and downsampling when increasing the network depth. we also delve into the distinct roles of weight sharing and locality in cnns. to this end, we compare the performance of cnns, locally-connected networks (lcns), and fully-connected networks (fcns) on a simple regression task, where lcns can be viewed as cnns without weight sharing. on the one hand, we prove that lcns require ${\omega}(d)$ samples while cnns need only $\widetilde{\mathcal{o}}(\log^2d)$ samples, highlighting the critical role of weight sharing. on the other hand, we prove that fcns require $\omega(d^2)$ samples, whereas lcns need only $\widetilde{\mathcal{o}}(d)$ samples, underscoring the importance of locality. these provable separations quantify the difference between the two biases, and the major observation behind our proof is that weight sharing and locality break different symmetries in the learning process.",,2023-05-15,2024-01-20,"['zihao wang', 'lei wu']"
2305.08999,wavelet-based density estimation for persistent homology,math.st stat.th,"persistent homology is a central methodology in topological data analysis that has been successfully implemented in many fields and is becoming increasingly popular and relevant. the output of persistent homology is a persistence diagram -- a multiset of points supported on the upper half plane -- that is often used as a statistical summary of the topological features of data. in this paper, we study the random nature of persistent homology and estimate the density of expected persistence diagrams from observations using wavelets; we show that our wavelet-based estimator is optimal. furthermore, we propose an estimator that offers a sparse representation of the expected persistence diagram that achieves near-optimality. we demonstrate the utility of our contributions in a machine learning task in the context of dynamical systems.",,2023-05-15,2024-01-05,"['konstantin häberle', 'barbara bravi', 'anthea monod']"
2305.09126,transfer learning for causal effect estimation,cs.lg math.st stat.me stat.ml stat.th,"we present a transfer causal learning (tcl) framework when target and source domains share the same covariate/feature spaces, aiming to improve causal effect estimation accuracy in limited data. limited data is very common in medical applications, where some rare medical conditions, such as sepsis, are of interest. our proposed method, named \texttt{$\ell_1$-tcl}, incorporates $\ell_1$ regularized tl for nuisance models (e.g., propensity score model); the tl estimator of the nuisance parameters is plugged into downstream average causal/treatment effect estimators (e.g., inverse probability weighted estimator). we establish non-asymptotic recovery guarantees for the \texttt{$\ell_1$-tcl} with generalized linear model (glm) under the sparsity assumption in the high-dimensional setting, and demonstrate the empirical benefits of \texttt{$\ell_1$-tcl} through extensive numerical simulation for glm and recent neural network nuisance models. our method is subsequently extended to real data and generates meaningful insights consistent with medical literature, a case where all baseline methods fail.",,2023-05-15,2024-01-01,"['song wei', 'hanyu zhang', 'ronald moore', 'rishikesan kamaleswaran', 'yao xie']"
2305.10500,learning likelihood ratios with neural network classifiers,hep-ph physics.data-an stat.ap stat.ml,"the likelihood ratio is a crucial quantity for statistical inference in science that enables hypothesis testing, construction of confidence intervals, reweighting of distributions, and more. many modern scientific applications, however, make use of data- or simulation-driven models for which computing the likelihood ratio can be very difficult or even impossible. by applying the so-called ``likelihood ratio trick,'' approximations of the likelihood ratio may be computed using clever parametrizations of neural network-based classifiers. a number of different neural network setups can be defined to satisfy this procedure, each with varying performance in approximating the likelihood ratio when using finite training data. we present a series of empirical studies detailing the performance of several common loss functionals and parametrizations of the classifier output in approximating the likelihood ratio of two univariate and multivariate gaussian distributions as well as simulated high-energy particle physics datasets.",,2023-05-17,2024-01-08,"['shahzar rizvi', 'mariel pettee', 'benjamin nachman']"
2305.10880,functional sufficient dimension reduction through information   maximization with application to classification,stat.ml cs.lg,"considering the case where the response variable is a categorical variable and the predictor is a random function, two novel functional sufficient dimensional reduction (fsdr) methods are proposed based on mutual information and square loss mutual information. compared to the classical fsdr methods, such as functional sliced inverse regression and functional sliced average variance estimation, the proposed methods are appealing because they are capable of estimating multiple effective dimension reduction directions in the case of a relatively small number of categories, especially for the binary response. moreover, the proposed methods do not require the restrictive linear conditional mean assumption and the constant covariance assumption. they avoid the inverse problem of the covariance operator which is often encountered in the functional sufficient dimension reduction. the functional principal component analysis with truncation be used as a regularization mechanism. under some mild conditions, the statistical consistency of the proposed methods is established. it is demonstrated that the two methods are competitive compared with some existing fsdr methods by simulations and real data analyses.",,2023-05-18,2024-02-26,"['xinyu li', 'jianjun xu', 'wenquan cui', 'haoyang cheng']"
2305.11241,"evidence networks: simple losses for fast, amortized, neural bayesian   model comparison",cs.lg astro-ph.co astro-ph.im stat.ml,"evidence networks can enable bayesian model comparison when state-of-the-art methods (e.g. nested sampling) fail and even when likelihoods or priors are intractable or unknown. bayesian model comparison, i.e. the computation of bayes factors or evidence ratios, can be cast as an optimization problem. though the bayesian interpretation of optimal classification is well-known, here we change perspective and present classes of loss functions that result in fast, amortized neural estimators that directly estimate convenient functions of the bayes factor. this mitigates numerical inaccuracies associated with estimating individual model probabilities. we introduce the leaky parity-odd power (l-pop) transform, leading to the novel ``l-pop-exponential'' loss function. we explore neural density estimation for data probability in different models, showing it to be less accurate and scalable than evidence networks. multiple real-world and synthetic examples illustrate that evidence networks are explicitly independent of dimensionality of the parameter space and scale mildly with the complexity of the posterior probability density function. this simple yet powerful approach has broad implications for model inference tasks. as an application of evidence networks to real-world data we compute the bayes factor for two models with gravitational lensing data of the dark energy survey. we briefly discuss applications of our methods to other, related problems of model comparison and evaluation in implicit inference settings.",10.1088/2632-2153/ad1a4d,2023-05-18,2024-01-10,"['niall jeffrey', 'benjamin d. wandelt']"
2305.11417,complexity of deep neural networks from the perspective of functional   equivalence,cs.lg math.st stat.th,"in this paper, we investigate the complexity of feed-forward neural networks by examining the concept of functional equivalence, which suggests that different network parameterizations can lead to the same function. we utilize the permutation invariance property to derive a novel covering number bound for the class of feedforward neural networks, which reveals that the complexity of a neural network can be reduced by exploiting this property. we discuss the extensions to convolutional neural networks, residual networks, and attention-based models. we demonstrate that functional equivalence benefits optimization, as overparameterized networks tend to be easier to train since increasing network width leads to a diminishing volume of the effective parameter space. our findings offer new insights into overparameterization and have significant implications for understanding generalization and optimization in deep learning.",,2023-05-19,2024-01-16,['guohao shen']
2305.11463,generative sliced mmd flows with riesz kernels,cs.lg math.pr stat.ml,"maximum mean discrepancy (mmd) flows suffer from high computational costs in large scale computations. in this paper, we show that mmd flows with riesz kernels $k(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which allow their efficient computation. we prove that the mmd of riesz kernels, which is also known as energy distance, coincides with the mmd of their sliced version. as a consequence, the computation of gradients of mmds can be performed in the one-dimensional setting. here, for $r=1$, a simple sorting algorithm can be applied to reduce the complexity from $o(mn+n^2)$ to $o((m+n)\log(m+n))$ for two measures with $m$ and $n$ support points. as another interesting follow-up result, the mmd of compactly supported measures can be estimated from above and below by the wasserstein-1 distance. for the implementations we approximate the gradient of the sliced mmd by using only a finite number $p$ of slices. we show that the resulting error has complexity $o(\sqrt{d/p})$, where $d$ is the data dimension. these results enable us to train generative models by approximating mmd gradient flows by neural networks even for image applications. we demonstrate the efficiency of our model by image generation on mnist, fashionmnist and cifar10.",,2023-05-19,2024-02-20,"['johannes hertrich', 'christian wald', 'fabian altekrüger', 'paul hagemann']"
2305.11509,from random search to bandit learning in metric measure spaces,cs.lg stat.ml,"random search is one of the most widely-used method for hyperparameter optimization, and is critical to the success of deep learning models. despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. this paper gives a theoretical accounting of random search. we introduce the concept of \emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. we show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{o}} \left( \left( \frac{1}{t} \right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering dimension of the underlying function. when the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{o}} \left( \left( \frac{1}{t} \right)^{ \frac{1}{d_s + 1} } \right) $. in addition, based on the principles of random search, we introduce an algorithm, called blin-mos, for lipschitz bandits in doubling metric spaces that are also endowed with a probability measure, and show that under mild conditions, blin-mos achieves a regret rate of order $ \widetilde{\mathcal{o}} \left( t^{ \frac{d_z}{d_z + 1} } \right) $, where $d_z$ is the zooming dimension of the problem instance.",,2023-05-19,2024-02-12,"['chuying han', 'yasong feng', 'tianyu wang']"
2305.11766,transfer operators on graphs: spectral clustering and beyond,stat.ml cs.lg cs.si math.ds,"graphs and networks play an important role in modeling and analyzing complex interconnected systems such as transportation networks, integrated circuits, power grids, citation graphs, and biological and artificial neural networks. graph clustering algorithms can be used to detect groups of strongly connected vertices and to derive coarse-grained models. we define transfer operators such as the koopman operator and the perron-frobenius operator on graphs, study their spectral properties, introduce galerkin projections of these operators, and illustrate how reduced representations can be estimated from data. in particular, we show that spectral clustering of undirected graphs can be interpreted in terms of eigenfunctions of the koopman operator and propose novel clustering algorithms for directed graphs based on generalized transfer operators. we demonstrate the efficacy of the resulting algorithms on several benchmark problems and provide different interpretations of clusters.",10.1088/2632-072x/ad28fe,2023-05-19,2024-02-14,"['stefan klus', 'maia trower']"
2305.11854,multimodal web navigation with instruction-finetuned foundation models,cs.lg cs.ai stat.ml,"the progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. in this work, we study data-driven offline training for web agents with vision-language foundation models. we propose an instruction-following multimodal agent, webgum, that observes both webpage screenshots and html pages and outputs web navigation actions, such as click and type. webgum is trained by jointly finetuning an instruction-finetuned language model and a vision encoder with temporal and local perception on a large corpus of demonstrations. we empirically demonstrate this recipe improves the agent's ability of grounded multimodal perception, html comprehension, and multi-step reasoning, outperforming prior works by a significant margin. on the miniwob, we improve over the previous best offline methods by more than 45.8%, even outperforming online-finetuned sota, humans, and gpt-4-based agent. on the webshop benchmark, our 3-billion-parameter model achieves superior performance to the existing sota, palm-540b. furthermore, webgum exhibits strong positive transfer to the real-world planning tasks on the mind2web. we also collect 347k high-quality demonstrations using our trained models, 38 times larger than prior work, and make them available to promote future research in this direction.",,2023-05-19,2024-02-25,"['hiroki furuta', 'kuang-huei lee', 'ofir nachum', 'yutaka matsuo', 'aleksandra faust', 'shixiang shane gu', 'izzeddin gur']"
2305.11857,computing high-dimensional optimal transport by flow neural networks,stat.ml cs.lg stat.me,"flow-based models are widely used in generative tasks, including normalizing flow, where a neural network transports from a data distribution $p$ to a normal distribution. this work develops a flow-based model that transports from $p$ to an arbitrary $q$ where both distributions are only accessible via finite samples. we propose to learn the dynamic optimal transport between $p$ and $q$ by training a flow neural network. the model is trained to optimally find an invertible transport map between $p$ and $q$ by minimizing the transport cost. the trained optimal transport flow subsequently allows for performing many downstream tasks, including infinitesimal density ratio estimation (dre) and distribution interpolation in the latent space for generative models. the effectiveness of the proposed model on high-dimensional data is demonstrated by strong empirical performance on high-dimensional dre, ot baselines, and image-to-image translation.",,2023-05-19,2024-02-04,"['chen xu', 'xiuyuan cheng', 'yao xie']"
2305.12292,optimal low-rank matrix completion: semidefinite relaxations and   eigenvector disjunctions,cs.lg math.oc stat.ml,"low-rank matrix completion consists of computing a matrix of minimal complexity that recovers a given set of observations as accurately as possible. unfortunately, existing methods for matrix completion are heuristics that, while highly scalable and often identifying high-quality solutions, do not possess any optimality guarantees. we reexamine matrix completion with an optimality-oriented eye. we reformulate these low-rank problems as convex problems over the non-convex set of projection matrices and implement a disjunctive branch-and-bound scheme that solves them to certifiable optimality. further, we derive a novel and often tight class of convex relaxations by decomposing a low-rank matrix as a sum of rank-one matrices and incentivizing that two-by-two minors in each rank-one matrix have determinant zero. in numerical experiments, our new convex relaxations decrease the optimality gap by two orders of magnitude compared to existing attempts, and our disjunctive branch-and-bound scheme solves nxn rank-r matrix completion problems to certifiable optimality in hours for n<=150 and r<=5.",,2023-05-20,2024-01-26,"['dimitris bertsimas', 'ryan cory-wright', 'sean lo', 'jean pauphilet']"
2305.12569,conditional generative modeling for high-dimensional marked temporal   point processes,stat.ml cs.lg,"point processes offer a versatile framework for sequential event modeling. however, the computational challenges and constrained representational power of the existing point process models have impeded their potential for wider applications. this limitation becomes especially pronounced when dealing with event data that is associated with multi-dimensional or high-dimensional marks such as texts or images. to address this challenge, this study proposes a novel event-generation framework for modeling point processes with high-dimensional marks. we aim to capture the distribution of events without explicitly specifying the conditional intensity or probability density function. instead, we use a conditional generator that takes the history of events as input and generates the high-quality subsequent event that is likely to occur given the prior observations. the proposed framework offers a host of benefits, including considerable representational power to capture intricate dynamics in multi- or even high-dimensional event space, as well as exceptional efficiency in learning the model and generating samples. our numerical results demonstrate superior performance compared to other state-of-the-art baselines.",,2023-05-21,2024-02-14,"['zheng dong', 'zekai fan', 'shixiang zhu']"
2305.12789,the decaying missing-at-random framework: doubly robust causal inference   with partially labeled data,stat.me math.st stat.ml stat.th,"in real-world scenarios, data collection limitations often result in partially labeled datasets, leading to difficulties in drawing reliable causal inferences. traditional approaches in the semi-supervised (ss) and missing data literature may not adequately handle these complexities, leading to biased estimates. to address these challenges, our paper introduces a novel decaying missing-at-random (decaying mar) framework. this framework tackles missing outcomes in high-dimensional settings and accounts for selection bias arising from the dependence of labeling probability on covariates. notably, we relax the need for a positivity condition, commonly required in the missing data literature, and allow uniform decay of labeling propensity scores with sample size, accommodating faster growth of unlabeled data. our decaying mar framework enables easy rate double-robust (dr) estimation of average treatment effects, succeeding where other methods fail, even with correctly specified nuisance models. additionally, it facilitates asymptotic normality under model misspecification. to achieve this, we propose adaptive new targeted bias-reducing nuisance estimators and asymmetric cross-fitting, along with a novel semi-parametric approach that fully leverages large volumes of unlabeled data. our approach requires weak sparsity conditions. numerical results confirm our estimators' efficacy and versatility, addressing selection bias and model misspecification.",,2023-05-22,2023-12-31,"['yuqian zhang', 'abhishek chakrabortty', 'jelena bradic']"
2305.12809,relabeling minimal training subset to flip a prediction,cs.lg cs.ai stat.ml,"when facing an unsatisfactory prediction from a machine learning model, users can be interested in investigating the underlying reasons and exploring the potential for reversing the outcome. we ask: to flip the prediction on a test point $x_t$, how to identify the smallest training subset $\mathcal{s}_t$ that we need to relabel? we propose an efficient algorithm to identify and relabel such a subset via an extended influence function for binary classification models with convex loss. we find that relabeling fewer than 2% of the training points can always flip a prediction. this mechanism can serve multiple purposes: (1) providing an approach to challenge a model prediction by altering training points; (2) evaluating model robustness with the cardinality of the subset (i.e., $|\mathcal{s}_t|$); we show that $|\mathcal{s}_t|$ is highly related to the noise ratio in the training set and $|\mathcal{s}_t|$ is correlated with but complementary to predicted probabilities; and (3) revealing training points lead to group attribution bias. to the best of our knowledge, we are the first to investigate identifying and relabeling the minimal training subset required to flip a given prediction.",,2023-05-22,2024-02-03,"['jinghan yang', 'linjie xu', 'lequan yu']"
2305.14040,"incremental propensity score effects for criminology: an application   assessing the relationship between homelessness, behavioral health problems,   and recidivism",stat.ap,"this study examines the relationship between homelessness and recidivism among people on probation with and without behavioral health problems. the study also illustrates a new way to summarize the effect of an exposure on an outcome, the incremental propensity score (ips) effect, which avoids pitfalls of other approaches commonly used in criminology. we assessed the impact of homelessness at probation start on rearrest within one year among a cohort of people on probation (n = 2,453). we estimated ips effects, considering general and crime-specific recidivism if subjects were more or less likely to be unhoused, and assessed effect variation by behavioral health problem status. we used a doubly robust machine learning estimator to flexibly but efficiently estimate effects. a substantial intervention -- reducing homelessness by roughly 65% -- corresponded to a 9% reduction in the estimated average rate of recidivism (p < .05). milder interventions showed smaller, non-significant effect sizes. stratifying by behavioral health problem and rearrest type led to similar results without statistical significance. minding limitations related to observational data and generalizability, this study suggests large reductions in homelessness lead to significant reductions in rearrest rates. efforts to reduce recidivism should include interventions that make homelessness less likely, but notable differences in recidivism will require these interventions be sizable. meanwhile, efforts to establish recidivism risk factors should consider alternative effects, like ips effects, to maximize validity and reduce bias.",10.1007/s10940-024-09582-7,2023-05-23,2024-02-08,"['leah a. jacobs', 'alec mcclean', 'zach branson', 'edward h. kennedy', 'alex fixler']"
2305.14120,learning relevant contextual variables within bayesian optimization,cs.lg stat.ml,"contextual bayesian optimization (cbo) efficiently optimizes black-box functions with respect to design variables, while simultaneously integrating contextual information regarding the environment, such as experimental conditions. however, the relevance of contextual variables is not necessarily known beforehand. moreover, contextual variables can sometimes be optimized themselves at additional cost, a setting overlooked by current cbo algorithms. cost-sensitive cbo would simply include optimizable contextual variables as part of the design variables based on their cost. instead, we adaptively select a subset of contextual variables to include in the optimization, based on the trade-off between their \emph{relevance} and the additional cost incurred by optimizing them compared to leaving them to be determined by the environment. we learn the relevance of contextual variables by sensitivity analysis of the posterior surrogate model while minimizing the cost of optimization by leveraging recent developments on early stopping for bo. we empirically evaluate our proposed sensitivity-analysis-driven contextual bo (sadcbo) method against alternatives on both synthetic and real-world experiments, together with extensive ablation studies, and demonstrate a consistent improvement across examples.",,2023-05-23,2024-02-12,"['julien martinelli', 'ayush bharti', 'armi tiihonen', 's. t. john', 'louis filstroff', 'sabina j. sloman', 'patrick rinke', 'samuel kaski']"
2305.14496,optimal learning via moderate deviations theory,stat.ml math.oc math.pr math.st stat.th,"this paper proposes a statistically optimal approach for learning a function value using a confidence interval in a wide range of models, including general non-parametric estimation of an expected loss described as a stochastic programming problem or various sde models. more precisely, we develop a systematic construction of highly accurate confidence intervals by using a moderate deviation principle-based approach. it is shown that the proposed confidence intervals are statistically optimal in the sense that they satisfy criteria regarding exponential accuracy, minimality, consistency, mischaracterization probability, and eventual uniformly most accurate (uma) property. the confidence intervals suggested by this approach are expressed as solutions to robust optimization problems, where the uncertainty is expressed via the underlying moderate deviation rate function induced by the data-generating process. we demonstrate that for many models these optimization problems admit tractable reformulations as finite convex programs even when they are infinite-dimensional.",,2023-05-23,2024-02-13,"['arnab ganguly', 'tobias sutter']"
2305.14961,deep learning for survival analysis: a review,stat.ml cs.lg,"the influx of deep learning (dl) techniques into the field of survival analysis in recent years has led to substantial methodological progress; for instance, learning from unstructured or high-dimensional data such as images, text or omics data. in this work, we conduct a comprehensive systematic review of dl-based methods for time-to-event analysis, characterizing them according to both survival- and dl-related attributes. in summary, the reviewed methods often address only a small subset of tasks relevant to time-to-event data - e.g., single-risk right-censored data - and neglect to incorporate more complex settings. our findings are summarized in an editable, open-source, interactive table: https://survival-org.github.io/dl4survival. as this research area is advancing rapidly, we encourage community contribution in order to keep this database up to date.",10.1007/s10462-023-10681-3,2023-05-24,2024-02-22,"['simon wiegrebe', 'philipp kopper', 'raphael sonabend', 'bernd bischl', 'andreas bender']"
2305.15325,statistical post-processing of visibility ensemble forecasts,stat.ap stat.ml,"to be able to produce accurate and reliable predictions of visibility has crucial importance in aviation meteorology, as well as in water- and road transportation. nowadays, several meteorological services provide ensemble forecasts of visibility; however, the skill, and reliability of visibility predictions are far reduced compared to other variables, such as temperature or wind speed. hence, some form of calibration is strongly advised, which usually means estimation of the predictive distribution of the weather quantity at hand either by parametric or non-parametric approaches, including also machine learning-based techniques. as visibility observations - according to the suggestion of the world meteorological organization - are usually reported in discrete values, the predictive distribution for this particular variable is a discrete probability law, hence calibration can be reduced to a classification problem. based on visibility ensemble forecasts of the european centre for medium-range weather forecasts covering two slightly overlapping domains in central and western europe and two different time periods, we investigate the predictive performance of locally, semi-locally and regionally trained proportional odds logistic regression (polr) and multilayer perceptron (mlp) neural network classifiers. we show that while climatological forecasts outperform the raw ensemble by a wide margin, post-processing results in further substantial improvement in forecast skill and in general, polr models are superior to their mlp counterparts.",10.1002/met.2157,2023-05-24,2023-05-27,"['sándor baran', 'mária lakatos']"
2305.15349,on the convergence of black-box variational inference,cs.lg eess.sp math.oc stat.co stat.ml,"we provide the first convergence guarantee for full black-box variational inference (bbvi), also known as monte carlo variational inference. while preliminary investigations worked on simplified versions of bbvi (e.g., bounded domain, bounded support, only optimizing for the scale, and such), our setup does not need any such algorithmic modifications. our results hold for log-smooth posterior densities with and without strong log-concavity and the location-scale variational family. also, our analysis reveals that certain algorithm design choices commonly employed in practice, particularly, nonlinear parameterizations of the scale of the variational approximation, can result in suboptimal convergence rates. fortunately, running bbvi with proximal stochastic gradient descent fixes these limitations, and thus achieves the strongest known convergence rate guarantees. we evaluate this theoretical insight by comparing proximal sgd against other standard implementations of bbvi on large-scale bayesian inference problems.",,2023-05-24,2024-01-10,"['kyurae kim', 'jisu oh', 'kaiwen wu', 'yi-an ma', 'jacob r. gardner']"
2305.15577,minimizing $f$-divergences by interpolating velocity fields,stat.ml cs.lg,"many machine learning problems can be formulated as approximating a target distribution using a particle distribution by minimizing a statistical discrepancy. wasserstein gradient flow can be employed to move particles along a path that minimizes the $f$-divergence between the \textit{target} and \textit{particle} distributions. to perform such movements we need to calculate the corresponding velocity fields which include a density ratio function between these two distributions. while previous works estimated the density ratio function first and then differentiated the estimated ratio, this approach may suffer from overfitting, which leads to a less accurate estimate. inspired by non-parametric curve fitting, we directly estimate these velocity fields using interpolation. we prove that our method is asymptotically consistent under mild conditions. we validate the effectiveness using novel applications on domain adaptation and missing data imputation.",,2023-05-24,2024-02-02,"['song liu', 'jiahao yu', 'jack simons', 'mingxuan yi', 'mark beaumont']"
2305.15742,counterfactual generative models for time-varying treatments,stat.ml cs.lg stat.me,"estimating the counterfactual outcome of treatment is essential for decision-making in public health and clinical science, among others. often, treatments are administered in a sequential, time-varying manner, leading to an exponentially increased number of possible counterfactual outcomes. furthermore, in modern applications, the outcomes are high-dimensional and conventional average treatment effect estimation fails to capture disparities in individuals. to tackle these challenges, we propose a novel conditional generative framework capable of producing counterfactual samples under time-varying treatment, without the need for explicit density estimation. our method carefully addresses the distribution mismatch between the observed and counterfactual distributions via a loss function based on inverse probability re-weighting, and supports integration with state-of-the-art conditional generative models such as the guided diffusion and conditional variational autoencoder. we present a thorough evaluation of our method using both synthetic and real-world data. our results demonstrate that our method is capable of generating high-quality counterfactual samples and outperforms the state-of-the-art baselines.",,2023-05-25,2024-02-14,"['shenghao wu', 'wenbin zhou', 'minshuo chen', 'shixiang zhu']"
2305.15936,learning dags from data with few root causes,cs.lg cs.ai stat.me,"we present a novel perspective and algorithm for learning directed acyclic graphs (dags) from data generated by a linear structural equation model (sem). first, we show that a linear sem can be viewed as a linear transform that, in prior work, computes the data from a dense input vector of random valued root causes (as we will call them) associated with the nodes. instead, we consider the case of (approximately) few root causes and also introduce noise in the measurement of the data. intuitively, this means that the dag data is produced by few data-generating events whose effect percolates through the dag. we prove identifiability in this new setting and show that the true dag is the global minimizer of the $l^0$-norm of the vector of root causes. for data with few root causes, with and without noise, we show superior performance compared to prior dag learning methods.",,2023-05-25,2024-01-23,"['panagiotis misiakos', 'chris wendler', 'markus püschel']"
2305.15984,dynamic inter-treatment information sharing for individualized treatment   effects estimation,cs.lg stat.me,"estimation of individualized treatment effects (ite) from observational studies is a fundamental problem in causal inference and holds significant importance across domains, including healthcare. however, limited observational datasets pose challenges in reliable ite estimation as data have to be split among treatment groups to train an ite learner. while information sharing among treatment groups can partially alleviate the problem, there is currently no general framework for end-to-end information sharing in ite estimation. to tackle this problem, we propose a deep learning framework based on `\textit{soft weight sharing}' to train ite learners, enabling \textit{dynamic end-to-end} information sharing among treatment groups. the proposed framework complements existing ite learners, and introduces a new class of ite learners, referred to as \textit{hyperite}. we extend state-of-the-art ite learners with \textit{hyperite} versions and evaluate them on ihdp, acic-2016, and twins benchmarks. our experimental results show that the proposed framework improves ite estimation error, with increasing effectiveness for smaller datasets.",,2023-05-25,2024-02-12,"['vinod kumar chauhan', 'jiandong zhou', 'ghadeer ghosheh', 'soheila molaei', 'david a. clifton']"
2305.16102,demystifying oversmoothing in attention-based graph neural networks,cs.lg cs.si stat.ml,"oversmoothing in graph neural networks (gnns) refers to the phenomenon where increasing network depth leads to homogeneous node representations. while previous work has established that graph convolutional networks (gcns) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. in this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based gnns as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. we establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. the proposed framework extends the existing results on oversmoothing for symmetric gcns to a significantly broader class of gnn models, including random walk gcns, graph attention networks (gats) and (graph) transformers. in particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as relu, leakyrelu, gelu and silu.",,2023-05-25,2024-01-11,"['xinyi wu', 'amir ajorlou', 'zihui wu', 'ali jadbabaie']"
2305.16189,martian time-series unraveled: a multi-scale nested approach with   factorial variational autoencoders,cs.lg astro-ph.ep stat.ml,"unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. this problem is inherently ill-posed and is further challenged by the variety of timescales exhibited by sources. existing methods typically rely on a preselected window size that determines their operating timescale, limiting their capacity to handle multi-scale sources. to address this issue, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering spectra that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-gaussian stochastic processes. nested within this representation space, we develop a factorial gaussian-mixture variational autoencoder that is trained to (1) probabilistically cluster sources at different timescales and (2) independently sample scattering spectra representations associated with each cluster. as the final stage, using samples from each cluster as prior information, we formulate source separation as an optimization problem in the wavelet scattering spectra representation space, aiming to separate sources in the time domain. when applied to the entire seismic dataset recorded during the nasa insight mission on mars, containing sources varying greatly in timescale, our multi-scale nested approach proves to be a powerful tool for disentangling such different sources, e.g., minute-long transient one-sided pulses (known as ``glitches'') and structured ambient noises resulting from atmospheric activities that typically last for tens of minutes. these results provide an opportunity to conduct further investigations into the isolated sources related to atmospheric-surface interactions, thermal relaxations, and other complex phenomena.",,2023-05-25,2024-02-19,"['ali siahkoohi', 'rudy morel', 'randall balestriero', 'erwan allys', 'grégory sainton', 'taichi kawamura', 'maarten v. de hoop']"
2305.16215,koopman kernel regression,cs.lg cs.sy eess.sy math.ds stat.ml,"many machine learning approaches for decision making, such as reinforcement learning, rely on simulators or predictive models to forecast the time-evolution of quantities of interest, e.g., the state of an agent or the reward of a policy. forecasts of such complex phenomena are commonly described by highly nonlinear dynamical systems, making their use in optimization-based decision-making challenging. koopman operator theory offers a beneficial paradigm for addressing this problem by characterizing forecasts via linear time-invariant (lti) odes, turning multi-step forecasts into sparse matrix multiplication. though there exists a variety of learning approaches, they usually lack crucial learning-theoretic guarantees, making the behavior of the obtained models with increasing data and dimensionality unclear. we address the aforementioned by deriving a universal koopman-invariant reproducing kernel hilbert space (rkhs) that solely spans transformations into lti dynamical systems. the resulting koopman kernel regression (kkr) framework enables the use of statistical learning tools from function approximation for novel convergence results and generalization error bounds under weaker assumptions than existing work. our experiments demonstrate superior forecasting performance compared to koopman operator and sequential data predictors in rkhs.",,2023-05-25,2024-01-16,"['petar bevanda', 'max beier', 'armin lederer', 'stefan sosnowski', 'eyke hüllermeier', 'sandra hirche']"
2305.16284,dowg unleashed: an efficient universal parameter-free gradient descent   method,cs.lg math.oc stat.ml,"this paper proposes a new easy-to-implement parameter-free gradient-based optimizer: dowg (distance over weighted gradients). we prove that dowg is efficient -- matching the convergence rate of optimally tuned gradient descent in convex optimization up to a logarithmic factor without tuning any parameters, and universal -- automatically adapting to both smooth and nonsmooth problems. while popular algorithms following the adagrad framework compute a running average of the squared gradients to use for normalization, dowg maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties. to complement our theory, we also show empirically that dowg trains at the edge of stability, and validate its effectiveness on practical machine learning tasks.",,2023-05-25,2024-01-29,"['ahmed khaled', 'konstantin mishchenko', 'chi jin']"
2305.16368,neural incomplete factorization: learning preconditioners for the   conjugate gradient method,math.oc cs.lg cs.na math.na stat.ml,"finding suitable preconditioners to accelerate iterative solution methods, such as the conjugate gradient method, is an active area of research. in this paper, we develop a computationally efficient data-driven approach to replace the typically hand-engineered algorithms with neural networks. optimizing the condition number of the linear system directly is computationally infeasible. instead, our method generates an incomplete factorization of the matrix and is, therefore, referred to as neural incomplete factorization (neuralif). for efficient training, we utilize a stochastic approximation of the frobenius loss which only requires matrix-vector multiplications. at the core of our method is a novel messagepassing block, inspired by sparse matrix theory, that aligns with the objective of finding a sparse factorization of the matrix. by replacing conventional preconditioners used within the conjugate gradient method by data-driven models based on graph neural networks, we accelerate the iterative solving procedure. we evaluate our proposed method on both a synthetic and a real-world problem arising from scientific computing and show its ability to reduce the solving time while remaining computationally efficient.",,2023-05-25,2024-02-05,"['paul häusner', 'ozan öktem', 'jens sjölund']"
2305.16860,error bounds for flow matching methods,stat.ml cs.lg,"score-based generative models are a popular class of generative modelling techniques relying on stochastic differential equations (sde). from their inception, it was realized that it was also possible to perform generation using ordinary differential equations (ode) rather than sde. this led to the introduction of the probability flow ode approach and denoising diffusion implicit models. flow matching methods have recently further extended these ode-based approaches and approximate a flow between two arbitrary probability distributions. previous work derived bounds on the approximation error of diffusion models under the stochastic sampling regime, given assumptions on the $l^2$ loss. we present error bounds for the flow matching procedure using fully deterministic sampling, assuming an $l^2$ bound on the approximation error and a certain regularity condition on the data distributions.",,2023-05-26,2024-02-11,"['joe benton', 'george deligiannidis', 'arnaud doucet']"
2305.16905,improving neural additive models with bayesian principles,stat.ml cs.lg,"neural additive models (nams) enhance the transparency of deep neural networks by handling input features in separate additive sub-networks. however, they lack inherent mechanisms that provide calibrated uncertainties and enable selection of relevant features and interactions. approaching nams from a bayesian perspective, we augment them in three primary ways, namely by a) providing credible intervals for the individual additive sub-networks; b) estimating the marginal likelihood to perform an implicit selection of features via an empirical bayes procedure; and c) facilitating the ranking of feature pairs as candidates for second-order interaction in fine-tuned models. in particular, we develop laplace-approximated nams (la-nams), which show improved empirical performance on tabular datasets and challenging real-world medical tasks.",,2023-05-26,2024-02-05,"['kouroche bouchiat', 'alexander immer', 'hugo yèche', 'gunnar rätsch', 'vincent fortuin']"
2305.17028,better batch for deep probabilistic time series forecasting,stat.ml cs.lg,"deep probabilistic time series forecasting has gained attention for its superior performance in nonlinear approximation and its capability to offer valuable uncertainty quantification for decision-making. however, existing models often oversimplify the problem by assuming a time-independent error process, overlooking serial correlation. to overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance probabilistic forecasting accuracy. our method constructs a mini-batch as a collection of $d$ consecutive time series segments for model training. it explicitly learns a time-varying covariance matrix over each mini-batch, encoding error correlation among adjacent time steps. the learned covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantification. we evaluate our method on two different neural forecasting models and multiple public datasets. experimental results confirm the effectiveness of the proposed approach in improving the performance of both models across a range of datasets, resulting in notable improvements in predictive accuracy.",,2023-05-26,2024-02-06,"['vincent zhihao zheng', 'seongjin choi', 'lijun sun']"
2305.17224,fast and accurate estimation of low-rank matrices from noisy   measurements via preconditioned non-convex gradient descent,math.oc cs.lg stat.ml,"non-convex gradient descent is a common approach for estimating a low-rank $n\times n$ ground truth matrix from noisy measurements, because it has per-iteration costs as low as $o(n)$ time, and is in theory capable of converging to a minimax optimal estimate. however, the practitioner is often constrained to just tens to hundreds of iterations, and the slow and/or inconsistent convergence of non-convex gradient descent can prevent a high-quality estimate from being obtained. recently, the technique of preconditioning was shown to be highly effective at accelerating the local convergence of non-convex gradient descent when the measurements are noiseless. in this paper, we describe how preconditioning should be done for noisy measurements to accelerate local convergence to minimax optimality. for the symmetric matrix sensing problem, our proposed preconditioned method is guaranteed to locally converge to minimax error at a linear rate that is immune to ill-conditioning and/or over-parameterization. using our proposed preconditioned method, we perform a 60 megapixel medical image denoising task, and observe significantly reduced noise levels compared to previous approaches.",,2023-05-26,2024-02-27,"['gavin zhang', 'hong-ming chiu', 'richard y. zhang']"
2305.17225,causal component analysis,stat.ml cs.ai cs.lg,"independent component analysis (ica) aims to recover independent latent variables from observed mixtures thereof. causal representation learning (crl) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. we introduce an intermediate problem termed causal component analysis (cauca). cauca can be viewed as a generalization of ica, modelling the causal dependence among the latent components, and as a special case of crl. in contrast to crl, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. any impossibility results regarding the recovery of the ground truth in cauca also apply for crl, while possibility results may serve as a stepping stone for extensions to crl. we characterize cauca identifiability from multiple datasets generated through different types of interventions on the latent causal variables. as a corollary, this interventional perspective also leads to new identifiability results for nonlinear ica -- a special case of cauca with an empty graph -- requiring strictly fewer datasets than previous results. we introduce a likelihood-based approach using normalizing flows to estimate both the unmixing function and the causal mechanisms, and demonstrate its effectiveness through extensive synthetic experiments in the cauca and ica setting.",,2023-05-26,2024-01-17,"['liang wendong', 'armin kekić', 'julius von kügelgen', 'simon buchholz', 'michel besserve', 'luigi gresele', 'bernhard schölkopf']"
2305.17301,"stability-penalty-adaptive follow-the-regularized-leader: sparsity,   game-dependency, and best-of-both-worlds",cs.lg stat.ml,"adaptivity to the difficulties of a problem is a key property in sequential decision-making problems to broaden the applicability of algorithms. follow-the-regularized-leader (ftrl) has recently emerged as one of the most promising approaches for obtaining various types of adaptivity in bandit problems. aiming to further generalize this adaptivity, we develop a generic adaptive learning rate, called stability-penalty-adaptive (spa) learning rate for ftrl. this learning rate yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of ftrl is typically decomposed. with this result, we establish several algorithms with three types of adaptivity: sparsity, game-dependency, and best-of-both-worlds (bobw). despite the fact that sparsity appears frequently in real problems, existing sparse multi-armed bandit algorithms with $k$-arms assume that the sparsity level $s \leq k$ is known in advance, which is often not the case in real-world scenarios. to address this issue, we first establish $s$-agnostic algorithms with regret bounds of $\tilde{o}(\sqrt{st})$ in the adversarial regime for $t$ rounds, which matches the existing lower bound up to a logarithmic factor. meanwhile, bobw algorithms aim to achieve a near-optimal regret in both the stochastic and adversarial regimes. leveraging the spa learning rate and the technique for $s$-agnostic algorithms combined with a new analysis to bound the variation in ftrl output in response to changes in a regularizer, we establish the first bobw algorithm with a sparsity-dependent bound. additionally, we explore partial monitoring and demonstrate that the proposed spa learning rate framework allows us to achieve a game-dependent bound and the bobw simultaneously.",,2023-05-26,2024-02-13,"['taira tsuchiya', 'shinji ito', 'junya honda']"
2305.17517,stochastic nonparametric estimation of the density-flow curve,stat.me stat.ap,"recent advances in operations research and machine learning have revived interest in solving complex real-world, large-size traffic control problems. with the increasing availability of road sensor data, deterministic parametric models have proved inadequate in describing the variability of real-world data, especially in congested area of the density-flow diagram. in this paper we estimate the stochastic density-flow relation introducing a nonparametric method called convex quantile regression. the proposed method does not depend on any prior functional form assumptions, but thanks to the concavity constraints, the estimated function satisfies the theoretical properties of the density-flow curve. the second contribution is to develop the new convex quantile regression with bags (cqrb) approach to facilitate practical implementation of cqr to the real-world data. we illustrate the cqrb estimation process using the road sensor data from finland in years 2016-2018. our third contribution is to demonstrate the excellent out-of-sample predictive power of the proposed cqrb method in comparison to the standard parametric deterministic approach.",,2023-05-27,2024-02-16,"['iaroslav kriuchkov', 'timo kuosmanen']"
2305.17665,acceleration of stochastic gradient descent with momentum by averaging:   finite-sample rates and asymptotic normality,cs.lg stat.ml,"stochastic gradient descent with momentum (sgdm) has been widely used in many machine learning and statistical applications. despite the observed empirical benefits of sgdm over traditional sgd, the theoretical understanding of the role of momentum for different learning rates in the optimization process remains widely open. we analyze the finite-sample convergence rate of sgdm under the strongly convex settings and show that, with a large batch size, the mini-batch sgdm converges faster than the mini-batch sgd to a neighborhood of the optimal value. additionally, our findings, supported by theoretical analysis and numerical experiments, indicate that sgdm permits broader choices of learning rates. furthermore, we analyze the polyak-averaging version of the sgdm estimator, establish its asymptotic normality, and justify its asymptotic equivalence to the averaged sgd. the asymptotic distribution of the averaged sgdm enables uncertainty quantification of the algorithm output and statistical inference of the model parameters.",,2023-05-28,2024-02-01,"['kejie tang', 'weidong liu', 'yichen zhang', 'xi chen']"
2305.18435,statistically efficient bayesian sequential experiment design via   reinforcement learning with cross-entropy estimators,cs.lg stat.me,"reinforcement learning can learn amortised design policies for designing sequences of experiments. however, current amortised methods rely on estimators of expected information gain (eig) that require an exponential number of samples on the magnitude of the eig to achieve an unbiased estimation. we propose the use of an alternative estimator based on the cross-entropy of the joint model distribution and a flexible proposal distribution. this proposal distribution approximates the true posterior of the model parameters given the experimental history and the design policy. our method overcomes the exponential-sample complexity of previous approaches and provide more accurate estimates of high eig values. more importantly, it allows learning of superior design policies, and is compatible with continuous and discrete design spaces, non-differentiable likelihoods and even implicit probabilistic models.",,2023-05-28,2024-02-04,"['tom blau', 'iadine chades', 'amir dezfouli', 'daniel steinberg', 'edwin v. bonilla']"
2305.18484,neural fourier transform: a general approach to equivariant   representation learning,stat.ml cs.lg,"symmetry learning has proven to be an effective approach for extracting the hidden structure of data, with the concept of equivariance relation playing the central role. however, most of the current studies are built on architectural theory and corresponding assumptions on the form of data. we propose neural fourier transform (nft), a general framework of learning the latent linear action of the group without assuming explicit knowledge of how the group acts on data. we present the theoretical foundations of nft and show that the existence of a linear equivariant feature, which has been assumed ubiquitously in equivariance learning, is equivalent to the existence of a group invariant kernel on the dataspace. we also provide experimental results to demonstrate the application of nft in typical scenarios with varying levels of knowledge about the acting group.",,2023-05-29,2024-02-14,"['masanori koyama', 'kenji fukumizu', 'kohei hayashi', 'takeru miyato']"
2305.18671,perturbation-assisted sample synthesis: a novel approach for uncertainty   quantification,stat.ml cs.lg,"this paper introduces a novel perturbation-assisted inference (pai) framework utilizing synthetic data generated by the perturbation-assisted sample synthesis (pass) method. the framework focuses on uncertainty quantification in complex data scenarios, particularly involving unstructured data while utilizing deep learning models. on one hand, pass employs a generative model to create synthetic data that closely mirrors raw data while preserving its rank properties through data perturbation, thereby enhancing data diversity and bolstering privacy. by incorporating knowledge transfer from large pre-trained generative models, pass enhances estimation accuracy, yielding refined distributional estimates of various statistics via monte carlo experiments. on the other hand, pai boasts its statistically guaranteed validity. in pivotal inference, it enables precise conclusions even without prior knowledge of the pivotal's distribution. in non-pivotal situations, we enhance the reliability of synthetic data generation by training it with an independent holdout sample. we demonstrate the effectiveness of pai in advancing uncertainty quantification in complex, data-driven tasks by applying it to diverse areas such as image synthesis, sentiment word analysis, multimodal inference, and the construction of prediction intervals.",,2023-05-29,2024-02-13,"['yifei liu', 'rex shen', 'xiaotong shen']"
2305.19082,embedding inequalities for barron-type spaces,stat.ml cs.lg cs.na math.na,"an important problem in machine learning theory is to understand the approximation and generalization properties of two-layer neural networks in high dimensions. to this end, researchers have introduced the barron space $\mathcal{b}_s(\omega)$ and the spectral barron space $\mathcal{f}_s(\omega)$, where the index $s\in [0,\infty)$ indicates the smoothness of functions within these spaces and $\omega\subset\mathbb{r}^d$ denotes the input domain. however, the precise relationship between the two types of barron spaces remains unclear. in this paper, we establish a continuous embedding between them as implied by the following inequality: for any $\delta\in (0,1), s\in \mathbb{n}^{+}$ and $f: \omega \mapsto\mathbb{r}$, it holds that \[ \delta \|f\|_{\mathcal{f}_{s-\delta}(\omega)}\lesssim_s \|f\|_{\mathcal{b}_s(\omega)}\lesssim_s \|f\|_{\mathcal{f}_{s+1}(\omega)}. \]   importantly, the constants do not depend on the input dimension $d$, suggesting that the embedding is effective in high dimensions. moreover, we also show that the lower and upper bound are both tight.",10.4208/jml.230530,2023-05-30,2023-12-27,['lei wu']
2305.19215,"dotears: scalable, consistent dag estimation using observational and   interventional data",stat.ml cs.lg,"new biological assays like perturb-seq link highly parallel crispr interventions to a high-dimensional transcriptomic readout, providing insight into gene regulatory networks. causal gene regulatory networks can be represented by directed acyclic graph (dags), but learning dags from observational data is complicated by lack of identifiability and a combinatorial solution space. score-based structure learning improves practical scalability of inferring dags. previous score-based methods are sensitive to error variance structure; on the other hand, estimation of error variance is difficult without prior knowledge of structure. accordingly, we present $\texttt{dotears}$ [doo-tairs], a continuous optimization framework which leverages observational and interventional data to infer a single causal structure, assuming a linear structural equation model (sem). $\texttt{dotears}$ exploits structural consequences of hard interventions to give a marginal estimate of exogenous error structure, bypassing the circular estimation problem. we show that $\texttt{dotears}$ is a provably consistent estimator of the true dag under mild assumptions. $\texttt{dotears}$ outperforms other methods in varied simulations, and in real data infers edges that validate with higher precision and recall than state-of-the-art methods through differential expression tests and high-confidence protein-protein interactions.",,2023-05-30,2024-02-20,"['albert xue', 'jingyou rao', 'sriram sankararaman', 'harold pimentel']"
2305.19259,on convergence of incremental gradient for non-convex smooth functions,cs.lg math.oc stat.ml,"in machine learning and neural network optimization, algorithms like incremental gradient, and shuffle sgd are popular due to minimizing the number of cache misses and good practical convergence behavior. however, their optimization properties in theory, especially for non-convex smooth functions, remain incompletely explored.   this paper delves into the convergence properties of sgd algorithms with arbitrary data ordering, within a broad framework for non-convex smooth functions. our findings show enhanced convergence guarantees for incremental gradient and single shuffle sgd. particularly if $n$ is the training set size, we improve $n$ times the optimization term of convergence guarantee to reach accuracy $\varepsilon$ from $o(n / \varepsilon)$ to $o(1 / \varepsilon)$.",,2023-05-30,2024-02-12,"['anastasia koloskova', 'nikita doikov', 'sebastian u. stich', 'martin jaggi']"
2305.19265,probabilistic computation and uncertainty quantification with emerging   covariance,cs.lg cs.ne math.st stat.th,"building robust, interpretable, and secure ai system requires quantifying and representing uncertainty under a probabilistic perspective to mimic human cognitive abilities. however, probabilistic computation presents significant challenges for most conventional artificial neural network, as they are essentially implemented in a deterministic manner. in this paper, we develop an efficient probabilistic computation framework by truncating the probabilistic representation of neural activation up to its mean and covariance and construct a moment neural network that encapsulates the nonlinear coupling between the mean and covariance of the underlying stochastic network. we reveal that when only the mean but not the covariance is supervised during gradient-based learning, the unsupervised covariance spontaneously emerges from its nonlinear coupling with the mean and faithfully captures the uncertainty associated with model predictions. our findings highlight the inherent simplicity of probabilistic computation by seamlessly incorporating uncertainty into model prediction, paving the way for integrating it into large-scale ai systems.",,2023-05-30,2024-01-12,"['hengyuan ma', 'yang qi', 'li zhang', 'wenlian lu', 'jianfeng feng']"
2305.19510,mildly overparameterized relu networks have a favorable loss landscape,cs.lg math.co stat.ml,"we study the loss landscape of both shallow and deep, mildly overparameterized relu neural networks on a generic finite input dataset for the squared error loss. we show both by count and volume that most activation patterns correspond to parameter regions with no bad local minima. furthermore, for one-dimensional input data, we show most activation regions realizable by the network contain a high dimensional set of global minima and no bad local minima. we experimentally confirm these results by finding a phase transition from most regions having full rank jacobian to many regions having deficient rank depending on the amount of overparameterization.",,2023-05-30,2024-02-08,"['kedar karhadkar', 'michael murray', 'hanna tseran', 'guido montúfar']"
2305.19638,a unified framework for u-net design and analysis,stat.ml cs.cv cs.lg eess.iv,"u-nets are a go-to, state-of-the-art neural architecture across numerous tasks for continuous signals on a square such as images and partial differential equations (pde), however their design and architecture is understudied. in this paper, we provide a framework for designing and analysing general u-net architectures. we present theoretical results which characterise the role of the encoder and decoder in a u-net, their high-resolution scaling limits and their conjugacy to resnets via preconditioning. we propose multi-resnets, u-nets with a simplified, wavelet-based encoder without learnable parameters. further, we show how to design novel u-net architectures which encode function constraints, natural bases, or the geometry of the data. in diffusion models, our framework enables us to identify that high-frequency information is dominated by noise exponentially faster, and show how u-nets with average pooling exploit this. in our experiments, we demonstrate how multi-resnets achieve competitive and often superior performance compared to classical u-nets in image segmentation, pde surrogate modelling, and generative modelling with diffusion models. our u-net framework paves the way to study the theoretical properties of u-nets and design natural, scalable neural architectures for a multitude of problems beyond the square.",,2023-05-31,2024-01-10,"['christopher williams', 'fabian falck', 'george deligiannidis', 'chris holmes', 'arnaud doucet', 'saifuddin syed']"
2305.19685,deep stochastic mechanics,cs.lg quant-ph stat.ml,"this paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving schr\""odinger equation inspired by stochastic mechanics and generative diffusion models. unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the markovian diffusion. depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.",,2023-05-31,2024-02-14,"['elena orlova', 'aleksei ustimenko', 'ruoxi jiang', 'peter y. lu', 'rebecca willett']"
2305.20004,learning to solve bayesian inverse problems: an amortized variational   inference approach using gaussian and flow guides,stat.ml cs.lg physics.data-an,"inverse problems, i.e., estimating parameters of physical models from experimental data, are ubiquitous in science and engineering. the bayesian formulation is the gold standard because it alleviates ill-posedness issues and quantifies epistemic uncertainty. since analytical posteriors are not typically available, one resorts to markov chain monte carlo sampling or approximate variational inference. however, inference needs to be rerun from scratch for each new set of data. this drawback limits the applicability of the bayesian formulation to real-time settings, e.g., health monitoring of engineered systems, and medical diagnosis. the objective of this paper is to develop a methodology that enables real-time inference by learning the bayesian inverse map, i.e., the map from data to posteriors. our approach is as follows. we parameterize the posterior distribution as a function of data. this work outlines two distinct approaches to do this. the first method involves parameterizing the posterior using an amortized full-rank gaussian guide, implemented through neural networks. the second method utilizes a conditional normalizing flow guide, employing conditional invertible neural networks for cases where the target posterior is arbitrarily complex. in both approaches, we learn the network parameters by amortized variational inference which involves maximizing the expectation of evidence lower bound over all possible datasets compatible with the model. we demonstrate our approach by solving a set of benchmark problems from science and engineering. our results show that the posterior estimates of our approach are in agreement with the corresponding ground truth obtained by markov chain monte carlo. once trained, our approach provides the posterior distribution for a given observation just at the cost of a forward pass of the neural network.",,2023-05-31,2024-01-15,"['sharmila karumuri', 'ilias bilionis']"
2306.00074,human-aligned calibration for ai-assisted decision making,cs.lg cs.cy cs.hc stat.ml,"whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. in this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. however, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. in this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. we first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker would need to sometimes place more (less) trust on predictions with lower (higher) confidence values. however, we then show that, if the confidence values satisfy a natural alignment property with respect to the decision maker's confidence on her own predictions, there always exists an optimal decision policy under which the level of trust the decision maker would need to place on predictions is monotone on the confidence values, facilitating its discoverability. further, we show that multicalibration with respect to the decision maker's confidence on her own predictions is a sufficient condition for alignment. experiments on four different ai-assisted decision making tasks where a classifier provides decision support to real human experts validate our theoretical results and suggest that alignment may lead to better decisions.",,2023-05-31,2024-02-23,"['nina l. corvelo benz', 'manuel gomez rodriguez']"
2306.00196,restless bandits with average reward: breaking the uniform global   attractor assumption,cs.lg math.oc math.pr stat.ml,"we study the infinite-horizon restless bandit problem with the average reward criterion, in both discrete-time and continuous-time settings. a fundamental goal is to efficiently compute policies that achieve a diminishing optimality gap as the number of arms, $n$, grows large. existing results on asymptotic optimality all rely on the uniform global attractor property (ugap), a complex and challenging-to-verify assumption. in this paper, we propose a general, simulation-based framework, follow-the-virtual-advice, that converts any single-armed policy into a policy for the original $n$-armed problem. this is done by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. our framework can be instantiated to produce a policy with an $o(1/\sqrt{n})$ optimality gap. in the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that violate ugap. more notably, in the continuous-time setting, we do not require \emph{any} additional assumptions beyond the standard unichain condition. in both settings, our work is the first asymptotic optimality result that does not require ugap.",,2023-05-31,2024-01-16,"['yige hong', 'qiaomin xie', 'yudong chen', 'weina wang']"
2306.00353,constructing semantics-aware adversarial examples with probabilistic   perspective,stat.ml cs.cr cs.lg,"we propose a probabilistic perspective on adversarial examples. this perspective allows us to view geometric restrictions on adversarial examples as distributions, enabling a seamless shift towards data-driven, semantic constraints. building on this foundation, we present a method for creating semantics-aware adversarial examples in a principle way. leveraging the advanced generalization capabilities of contemporary probabilistic generative models, our method produces adversarial perturbations that maintain the original image's semantics. moreover, it offers users the flexibility to inject their own understanding of semantics into the adversarial examples. our empirical findings indicate that the proposed methods achieve enhanced transferability and higher success rates in circumventing adversarial defense mechanisms, while maintaining a low detection rate by human observers.",,2023-06-01,2024-02-11,"['andi zhang', 'mingtian zhang', 'damon wischik']"
2306.00684,balanced training of energy-based models with adaptive flow sampling,cs.lg stat.ml,"energy-based models (ebms) are versatile density estimation models that directly parameterize an unnormalized log density. although very flexible, ebms lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. these techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. in this work, we propose a new maximum likelihood training algorithm for ebms that uses a different type of generative model, normalizing flows (nf), which have recently been proposed to facilitate sampling. our method fits an nf to an ebm during training so that an nf-assisted sampling scheme provides an accurate gradient for the ebms at all times, ultimately leading to a fast sampler for generating new data.",,2023-06-01,2024-02-18,"['louis grenioux', 'éric moulines', 'marylou gabrié']"
2306.00732,sharper bounds for $\ell_p$ sensitivity sampling,cs.ds cs.lg stat.ml,"in large scale machine learning, random sampling is a popular way to approximate datasets by a small representative subset of examples. in particular, sensitivity sampling is an intensely studied technique which provides provable guarantees on the quality of approximation, while reducing the number of examples to the product of the vc dimension $d$ and the total sensitivity $\mathfrak s$ in remarkably general settings. however, guarantees going beyond this general bound of $\mathfrak s d$ are known in perhaps only one setting, for $\ell_2$ subspace embeddings, despite intense study of sensitivity sampling in prior work. in this work, we show the first bounds for sensitivity sampling for $\ell_p$ subspace embeddings for $p > 2$ that improve over the general $\mathfrak s d$ bound, achieving a bound of roughly $\mathfrak s^{2-2/p}$ for $2<p<\infty$. furthermore, our techniques yield further new results in the study of sampling algorithms, showing that the root leverage score sampling algorithm achieves a bound of roughly $d$ for $1\leq p<2$, and that a combination of leverage score and sensitivity sampling achieves an improved bound of roughly $d^{2/p}\mathfrak s^{2-4/p}$ for $2<p<\infty$. our sensitivity sampling results yield the best known sample complexity for a wide class of structured matrices that have small $\ell_p$ sensitivity.",,2023-06-01,2024-01-03,"['david p. woodruff', 'taisuke yasuda']"
2306.00740,on the limitations of temperature scaling for distributions with   overlaps,cs.lg stat.ml,"despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to be overconfident when they are wrong. fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures such as temperature scaling. while temperature scaling is frequently used because of its simplicity, it is often outperformed by modified training schemes. in this work, we identify a specific bottleneck for the performance of temperature scaling. we show that for empirical risk minimizers for a general set of distributions in which the supports of classes have overlaps, the performance of temperature scaling degrades with the amount of overlap between classes, and asymptotically becomes no better than random when there are a large number of classes. on the other hand, we prove that optimizing a modified form of the empirical risk induced by the mixup data augmentation technique can in fact lead to reasonably good calibration performance, showing that training-time calibration may be necessary in some situations. we also verify that our theoretical results reflect practice by showing that mixup significantly outperforms empirical risk minimization (with respect to multiple calibration metrics) on image classification benchmarks with class overlaps introduced in the form of label noise.",,2023-06-01,2024-02-13,"['muthu chidambaram', 'rong ge']"
2306.00742,the galerkin method beats graph-based approaches for spectral algorithms,cs.lg cs.ai stat.ml,"historically, the machine learning community has derived spectral decompositions from graph-based approaches. we break with this approach and prove the statistical and computational superiority of the galerkin method, which consists in restricting the study to a small set of test functions. in particular, we introduce implementation tricks to deal with differential operators in large dimensions with structured kernels. finally, we extend on the core principles beyond our approach to apply them to non-linear spaces of functions, such as the ones parameterized by deep neural networks, through loss-based optimization procedures.",,2023-06-01,2024-02-26,"['vivien cabannes', 'francis bach']"
2306.00788,understanding augmentation-based self-supervised representation learning   via rkhs approximation and regression,cs.lg stat.ml,"data augmentation is critical to the empirical success of modern self-supervised representation learning, such as contrastive learning and masked language modeling. however, a theoretical understanding of the exact role of augmentation remains limited. recent work has built the connection between self-supervised learning and the approximation of the top eigenspace of a graph laplacian operator, suggesting that learning a linear probe atop such representation can be connected to rkhs regression. building on this insight, this work delves into a statistical analysis of augmentation-based pretraining. starting from the isometry property, a geometric characterization of the target function given by the augmentation, we disentangle the effects of the model and the augmentation, and prove two generalization bounds that are free of model complexity. our first bound works for an arbitrary encoder, where the prediction error is decomposed as the sum of an estimation error incurred by fitting a linear probe with rkhs regression, and an approximation error entailed by rkhs approximation. our second bound specifically addresses the case where the encoder is near-optimal, that is it approximates the top-d eigenspace of the rkhs induced by the augmentation. a key ingredient in our analysis is the augmentation complexity, which we use to quantitatively compare different augmentations and analyze their impact on downstream performance.",,2023-06-01,2024-01-18,"['runtian zhai', 'bingbin liu', 'andrej risteski', 'zico kolter', 'pradeep ravikumar']"
2306.00809,initial guessing bias: how untrained networks favor some classes,cs.lg cond-mat.dis-nn stat.ml,"understanding and controlling biasing effects in neural networks is crucial for ensuring accurate and fair model performance. in the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a deep neural network (dnn) can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. we prove that, besides dataset properties, the presence of this phenomenon, which we call \textit{initial guessing bias} (igb), is influenced by model choices including dataset preprocessing methods, and architectural decisions, such as activation functions, max-pooling layers, and network depth. our analysis of igb provides information for architecture selection and model initialization. we also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging and the non-trivial effects that depth has on the phenomenon.",,2023-06-01,2024-02-12,"['emanuele francazi', 'aurelien lucchi', 'marco baity-jesi']"
2306.01237,bayesian regret minimization in offline bandits,cs.lg stat.ml,"we study how to make decisions that minimize bayesian regret in offline linear bandits. prior work suggests that one must take actions with maximum lower confidence bound (lcb) on their reward. we argue that reliance on lcb is inherently flawed in this setting and propose a new algorithm that directly minimizes upper bounds on the bayesian regret using efficient conic optimization solvers. our bounds build heavily on new connections to monetary risk measures. proving a matching lower bound, we show that our upper bounds are tight, and by minimizing them we are guaranteed to outperform the lcb approach. our numerical results on synthetic domains confirm that our approach is superior to maximizing lcb.",,2023-06-01,2024-02-06,"['mohammad ghavamzadeh', 'marek petrik', 'guy tennenholtz']"
2306.01271,towards understanding clean generalization and robust overfitting in   adversarial training,cs.lg stat.ml,"similar to surprising performance in the standard deep learning, deep nets trained by adversarial training also generalize well for $\textit{unseen clean data (natural data)}$. however, despite adversarial training can achieve low robust training error, there exists a significant $\textit{robust generalization gap}$. we call this phenomenon the $\textit{clean generalization and robust overfitting (cgro)}$. in this work, we study the cgro phenomenon in adversarial training from two views: $\textit{representation complexity}$ and $\textit{training dynamics}$. specifically, we consider a binary classification setting with $n$ separated training data points. $\textit{first}$, we prove that, based on the assumption that we assume there is $\operatorname{poly}(d)$-size clean classifier (where $d$ is the data dimension), relu net with only $o(n d)$ extra parameters is able to leverages robust memorization to achieve the cgro, while robust classifier still requires exponential representation complexity in worst case. $\textit{next}$, we focus on a structured-data case to analyze training dynamics, where we train a two-layer convolutional network with $o(n d)$ width against adversarial perturbation. we then show that a three-stage phase transition occurs during learning process and the network provably converges to robust memorization regime, which thereby results in the cgro. $\textit{besides}$, we also empirically verify our theoretical analysis by experiments in real-image recognition datasets.",,2023-06-02,2024-02-05,"['binghui li', 'yuanzhi li']"
2306.01424,partial counterfactual identification of continuous outcomes with a   curvature sensitivity model,stat.ml cs.lg,"counterfactual inference aims to answer retrospective ""what if"" questions and thus belongs to the most fine-grained type of inference in pearl's causality ladder. existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. in this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. we prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. as a remedy, we propose a novel sensitivity model called curvature sensitivity model. this allows us to obtain informative bounds by bounding the curvature of level sets of the functions. we further show that existing point counterfactual identification methods are special cases of our curvature sensitivity model when the bound of the curvature is set to zero. we then propose an implementation of our curvature sensitivity model in the form of a novel deep generative model, which we call augmented pseudo-invertible decoder. our implementation employs (i) residual normalizing flows with (ii) variational augmentations. we empirically demonstrate the effectiveness of our augmented pseudo-invertible decoder. to the best of our knowledge, ours is the first partial identification model for markovian structural causal models with continuous outcomes.",,2023-06-02,2024-01-11,"['valentyn melnychuk', 'dennis frauen', 'stefan feuerriegel']"
2306.01710,a data-driven measure of relative uncertainty for misclassification   detection,stat.ml cs.lg,"misclassification detection is an important problem in machine learning, as it allows for the identification of instances where the model's predictions are unreliable. however, conventional uncertainty measures such as shannon entropy do not provide an effective way to infer the real uncertainty associated with the model's predictions. in this paper, we introduce a novel data-driven measure of uncertainty relative to an observer for misclassification detection. by learning patterns in the distribution of soft-predictions, our uncertainty measure can identify misclassified samples based on the predicted class probabilities. interestingly, according to the proposed measure, soft-predictions corresponding to misclassified instances can carry a large amount of uncertainty, even though they may have low shannon entropy. we demonstrate empirical improvements over multiple image classification tasks, outperforming state-of-the-art misclassification detection methods.",,2023-06-02,2024-02-08,"['eduardo dadalto', 'marco romanelli', 'georg pichler', 'pablo piantanida']"
2306.01727,broadcasting in random recursive dags,stat.ml cs.lg cs.si,"a uniform $k$-{\sc dag} generalizes the uniform random recursive tree by picking $k$ parents uniformly at random from the existing nodes. it starts with $k$ ''roots''. each of the $k$ roots is assigned a bit. these bits are propagated by a noisy channel. the parents' bits are flipped with probability $p$, and a majority vote is taken. when all nodes have received their bits, the $k$-{\sc dag} is shown without identifying the roots. the goal is to estimate the majority bit among the roots. we identify the threshold for $p$ as a function of $k$ below which the majority rule among all nodes yields an error $c+o(1)$ with $c<1/2$. above the threshold the majority rule errs with probability $1/2+o(1)$.",,2023-06-02,2024-02-24,"['simon briend', 'luc devroye', 'gabor lugosi']"
2306.01992,on size-independent sample complexity of relu networks,cs.lg stat.ml,"we study the sample complexity of learning relu neural networks from the point of view of generalization. given norm constraints on the weight matrices, a common approach is to estimate the rademacher complexity of the associated function class. previously golowich-rakhlin-shamir (2020) obtained a bound independent of the network size (scaling with a product of frobenius norms) except for a factor of the square-root depth. we give a refinement which often has no explicit depth-dependence at all.",,2023-06-02,2024-02-04,['mark sellke']
2306.02066,variational gaussian process diffusion processes,cs.lg stat.ml,"diffusion processes are a class of stochastic differential equations (sdes) providing a rich family of expressive models that arise naturally in dynamic modelling tasks. probabilistic inference and learning under generative models with latent processes endowed with a non-linear diffusion process prior are intractable problems. we build upon work within variational inference, approximating the posterior process as a linear diffusion process, and point out pathologies in the approach. we propose an alternative parameterization of the gaussian variational process using a site-based exponential family description. this allows us to trade a slow inference algorithm with fixed-point iterations for a fast algorithm for convex optimization akin to natural gradient descent, which also provides a better objective for learning model parameters.",,2023-06-03,2024-02-27,"['prakhar verma', 'vincent adam', 'arno solin']"
2306.02426,resilient constrained learning,cs.lg stat.ml,"when deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. these requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on lagrangian duality. either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. this paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. to do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. we call this approach resilient constrained learning after the term used to describe ecological systems that adapt to disruptions by modifying their operation. we show conditions under which this balance can be achieved and introduce a practical algorithm to compute it, for which we derive approximation and generalization guarantees. we showcase the advantages of this resilient learning method in image classification tasks involving multiple potential invariances and in heterogeneous federated learning.",,2023-06-04,2024-01-11,"['ignacio hounie', 'alejandro ribeiro', 'luiz f. o. chamon']"
2306.02689,equity-transformer: solving np-hard min-max routing problems as   sequential generation with equity context,cs.lg math.oc stat.ml,"min-max routing problems aim to minimize the maximum tour length among multiple agents by having agents conduct tasks in a cooperative manner. these problems include impactful real-world applications but are known as np-hard. existing methods are facing challenges, particularly in large-scale problems that require the coordination of numerous agents to cover thousands of cities. this paper proposes equity-transformer to solve large-scale min-max routing problems. first, we employ sequential planning approach to address min-max routing problems, allowing us to harness the powerful sequence generators (e.g., transformer). second, we propose key inductive biases that ensure equitable workload distribution among agents. the effectiveness of equity-transformer is demonstrated through its superior performance in two representative min-max routing tasks: the min-max multi-agent traveling salesman problem (min-max mtsp) and the min-max multi-agent pick-up and delivery problem (min-max mpdp). notably, our method achieves significant reductions of runtime, approximately 335 times, and cost values of about 53\% compared to a competitive heuristic (lkh3) in the case of 100 vehicles with 1,000 cities of mtsp. we provide reproducible source code: \url{https://github.com/kaist-silab/equity-transformer}.",,2023-06-05,2024-02-04,"['jiwoo son', 'minsu kim', 'sanghyeok choi', 'hyeonah kim', 'jinkyoo park']"
2306.02729,gibbs sampling the posterior of neural networks,cs.lg stat.ml,"in this paper, we study sampling from a posterior derived from a neural network. we propose a new probabilistic model consisting of adding noise at every pre- and post-activation in the network, arguing that the resulting posterior can be sampled using an efficient gibbs sampler. for small models, the gibbs sampler attains similar performances as the state-of-the-art markov chain monte carlo (mcmc) methods, such as the hamiltonian monte carlo (hmc) or the metropolis adjusted langevin algorithm (mala), both on real and synthetic data. by framing our analysis in the teacher-student setting, we introduce a thermalization criterion that allows us to detect when an algorithm, when run on data with synthetic labels, fails to sample from the posterior. the criterion is based on the fact that in the teacher-student setting we can initialize an algorithm directly at equilibrium.",,2023-06-05,2024-01-11,"['giovanni piccioli', 'emanuele troiani', 'lenka zdeborová']"
2306.02869,data-driven online model selection with regret guarantees,cs.lg cs.ai stat.ml,"we consider model selection for sequential decision making in stochastic environments with bandit feedback, where a meta-learner has at its disposal a pool of base learners, and decides on the fly which action to take based on the policies recommended by each base learner. model selection is performed by regret balancing but, unlike the recent literature on this subject, we do not assume any prior knowledge about the base learners like candidate regret guarantees; instead, we uncover these quantities in a data-driven manner. the meta-learner is therefore able to leverage the realized regret incurred by each base learner for the learning environment at hand (as opposed to the expected regret), and single out the best such regret. we design two model selection algorithms operating with this more ambitious notion of regret and, besides proving model selection guarantees via regret balancing, we experimentally demonstrate the compelling practical benefits of dealing with actual regrets instead of candidate regret bounds.",,2023-06-05,2024-01-23,"['aldo pacchiano', 'christoph dann', 'claudio gentile']"
2306.02895,evading black-box classifiers without breaking eggs,cs.cr cs.lg stat.ml,"decision-based evasion attacks repeatedly query a black-box classifier to generate adversarial examples. prior work measures the cost of such attacks by the total number of queries made to the classifier. we argue this metric is flawed. most security-critical machine learning systems aim to weed out ""bad"" data (e.g., malware, harmful content, etc). queries to such systems carry a fundamentally asymmetric cost: queries detected as ""bad"" come at a higher cost because they trigger additional security filters, e.g., usage throttling or account suspension. yet, we find that existing decision-based attacks issue a large number of ""bad"" queries, which likely renders them ineffective against security-critical systems. we then design new attacks that reduce the number of bad queries by $1.5$-$7.3\times$, but often at a significant increase in total (non-bad) queries. we thus pose it as an open problem to build black-box attacks that are more effective under realistic cost metrics.",,2023-06-05,2024-02-14,"['edoardo debenedetti', 'nicholas carlini', 'florian tramèr']"
2306.02939,improved stability and generalization guarantees of the decentralized   sgd algorithm,cs.lg stat.ml,"this paper presents a new generalization error analysis for decentralized stochastic gradient descent (d-sgd) based on algorithmic stability. the obtained results overhaul a series of recent works that suggested an increased instability due to decentralization and a detrimental impact of poorly-connected communication graphs on generalization. on the contrary, we show, for convex, strongly convex and non-convex functions, that d-sgd can always recover generalization bounds analogous to those of classical sgd, suggesting that the choice of graph does not matter. we then argue that this result is coming from a worst-case analysis, and we provide a refined data-dependent generalization bound for general convex functions. this new bound reveals that the choice of graph can in fact improve the worst-case bound in certain regimes, and that surprisingly, a poorly-connected graph can even be beneficial.",,2023-06-05,2024-02-15,"['batiste le bars', 'aurélien bellet', 'marc tommasi', 'kevin scaman', 'giovanni neglia']"
2306.03266,extending the design space of graph neural networks by rethinking   folklore weisfeiler-lehman,cs.lg stat.ml,"message passing neural networks (mpnns) have emerged as the most popular framework of graph neural networks (gnns) in recent years. however, their expressive power is limited by the 1-dimensional weisfeiler-lehman (1-wl) test. some works are inspired by $k$-wl/fwl (folklore wl) and design the corresponding neural versions. despite the high expressive power, there are serious limitations in this line of research. in particular, (1) $k$-wl/fwl requires at least $o(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) the design space of $k$-wl/fwl is rigid, with the only adjustable hyper-parameter being $k$. to tackle the first limitation, we propose an extension, $(k,t)$-fwl. we theoretically prove that even if we fix the space complexity to $o(n^k)$ (for any $k\geq 2$) in $(k,t)$-fwl, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. to tackle the second problem, we propose $k$-fwl+, which considers any equivariant set as neighbors instead of all nodes, thereby greatly expanding the design space of $k$-fwl. combining these two modifications results in a flexible and powerful framework $(k,t)$-fwl+. we demonstrate $(k,t)$-fwl+ can implement most existing models with matching expressiveness. we then introduce an instance of $(k,t)$-fwl+ called neighborhood$^2$-fwl (n$^2$-fwl), which is practically and theoretically sound. we prove that n$^2$-fwl is no less powerful than 3-wl, and can encode many substructures while only requiring $o(n^2)$ space. finally, we design its neural version named n$^2$-gnn and evaluate its performance on various tasks. n$^2$-gnn achieves record-breaking results on zinc-subset (0.059), outperforming previous sota results by 10.6%. moreover, n$^2$-gnn achieves new sota results on the brec dataset (71.8%) among all existing high-expressive gnn methods.",,2023-06-05,2024-01-14,"['jiarui feng', 'lecheng kong', 'hao liu', 'dacheng tao', 'fuhai li', 'muhan zhang', 'yixin chen']"
2306.03401,a lightweight method for tackling unknown participation statistics in   federated averaging,cs.lg cs.dc cs.it math.it math.oc stat.ml,"in federated learning (fl), clients usually have diverse participation statistics that are unknown a priori, which can significantly harm the performance of fl if not handled properly. existing works aiming at addressing this problem are usually based on global variance reduction, which requires a substantial amount of additional memory in a multiplicative factor equal to the total number of clients. an important open problem is to find a lightweight method for fl in the presence of clients with unknown participation rates. in this paper, we address this problem by adapting the aggregation weights in federated averaging (fedavg) based on the participation history of each client. we first show that, with heterogeneous participation statistics, fedavg with non-optimal aggregation weights can diverge from the optimal solution of the original fl objective, indicating the need of finding optimal aggregation weights. however, it is difficult to compute the optimal weights when the participation statistics are unknown. to address this problem, we present a new algorithm called fedau, which improves fedavg by adaptively weighting the client updates based on online estimates of the optimal weights without knowing the statistics of client participation. we provide a theoretical convergence analysis of fedau using a novel methodology to connect the estimation error and convergence. our theoretical results reveal important and interesting insights, while showing that fedau converges to an optimal solution of the original objective and has desirable properties such as linear speedup. our experimental results also verify the advantage of fedau over baseline methods with various participation patterns.",,2023-06-06,2024-01-23,"['shiqiang wang', 'mingyue ji']"
2306.03589,how does over-squashing affect the power of gnns?,cs.lg stat.ml,"graph neural networks (gnns) are the state-of-the-art model for machine learning on graph-structured data. the most popular class of gnns operate by exchanging information between adjacent nodes, and are known as message passing neural networks (mpnns). given their widespread use, understanding the expressive power of mpnns is a key question. however, existing results typically consider settings with uninformative node features. in this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an mpnn of a given capacity. we do so by measuring the level of pairwise interactions between nodes that mpnns allow for. this measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the mpnn must be large enough, depending on properties of the input graph structure, such as commute times. for many relevant scenarios, our analysis results in impossibility statements in practice, showing that over-squashing hinders the expressive power of mpnns. we validate our theoretical findings through extensive controlled experiments and ablation studies.",,2023-06-06,2024-02-12,"['francesco di giovanni', 't. konstantin rusch', 'michael m. bronstein', 'andreea deac', 'marc lackenby', 'siddhartha mishra', 'petar veličković']"
2306.03801,stable vectorization of multiparameter persistent homology using signed   barcodes as measures,cs.lg cs.cg math.at stat.ml,"persistent homology (ph) provides topological descriptors for geometric data, such as weighted graphs, which are interpretable, stable to perturbations, and invariant under, e.g., relabeling. most applications of ph focus on the one-parameter case -- where the descriptors summarize the changes in topology of data as it is filtered by a single quantity of interest -- and there is now a wide array of methods enabling the use of one-parameter ph descriptors in data science, which rely on the stable vectorization of these descriptors as elements of a hilbert space. although the multiparameter ph (mph) of data that is filtered by several quantities of interest encodes much richer information than its one-parameter counterpart, the scarceness of stability results for mph descriptors has so far limited the available options for the stable vectorization of mph. in this paper, we aim to bring together the best of both worlds by showing how the interpretation of signed barcodes -- a recent family of mph descriptors -- as signed measures leads to natural extensions of vectorization strategies from one parameter to multiple parameters. the resulting feature vectors are easy to define and to compute, and provably stable. while, as a proof of concept, we focus on simple choices of signed barcodes and vectorizations, we already see notable performance improvements when comparing our feature vectors to state-of-the-art topology-based methods on various types of data.",,2023-06-06,2024-02-07,"['david loiseaux', 'luis scoccola', 'mathieu carrière', 'magnus bakke botnan', 'steve oudot']"
2306.04120,messy estimation: maximum-entropy based stochastic and symbolic density   estimation,cs.lg cs.ai cs.it math.it math.st stat.ml stat.th,"we introduce messy estimation, a maximum-entropy based stochastic and symbolic density estimation method. the proposed approach recovers probability density functions symbolically from samples using moments of a gradient flow in which the ansatz serves as the driving force. in particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. we then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. furthermore, we use symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. the cost of the proposed method for each set of selected basis functions is linear with the number of samples and quadratic with the number of basis functions. however, the underlying acceptance/rejection procedure for finding optimal and well-conditioned bases adds to the computational cost. we validate the proposed messy estimation method against other benchmark methods for the case of a bi-modal and a discontinuous density, as well as a density at the limit of physical realizability. we find that the addition of a symbolic search for basis functions improves the accuracy of the estimation at a reasonable additional computational cost. our results suggest that the proposed method outperforms existing density recovery methods in the limit of a small to moderate number of samples by providing a low-bias and tractable symbolic description of the unknown density at a reasonable computational cost.",,2023-06-06,2024-02-10,"['tony tohme', 'mohsen sadr', 'kamal youcef-toumi', 'nicolas g. hadjiconstantinou']"
2306.04746,using imperfect surrogates for downstream inference: design-based   supervised learning for social science applications of large language models,stat.me cs.cl cs.lg stat.ml,"in computational social science (css), researchers analyze documents to explain social and political phenomena. in most scenarios, css researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. one increasingly common way to annotate documents cheaply at scale is through large language models (llms). however, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased. we present a new algorithm for using imperfect annotation surrogates for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to css research. we show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80-90%. to address this, we build on debiased machine learning to propose the design-based supervised learning (dsl) estimator. dsl employs a doubly-robust procedure to combine surrogate labels with a smaller number of high-quality, gold-standard labels. our approach guarantees valid inference for downstream statistical analyses, even when surrogates are arbitrarily biased and without requiring stringent assumptions, by controlling the probability of sampling documents for gold-standard labeling. both our theoretical analysis and experimental results show that dsl provides valid statistical inference while achieving root mean squared errors comparable to existing alternatives that focus only on prediction without inferential guarantees.",,2023-06-07,2024-01-14,"['naoki egami', 'musashi hinck', 'brandon m. stewart', 'hanying wei']"
2306.04775,exploiting observation bias to improve matrix completion,cs.lg stat.ml,"we consider a variant of matrix completion where entries are revealed in a biased manner, adopting a model akin to that introduced by ma and chen. instead of treating this observation bias as a disadvantage, as is typically the case, the goal is to exploit the shared information between the bias and the outcome of interest to improve predictions. towards this, we consider a natural model where the observation pattern and outcome of interest are driven by the same set of underlying latent or unobserved factors. this leads to a two stage matrix completion algorithm: first, recover (distances between) the latent factors by utilizing matrix completion for the fully observed noisy binary matrix corresponding to the observation pattern; second, utilize the recovered latent factors as features and sparsely observed noisy outcomes as labels to perform non-parametric supervised learning. the finite-sample error rates analysis suggests that, ignoring logarithmic factors, this approach is competitive with the corresponding supervised learning parametric rates. this implies the two-stage method has performance that is comparable to having access to the unobserved latent factors through exploiting the shared information between the bias and outcomes. through empirical evaluation using a real-world dataset, we find that with this two-stage algorithm, the estimates have 30x smaller mean squared error compared to traditional matrix completion methods, suggesting the utility of the model and the method proposed in this work.",,2023-06-07,2024-02-04,"['yassir jedra', 'sean mann', 'charlotte park', 'devavrat shah']"
2306.04817,sibblings: similarity-driven building-block inference using graphs   across states,stat.ml cs.lg,"time series data across scientific domains are often collected under distinct states (e.g., tasks), wherein latent processes (e.g., biological factors) create complex inter- and intra-state variability. a key approach to capture this complexity is to uncover fundamental interpretable units within the data, i.e., building blocks (bbs), that modulate their activity and adjust their structure across observations. existing methods for identifying bbs in multi-way data often overlook inter- vs. intra-state variability, produce uninterpretable components, or do not align with some real-world data properties including missing samples and sessions of different durations. here, we present a framework for similarity-driven building block inference using graphs across states (sibblings). sibblings offers a graph-based dictionary learning approach for discovering sparse bbs along with their temporal traces, based on co-activity patterns and inter- vs. intra-state relationships. moreover, sibblings captures per-trial temporal variability and controlled cross-state structural bb adaptations, identifies state-specific vs. state-invariant components, and is robust to noise, missing samples, and variability in the number and duration of observed sessions across states. we demonstrate sibblings ability to reveal insights into complex phenomena through several synthetic and real-world examples, including web search and neural data.",,2023-06-07,2024-01-31,"['noga mudrik', 'gal mishne', 'adam s. charles']"
2306.04836,$k$-nearest-neighbor resampling for off-policy evaluation in stochastic   control,stat.ml cs.lg math.oc math.st stat.me stat.th,"in this paper, we propose a novel $k$-nearest neighbor resampling procedure for estimating the performance of a policy from historical data containing realized episodes of a decision process generated under a different policy. we provide statistical consistency results under weak conditions. in particular, we avoid the common assumption of identically and independently distributed transitions and rewards. instead, our analysis allows for the sampling of entire episodes, as is common practice in most applications. to establish the consistency in this setting, we generalize stone's theorem, a well-known result in nonparametric statistics on local averaging, to include episodic data and the counterfactual estimation underlying off-policy evaluation (ope). by focusing on feedback policies that depend deterministically on the current state in environments with continuous state-action spaces and system-inherent stochasticity effected by chosen actions, and relying on trajectory simulation similar to monte carlo methods, the proposed method is particularly well suited for stochastic control environments. compared to other ope methods, our algorithm does not require optimization, can be efficiently implemented via tree-based nearest neighbor search and parallelization, and does not explicitly assume a parametric model for the environment's dynamics. numerical experiments demonstrate the effectiveness of the algorithm compared to existing baselines in a variety of stochastic control settings, including a linear quadratic regulator, trade execution in limit order books, and online stochastic bin packing.",,2023-06-07,2024-01-10,"['michael giegrich', 'roel oomen', 'christoph reisinger']"
2306.04843,classical verification of quantum learning,quant-ph cs.cc cs.lg stat.ml,"quantum data access and quantum processing can make certain classically intractable learning tasks feasible. however, quantum capabilities will only be available to a select few in the near future. thus, reliable schemes that allow classical clients to delegate learning to untrusted quantum servers are required to facilitate widespread access to quantum learning advantages. building on a recently introduced framework of interactive proof systems for classical machine learning, we develop a framework for classical verification of quantum learning. we exhibit learning problems that a classical learner cannot efficiently solve on their own, but that they can efficiently and reliably solve when interacting with an untrusted quantum prover. concretely, we consider the problems of agnostic learning parities and fourier-sparse functions with respect to distributions with uniform input marginal. we propose a new quantum data access model that we call ""mixture-of-superpositions"" quantum examples, based on which we give efficient quantum learning algorithms for these tasks. moreover, we prove that agnostic quantum parity and fourier-sparse learning can be efficiently verified by a classical verifier with only random example or statistical query access. finally, we showcase two general scenarios in learning and verification in which quantum mixture-of-superpositions examples do not lead to sample complexity improvements over classical data. our results demonstrate that the potential power of quantum data for learning tasks, while not unlimited, can be utilized by classical agents through interaction with untrusted quantum entities.",10.4230/lipics.itcs.2024.24,2023-06-07,2023-12-07,"['matthias c. caro', 'marcel hinsche', 'marios ioannou', 'alexander nietner', 'ryan sweke']"
2306.04848,interpreting and improving diffusion models using the euclidean distance   function,cs.lg cs.cv math.oc stat.ml,"denoising is intuitively related to projection. indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. hence, learning to denoise is approximately learning to project. in this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the euclidean distance function. we then provide straight-forward convergence analysis of the ddim sampler under simple assumptions on the projection-error of the denoiser. finally, we propose a new sampler based on two simple modifications to ddim using insights from our theoretical results. in as few as 5-10 function evaluations, our sampler achieves state-of-the-art fid scores on pretrained cifar-10 and celeba models and can generate high quality samples on latent diffusion models.",,2023-06-07,2024-02-13,"['frank permenter', 'chenyang yuan']"
2306.05023,beyond vanilla variational autoencoders: detecting posterior collapse in   conditional and hierarchical variational autoencoders,stat.ml cs.lg,"the posterior collapse phenomenon in variational autoencoder (vae), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. as a consequence of posterior collapse, the latent variables extracted by the encoder in vae preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. while this phenomenon has been an actively addressed topic related to vae performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard vae. in this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of vae: conditional vae and hierarchical vae. specifically, via a non-trivial theoretical analysis of linear conditional vae and hierarchical vae with two levels of latent, we prove that the cause of posterior collapses in these models includes the correlation between the input and output of the conditional vae and the effect of learnable encoder variance in the hierarchical vae. we empirically validate our theoretical findings for linear conditional and hierarchical vae and demonstrate that these results are also predictive for non-linear cases with extensive experiments.",,2023-06-08,2024-01-22,"['hien dang', 'tho tran', 'tan nguyen', 'nhat ho']"
2306.05843,adaptive batch sizes for active learning a probabilistic numerics   approach,cs.lg cs.ai cs.na math.na stat.co stat.ml,"active learning parallelization is widely used, but typically relies on fixing the batch size throughout experimentation. this fixed approach is inefficient because of a dynamic trade-off between cost and speed -- larger batches are more costly, smaller batches lead to slower wall-clock run-times -- and the trade-off may change over the run (larger batches are often preferable earlier). to address this trade-off, we propose a novel probabilistic numerics framework that adaptively changes batch sizes. by framing batch selection as a quadrature task, our integration-error-aware algorithm facilitates the automatic tuning of batch sizes to meet predefined quadrature precision objectives, akin to how typical optimizers terminate based on convergence thresholds. this approach obviates the necessity for exhaustive searches across all potential batch sizes. we also extend this to scenarios with constrained active learning and constrained optimization, interpreting constraint violations as reductions in the precision requirement, to subsequently adapt batch construction. through extensive experiments, we demonstrate that our approach significantly enhances learning efficiency and flexibility in diverse bayesian batch active learning and bayesian optimization applications.",,2023-06-09,2024-02-21,"['masaki adachi', 'satoshi hayakawa', 'martin jørgensen', 'xingchen wan', 'vu nguyen', 'harald oberhauser', 'michael a. osborne']"
2306.05857,how sparse can we prune a deep network: a fundamental limit viewpoint,stat.ml cs.lg,"network pruning is an effective measure to alleviate the storage and computational burden of deep neural networks arising from its high overparameterization. thus raises a fundamental question: how sparse can we prune a deep network without sacrifice on the performance? to address this problem, in this work we'll take a first principles approach, i.e. we directly impose the sparsity constraint on the original loss function and then characterize the necessary and sufficient condition of the sparsity (\textit{which turns out to nearly coincide}) by leveraging the notion of \textit{statistical dimension} in convex geometry. through this fundamental limit, we're able to identify two key factors that determine the pruning ratio limit, i.e., weight magnitude and network flatness. generally speaking, the flatter the loss landscape or the smaller the weight magnitude, the smaller pruning ratio. in addition, we provide efficient countermeasures to address the challenges in computing the pruning limit, which involves accurate spectrum estimation of a large-scale and non-positive hessian matrix. moreover, through the lens of the pruning ratio threshold, we can provide rigorous interpretations on several heuristics in existing pruning algorithms. extensive experiments are performed that demonstrate that the our theoretical pruning ratio threshold coincides very well with the experiments. all codes are available at: https://github.com/qiaozhezhang/global-one-shot-pruning",,2023-06-09,2024-02-21,"['qiaozhe zhang', 'ruijie zhang', 'jun sun', 'yingzhuang liu']"
2306.06155,intensity profile projection: a framework for continuous-time   representation learning for dynamic networks,cs.lg stat.me stat.ml,"we present a new representation learning framework, intensity profile projection, for continuous-time dynamic network data. given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. the framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. the trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. the theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.",,2023-06-09,2024-01-17,"['alexander modell', 'ian gallagher', 'emma ceccherini', 'nick whiteley', 'patrick rubin-delanchy']"
2306.07119,"improving forecasts for heterogeneous time series by ""averaging"", with   application to food demand forecast",stat.me stat.ml,"a common forecasting setting in real world applications considers a set of possibly heterogeneous time series of the same domain. due to different properties of each time series such as length, obtaining forecasts for each individual time series in a straight-forward way is challenging. this paper proposes a general framework utilizing a similarity measure in dynamic time warping to find similar time series to build neighborhoods in a k-nearest neighbor fashion, and improve forecasts of possibly simple models by averaging. several ways of performing the averaging are suggested, and theoretical arguments underline the usefulness of averaging for forecasting. additionally, diagnostics tools are proposed allowing a deep understanding of the procedure.",,2023-06-12,2024-01-30,"['lukas neubauer', 'peter filzmoser']"
2306.07774,the rank-reduced kalman filter: approximate dynamical-low-rank filtering   in high dimensions,stat.ml cs.lg,"inference and simulation in the context of high-dimensional dynamical systems remain computationally challenging problems. some form of dimensionality reduction is required to make the problem tractable in general. in this paper, we propose a novel approximate gaussian filtering and smoothing method which propagates low-rank approximations of the covariance matrices. this is accomplished by projecting the lyapunov equations associated with the prediction step to a manifold of low-rank matrices, which are then solved by a recently developed, numerically stable, dynamical low-rank integrator. meanwhile, the update steps are made tractable by noting that the covariance update only transforms the column space of the covariance matrix, which is low-rank by construction. the algorithm differentiates itself from existing ensemble-based approaches in that the low-rank approximations of the covariance matrices are deterministic, rather than stochastic. crucially, this enables the method to reproduce the exact kalman filter as the low-rank dimension approaches the true dimensionality of the problem. our method reduces computational complexity from cubic (for the kalman filter) to \emph{quadratic} in the state-space size in the worst-case, and can achieve \emph{linear} complexity if the state-space model satisfies certain criteria. through a set of experiments in classical data-assimilation and spatio-temporal regression, we show that the proposed method consistently outperforms the ensemble-based methods in terms of error in the mean and covariance with respect to the exact kalman filter. this comes at no additional cost in terms of asymptotic computational complexity.",,2023-06-13,2024-01-03,"['jonathan schmidt', 'philipp hennig', 'jörg nick', 'filip tronarp']"
2306.08125,implicit compressibility of overparametrized neural networks trained   with heavy-tailed sgd,stat.ml cs.lg math.pr,"neural network compression has been an increasingly important subject, not only due to its practical relevance, but also due to its theoretical implications, as there is an explicit connection between compressibility and generalization error. recent studies have shown that the choice of the hyperparameters of stochastic gradient descent (sgd) can have an effect on the compressibility of the learned parameter vector. these results, however, rely on unverifiable assumptions and the resulting theory does not provide a practical guideline due to its implicitness. in this study, we propose a simple modification for sgd, such that the outputs of the algorithm will be provably compressible without making any nontrivial assumptions. we consider a one-hidden-layer neural network trained with sgd, and show that if we inject additive heavy-tailed noise to the iterates at each iteration, for any compression rate, there exists a level of overparametrization such that the output of the algorithm will be compressible with high probability. to achieve this result, we make two main technical contributions: (i) we prove a 'propagation of chaos' result for a class of heavy-tailed stochastic differential equations, and (ii) we derive error estimates for their euler discretization. our experiments suggest that the proposed approach not only achieves increased compressibility with various models and datasets, but also leads to robust test performance under pruning, even in more realistic architectures that lie beyond our theoretical setting.",,2023-06-13,2024-02-12,"['yijun wan', 'melih barsbey', 'abdellatif zaidi', 'umut simsekli']"
2306.08489,analysis and approximate inference of large random kronecker graphs,stat.ml cs.lg math.sp,"random graph models are playing an increasingly important role in various fields ranging from social networks, telecommunication systems, to physiologic and biological networks. within this landscape, the random kronecker graph model, emerges as a prominent framework for scrutinizing intricate real-world networks. in this paper, we investigate large random kronecker graphs, i.e., the number of graph vertices $n$ is large. built upon recent advances in random matrix theory (rmt) and high-dimensional statistics, we prove that the adjacency of a large random kronecker graph can be decomposed, in a spectral norm sense, into two parts: a small-rank (of rank $o(\log n)$) signal matrix that is linear in the graph parameters and a zero-mean random noise matrix. based on this result, we propose a ``denoise-and-solve'' approach to infer the key graph parameters, with significantly reduced computational complexity. experiments on both graph inference and classification are presented to evaluate the our proposed method. in both tasks, the proposed approach yields comparable or advantageous performance, than widely-used graph inference (e.g., kronfit) and graph neural net baselines, at a time cost that scales linearly as the graph size $n$.",,2023-06-14,2024-02-05,"['zhenyu liao', 'yuanqian xia', 'chengmei niu', 'yong xiao']"
2306.08598,"kernel debiased plug-in estimation: simultaneous, automated debiasing   without influence functions for many target parameters",stat.me stat.ml,"in the problem of estimating target parameters in nonparametric models with nuisance parameters, substituting the unknown nuisances with nonparametric estimators can introduce ""plug-in bias."" traditional methods addressing this sub-optimal bias-variance trade-offs rely on the influence function (if) of the target parameter. when estimating multiple target parameters, these methods require debiasing the nuisance parameter multiple times using the corresponding ifs, posing analytical and computational challenges. in this work, we leverage the targeted maximum likelihood estimation framework to propose a novel method named kernel debiased plug-in estimation (kdpe). kdpe refines an initial estimate through regularized likelihood maximization steps, employing a nonparametric model based on reproducing kernel hilbert spaces. we show that kdpe (i) simultaneously debiases all pathwise differentiable target parameters that satisfy our regularity conditions, (ii) does not require the if for implementation, and (iii) remains computationally tractable. we numerically illustrate the use of kdpe and validate our theoretical results.",,2023-06-14,2024-02-08,"['brian cho', 'yaroslav mukhin', 'kyra gan', 'ivana malenica']"
2306.08838,differentially private domain adaptation with theoretical guarantees,cs.lg cs.cr stat.ml,"in many applications, the labeled data at the learner's disposal is subject to privacy constraints and is relatively limited. to derive a more accurate predictor for the target domain, it is often beneficial to leverage publicly available labeled data from an alternative domain, somewhat close to the target domain. this is the modern problem of supervised domain adaptation from a public source to a private target domain. we present two $(\epsilon, \delta)$-differentially private adaptation algorithms for supervised adaptation, for which we make use of a general optimization problem, recently shown to benefit from favorable theoretical learning guarantees. our first algorithm is designed for regression with linear predictors and shown to solve a convex optimization problem. our second algorithm is a more general solution for loss functions that may be non-convex but lipschitz and smooth. while our main objective is a theoretical analysis, we also report the results of several experiments first demonstrating that the non-private versions of our algorithms outperform adaptation baselines and next showing that, for larger values of the target sample size or $\epsilon$, the performance of our private algorithms remains close to that of the non-private formulation.",,2023-06-15,2024-02-04,"['raef bassily', 'corinna cortes', 'anqi mao', 'mehryar mohri']"
2306.08946,bootstrap aggregation and confidence measures to improve time series   causal discovery,stat.me stat.ml,"learning causal graphs from multivariate time series is a ubiquitous challenge in all application domains dealing with time-dependent systems, such as in earth sciences, biology, or engineering, to name a few. recent developments for this causal discovery learning task have shown considerable skill, notably the specific time-series adaptations of the popular conditional independence-based learning framework. however, uncertainty estimation is challenging for conditional independence-based methods. here, we introduce a novel bootstrap approach designed for time series causal discovery that preserves the temporal dependencies and lag structure. it can be combined with a range of time series causal discovery methods and provides a measure of confidence for the links of the time series graphs. furthermore, next to confidence estimation, an aggregation, also called bagging, of the bootstrapped graphs by majority voting results in bagged causal discovery methods. in this work, we combine this approach with the state-of-the-art conditional-independence-based algorithm pcmci+. with extensive numerical experiments we empirically demonstrate that, in addition to providing confidence measures for links, bagged-pcmci+ improves in precision and recall as compared to its base algorithm pcmci+, at the cost of higher computational demands. these statistical performance improvements are especially pronounced in the more challenging settings (short time sample size, large number of variables, high autocorrelation). our bootstrap approach can also be combined with other time series causal discovery algorithms and can be of considerable use in many real-world applications.",,2023-06-15,2024-02-22,"['kevin debeire', 'jakob runge', 'andreas gerhardus', 'veronika eyring']"
2306.09136,finite-time logarithmic bayes regret upper bounds,cs.lg stat.ml,"we derive the first finite-time logarithmic bayes regret upper bounds for bayesian bandits. in a multi-armed bandit, we obtain $o(c_\delta \log n)$ and $o(c_h \log^2 n)$ upper bounds for an upper confidence bound algorithm, where $c_h$ and $c_\delta$ are constants depending on the prior distribution and the gaps of bandit instances sampled from it, respectively. the latter bound asymptotically matches the lower bound of lai (1987). our proofs are a major technical departure from prior works, while being simple and general. to show the generality of our techniques, we apply them to linear bandits. our results provide insights on the value of prior in the bayesian setting, both in the objective and as a side information given to the learner. they significantly improve upon existing $\tilde{o}(\sqrt{n})$ bounds, which have become standard in the literature despite the logarithmic lower bound of lai (1987).",,2023-06-15,2024-01-21,"['alexia atsidakou', 'branislav kveton', 'sumeet katariya', 'constantine caramanis', 'sujay sanghavi']"
2306.09686,collapsed inference for bayesian deep learning,cs.lg cs.ai stat.ml,"bayesian neural networks (bnns) provide a formalism to quantify and calibrate uncertainty in deep learning. current inference approaches for bnns often resort to few-sample estimation for scalability, which can harm predictive performance, while its alternatives tend to be computationally prohibitively expensive. we tackle this challenge by revealing a previously unseen connection between inference on bnns and volume computation problems. with this observation, we introduce a novel collapsed inference scheme that performs bayesian model averaging using collapsed samples. it improves over a monte-carlo sample by limiting sampling to a subset of the network weights while pairing it with some closed-form conditional distribution over the rest. a collapsed sample represents uncountably many models drawn from the approximate posterior and thus yields higher sample efficiency. further, we show that the marginalization of a collapsed sample can be solved analytically and efficiently despite the non-linearity of neural networks by leveraging existing volume computation solvers. our proposed use of collapsed samples achieves a balance between scalability and accuracy. on various regression and classification tasks, our collapsed bayesian deep learning approach demonstrates significant improvements over existing methods and sets a new state of the art in terms of uncertainty estimation as well as predictive performance.",,2023-06-16,2024-02-12,"['zhe zeng', 'guy van den broeck']"
2306.09739,stabilized neural differential equations for learning dynamics with   explicit constraints,cs.lg physics.comp-ph stat.ml,"many successful methods to learn dynamical systems from data have recently been introduced. however, ensuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. we propose stabilized neural differential equations (sndes), a method to enforce arbitrary manifold constraints for neural differential equations. our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. due to its simplicity, our method is compatible with all common neural differential equation (nde) models and broadly applicable. in extensive empirical evaluations, we demonstrate that sndes outperform existing methods while broadening the types of constraints that can be incorporated into nde training.",,2023-06-16,2024-02-15,"['alistair white', 'niki kilbertus', 'maximilian gelbrecht', 'niklas boers']"
2306.09882,uncertainty quantification via spatial-temporal tweedie model for   zero-inflated and long-tail travel demand prediction,cs.lg stat.ml stat.ot,"understanding origin-destination (o-d) travel demand is crucial for transportation management. however, traditional spatial-temporal deep learning models grapple with addressing the sparse and long-tail characteristics in high-resolution o-d matrices and quantifying prediction uncertainty. this dilemma arises from the numerous zeros and over-dispersed demand patterns within these matrices, which challenge the gaussian assumption inherent to deterministic deep learning models. to address these challenges, we propose a novel approach: the spatial-temporal tweedie graph neural network (sttd). the sttd introduces the tweedie distribution as a compelling alternative to the traditional 'zero-inflated' model and leverages spatial and temporal embeddings to parameterize travel demand distributions. our evaluations using real-world datasets highlight sttd's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.",10.1145/3583780.3615215,2023-06-16,2024-01-30,"['xinke jiang', 'dingyi zhuang', 'xianghui zhang', 'hao chen', 'jiayuan luo', 'xiaowei gao']"
2306.10155,fairness in multi-task learning via wasserstein barycenters,stat.ml cs.cy cs.lg,"algorithmic fairness is an established field in machine learning that aims to reduce biases in data. recent advances have proposed various methods to ensure fairness in a univariate environment, where the goal is to de-bias a single task. however, extending fairness to a multi-task setting, where more than one objective is optimised using a shared representation, remains underexplored. to bridge this gap, we develop a method that extends the definition of strong demographic parity to multi-task learning using multi-marginal wasserstein barycenters. our approach provides a closed form solution for the optimal fair multi-task predictor including both regression and binary classification tasks. we develop a data-driven estimation procedure for the solution and run numerical experiments on both synthetic and real datasets. the empirical results highlight the practical value of our post-processing methodology in promoting fair decision-making.",10.1007/978-3-031-43415-0_18,2023-06-16,2023-07-06,"['françois hu', 'philipp ratz', 'arthur charpentier']"
2306.10592,conditional expectation using compactification operators,stat.ml cs.lg math.fa math.pr,"the separate tasks of denoising, least squares expectation, and manifold learning can often be posed in a common setting of finding the conditional expectations arising from a product of two random variables. this paper focuses on this more general problem and describes an operator theoretic approach to estimating the conditional expectation. kernel integral operators are used as a compactification tool, to set up the estimation problem as a linear inverse problem in a reproducing kernel hilbert space. this equation is shown to have solutions that allow numerical approximation, thus guaranteeing the convergence of data-driven implementations. the overall technique is easy to implement, and their successful application to some real-world problems are also shown.",10.1016/j.acha.2024.101638,2023-06-18,2024-01-08,['suddhasattwa das']
2306.10816,$\texttt{causalassembly}$: generating realistic production data for   benchmarking causal discovery,stat.ml cs.lg stat.me,"algorithms for causal discovery have recently undergone rapid advances and increasingly draw on flexible nonparametric methods to process complex data. with these advances comes a need for adequate empirical validation of the causal relationships learned by different algorithms. however, for most real data sources true causal relations remain unknown. this issue is further compounded by privacy concerns surrounding the release of suitable high-quality data. to help address these challenges, we gather a complex dataset comprising measurements from an assembly line in a manufacturing context. this line consists of numerous physical processes for which we are able to provide ground truth causal relationships on the basis of a detailed study of the underlying physics. we use the assembly line data and associated ground truth information to build a system for generation of semisynthetic manufacturing data that supports benchmarking of causal discovery methods. to accomplish this, we employ distributional random forests in order to flexibly estimate and represent conditional distributions that may be combined into joint distributions that strictly adhere to a causal model over the observed variables. the estimated conditionals and tools for data generation are made available in our python library $\texttt{causalassembly}$. using the library, we showcase how to benchmark several well-known causal discovery algorithms.",,2023-06-19,2024-02-14,"['konstantin göbler', 'tobias windisch', 'mathias drton', 'tim pychynski', 'steffen sonntag', 'martin roth']"
2306.10822,interpreting deep neural networks with the package innsight,stat.ml cs.lg,"the r package innsight offers a general toolbox for revealing variable-wise interpretations of deep neural networks' predictions with so-called feature attribution methods. aside from the unified and user-friendly framework, the package stands out in three ways: it is generally the first r package implementing feature attribution methods for neural networks. secondly, it operates independently of the deep learning library allowing the interpretation of models from any r package, including keras, torch, neuralnet, and even custom models. despite its flexibility, innsight benefits internally from the torch package's fast and efficient array calculations, which builds on libtorch $-$ pytorch's c++ backend $-$ without a python dependency. finally, it offers a variety of visualization tools for tabular, signal, image data or a combination of these. additionally, the plots can be rendered interactively using the plotly package.",,2023-06-19,2024-01-18,"['niklas koenen', 'marvin n. wright']"
2306.10882,adastop: adaptive statistical testing for sound comparisons of deep rl   agents,cs.lg stat.ml,"recently, the scientific community has questioned the statistical reproducibility of many empirical results, especially in the field of machine learning. to solve this reproducibility crisis, we propose a theoretically sound methodology to compare the overall performance of multiple algorithms with stochastic returns. we exemplify our methodology in deep rl. indeed, the performance of one execution of a deep rl algorithm is random. therefore, several independent executions are needed to accurately evaluate the overall performance. when comparing several rl algorithms, a major question is how many executions must be made and how can we ensure that the results of such a comparison are theoretically sound. when comparing several algorithms at once, the error of each comparison may accumulate and must be taken into account with a multiple tests procedure to preserve low error guarantees. we introduce adastop, a new statistical test based on multiple group sequential tests. when comparing algorithms, adastop adapts the number of executions to stop as early as possible while ensuring that we have enough information to distinguish algorithms that perform better than the others in a statistical significant way. we prove theoretically and empirically that adastop has a low probability of making a (family-wise) error. finally, we illustrate the effectiveness of adastop in multiple deep rl use-cases, including toy examples and challenging mujoco environments. adastop is the first statistical test fitted to this sort of comparisons: adastop is both a significant contribution to statistics, and a major contribution to computational studies performed in reinforcement learning and in other domains. to summarize our contribution, we introduce adastop, a formally grounded statistical tool to let anyone answer the practical question: ``is my algorithm the new state-of-the-art?''.",,2023-06-19,2024-01-27,"['timothée mathieu', 'riccardo della vecchia', 'alena shilova', 'matheus medeiros centa', 'hector kohler', 'odalric-ambrym maillard', 'philippe preux']"
2306.10943,probabilistic matching of real and generated data statistics in   generative adversarial networks,stat.ml cs.lg,"generative adversarial networks constitute a powerful approach to generative modeling. while generated samples often are indistinguishable from real data, mode-collapse may occur and there is no guarantee that they will follow the true data distribution. for scientific applications in particular, it is essential that the true distribution is well captured by the generated distribution. in this work, we propose a method to ensure that the distributions of certain generated data statistics coincide with the respective distributions of the real data. in order to achieve this, we add a new loss term to the generator loss function, which quantifies the difference between these distributions via suitable f-divergences. kernel density estimation is employed to obtain representations of the true distributions, and to estimate the corresponding generated distributions from minibatch values at each iteration. when compared to other methods, our approach has the advantage that the complete shapes of the distributions are taken into account. we evaluate the method on a synthetic dataset and a real-world dataset and demonstrate improved performance of our approach.",,2023-06-19,2024-02-08,"['philipp pilar', 'niklas wahlström']"
2306.10947,pac-chernoff bounds: understanding generalization in the interpolation   regime,cs.lg math.st stat.ml stat.th,"in this paper, we present a distribution-dependent pac-chernoff bound that is perfectly tight for interpolators even under overparametrized model classes. this bound relies on basic principles of large deviation theory and naturally provides a characterization of the smoothness of a model described as a simple real-valued function. based on this distribution-dependent bound and the novel definition of smoothness, we propose an unifying theoretical explanation of why some interpolators generalize remarkably well while others not. and why a wide range of modern learning techniques (i.e., $\ell_2$-norm, distance-from-initialization, input-gradient and variance regularization together with data augmentation, invariant architectures, and overparameterization) are able to find them. the emergent conclusion is that all these methods provide complimentary procedures that bias the optimizer to smoother interpolators, which, according to this theoretical analysis, are the ones with better generalization error. one of the main insights of this study is that distribution-dependent bounds serve as a powerful tool better understand the complex dynamics behind the generalization capabilities of highly-overparameterized interpolators.",,2023-06-19,2024-02-07,"['andrés r. masegosa', 'luis a. ortega']"
2306.11157,human limits in machine learning: prediction of plant phenotypes using   soil microbiome data,stat.ml cs.lg stat.ap,"the preservation of soil health is a critical challenge in the 21st century due to its significant impact on agriculture, human health, and biodiversity. we provide the first deep investigation of the predictive potential of machine learning models to understand the connections between soil and biological phenotypes. we investigate an integrative framework performing accurate machine learning-based prediction of plant phenotypes from biological, chemical, and physical properties of the soil via two models: random forest and bayesian neural network. we show that prediction is improved when incorporating environmental features like soil physicochemical properties and microbial population density into the models, in addition to the microbiome information. exploring various data preprocessing strategies confirms the significant impact of human decisions on predictive performance. we show that the naive total sum scaling normalization that is commonly used in microbiome research is not the optimal strategy to maximize predictive power. also, we find that accurately defined labels are more important than normalization, taxonomic level or model characteristics. in cases where humans are unable to classify samples accurately, machine learning model performance is limited. lastly, we provide domain scientists via a full model selection decision tree to identify the human choices that optimize model prediction power. our work is accompanied by open source reproducible scripts (https://github.com/solislemuslab/soil-microbiome-nn) for maximum outreach among the microbiome research community.",,2023-06-19,2024-02-16,"['rosa aghdam', 'xudong tang', 'shan shan', 'richard lankau', 'claudia solís-lemus']"
2306.11313,deep graph kernel point processes,stat.ml cs.lg,"point process models are widely used for continuous asynchronous event data, where each data point includes time and additional information called ""marks"", which can be locations, nodes, or event types. this paper presents a novel point process model for discrete event data over graphs, where the event interaction occurs within a latent graph structure. our model builds upon hawkes's classic influence kernel-based formulation in the original self-exciting point processes work to capture the influence of historical events on future events' occurrence. the key idea is to represent the influence kernel by graph neural networks (gnn) to capture the underlying graph structure while harvesting the strong representation power of gnns. compared with prior works focusing on directly modeling the conditional intensity function using neural networks, our kernel presentation herds the repeated event influence patterns more effectively by combining statistical and deep models, achieving better model estimation/learning efficiency and superior predictive performance. our work significantly extends the existing deep spatio-temporal kernel for point process data, which is inapplicable to our setting due to the fundamental difference in the nature of the observation space being euclidean rather than a graph. we present comprehensive experiments on synthetic and real-world data to show the superior performance of the proposed approach against the state-of-the-art in predicting future events and uncovering the relational structure among data.",,2023-06-20,2024-02-02,"['zheng dong', 'matthew repasky', 'xiuyuan cheng', 'yao xie']"
2306.11380,a bayesian take on gaussian process networks,stat.ml cs.lg stat.me,"gaussian process networks (gpns) are a class of directed graphical models which employ gaussian processes as priors for the conditional expectation of each variable given its parents in the network. the model allows the description of continuous joint distributions in a compact but flexible manner with minimal parametric assumptions on the dependencies between variables. bayesian structure learning of gpns requires computing the posterior over graphs of the network and is computationally infeasible even in low dimensions. this work implements monte carlo and markov chain monte carlo methods to sample from the posterior distribution of network structures. as such, the approach follows the bayesian paradigm, comparing models via their marginal likelihood and computing the posterior probability of the gpn features. simulation studies show that our method outperforms state-of-the-art algorithms in recovering the graphical structure of the network and provides an accurate approximation of its posterior distribution.",,2023-06-20,2023-11-24,"['enrico giudice', 'jack kuipers', 'giusi moffa']"
2306.11589,sampling from gaussian process posteriors using stochastic gradient   descent,cs.lg stat.ml,"gaussian processes are a powerful framework for quantifying uncertainty and for sequential decision-making but are limited by the requirement of solving linear systems. in general, this has a cubic cost in dataset size and is sensitive to conditioning. we explore stochastic gradient algorithms as a computationally efficient method of approximately solving these linear systems: we develop low-variance optimization objectives for sampling from the posterior and extend these to inducing points. counterintuitively, stochastic gradient descent often produces accurate predictions, even in cases where it does not converge quickly to the optimum. we explain this through a spectral characterization of the implicit bias from non-convergence. we show that stochastic gradient descent produces predictive distributions close to the true posterior both in regions with sufficient data coverage, and in regions sufficiently far away from the data. experimentally, stochastic gradient descent achieves state-of-the-art performance on sufficiently large-scale or ill-conditioned regression tasks. its uncertainty estimates match the performance of significantly more expensive baselines on a large-scale bayesian optimization task.",,2023-06-20,2024-01-15,"['jihao andreas lin', 'javier antorán', 'shreyas padhy', 'david janz', 'josé miguel hernández-lobato', 'alexander terenin']"
2306.12214,"more pac-bayes bounds: from bounded losses, to losses with general tail   behaviors, to anytime-validity",stat.ml cs.lg,"in this paper, we present new high-probability pac-bayes bounds for different types of losses. firstly, for losses with a bounded range, we recover a strengthened version of catoni's bound that holds uniformly for all parameter values. this leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. in particular, the fast rate bound is equivalent to the seeger--langford bound. secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a pac-bayes chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. these two bounds are obtained using a new technique based on a discretization of the space of possible events for the ""in probability"" parameter optimization problem. this technique is both simpler and more general than previous approaches optimizing over a grid on the parameters' space. finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.",,2023-06-21,2024-02-14,"['borja rodríguez-gálvez', 'ragnar thobaben', 'mikael skoglund']"
2306.12528,structured learning in time-dependent cox models,stat.me stat.ap stat.co stat.ml,"cox models with time-dependent coefficients and covariates are widely used in survival analysis. in high-dimensional settings, sparse regularization techniques are employed for variable selection, but existing methods for time-dependent cox models lack flexibility in enforcing specific sparsity patterns (i.e., covariate structures). we propose a flexible framework for variable selection in time-dependent cox models, accommodating complex selection rules. our method can adapt to arbitrary grouping structures, including interaction selection, temporal, spatial, tree, and directed acyclic graph structures. it achieves accurate estimation with low false alarm rates. we develop the sox package, implementing a network flow algorithm for efficiently solving models with complex covariate structures. sox offers a user-friendly interface for specifying grouping structures and delivers fast computation. through examples, including a case study on identifying predictors of time to all-cause death in atrial fibrillation patients, we demonstrate the practical application of our method with specific selection rules.",,2023-06-21,2024-01-06,"['guanbo wang', 'yi lian', 'archer y. yang', 'robert w. platt', 'rui wang', 'sylvie perreault', 'marc dorais', 'mireille e. schnitzer']"
2306.12584,hierarchical neural simulation-based inference over event ensembles,stat.ml astro-ph.im cs.lg hep-ex,"when analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. such models often have a hierarchical structure, where ""local"" parameters impact individual events and ""global"" parameters influence the entire dataset. we introduce practical approaches for frequentist and bayesian dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via a hierarchical forward model. we construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to significantly tighter parameter constraints. we ground our discussion using case studies from the physical sciences, focusing on examples from particle physics and cosmology.",,2023-06-21,2024-02-21,"['lukas heinrich', 'siddharth mishra-sharma', 'chris pollard', 'philipp windischhofer']"
2306.13119,adversarial resilience in sequential prediction via abstention,cs.lg cs.ds stat.ml,"we study the problem of sequential prediction in the stochastic setting with an adversary that is allowed to inject clean-label adversarial (or out-of-distribution) examples. algorithms designed to handle purely stochastic data tend to fail in the presence of such adversarial examples, often leading to erroneous predictions. this is undesirable in many high-stakes applications such as medical recommendations, where abstaining from predictions on adversarial examples is preferable to misclassification. on the other hand, assuming fully adversarial data leads to very pessimistic bounds that are often vacuous in practice.   to capture this motivation, we propose a new model of sequential prediction that sits between the purely stochastic and fully adversarial settings by allowing the learner to abstain from making a prediction at no cost on adversarial examples. assuming access to the marginal distribution on the non-adversarial examples, we design a learner whose error scales with the vc dimension (mirroring the stochastic setting) of the hypothesis class, as opposed to the littlestone dimension which characterizes the fully adversarial setting. furthermore, we design a learner for vc dimension~1 classes, which works even in the absence of access to the marginal distribution. our key technical contribution is a novel measure for quantifying uncertainty for learning vc classes, which may be of independent interest.",,2023-06-22,2024-01-24,"['surbhi goel', 'steve hanneke', 'shay moran', 'abhishek shetty']"
2306.13746,revisiting inference after prediction,stat.ml cs.lg,"recent work has focused on the very common practice of prediction-based inference: that is, (i) using a pre-trained machine learning model to predict an unobserved response variable, and then (ii) conducting inference on the association between that predicted response and some covariates. as pointed out by wang et al. (2020), applying a standard inferential approach in (ii) does not accurately quantify the association between the unobserved (as opposed to the predicted) response and the covariates. in recent work, wang et al. (2020) and angelopoulos et al. (2023) propose corrections to step (ii) in order to enable valid inference on the association between the unobserved response and the covariates. here, we show that the method proposed by angelopoulos et al. (2023) successfully controls the type 1 error rate and provides confidence intervals with correct nominal coverage, regardless of the quality of the pre-trained machine learning model used to predict the unobserved response. however, the method proposed by wang et al. (2020) provides valid inference only under very strong conditions that rarely hold in practice: for instance, if the machine learning model perfectly estimates the true regression function in the study population of interest.",,2023-06-23,2024-01-01,"['keshav motwani', 'daniela witten']"
2306.14010,learned mappings for targeted free energy perturbation between peptide   conformations,cond-mat.stat-mech stat.ml,"targeted free energy perturbation uses an invertible mapping to promote configuration space overlap and the convergence of free energy estimates. however, developing suitable mappings can be challenging. wirnsberger et al. (2020) demonstrated the use of machine learning to train deep neural networks that map between boltzmann distributions for different thermodynamic states. here, we adapt their approach to free energy differences of a flexible bonded molecule, deca-alanine, with harmonic biases with different spring centers. when the neural network is trained until ``early stopping'' - when the loss value of the test set increases - we calculate accurate free energy differences between thermodynamic states with spring centers separated by 1 \r{a} and sometimes 2 \r{a}. for more distant thermodynamic states, the mapping does not produce structures representative of the target state and the method does not reproduce reference calculations.",10.1063/5.0164662,2023-06-24,,"['soohaeng yoo willow', 'lulu kang', 'david d. l. minh']"
2306.14012,private networked federated learning for nonsmooth objectives,math.oc stat.ml,"this paper develops a networked federated learning algorithm to solve nonsmooth objective functions. to guarantee the confidentiality of the participants with respect to each other and potential eavesdroppers, we use the zero-concentrated differential privacy notion (zcdp). privacy is achieved by perturbing the outcome of the computation at each client with a variance-decreasing gaussian noise. zcdp allows for better accuracy than the conventional $(\epsilon, \delta)$-dp and stronger guarantees than the more recent r\'enyi-dp by assuming adversaries aggregate all the exchanged messages. the proposed algorithm relies on the distributed alternating direction method of multipliers (admm) and uses the approximation of the augmented lagrangian to handle nonsmooth objective functions. the developed private networked federated learning algorithm has a competitive privacy accuracy trade-off and handles nonsmooth and non-strongly convex problems. we provide complete theoretical proof for the privacy guarantees and the algorithm's convergence to the exact solution. we also prove under additional assumptions that the algorithm converges in $o(1/n)$ admm iterations. finally, we observe the performance of the algorithm in a series of numerical simulations.",,2023-06-24,2024-02-21,"['françois gauthier', 'cristiano gratton', 'naveen k. d. venkategowda', 'stefan werner']"
2306.14670,improved bayes risk can yield reduced social welfare under competition,cs.gt cs.cy cs.lg stat.ml,"as the scale of machine learning models increases, trends such as scaling laws anticipate consistent downstream improvements in predictive accuracy. however, these trends take the perspective of a single model-provider in isolation, while in reality providers often compete with each other for users. in this work, we demonstrate that competition can fundamentally alter the behavior of these scaling trends, even causing overall predictive accuracy across users to be non-monotonic or decreasing with scale. we define a model of competition for classification tasks, and use data representations as a lens for studying the impact of increases in scale. we find many settings where improving data representation quality (as measured by bayes risk) decreases the overall predictive accuracy across users (i.e., social welfare) for a marketplace of competing model-providers. our examples range from closed-form formulas in simple settings to simulations with pretrained representations on cifar-10. at a conceptual level, our work suggests that favorable scaling trends for individual model-providers need not translate to downstream improvements in social welfare in marketplaces with multiple model providers.",,2023-06-26,2024-02-06,"['meena jagadeesan', 'michael i. jordan', 'jacob steinhardt', 'nika haghtalab']"
2306.14753,the deep arbitrary polynomial chaos neural network or how deep   artificial neural networks could benefit from data-driven homogeneous chaos   theory,cs.ne stat.ml,"artificial intelligence and machine learning have been widely used in various fields of mathematical computing, physical modeling, computational science, communication science, and stochastic analysis. approaches based on deep artificial neural networks (dann) are very popular in our days. depending on the learning task, the exact form of danns is determined via their multi-layer architecture, activation functions and the so-called loss function. however, for a majority of deep learning approaches based on danns, the kernel structure of neural signal processing remains the same, where the node response is encoded as a linear superposition of neural activity, while the non-linearity is triggered by the activation functions. in the current paper, we suggest to analyze the neural signal processing in danns from the point of view of homogeneous chaos theory as known from polynomial chaos expansion (pce). from the pce perspective, the (linear) response on each node of a dann could be seen as a $1^{st}$ degree multi-variate polynomial of single neurons from the previous layer, i.e. linear weighted sum of monomials. from this point of view, the conventional dann structure relies implicitly (but erroneously) on a gaussian distribution of neural signals. additionally, this view revels that by design danns do not necessarily fulfill any orthogonality or orthonormality condition for a majority of data-driven applications. therefore, the prevailing handling of neural signals in danns could lead to redundant representation as any neural signal could contain some partial information from other neural signals. to tackle that challenge, we suggest to employ the data-driven generalization of pce theory known as arbitrary polynomial chaos (apc) to construct a corresponding multi-variate orthonormal representations on each node of a dann to obtain deep arbitrary polynomial chaos neural networks.",10.1016/j.neunet.2023.06.036,2023-06-26,,"['sergey oladyshkin', 'timothy praditia', 'ilja kröker', 'farid mohammadi', 'wolfgang nowak', 'sebastian otte']"
2306.14872,geometry-aware approaches for balancing performance and theoretical   guarantees in linear bandits,cs.lg stat.ml,"this paper is motivated by recent research in the $d$-dimensional stochastic linear bandit literature, which has revealed an unsettling discrepancy: algorithms like thompson sampling and greedy demonstrate promising empirical performance, yet this contrasts with their pessimistic theoretical regret bounds. the challenge arises from the fact that while these algorithms may perform poorly in certain problem instances, they generally excel in typical instances. to address this, we propose a new data-driven technique that tracks the geometric properties of the uncertainty ellipsoid around the main problem parameter. this methodology enables us to formulate an instance-dependent frequentist regret bound, which incorporates the geometric information, for a broad class of base algorithms, including greedy, oful, and thompson sampling. this result allows us to identify and ``course-correct"" problem instances in which the base algorithms perform poorly. the course-corrected algorithms achieve the minimax optimal regret of order $\tilde{\mathcal{o}}(d\sqrt{t})$ for a $t$-period decision-making scenario, effectively maintaining the desirable attributes of the base algorithms, including their empirical efficacy. we present simulation results to validate our findings using synthetic and real data.",,2023-06-26,2023-12-30,"['yuwei luo', 'mohsen bayati']"
2306.15012,statistical component separation for targeted signal recovery in noisy   mixtures,stat.ml astro-ph.im cs.lg eess.sp,"separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. in this work, we tackle simpler ""statistical component separation"" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. we first analyze the behavior of this method using simple examples with analytically tractable calculations. then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) convnet-based descriptors on astrophysics and imagenet data. in the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. additionally, despite not constructed for this purpose, it performs surprisingly well in terms of peak signal-to-noise ratio on full signal reconstruction. in comparison, representation 2) appears less suitable for image denoising. finally, we extend this method by introducing a diffusive stepwise algorithm which gives a new perspective to the initial method and leads to promising results for image denoising under specific circumstances.",,2023-06-26,2024-02-28,"['bruno régaldo-saint blancard', 'michael eickenberg']"
2306.15056,optimal differentially private model training with public data,cs.lg cs.cr math.oc stat.ml,"differential privacy (dp) ensures that training a machine learning model does not leak private data. in practice, we may have access to auxiliary public data that is free of privacy concerns. in this work, we assume access to a given amount of public data and settle the following fundamental open questions: 1. what is the optimal (worst-case) error of a dp model trained over a private data set while having access to side public data? 2. how can we harness public data to improve dp model training in practice? we consider these questions in both the local and central models of pure and approximate dp. to answer the first question, we prove tight (up to log factors) lower and upper bounds that characterize the optimal error rates of three fundamental problems: mean estimation, empirical risk minimization, and stochastic convex optimization. we show that the optimal error rates can be attained (up to log factors) by either discarding private data and training a public model, or treating public data like it is private and using an optimal dp algorithm. to address the second question, we develop novel algorithms that are ""even more optimal"" (i.e. better constants) than the asymptotically optimal approaches described above. for local dp mean estimation, our algorithm is \ul{optimal including constants}. empirically, our algorithms show benefits over the state-of-the-art.",,2023-06-26,2024-02-13,"['andrew lowy', 'zeman li', 'tianjian huang', 'meisam razaviyayn']"
2306.16593,autoregressive with slack time series model for forecasting a   partially-observed dynamical time series,stat.me cs.lg stat.ml,"this study delves into the domain of dynamical systems, specifically the forecasting of dynamical time series defined through an evolution function. traditional approaches in this area predict the future behavior of dynamical systems by inferring the evolution function. however, these methods may confront obstacles due to the presence of missing variables, which are usually attributed to challenges in measurement and a partial understanding of the system of interest. to overcome this obstacle, we introduce the autoregressive with slack time series (ars) model, that simultaneously estimates the evolution function and imputes missing variables as a slack time series. assuming time-invariance and linearity in the (underlying) entire dynamical time series, our experiments demonstrate the ars model's capability to forecast future time series. from a theoretical perspective, we prove that a 2-dimensional time-invariant and linear system can be reconstructed by utilizing observations from a single, partially observed dimension of the system.",,2023-06-28,2024-02-09,"['akifumi okuno', 'yuya morishita', 'yoh-ichi mototake']"
2306.16717,understanding pathologies of deep heteroskedastic regression,stat.ml cs.lg,"deep, overparameterized regression models are notorious for their tendency to overfit. this problem is exacerbated in heteroskedastic models, which predict both mean and residual noise for each data point. at one extreme, these models fit all training data perfectly, eliminating residual noise entirely; at the other, they overfit the residual noise while predicting a constant, uninformative mean. we observe a lack of middle ground, suggesting a phase transition dependent on model regularization strength. empirical verification supports this conjecture by fitting numerous models with varying mean and variance regularization. to explain the transition, we develop a theoretical framework based on a statistical field theory, yielding qualitative agreement with experiments. as a practical consequence, our analysis simplifies hyperparameter tuning from a two-dimensional to a one-dimensional search, substantially reducing the computational burden. experiments on diverse datasets, including uci datasets and the large-scale climsim climate dataset, demonstrate significantly improved performance in various calibration tasks.",,2023-06-29,2024-02-13,"['eliot wong-toi', 'alex boyd', 'vincent fortuin', 'stephan mandt']"
2306.16838,solving kernel ridge regression with gradient-based optimization methods,stat.ml cs.lg math.oc stat.me,"kernel ridge regression, krr, is a generalization of linear ridge regression that is non-linear in the data, but linear in the parameters. here, we introduce an equivalent formulation of the objective function of krr, opening up both for using penalties other than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. using a continuous-time perspective, we derive a closed-form solution for solving kernel regression with gradient descent, something we refer to as kernel gradient flow, kgf, and theoretically bound the differences between krr and kgf, where, for the latter, regularization is obtained through early stopping. we also generalize krr by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties, respectively, and use the fact that analogous to the similarities between kgf and krr, $\ell_1$ regularization and forward stagewise regression (also known as coordinate descent), and $\ell_\infty$ regularization and sign gradient descent, follow similar solution paths. we can thus alleviate the need for computationally heavy algorithms based on proximal gradient descent. we show theoretically and empirically how the $\ell_1$ and $\ell_\infty$ penalties, and the corresponding gradient-based optimization algorithms, produce sparse and robust kernel regression solutions, respectively.",,2023-06-29,2024-02-26,['oskar allerbo']
2306.17004,learning thermodynamically constrained equations of state with   uncertainty,physics.data-an stat.ml,"numerical simulations of high energy-density experiments require equation of state (eos) models that relate a material's thermodynamic state variables -- specifically pressure, volume/density, energy, and temperature. eos models are typically constructed using a semi-empirical parametric methodology, which assumes a physics-informed functional form with many tunable parameters calibrated using experimental/simulation data. since there are inherent uncertainties in the calibration data (parametric uncertainty) and the assumed functional eos form (model uncertainty), it is essential to perform uncertainty quantification (uq) to improve confidence in the eos predictions. model uncertainty is challenging for uq studies since it requires exploring the space of all possible physically consistent functional forms. thus, it is often neglected in favor of parametric uncertainty, which is easier to quantify without violating thermodynamic laws. this work presents a data-driven machine learning approach to constructing eos models that naturally captures model uncertainty while satisfying the necessary thermodynamic consistency and stability constraints. we propose a novel framework based on physics-informed gaussian process regression (gpr) that automatically captures total uncertainty in the eos and can be jointly trained on both simulation and experimental data sources. a gpr model for the shock hugoniot is derived and its uncertainties are quantified using the proposed framework. we apply the proposed model to learn the eos for the diamond solid state of carbon, using both density functional theory data and experimental shock hugoniot data to train the model and show that the prediction uncertainty reduces by considering the thermodynamic constraints.",10.1063/5.0165298,2023-06-29,2024-02-23,"['himanshu sharma', 'jim a. gaffney', 'dimitrios tsapetis', 'michael d. shields']"
2306.17248,temperaturegan: generative modeling of regional atmospheric temperatures,cs.lg physics.ao-ph stat.ml,"stochastic generators are useful for estimating climate impacts on various sectors. projecting climate risk in various sectors, e.g. energy systems, requires generators that are accurate (statistical resemblance to ground-truth), reliable (do not produce erroneous examples), and efficient. leveraging data from the north american land data assimilation system, we introduce temperaturegan, a generative adversarial network conditioned on months, locations, and time periods, to generate 2m above ground atmospheric temperatures at an hourly resolution. we propose evaluation methods and metrics to measure the quality of generated samples. we show that temperaturegan produces high-fidelity examples with good spatial representation and temporal dynamics consistent with known diurnal cycles.",,2023-06-29,2024-01-19,"['emmanuel balogun', 'ram rajagopal', 'arun majumdar']"
2306.17361,iscan: identifying causal mechanism shifts among nonlinear additive   noise models,cs.lg cs.ai stat.ap stat.me stat.ml,"structural causal models (scms) are widely used in various disciplines to represent causal relationships among variables in complex systems. unfortunately, the underlying causal structure is often unknown, and estimating it from data remains a challenging task. in many situations, however, the end goal is to localize the changes (shifts) in the causal mechanisms between related datasets instead of learning the full causal structure of the individual datasets. some applications include root cause analysis, analyzing gene regulatory network structure changes between healthy and cancerous individuals, or explaining distribution shifts. this paper focuses on identifying the causal mechanism shifts in two or more related datasets over the same set of variables -- without estimating the entire dag structure of each scm. prior work under this setting assumed linear models with gaussian noises; instead, in this work we assume that each scm belongs to the more general class of nonlinear additive noise models (anms). a key technical contribution of this work is to show that the jacobian of the score function for the mixture distribution allows for the identification of shifts under general non-parametric functional mechanisms. once the shifted variables are identified, we leverage recent work to estimate the structural differences, if any, for the shifted variables. experiments on synthetic and real-world data are provided to showcase the applicability of this approach. code implementing the proposed method is open-source and publicly available at https://github.com/kevinsbello/iscan.",,2023-06-29,2024-01-12,"['tianyu chen', 'kevin bello', 'bryon aragam', 'pradeep ravikumar']"
2307.00048,learned harmonic mean estimation of the marginal likelihood with   normalizing flows,stat.me astro-ph.im stat.ml,"computing the marginal likelihood (also called the bayesian model evidence) is an important task in bayesian model selection, providing a principled quantitative way to compare models. the learned harmonic mean estimator solves the exploding variance problem of the original harmonic mean estimation of the marginal likelihood. the learned harmonic mean estimator learns an importance sampling target distribution that approximates the optimal distribution. while the approximation need not be highly accurate, it is critical that the probability mass of the learned distribution is contained within the posterior in order to avoid the exploding variance problem. in previous work a bespoke optimization problem is introduced when training models in order to ensure this property is satisfied. in the current article we introduce the use of normalizing flows to represent the importance sampling target distribution. a flow-based model is trained on samples from the posterior by maximum likelihood estimation. then, the probability density of the flow is concentrated by lowering the variance of the base distribution, i.e. by lowering its ""temperature"", ensuring its probability mass is contained within the posterior. this approach avoids the need for a bespoke optimisation problem and careful fine tuning of parameters, resulting in a more robust method. moreover, the use of normalizing flows has the potential to scale to high dimensional settings. we present preliminary experiments demonstrating the effectiveness of the use of flows for the learned harmonic mean estimator. the harmonic code implementing the learned harmonic mean, which is publicly available, has been updated to now support normalizing flows.",,2023-06-30,2024-01-19,"['alicja polanska', 'matthew a. price', 'alessio spurio mancini', 'jason d. mcewen']"
2307.00238,unified transfer learning models in high-dimensional linear regression,stat.ml cs.lg,"transfer learning plays a key role in modern data analysis when: (1) the target data are scarce but the source data are sufficient; (2) the distributions of the source and target data are heterogeneous. this paper develops an interpretable unified transfer learning model, termed as utrans, which can detect both transferable variables and source data. more specifically, we establish the estimation error bounds and prove that our bounds are lower than those with target data only. besides, we propose a source detection algorithm based on hypothesis testing to exclude the nontransferable data. we evaluate and compare utrans to the existing algorithms in multiple experiments. it is shown that utrans attains much lower estimation and prediction errors than the existing methods, while preserving interpretability. we finally apply it to the us intergenerational mobility data and compare our proposed algorithms to the classical machine learning algorithms.",,2023-07-01,2024-01-29,['shuo shuo liu']
2307.00405,provably efficient ucb-type algorithms for learning predictive state   representations,cs.lg stat.ml,"the general sequential decision-making problem, which includes markov decision processes (mdps) and partially observable mdps (pomdps) as special cases, aims at maximizing a cumulative reward by making a sequence of decisions based on a history of observations and actions over time. recent studies have shown that the sequential decision-making problem is statistically learnable if it admits a low-rank structure modeled by predictive state representations (psrs). despite these advancements, existing approaches typically involve oracles or steps that are computationally intractable. on the other hand, the upper confidence bound (ucb) based approaches, which have served successfully as computationally efficient methods in bandits and mdps, have not been investigated for more general psrs, due to the difficulty of optimistic bonus design in these more challenging settings. this paper proposes the first known ucb-type approach for psrs, featuring a novel bonus term that upper bounds the total variation distance between the estimated and true models. we further characterize the sample complexity bounds for our designed ucb-type algorithms for both online and offline psrs. in contrast to existing approaches for psrs, our ucb-type algorithms enjoy computational tractability, last-iterate guaranteed near-optimal policy, and guaranteed model accuracy.",,2023-07-01,2024-02-06,"['ruiquan huang', 'yingbin liang', 'jing yang']"
2307.00859,cardigraphormer: unveiling the power of self-supervised learning in   revolutionizing drug discovery,cs.lg q-bio.qm stat.ap stat.ml,"in the expansive realm of drug discovery, with approximately 15,000 known drugs and only around 4,200 approved, the combinatorial nature of the chemical space presents a formidable challenge. while artificial intelligence (ai) has emerged as a powerful ally, traditional ai frameworks face significant hurdles. this manuscript introduces cardigraphormer, a groundbreaking approach that synergizes self-supervised learning (ssl), graph neural networks (gnns), and cardinality preserving attention to revolutionize drug discovery. cardigraphormer, a novel combination of graphormer and cardinality preserving attention, leverages ssl to learn potent molecular representations and employs gnns to extract molecular fingerprints, enhancing predictive performance and interpretability while reducing computation time. it excels in handling complex data like molecular structures and performs tasks associated with nodes, pairs of nodes, subgraphs, or entire graph structures. cardigraphormer's potential applications in drug discovery and drug interactions are vast, from identifying new drug targets to predicting drug-to-drug interactions and enabling novel drug discovery. this innovative approach provides an ai-enhanced methodology in drug development, utilizing ssl combined with gnns to overcome existing limitations and pave the way for a richer exploration of the vast combinatorial chemical space in drug discovery.",,2023-07-03,2024-01-13,['abhijit gupta']
2307.02129,how deep neural networks learn compositional data: the random hierarchy   model,cs.lg cs.cv stat.ml,"deep learning algorithms demonstrate a surprising ability to learn high-dimensional tasks from limited examples. this is commonly attributed to the depth of neural networks, enabling them to build a hierarchy of abstract, low-dimensional data representations. however, how many training examples are required to learn such representations remains unknown. to quantitatively study this question, we introduce the random hierarchy model: a family of synthetic tasks inspired by the hierarchical structure of language and images. the model is a classification task where each class corresponds to a group of high-level features, chosen among several equivalent groups associated with the same class. in turn, each feature corresponds to a group of sub-features chosen among several equivalent ones and so on, following a hierarchy of composition rules. we find that deep networks learn the task by developing internal representations invariant to exchanging equivalent groups. moreover, the number of data required corresponds to the point where correlations between low-level features and classes become detectable. overall, our results indicate how deep networks overcome the curse of dimensionality by building invariant representations, and provide an estimate of the number of data required to learn a hierarchical task.",,2023-07-05,2024-01-09,"['francesco cagnetta', 'leonardo petrini', 'umberto m. tomasini', 'alessandro favero', 'matthieu wyart']"
2307.02388,multi-task learning with summary statistics,stat.me stat.ml,"multi-task learning has emerged as a powerful machine learning paradigm for integrating data from multiple sources, leveraging similarities between tasks to improve overall model performance. however, the application of multi-task learning to real-world settings is hindered by data-sharing constraints, especially in healthcare settings. to address this challenge, we propose a flexible multi-task learning framework utilizing summary statistics from various sources. additionally, we present an adaptive parameter selection approach based on a variant of lepski's method, allowing for data-driven tuning parameter selection when only summary statistics are available. our systematic non-asymptotic analysis characterizes the performance of the proposed methods under various regimes of the sample complexity and overlap. we demonstrate our theoretical findings and the performance of the method through extensive simulations. this work offers a more flexible tool for training related models across various domains, with practical implications in genetic risk prediction and many other fields.",,2023-07-05,2024-02-08,"['parker knight', 'rui duan']"
2307.02764,when does confidence-based cascade deferral suffice?,cs.lg stat.ml,"cascades are a classical strategy to enable inference cost to vary adaptively across samples, wherein a sequence of classifiers are invoked in turn. a deferral rule determines whether to invoke the next classifier in the sequence, or to terminate prediction. one simple deferral rule employs the confidence of the current classifier, e.g., based on the maximum predicted softmax probability. despite being oblivious to the structure of the cascade -- e.g., not modelling the errors of downstream models -- such confidence-based deferral often works remarkably well in practice. in this paper, we seek to better understand the conditions under which confidence-based deferral may fail, and when alternate deferral strategies can perform better. we first present a theoretical characterisation of the optimal deferral rule, which precisely characterises settings under which confidence-based deferral may suffer. we then study post-hoc deferral mechanisms, and demonstrate they can significantly improve upon confidence-based deferral in settings where (i) downstream models are specialists that only work well on a subset of inputs, (ii) samples are subject to label noise, and (iii) there is distribution shift between the train and test set.",,2023-07-06,2024-01-23,"['wittawat jitkrittum', 'neha gupta', 'aditya krishna menon', 'harikrishna narasimhan', 'ankit singh rawat', 'sanjiv kumar']"
2307.03176,learning curves for noisy heterogeneous feature-subsampled ridge   ensembles,stat.ml cond-mat.dis-nn cs.lg q-bio.nc,"feature bagging is a well-established ensembling method which aims to reduce prediction variance by combining predictions of many estimators trained on subsets or projections of features. here, we develop a theory of feature-bagging in noisy least-squares ridge ensembles and simplify the resulting learning curves in the special case of equicorrelated data. using analytical learning curves, we demonstrate that subsampling shifts the double-descent peak of a linear predictor. this leads us to introduce heterogeneous feature ensembling, with estimators built on varying numbers of feature dimensions, as a computationally efficient method to mitigate double-descent. then, we compare the performance of a feature-subsampling ensemble to a single linear predictor, describing a trade-off between noise amplification due to subsampling and noise reduction due to ensembling. our qualitative insights carry over to linear classifiers applied to image classification tasks with realistic datasets constructed using a state-of-the-art deep learning feature map.",,2023-07-06,2024-01-09,"['benjamin s. ruben', 'cengiz pehlevan']"
2307.03748,incentive-theoretic bayesian inference for collaborative science,stat.me cs.gt cs.lg stat.ml,"contemporary scientific research is a distributed, collaborative endeavor, carried out by teams of researchers, regulatory institutions, funding agencies, commercial partners, and scientific bodies, all interacting with each other and facing different incentives. to maintain scientific rigor, statistical methods should acknowledge this state of affairs. to this end, we study hypothesis testing when there is an agent (e.g., a researcher or a pharmaceutical company) with a private prior about an unknown parameter and a principal (e.g., a policymaker or regulator) who wishes to make decisions based on the parameter value. the agent chooses whether to run a statistical trial based on their private prior and then the result of the trial is used by the principal to reach a decision. we show how the principal can conduct statistical inference that leverages the information that is revealed by an agent's strategic behavior -- their choice to run a trial or not. in particular, we show how the principal can design a policy to elucidate partial information about the agent's private prior beliefs and use this to control the posterior probability of the null. one implication is a simple guideline for the choice of significance threshold in clinical trials: the type-i error level should be set to be strictly less than the cost of the trial divided by the firm's profit if the trial is successful.",,2023-07-07,2024-02-08,"['stephen bates', 'michael i. jordan', 'michael sklar', 'jake a. soloff']"
2307.03927,fast empirical scenarios,stat.ml cs.lg cs.na math.na q-fin.rm,"we seek to extract a small number of representative scenarios from large and high-dimensional panel data that are consistent with sample moments. among two novel algorithms, the first identifies scenarios that have not been observed before, and comes with a scenario-based representation of covariance matrices. the second proposal picks important data points from states of the world that have already realized, and are consistent with higher-order sample moment information. both algorithms are efficient to compute, and lend themselves to consistent scenario-based modeling and high-dimensional numerical integration. extensive numerical benchmarking studies and an application in portfolio optimization favor the proposed algorithms.",,2023-07-08,2024-02-05,"['michael multerer', 'paul schneider', 'rohan sen']"
2307.04191,on the sample complexity of parameter estimation in logistic regression   with normal design,math.st cs.it cs.lg math.it stat.ml stat.th,"the logistic regression model is one of the most popular data generation model in noisy binary classification problems. in this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. the inverse temperature controls the signal-to-noise ratio of the data generation process. while both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. we show that the sample complexity curve has two change-points in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.",,2023-07-09,2024-02-08,"['daniel hsu', 'arya mazumdar']"
2307.05251,minimizing robust density power-based divergences for general parametric   density models,stat.me stat.ml,"density power divergence (dpd) is designed to robustly estimate the underlying distribution of observations, in the presence of outliers. however, dpd involves an integral of the power of the parametric density models to be estimated; the explicit form of the integral term can be derived only for specific densities, such as normal and exponential densities. while we may perform a numerical integration for each iteration of the optimization algorithms, the computational complexity has hindered the practical application of dpd-based estimation to more general parametric densities. to address the issue, this study introduces a stochastic approach to minimize dpd for general parametric density models. the proposed approach can also be employed to minimize other density power-based $\gamma$-divergences, by leveraging unnormalized models. we provide \verb|r| package for implementation of the proposed approach in \url{https://github.com/oknakfm/sgdpd}.",,2023-07-11,2024-02-08,['akifumi okuno']
2307.05304,evidence of social learning across symbolic cultural barriers in sperm   whales,cs.si stat.ap,"we provide quantitative evidence suggesting social learning in sperm whales across socio-cultural boundaries, using acoustic data from the pacific and atlantic oceans. traditionally, sperm whale populations are categorized into clans based on their vocal repertoire: the rhythmically patterned click sequences (codas) that they use. among these codas, identity codas function as symbolic markers for each clan, accounting for 35-60% of codas they produce. we introduce a computational method to model whale speech, which encodes rhythmic micro-variations within codas, capturing their vocal style. we find that vocal style-clans closely align with repertoire-clans. however, contrary to vocal repertoire, we show that sympatry increases vocal style similarity between clans for non-identity codas, i.e. most codas, suggesting social learning across cultural boundaries. more broadly, this subcoda structure model offers a framework for comparing communication systems in other species, with potential implications for deeper understanding of vocal and cultural transmission within animal societies.",,2023-07-07,2024-01-18,"['antónio leitão', 'maxime lucas', 'simone poetto', 'taylor a. hersh', 'shane gero', 'david gruber', 'michael bronstein', 'giovanni petri']"
2307.05592,functional pca and deep neural networks-based bayesian inverse   uncertainty quantification with transient experimental data,stat.ml cs.lg,"inverse uq is the process to inversely quantify the model input uncertainties based on experimental data. this work focuses on developing an inverse uq process for time-dependent responses, using dimensionality reduction by functional principal component analysis (pca) and deep neural network (dnn)-based surrogate models. the demonstration is based on the inverse uq of trace physical model parameters using the feba transient experimental data. the measurement data is time-dependent peak cladding temperature (pct). since the quantity-of-interest (qoi) is time-dependent that corresponds to infinite-dimensional responses, pca is used to reduce the qoi dimension while preserving the transient profile of the pct, in order to make the inverse uq process more efficient. however, conventional pca applied directly to the pct time series profiles can hardly represent the data precisely due to the sudden temperature drop at the time of quenching. as a result, a functional alignment method is used to separate the phase and amplitude information of the transient pct profiles before dimensionality reduction. dnns are then trained using pc scores from functional pca to build surrogate models of trace in order to reduce the computational cost in markov chain monte carlo sampling. bayesian neural networks are used to estimate the uncertainties of dnn surrogate model predictions. in this study, we compared four different inverse uq processes with different dimensionality reduction methods and surrogate models. the proposed approach shows an improvement in reducing the dimension of the trace transient simulations, and the forward propagation of inverse uq results has a better agreement with the experimental data.",10.1016/j.cma.2023.116721,2023-07-10,,"['ziyu xie', 'mahmoud yaseen', 'xu wu']"
2307.06093,online laplace model selection revisited,cs.lg stat.ml,"the laplace approximation provides a closed-form model selection objective for neural networks (nn). online variants, which optimise nn parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the bayesian deep learning community. however, these methods violate laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. this work re-derives online laplace methods, showing them to target a variational bound on a mode-corrected variant of the laplace evidence which does not make stationarity assumptions. online laplace and its mode-corrected counterpart share stationary points where 1. the nn parameters are a maximum a posteriori, satisfying the laplace method's assumption, and 2. the hyperparameters maximise the laplace evidence, motivating online methods. we demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradient descent on uci regression datasets. the optimised hyperparameters prevent overfitting and outperform validation-based early stopping.",,2023-07-12,2024-01-09,"['jihao andreas lin', 'javier antorán', 'josé miguel hernández-lobato']"
2307.06555,deep network approximation: beyond relu to diverse activation functions,cs.lg stat.ml,"this paper explores the expressive power of deep neural networks for a diverse range of activation functions. an activation function set $\mathscr{a}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{relu}$, $\mathtt{leakyrelu}$, $\mathtt{relu}^2$, $\mathtt{elu}$, $\mathtt{celu}$, $\mathtt{selu}$, $\mathtt{softplus}$, $\mathtt{gelu}$, $\mathtt{silu}$, $\mathtt{swish}$, $\mathtt{mish}$, $\mathtt{sigmoid}$, $\mathtt{tanh}$, $\mathtt{arctan}$, $\mathtt{softsign}$, $\mathtt{dsilu}$, and $\mathtt{srs}$. we demonstrate that for any activation function $\varrho\in \mathscr{a}$, a $\mathtt{relu}$ network of width $n$ and depth $l$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $3n$ and depth $2l$ on any bounded set. this finding enables the extension of most approximation results achieved with $\mathtt{relu}$ networks to a wide variety of other activation functions, albeit with slightly increased constants. significantly, we establish that the (width,$\,$depth) scaling factors can be further reduced from $(3,2)$ to $(1,1)$ if $\varrho$ falls within a specific subset of $\mathscr{a}$. this subset includes activation functions such as $\mathtt{elu}$, $\mathtt{celu}$, $\mathtt{selu}$, $\mathtt{softplus}$, $\mathtt{gelu}$, $\mathtt{silu}$, $\mathtt{swish}$, and $\mathtt{mish}$.",,2023-07-13,2024-01-31,"['shijun zhang', 'jianfeng lu', 'hongkai zhao']"
2307.08138,neural orientation distribution fields for estimation and uncertainty   quantification in diffusion mri,eess.iv stat.ap stat.co,"inferring brain connectivity and structure \textit{in-vivo} requires accurate estimation of the orientation distribution function (odf), which encodes key local tissue properties. however, estimating the odf from diffusion mri (dmri) signals is a challenging inverse problem due to obstacles such as significant noise, high-dimensional parameter spaces, and sparse angular measurements. in this paper, we address these challenges by proposing a novel deep-learning based methodology for continuous estimation and uncertainty quantification of the spatially varying odf field. we use a neural field (nf) to parameterize a random series representation of the latent odfs, implicitly modeling the often ignored but valuable spatial correlation structures in the data, and thereby improving efficiency in sparse and noisy regimes. an analytic approximation to the posterior predictive distribution is derived which can be used to quantify the uncertainty in the odf estimate at any spatial location, avoiding the need for expensive resampling-based approaches that are typically employed for this purpose. we present empirical evaluations on both synthetic and real in-vivo diffusion data, demonstrating the advantages of our method over existing approaches.",,2023-07-16,2024-02-06,"['william consagra', 'lipeng ning', 'yogesh rathi']"
2307.09055,robust data clustering with outliers via transformed tensor low-rank   representation,stat.ml cs.cv cs.lg,"recently, tensor low-rank representation (tlrr) has become a popular tool for tensor data recovery and clustering, due to its empirical success and theoretical guarantees. however, existing tlrr methods consider gaussian or gross sparse noise, inevitably leading to performance degradation when the tensor data are contaminated by outliers or sample-specific corruptions. this paper develops an outlier-robust tensor low-rank representation (or-tlrr) method that provides outlier detection and tensor data clustering simultaneously based on the t-svd framework. for tensor observations with arbitrary outlier corruptions, or-tlrr has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. moreover, an extension of or-tlrr is proposed to handle the case when parts of the data are missing. finally, extensive experimental results on synthetic and real data demonstrate the effectiveness of the proposed algorithms. we release our code at https://github.com/twugithub/2024-aistats-ortlrr.",,2023-07-18,2024-02-28,['tong wu']
2307.09700,the connection between r-learning and inverse-variance weighting for   estimation of heterogeneous treatment effects,stat.me stat.ml,"many methods for estimating conditional average treatment effects (cates) can be expressed as weighted pseudo-outcome regressions (pors). previous comparisons of por techniques have paid careful attention to the choice of pseudo-outcome transformation. however, we argue that the dominant driver of performance is actually the choice of weights. for example, we point out that r-learning implicitly performs a por with inverse-variance weights (ivws). in the cate setting, ivws mitigate the instability associated with inverse-propensity weights, and lead to convenient simplifications of bias terms. we demonstrate the superior performance of ivws in simulations, and derive convergence rates for ivws that are, to our knowledge, the fastest yet shown without assuming knowledge of the covariate distribution.",,2023-07-18,2024-02-02,['aaron fisher']
2307.10187,personalized privacy amplification via importance sampling,cs.cr cs.lg stat.ml,"we examine the privacy-enhancing properties of importance sampling. in importance sampling, selection probabilities are heterogeneous and each selected data point is weighted by the reciprocal of its selection probability. due to the heterogeneity of importance sampling, we express our results within the framework of personalized differential privacy. we first consider the general case where an arbitrary personalized differentially private mechanism is subsampled with an arbitrary importance sampling distribution and show that the resulting mechanism also satisfies personalized differential privacy. this constitutes an extension of the established privacy amplification by subsampling result to importance sampling. then, for any fixed mechanism, we derive the sampling distribution that achieves the optimal sampling rate subject to a worst-case privacy constraint. empirically, we evaluate the privacy, efficiency, and accuracy of importance sampling on the example of k-means clustering.",,2023-07-05,2024-02-15,"['dominik fay', 'sebastian mair', 'jens sjölund']"
2307.10870,nonlinear meta-learning can guarantee faster rates,stat.ml cs.lg math.st stat.th,"many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $n$ of tasks} (as well as the number of samples per task). first steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. this linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. in practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. in the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. in particular, assuming the shared nonlinearity maps to an infinite-dimensional rkhs, we show that additional biases can be mitigated with careful regularization that leverages the smoothness of task-specific regression functions,",,2023-07-20,2024-01-12,"['dimitri meunier', 'zhu li', 'arthur gretton', 'samory kpotufe']"
2307.11018,amortized variational inference: when and why?,stat.ml cs.lg,"in a probabilistic latent variable model, factorized (or mean-field) variational inference (f-vi) fits a separate parametric distribution for each latent variable. amortized variational inference (a-vi) instead learns a common inference function, which maps each observation to its corresponding latent variable's approximate posterior. typically, a-vi is used as a cog in the training of variational autoencoders, however it stands to reason that a-vi could also be used as a general alternative to f-vi. in this paper we study when and why a-vi can be used for approximate bayesian inference. we derive conditions on a latent variable model which are necessary, sufficient, and verifiable under which a-vi can attain f-vi's optimal solution, thereby closing the amortization gap. we prove these conditions are uniquely verified by simple hierarchical models, a broad class that encompasses many models in machine learning. we then show, on a broader class of models, how to expand the domain of avi's inference function to improve its solution, and we provide examples, e.g. hidden markov models, where the amortization gap cannot be closed.",,2023-07-20,2024-02-13,"['charles c. margossian', 'david m. blei']"
2307.11423,attention to entropic communication,cs.it cs.lg math.it physics.data-an stat.ml,"the concept of attention, numerical weights that emphasize the importance of particular data, has proven to be very relevant in artificial intelligence. relative entropy (re, aka kullback-leibler divergence) plays a central role in communication theory. here we combine these concepts, attention and re. re guides optimal encoding of messages in bandwidth-limited communication as well as optimal message decoding via the maximum entropy principle (mep). in the coding scenario, re can be derived from four requirements, namely being analytical, local, proper, and calibrated. weighted re, used for attention steering in communications, turns out to be improper. to see how proper attention communication can emerge, we analyze a scenario of a message sender who wants to ensure that the receiver of the message can perform well-informed actions. if the receiver decodes the message using the mep, the sender only needs to know the receiver's utility function to inform optimally, but not the receiver's initial knowledge state. in case only the curvature of the utility function maxima are known, it becomes desirable to accurately communicate an attention function, in this case a by this curvature weighted and re-normalized probability function. entropic attention communication is here proposed as the desired generalization of entropic communication that permits weighting while being proper, thereby aiding the design of optimal communication protocols in technical applications and helping to understand human communication. for example, our analysis shows how to derive the level of cooperation expected under misaligned interests of otherwise honest communication partners.",,2023-07-21,2024-01-09,"['torsten enßlin', 'carolin weidinger', 'philipp frank']"
2307.12971,big data - supply chain management framework for forecasting: data   preprocessing and machine learning techniques,cs.lg stat.ml,"this article intends to systematically identify and comparatively analyze state-of-the-art supply chain (sc) forecasting strategies and technologies. a novel framework has been proposed incorporating big data analytics in sc management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall sc. initially, the need to collect data according to sc strategy and how to collect them has been discussed. the article discusses the need for different types of forecasting according to the period or sc objective. the sc kpis and the error-measurement systems have been recommended to optimize the top-performing model. the adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the sc kpis for determining model performance parameters and improving operations management, transparency, and planning efficiency have been illustrated. the cyclic connection within the framework introduces preprocessing optimization based on the post-process kpis, optimizing the overall control process (inventory management, workforce determination, cost, production and capacity planning). the contribution of this research lies in the standard sc process framework proposal, recommended forecasting data analysis, forecasting effects on sc performance, machine learning algorithms optimization followed, and in shedding light on future research.",,2023-07-24,2024-02-07,"['md abrar jahin', 'md sakib hossain shovon', 'jungpil shin', 'istiyaque ahmed ridoy', 'm. f. mridha']"
2307.13068,detection of common subtrees with identical label distribution,cs.ds stat.ml,"frequent pattern mining is a relevant method to analyse structured data, like sequences, trees or graphs. it consists in identifying characteristic substructures of a dataset. this paper deals with a new type of patterns for tree data: common subtrees with identical label distribution. their detection is far from obvious since the underlying isomorphism problem is graph isomorphism complete. an elaborated search algorithm is developed and analysed from both theoretical and numerical perspectives. based on this, the enumeration of patterns is performed through a new lossless compression scheme for trees, called dag-rw, whose complexity is investigated as well. the method shows very good properties, both in terms of computation times and analysis of real datasets from the literature. compared to other substructures like topological subtrees and labelled subtrees for which the isomorphism problem is linear, the patterns found provide a more parsimonious representation of the data.",10.1016/j.tcs.2023.114366,2023-07-24,,"['romain azaïs', 'florian ingels']"
2307.13147,extending path-dependent nj-odes to noisy observations and a dependent   observation framework,stat.ml cs.lg cs.na math.na math.pr,"the path-dependent neural jump ordinary differential equation (pd-nj-ode) is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. in particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. so far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. in this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them. in particular, we can lift the assumption of independence by extending the theory to much more realistic settings of conditional independence without any need to change the algorithm. moreover, we introduce a new loss function, which allows us to deal with noisy observations and explain why the previously used loss function did not lead to a consistent estimator.",,2023-07-24,2024-02-05,"['william andersson', 'jakob heiss', 'florian krach', 'josef teichmann']"
2307.13535,do algorithms and barriers for sparse principal component analysis   extend to other structured settings?,stat.ml cs.lg,"we study a principal component analysis problem under the spiked wishart model in which the structure in the signal is captured by a class of union-of-subspace models. this general class includes vanilla sparse pca as well as its variants with graph sparsity. with the goal of studying these problems under a unified statistical and computational lens, we establish fundamental limits that depend on the geometry of the problem instance, and show that a natural projected power method exhibits local convergence to the statistically near-optimal neighborhood of the solution. we complement these results with end-to-end analyses of two important special cases given by path and tree sparsity in a general basis, showing initialization methods and matching evidence of computational hardness. overall, our results indicate that several of the phenomena observed for vanilla sparse pca extend in a natural fashion to its structured counterparts.",,2023-07-25,2023-12-31,"['guanyi wang', 'mengqi lou', 'ashwin pananjady']"
2307.13679,red comets: an ensemble classifier for symbolically represented   multivariate time series,cs.lg stat.ml,"multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more. the complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths. this paper introduces a novel ensemble classifier called red comets (random enhanced co-eye for multivariate time series), which addresses these challenges. red comets builds upon the success of co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data. the performance of red comets is evaluated on benchmark datasets from the ucr archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings. notably, it achieves the highest reported accuracy in the literature for the 'handmovementdirection' dataset. moreover, the proposed method significantly reduces computation time compared to co-eye, making it an efficient and effective choice for multivariate time series classification.",10.1007/978-3-031-49896-1_6,2023-07-25,2023-09-16,"['luca a. bennett', 'zahraa s. abdallah']"
2307.13763,sobolev space regularised pre density models,stat.ml cs.ai cs.lg,"we propose a new approach to non-parametric density estimation that is based on regularizing a sobolev norm of the density. this method is statistically consistent, and makes the inductive bias of the model clear and interpretable. while there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. the optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. however, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. finally, while the approach provides pre-densities (i.e. not necessarily integrating to 1), which prevents the use of log-likelihood for cross validation, we show that one can instead adapt fisher divergence based score matching methods for this task. we evaluate the resulting method on the comprehensive recent anomaly detection benchmark suite, adbench, and find that it ranks second best, among more than 15 algorithms.",,2023-07-25,2024-02-13,"['mark kozdoba', 'binyamin perets', 'shie mannor']"
2307.13903,corruption-robust lipschitz contextual search,cs.lg stat.ml,"i study the problem of learning a lipschitz function with corrupted binary signals. the learner tries to learn a $l$-lipschitz function $f: [0,1]^d \rightarrow [0, l]$ that the adversary chooses. there is a total of $t$ rounds. in each round $t$, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess is high or low. in a total of $c$ rounds, the signal may be corrupted, though the value of $c$ is \emph{unknown} to the learner. the learner's goal is to incur a small cumulative loss. this work introduces the new algorithmic technique \emph{agnostic checking} as well as new analysis techniques. i design algorithms which: for the symmetric loss, the learner achieves regret $l\cdot o(c\log t)$ with $d = 1$ and $l\cdot o_d(c\log t + t^{(d-1)/d})$ with $d > 1$; for the pricing loss, the learner achieves regret $l\cdot \widetilde{o} (t^{d/(d+1)} + c\cdot t^{1/(d+1)})$.",,2023-07-25,2024-02-01,['shiliang zuo']
2307.15154,a/b testing and best-arm identification for linear bandits with   robustness to non-stationarity,cs.lg stat.ml,"we investigate the fixed-budget best-arm identification (bai) problem for linear bandits in a potentially non-stationary environment. given a finite arm set $\mathcal{x}\subset\mathbb{r}^d$, a fixed budget $t$, and an unpredictable sequence of parameters $\left\lbrace\theta_t\right\rbrace_{t=1}^{t}$, an algorithm will aim to correctly identify the best arm $x^* := \arg\max_{x\in\mathcal{x}}x^\top\sum_{t=1}^{t}\theta_t$ with probability as high as possible. prior work has addressed the stationary setting where $\theta_t = \theta_1$ for all $t$ and demonstrated that the error probability decreases as $\exp(-t /\rho^*)$ for a problem-dependent constant $\rho^*$. but in many real-world $a/b/n$ multivariate testing scenarios that motivate our work, the environment is non-stationary and an algorithm expecting a stationary setting can easily fail. for robust identification, it is well-known that if arms are chosen randomly and non-adaptively from a g-optimal design over $\mathcal{x}$ at each time then the error probability decreases as $\exp(-t\delta^2_{(1)}/d)$, where $\delta_{(1)} = \min_{x \neq x^*} (x^* - x)^\top \frac{1}{t}\sum_{t=1}^t \theta_t$. as there exist environments where $\delta_{(1)}^2/ d \ll 1/ \rho^*$, we are motivated to propose a novel algorithm $\mathsf{p1}$-$\mathsf{rage}$ that aims to obtain the best of both worlds: robustness to non-stationarity and fast rates of identification in benign settings. we characterize the error probability of $\mathsf{p1}$-$\mathsf{rage}$ and demonstrate empirically that the algorithm indeed never performs worse than g-optimal design but compares favorably to the best algorithms in the stationary setting.",,2023-07-27,2024-02-15,"['zhihan xiong', 'romain camilleri', 'maryam fazel', 'lalit jain', 'kevin jamieson']"
2307.15176,rct rejection sampling for causal estimation evaluation,cs.ai cs.cl cs.lg stat.me,"confounding is a significant obstacle to unbiased estimation of causal effects from observational data. for settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -- researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. however, empirical evaluation of these adjustment methods has been challenging and limited. in this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (rcts) to create confounded observational datasets while using the average causal effects from the rcts as ground-truth. we contribute a new sampling algorithm, which we call rct rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth rct. using synthetic data, we show our algorithm indeed results in low bias when oracle estimators are evaluated on the confounded samples, which is not always the case for a previously proposed algorithm. in addition to this identification result, we highlight several finite data considerations for evaluation designers who plan to use rct rejection sampling on their own datasets. as a proof of concept, we implement an example evaluation pipeline and walk through these finite data considerations with a novel, real-world rct -- which we release publicly -- consisting of approximately 70k observations and text data as high-dimensional covariates. together, these contributions build towards a broader agenda of improved empirical evaluation for causal estimation.",,2023-07-27,2024-01-31,"['katherine a. keith', 'sergey feldman', 'david jurgens', 'jonathan bragg', 'rohit bhattacharya']"
2307.15404,information-based preprocessing of plc data for automatic behavior   modeling,eess.sy cs.ce cs.sy stat.me,"cyber-physical systems (cps) offer immense optimization potential for manufacturing processes through the availability of multivariate time series data of actors and sensors. based on automated analysis software, the deployment of adaptive and responsive measures is possible for time series data. due to the complex and dynamic nature of modern manufacturing, analysis and modeling often cannot be entirely automated. even machine- or deep learning approaches often depend on a priori expert knowledge and labelling. in this paper, an information-based data preprocessing approach is proposed. by applying statistical methods including variance and correlation analysis, an approximation of the sampling rate in event-based systems and the utilization of spectral analysis, knowledge about the underlying manufacturing processes can be gained prior to modeling. the paper presents, how statistical analysis enables the pruning of a dataset's least important features and how the sampling rate approximation approach sets the base for further data analysis and modeling. the data's underlying periodicity, originating from the cyclic nature of an automated manufacturing process, will be detected by utilizing the fast fourier transform. this information-based preprocessing method will then be validated for process time series data of cyber-physical systems' programmable logic controllers (plc).",10.1016/j.procir.2023.09.038,2023-07-28,2024-01-15,"['brandon k. sai', 'jonas gram', 'thomas bauernhansl']"
2308.01054,simulation-based inference using surjective sequential neural likelihood   estimation,stat.ml cs.lg stat.me,"we present surjective sequential neural likelihood (ssnl) estimation, a novel method for simulation-based inference in models where the evaluation of the likelihood function is not tractable and only a simulator that can generate synthetic data is available. ssnl fits a dimensionality-reducing surjective normalizing flow model and uses it as a surrogate likelihood function which allows for conventional bayesian inference using either markov chain monte carlo methods or variational inference. by embedding the data in a low-dimensional space, ssnl solves several issues previous likelihood-based methods had when applied to high-dimensional data sets that, for instance, contain non-informative data dimensions or lie along a lower-dimensional manifold. we evaluate ssnl on a wide variety of experiments and show that it generally outperforms contemporary methods used in simulation-based inference, for instance, on a challenging real-world example from astrophysics which models the magnetic field strength of the sun using a solar dynamo model.",,2023-08-02,2024-02-23,"['simon dirmeier', 'carlo albert', 'fernando perez-cruz']"
2308.01566,fast slate policy optimization: going beyond plackett-luce,cs.lg cs.ir stat.ml,"an increasingly important building block of large scale machine learning systems is based on returning slates; an ordered lists of items given a query. applications of this technology include: search, information retrieval and recommender systems. when the action space is large, decision systems are restricted to a particular structure to complete online queries quickly. this paper addresses the optimization of these large scale decision systems given an arbitrary reward function. we cast this learning problem in a policy optimization framework and propose a new class of policies, born from a novel relaxation of decision functions. this results in a simple, yet efficient learning algorithm that scales to massive action spaces. we compare our method to the commonly adopted plackett-luce policy class and demonstrate the effectiveness of our approach on problems with action space sizes in the order of millions.",,2023-08-03,2023-12-29,"['otmane sakhi', 'david rohde', 'nicolas chopin']"
2308.02261,adaptive proximal gradient method for convex optimization,math.oc cs.lg cs.na math.na stat.ml,"in this paper, we explore two fundamental first-order algorithms in convex optimization, namely, gradient descent (gd) and proximal gradient method (proxgd). our focus is on making these algorithms entirely adaptive by leveraging local curvature information of smooth functions. we propose adaptive versions of gd and proxgd that are based on observed gradient differences and, thus, have no added computational costs. moreover, we prove convergence of our methods assuming only local lipschitzness of the gradient. in addition, the proposed versions allow for even larger stepsizes than those initially suggested in [mm20].",,2023-08-04,2024-02-12,"['yura malitsky', 'konstantin mishchenko']"
2308.03398,do machine learning methods lead to similar individualized treatment   rules? a comparison study on real data,stat.ap,"identifying patients who benefit from a treatment is a key aspect of personalized medicine, which allows the development of individualized treatment rules (itrs). many machine learning methods have been proposed to create such rules. however, to what extent the methods lead to similar itrs, i.e., recommending the same treatment for the same individuals is unclear. in this work, we compared 22 of the most common approaches in two randomized control trials. two classes of methods can be distinguished. the first class of methods relies on predicting individualized treatment effects from which an itr is derived by recommending the treatment evaluated to the individuals with a predicted benefit. in the second class, methods directly estimate the itr without estimating individualized treatment effects. for each trial, the performance of itrs was assessed by various metrics, and the pairwise agreement between all itrs was also calculated. results showed that the itrs obtained via the different methods generally had considerable disagreements regarding the patients to be treated. a better concordance was found among akin methods. overall, when evaluating the performance of itrs in a validation sample, all methods produced itrs with limited performance, suggesting a high potential for optimism. for non-parametric methods, this optimism was likely due to overfitting. the different methods do not lead to similar itrs and are therefore not interchangeable. the choice of the method strongly influences for which patients a certain treatment is recommended, drawing some concerns about their practical use.",,2023-08-07,2024-01-30,"['florie bouvier', 'etienne peyrot', 'alan balendran', 'corentin ségalas', 'ian roberts', 'françois petit', 'raphaël porcher']"
2308.03970,dependent cluster mapping (dcmap): optimal clustering of directed   acyclic graphs for statistical inference,cs.ds stat.ml,"a directed acyclic graph (dag) can be partitioned or mapped into clusters to support and make inference more computationally efficient in bayesian network (bn), markov process and other models. however, optimal partitioning with an arbitrary cost function is challenging, especially in statistical inference as the local cluster cost is dependent on both nodes within a cluster, and the mapping of clusters connected via parent and/or child nodes, which we call dependent clusters. we propose a novel algorithm called dcmap for optimal cluster mapping with dependent clusters. given an arbitrarily defined, positive cost function based on the dag, we show that dcmap converges to find all optimal clusters, and returns near-optimal solutions along the way. empirically, we find that the algorithm is time-efficient for a dynamic bn (dbn) model of a seagrass complex system using a computation cost function. for a 25 and 50-node dbn, the search space size was $9.91\times 10^9$ and $1.51\times10^{21}$ possible cluster mappings, and the first optimal solution was found at iteration 934 $(\text{95\% ci } 926,971)$, and 2256 $(2150,2271)$ with a cost that was 4\% and 0.2\% of the naive heuristic cost, respectively.",,2023-08-07,2024-02-07,"['paul pao-yen wu', 'fabrizio ruggeri', 'kerrie mengersen']"
2308.04106,parallel learning by multitasking neural networks,cond-mat.dis-nn physics.bio-ph stat.ml,"a modern challenge of artificial intelligence is learning multiple patterns at once (i.e.parallel learning). while this can not be accomplished by standard hebbian associative neural networks, in this paper we show how the multitasking hebbian network (a variation on theme of the hopfield model working on sparse data-sets) is naturally able to perform this complex task. we focus on systems processing in parallel a finite (up to logarithmic growth in the size of the network) amount of patterns, mirroring the low-storage level of standard associative neural networks at work with pattern recognition. for mild dilution in the patterns, the network handles them hierarchically, distributing the amplitudes of their signals as power-laws w.r.t. their information content (hierarchical regime), while, for strong dilution, all the signals pertaining to all the patterns are raised with the same strength (parallel regime). further, confined to the low-storage setting (i.e., far from the spin glass limit), the presence of a teacher neither alters the multitasking performances nor changes the thresholds for learning: the latter are the same whatever the training protocol is supervised or unsupervised. results obtained through statistical mechanics, signal-to-noise technique and monte carlo simulations are overall in perfect agreement and carry interesting insights on multiple learning at once: for instance, whenever the cost-function of the model is minimized in parallel on several patterns (in its description via statistical mechanics), the same happens to the standard sum-squared error loss function (typically used in machine learning).",10.1088/1742-5468/ad0a86,2023-08-08,,"['elena agliari', 'andrea alessandrelli', 'adriano barra', 'federico ricci-tersenghi']"
2308.04365,slem: machine learning for path modeling and causal inference with super   learner equation modeling,stat.ml cs.lg stat.ap,"causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. path models, structural equation models (sems), and, more generally, directed acyclic graphs (dags), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. unlike dags, which make very few assumptions about the functional and parametric form, sem assumes linearity. this can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. in contrast, we propose super learner equation modeling, a path modeling technique integrating machine learning super learner ensembles. we empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with sem, and highlight its superiority over sem when dealing with non-linear relationships. we provide open-source code, and a tutorial notebook with example usage, accentuating the easy-to-use nature of the method.",,2023-08-08,2024-01-05,['matthew j. vowels']
2308.04585,kernel single proxy control for deterministic confounding,stat.ml cs.lg,"we consider the problem of causal effect estimation with an unobserved confounder, where we observe a proxy variable that is associated with the confounder. although proxy causal learning (pcl) uses two proxy variables to recover the true causal effect, we show that a single proxy variable is sufficient for causal estimation if the outcome is generated deterministically, generalizing control outcome calibration approach (coca). we propose two kernel-based methods for this setting: the first based on the two-stage regression approach, and the second based on a maximum moment restriction approach. we prove that both approaches can consistently estimate the causal effect, and we empirically demonstrate that we can successfully recover the causal effect on challenging synthetic benchmarks.",,2023-08-08,2024-02-20,"['liyuan xu', 'arthur gretton']"
2308.04620,multiclass online learnability under bandit feedback,cs.lg stat.ml,"we study online multiclass classification under bandit feedback. we extend the results of daniely and helbertal [2013] by showing that the finiteness of the bandit littlestone dimension is necessary and sufficient for bandit online learnability even when the label space is unbounded. moreover, we show that, unlike the full-information setting, sequential uniform convergence is necessary but not sufficient for bandit online learnability. our result complements the recent work by hanneke, moran, raman, subedi, and tewari [2023] who show that the littlestone dimension characterizes online multiclass learnability in the full-information setting even when the label space is unbounded.",,2023-08-08,2024-01-20,"['ananth raman', 'vinod raman', 'unique subedi', 'idan mehalel', 'ambuj tewari']"
2308.05061,fine-tune language models as multi-modal differential equation solvers,cs.lg cs.na math.na stat.ml,"in the growing domain of scientific machine learning, in-context operator learning has shown notable potential in building foundation models, as in this framework the model is trained to learn operators and solve differential equations using prompted data, during the inference stage without weight updates. however, the current model's overdependence on function data overlooks the invaluable human insight into the operator. to address this, we present a transformation of in-context operator learning into a multi-modal paradigm. in particular, we take inspiration from the recent success of large language models, and propose using ""captions"" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. also, we introduce a novel approach to train a language-model-like architecture, or directly fine-tune existing language models, for in-context operator learning. we beat the baseline on single-modal learning tasks, and also demonstrated the effectiveness of multi-modal learning in enhancing performance and reducing function data requirements. the proposed method not only significantly enhanced the development of the in-context operator learning paradigm, but also created a new path for the application of language models.",,2023-08-09,2024-02-01,"['liu yang', 'siting liu', 'stanley j. osher']"
2308.06399,learning bayesian networks with heterogeneous agronomic data sets via   mixed-effect models and hierarchical clustering,stat.ml cs.lg stat.ap,"maize, a crucial crop globally cultivated across vast regions, especially in sub-saharan africa, asia, and latin america, occupies 197 million hectares as of 2021. various statistical and machine learning models, including mixed-effect models, random coefficients models, random forests, and deep learning architectures, have been devised to predict maize yield. these models consider factors such as genotype, environment, genotype-environment interaction, and field management. however, the existing models often fall short of fully exploiting the complex network of causal relationships among these factors and the hierarchical structure inherent in agronomic data. this study introduces an innovative approach integrating random effects into bayesian networks (bns), leveraging their capacity to model causal and probabilistic relationships through directed acyclic graphs. rooted in the linear mixed-effects models framework and tailored for hierarchical data, this novel approach demonstrates enhanced bn learning. application to a real-world agronomic trial produces a model with improved interpretability, unveiling new causal connections. notably, the proposed method significantly reduces the error rate in maize yield prediction from 28% to 17%. these results advocate for the preference of bns in constructing practical decision support tools for hierarchical agronomic data, facilitating causal inference.",,2023-08-11,2024-01-15,"['lorenzo valleggi', 'marco scutari', 'federico mattia stefanini']"
2308.07480,order-based structure learning with normalizing flows,cs.lg stat.me,"estimating the causal structure of observational data is a challenging combinatorial search problem that scales super-exponentially with graph size. existing methods use continuous relaxations to make this problem computationally tractable but often restrict the data-generating process to additive noise models (anms) through explicit or implicit assumptions. we present order-based structure learning with normalizing flows (oslow), a framework that relaxes these assumptions using autoregressive normalizing flows. we leverage the insight that searching over topological orderings is a natural way to enforce acyclicity in structure discovery and propose a novel, differentiable permutation learning method to find such orderings. through extensive experiments on synthetic and real-world data, we demonstrate that oslow outperforms prior baselines and improves performance on the observational sachs and syntren datasets as measured by structural hamming distance and structural intervention distance, highlighting the importance of relaxing the anm assumption made by existing methods.",,2023-08-14,2024-02-17,"['hamidreza kamkari', 'vahid balazadeh', 'vahid zehtab', 'rahul g. krishnan']"
2308.07520,"nonlinearity, feedback and uniform consistency in causal structural   learning",stat.ml cs.ai cs.lg,"the goal of causal discovery is to find automated search methods for learning causal structures from observational data. in some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. in contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. these are referred to as latent variables. one commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as iq tests. in this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. this thesis focuses on two questions in causal discovery: providing an alternative definition of k-triangle faithfulness that (i) is weaker than strong faithfulness when applied to the gaussian family of distributions, (ii) can be applied to non-gaussian families of distributions, and (iii) under the assumption that the modified version of strong faithfulness holds, can be used to show the uniform consistency of a modified causal discovery algorithm; relaxing the sufficiency assumption to learn causal structures with latent variables. given the importance of inferring cause-and-effect relationships for understanding and forecasting complex systems, the work in this thesis of relaxing various simplification assumptions is expected to extend the causal discovery method to be applicable in a wider range with diversified causal mechanism and statistical phenomena.",,2023-08-14,2024-01-09,['shuyan wang']
2308.08055,simple online learning with consistent oracle,cs.lg cs.ai stat.ml,"we consider online learning in the model where a learning algorithm can access the class only via the \emph{consistent oracle} -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. this model was recently considered by assos et al.~(colt'23). it is motivated by the fact that standard methods of online learning rely on computing the littlestone dimension of subclasses, a computationally intractable problem.   assos et al.~gave an online learning algorithm in this model that makes at most $c^d$ mistakes on classes of littlestone dimension $d$, for some absolute unspecified constant $c > 0$. we give a novel algorithm that makes at most $o(256^d)$ mistakes. our proof is significantly simpler and uses only very basic properties of the littlestone dimension. we also show that there exists no algorithm in this model that makes less than $3^d$ mistakes.",,2023-08-15,2024-02-06,"['alexander kozachinskiy', 'tomasz steifer']"
2308.09113,multi-fidelity fourier neural operator for fast modeling of large-scale   geological carbon storage,stat.ml cs.lg,"deep learning-based surrogate models have been widely applied in geological carbon storage (gcs) problems to accelerate the prediction of reservoir pressure and co2 plume migration. large amounts of data from physics-based numerical simulators are required to train a model to accurately predict the complex physical behaviors associated with this process. in practice, the available training data are always limited in large-scale 3d problems due to the high computational cost. therefore, we propose to use a multi-fidelity fourier neural operator (fno) to solve large-scale gcs problems with more affordable multi-fidelity training datasets. fno has a desirable grid-invariant property, which simplifies the transfer learning procedure between datasets with different discretization. we first test the model efficacy on a gcs reservoir model being discretized into 110k grid cells. the multi-fidelity model can predict with accuracy comparable to a high-fidelity model trained with the same amount of high-fidelity data with 81% less data generation costs. we further test the generalizability of the multi-fidelity model on a same reservoir model with a finer discretization of 1 million grid cells. this case was made more challenging by employing high-fidelity and low-fidelity datasets generated by different geostatistical models and reservoir simulators. we observe that the multi-fidelity fno model can predict pressure fields with reasonable accuracy even when the high-fidelity data are extremely limited. the findings of this study can help for better understanding of the transferability of multi-fidelity deep learning surrogate models.",,2023-08-17,2024-01-09,"['hewei tang', 'qingkai kong', 'joseph p. morris']"
2308.09460,accelerated bayesian imaging by relaxed proximal-point langevin sampling,stat.co cs.cv cs.na math.na stat.ml,"this paper presents a new accelerated proximal markov chain monte carlo methodology to perform bayesian inference in imaging inverse problems with an underlying convex geometry. the proposed strategy takes the form of a stochastic relaxed proximal-point iteration that admits two complementary interpretations. for models that are smooth or regularised by moreau-yosida smoothing, the algorithm is equivalent to an implicit midpoint discretisation of an overdamped langevin diffusion targeting the posterior distribution of interest. this discretisation is asymptotically unbiased for gaussian targets and shown to converge in an accelerated manner for any target that is $\kappa$-strongly log-concave (i.e., requiring in the order of $\sqrt{\kappa}$ iterations to converge, similarly to accelerated optimisation schemes), comparing favorably to [m. pereyra, l. vargas mieles, k.c. zygalakis, siam j. imaging sciences, 13,2 (2020), pp. 905-935] which is only provably accelerated for gaussian targets and has bias. for models that are not smooth, the algorithm is equivalent to a leimkuhler-matthews discretisation of a langevin diffusion targeting a moreau-yosida approximation of the posterior distribution of interest, and hence achieves a significantly lower bias than conventional unadjusted langevin strategies based on the euler-maruyama discretisation. for targets that are $\kappa$-strongly log-concave, the provided non-asymptotic convergence analysis also identifies the optimal time step which maximizes the convergence speed. the proposed methodology is demonstrated through a range of experiments related to image deconvolution with gaussian and poisson noise, with assumption-driven and data-driven convex priors. source codes for the numerical experiments of this paper are available from https://github.com/mi2g/accelerated-langevin-imla.",,2023-08-18,2024-01-12,"['teresa klatzer', 'paul dobson', 'yoann altmann', 'marcelo pereyra', 'jesús maría sanz-serna', 'konstantinos c. zygalakis']"
2308.09565,understanding the role of layer normalization in label-skewed federated   learning,cs.lg stat.ml,"layer normalization (ln) is a widely adopted deep learning technique especially in the era of foundation models. recently, ln has been shown to be surprisingly effective in federated learning (fl) with non-i.i.d. data. however, exactly why and how it works remains mysterious. in this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. to understand layer normalization better in fl, we identify the key contributing mechanism of normalization methods in fl, called feature normalization (fn), which applies normalization to the latent feature representation before the classifier head. although ln and fn do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. moreover, we conduct extensive ablation studies to understand the critical factors of layer normalization in fl. our results verify that fn is an essential ingredient inside ln to significantly improve the convergence of fl while remaining robust to learning rate choices, especially under extreme label shift where each client has access to few classes. our code is available at \url{https://github.com/huawei-noah/federated-learning/tree/main/layer_normalization}.",,2023-08-18,2024-02-14,"['guojun zhang', 'mahdi beitollahi', 'alex bie', 'xi chen']"
2308.09647,robust uncertainty quantification using conformalised monte carlo   prediction,cs.lg cs.ai stat.ml,"deploying deep learning models in safety-critical applications remains a very challenging task, mandating the provision of assurances for the dependable operation of these models. uncertainty quantification (uq) methods estimate the model's confidence per prediction, informing decision-making by considering the effect of randomness and model misspecification. despite the advances of state-of-the-art uq methods, they are computationally expensive or produce conservative prediction sets/intervals. we introduce mc-cp, a novel hybrid uq method that combines a new adaptive monte carlo (mc) dropout method with conformal prediction (cp). mc-cp adaptively modulates the traditional mc dropout at runtime to save memory and computation resources, enabling predictions to be consumed by cp, yielding robust prediction sets/intervals. throughout comprehensive experiments, we show that mc-cp delivers significant improvements over advanced uq methods, like mc dropout, raps and cqr, both in classification and regression benchmarks. mc-cp can be easily added to existing models, making its deployment simple.",,2023-08-18,2024-01-22,"['daniel bethell', 'simos gerasimou', 'radu calinescu']"
2308.11978,will more expressive graph neural networks do better on generative   tasks?,cs.lg cs.ai q-bio.bm stat.ml,"graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. this task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. in recent years, several successful methods have emerged in the field of graph generation. however, these approaches suffer from two significant shortcomings: (1) the underlying graph neural network (gnn) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. to fill this gap, we investigate the expressiveness of gnns under the context of the molecular graph generation task, by replacing the underlying gnns of graph generative models with more expressive gnns. specifically, we analyse the performance of six gnns in two different generative frameworks -- autoregressive generation models, such as gcpn and graphaf, and one-shot generation models, such as graphebm -- on six different molecular generative objectives on the zinc-250k dataset. through our extensive experiments, we demonstrate that advanced gnns can indeed improve the performance of gcpn, graphaf, and graphebm on molecular generation tasks, but gnn expressiveness is not a necessary condition for a good gnn-based generative model. moreover, we show that gcpn and graphaf with advanced gnns can achieve state-of-the-art results across 17 other non-gnn-based graph generative approaches, such as variational autoencoders and bayesian optimisation models, on the proposed molecular generative objectives (drd2, median1, median2), which are important metrics for de-novo molecular design.",,2023-08-23,2024-02-20,"['xiandong zou', 'xiangyu zhao', 'pietro liò', 'yiren zhao']"
2308.13737,survivalcontour: visualizing predicted survival via colored contour   plots,stat.ap,"advances in survival analysis have facilitated unprecedented flexibility in data modeling, yet there remains a lack of tools for graphically illustrating the influence of continuous covariates on predicted survival outcomes. we propose the utilization of a colored contour plot to depict the predicted survival probabilities over time, and provide a shiny app and r package as implementations of this tool. our approach is capable of supporting conventional models, including the cox and fine-gray models. however, its capability shines when coupled with cutting-edge machine learning models such as random survival forests and deep neural networks.",,2023-08-25,2024-01-12,"['yushu shi', 'liangliang zhang', 'kim-anh do', 'robert r. jenq', 'christine b. peterson']"
2308.14407,identifying topology of leaky photonic lattices with machine learning,physics.optics cs.lg physics.data-an stat.ml,"we show how machine learning techniques can be applied for the classification of topological phases in leaky photonic lattices using limited measurement data. we propose an approach based solely on bulk intensity measurements, thus exempt from the need for complicated phase retrieval procedures. in particular, we design a fully connected neural network that accurately determines topological properties from the output intensity distribution in dimerized waveguide arrays with leaky channels, after propagation of a spatially localized initial excitation at a finite distance, in a setting that closely emulates realistic experimental conditions.",10.1515/nanoph-2023-0564,2023-08-28,,"['ekaterina o. smolina', 'lev a. smirnov', 'daniel leykam', 'franco nori', 'daria a. smirnova']"
2308.14547,deep graphical regression for jointly moderate and extreme australian   wildfires,stat.ap stat.ml,"recent wildfires in australia have led to considerable economic loss and property destruction, and there is increasing concern that climate change may exacerbate their intensity, duration, and frequency. hazard quantification for extreme wildfires is an important component of wildfire management, as it facilitates efficient resource distribution, adverse effect mitigation, and recovery efforts. however, although extreme wildfires are typically the most impactful, both small and moderate fires can still be devastating to local communities and ecosystems. therefore, it is imperative to develop robust statistical methods to reliably model the full distribution of wildfire spread. we do so for a novel dataset of australian wildfires from 1999 to 2019, and analyse monthly spread over areas approximately corresponding to statistical areas level~1 and~2 (sa1/sa2) regions. given the complex nature of wildfire ignition and spread, we exploit recent advances in statistical deep learning and extreme value theory to construct a parametric regression model using graph convolutional neural networks and the extended generalized pareto distribution, which allows us to model wildfire spread observed on an irregular spatial domain. we highlight the efficacy of our newly proposed model and perform a wildfire hazard assessment for australia and population-dense communities, namely tasmania, sydney, melbourne, and perth.",,2023-08-28,2024-01-11,"['daniela cisneros', 'jordan richards', 'ashok dahal', 'luigi lombardo', 'raphaël huser']"
2308.15552,pure exploration under mediators' feedback,cs.lg stat.ml,"stochastic multi-armed bandits are a sequential-decision-making framework, where, at each interaction step, the learner selects an arm and observes a stochastic reward. within the context of best-arm identification (bai) problems, the goal of the agent lies in finding the optimal arm, i.e., the one with highest expected reward, as accurately and efficiently as possible. nevertheless, the sequential interaction protocol of classical bai problems, where the agent has complete control over the arm being pulled at each round, does not effectively model several decision-making problems of interest (e.g., off-policy learning, partially controllable environments, and human feedback). for this reason, in this work, we propose a novel strict generalization of the classical bai problem that we refer to as best-arm identification under mediators' feedback (bai-mf). more specifically, we consider the scenario in which the learner has access to a set of mediators, each of which selects the arms on the agent's behalf according to a stochastic and possibly unknown policy. the mediator, then, communicates back to the agent the pulled arm together with the observed reward. in this setting, the agent's goal lies in sequentially choosing which mediator to query to identify with high probability the optimal arm while minimizing the identification time, i.e., the sample complexity. to this end, we first derive and analyze a statistical lower bound on the sample complexity specific to our general mediator feedback scenario. then, we propose a sequential decision-making strategy for discovering the best arm under the assumption that the mediators' policies are known to the learner. as our theory verifies, this algorithm matches the lower bound both almost surely and in expectation. finally, we extend these results to cases where the mediators' policies are unknown to the learner obtaining comparable results.",,2023-08-29,2024-01-12,"['riccardo poiani', 'alberto maria metelli', 'marcello restelli']"
2308.15613,mixed variational flows for discrete variables,stat.co cs.lg stat.ml,"variational flows allow practitioners to learn complex continuous distributions, but approximating discrete distributions remains a challenge. current methodologies typically embed the discrete target in a continuous space - usually via continuous relaxation or dequantization - and then apply a continuous flow. these approaches involve a surrogate target that may not capture the original discrete target, might have biased or unstable gradients, and can create a difficult optimization problem. in this work, we develop a variational flow family for discrete distributions without any continuous embedding. first, we develop a measure-preserving and discrete (mad) invertible map that leaves the discrete target invariant, and then create a mixed variational flow (mad mix) based on that map. our family provides access to i.i.d. sampling and density evaluation with virtually no tuning effort. we also develop an extension to mad mix that handles joint discrete and continuous models. our experiments suggest that mad mix produces more reliable approximations than continuous-embedding flows while being significantly faster to train.",,2023-08-29,2024-02-26,"['gian carlo diluvi', 'benjamin bloem-reddy', 'trevor campbell']"
2308.16681,one model many scores: using multiverse analysis to prevent fairness   hacking and evaluate the influence of model design decisions,stat.ml cs.lg,"a vast number of systems across the world use algorithmic decision making (adm) to (partially) automate decisions that have previously been made by humans. the downstream effects of adm systems critically depend on the decisions made during a systems' design, implementation, and evaluation, as biases in data can be mitigated or reinforced along the modeling pipeline. many of these decisions are made implicitly, without knowing exactly how they will influence the final system. to study this issue, we draw on insights from the field of psychology and introduce the method of multiverse analysis for algorithmic fairness. in our proposed method, we turn implicit decisions during design and evaluation into explicit ones and demonstrate their fairness implications. by combining decisions, we create a grid of all possible ""universes"" of decision combinations. for each of these universes, we compute metrics of fairness and performance. using the resulting dataset, one can investigate the variability and robustness of fairness scores and see how and which decisions impact fairness. we demonstrate how multiverse analyses can be used to better understand fairness implications of design and evaluation decisions using an exemplary case study of predicting public health care coverage for vulnerable populations. our results highlight how decisions regarding the evaluation of a system can lead to vastly different fairness metrics for the same model. this is problematic, as a nefarious actor could optimise or ""hack"" a fairness metric to portray a discriminating model as fair merely by changing how it is evaluated. we illustrate how a multiverse analysis can help to address this issue.",,2023-08-31,2024-02-02,"['jan simson', 'florian pfisterer', 'christoph kern']"
2309.00557,concentrated differential privacy for bandits,stat.ml cs.cr cs.it cs.lg math.it math.st stat.th,"bandits serve as the theoretical foundation of sequential learning and an algorithmic foundation of modern recommender systems. however, recommender systems often rely on user-sensitive data, making privacy a critical concern. this paper contributes to the understanding of differential privacy (dp) in bandits with a trusted centralised decision-maker, and especially the implications of ensuring zero concentrated differential privacy (zcdp). first, we formalise and compare different adaptations of dp to bandits, depending on the considered input and the interaction protocol. then, we propose three private algorithms, namely adac-ucb, adac-gope and adac-oful, for three bandit settings, namely finite-armed bandits, linear bandits, and linear contextual bandits. the three algorithms share a generic algorithmic blueprint, i.e. the gaussian mechanism and adaptive episodes, to ensure a good privacy-utility trade-off. we analyse and upper bound the regret of these three algorithms. our analysis shows that in all of these settings, the prices of imposing zcdp are (asymptotically) negligible in comparison with the regrets incurred oblivious to privacy. next, we complement our regret upper bounds with the first minimax lower bounds on the regret of bandits with zcdp. to prove the lower bounds, we elaborate a new proof technique based on couplings and optimal transport. we conclude by experimentally validating our theoretical results for the three different settings of bandits.",,2023-09-01,2024-02-15,"['achraf azize', 'debabrota basu']"
2309.00612,bayesian deep learning for cosmic volumes with modified gravity,astro-ph.co astro-ph.im cs.lg stat.ml,"the new generation of galaxy surveys will provide unprecedented data allowing us to test gravity at cosmological scales. a robust cosmological analysis of the large-scale structure demands exploiting the nonlinear information encoded in the cosmic web. machine learning techniques provide such tools, however, do not provide a priori assessment of uncertainties. this study aims at extracting cosmological parameters from modified gravity (mg) simulations through deep neural networks endowed with uncertainty estimations. we implement bayesian neural networks (bnns) with an enriched approximate posterior distribution considering two cases: one with a single bayesian last layer (bll), and another one with bayesian layers at all levels (fullb). we train both bnns with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh $n$-body simulations including modified gravity models relying on mg-picola covering 256 $h^{-1}$ mpc side cubical volumes with 128$^3$ particles. bnns excel in accurately predicting parameters for $\omega_m$ and $\sigma_8$ and their respective correlation with the mg parameter. we find out that bnns yield well-calibrated uncertainty estimates overcoming the over- and under-estimation issues in traditional neural networks. we observe that the presence of mg parameter leads to a significant degeneracy with $\sigma_8$ being one of the possible explanations of the poor mg predictions. ignoring mg, we obtain a deviation of the relative errors in $\omega_m$ and $\sigma_8$ by at least $30\%$. moreover, we report consistent results from the density field and power spectra analysis, and comparable results between bll and fullb experiments which permits us to save computing time by a factor of two. this work contributes in setting the path to extract cosmological parameters from complete small cosmic volumes towards the highly nonlinear regime.",10.1051/0004-6361/202347929,2023-09-01,2024-02-12,"['jorge enrique garcía-farieta', 'héctor j hortúa', 'francisco-shu kitaura']"
2309.01592,les houches lectures on deep learning at large & infinite width,stat.ml cs.ai cs.lg hep-th math.pr,"these lectures, presented at the 2022 les houches summer school on statistical physics and machine learning, focus on the infinite-width limit and large-width regime of deep neural networks. topics covered include various statistical and dynamical properties of these networks. in particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.",,2023-09-04,2024-02-12,"['yasaman bahri', 'boris hanin', 'antonin brossollet', 'vittorio erba', 'christian keup', 'rosalba pacelli', 'james b. simon']"
2309.01657,locally stationary graph processes,stat.ml cs.lg,"stationary graph process models are commonly used in the analysis and inference of data sets collected on irregular network topologies. while most of the existing methods represent graph signals with a single stationary process model that is globally valid on the entire graph, in many practical problems, the characteristics of the process may be subject to local variations in different regions of the graph. in this work, we propose a locally stationary graph process (lsgp) model that aims to extend the classical concept of local stationarity to irregular graph domains. we characterize local stationarity by expressing the overall process as the combination of a set of component processes such that the extent to which the process adheres to each component varies smoothly over the graph. we propose an algorithm for computing lsgp models from realizations of the process, and also study the approximation of lsgps locally with wss processes. experiments on signal interpolation problems show that the proposed process model provides accurate signal representations competitive with the state of the art.",,2023-09-04,2024-02-27,"['abdullah canbolat', 'elif vural']"
2309.02334,polylut: learning piecewise polynomials for ultra-low latency fpga   lut-based inference,cs.lg cs.ar stat.ml,"field-programmable gate arrays (fpgas) are widely used to implement deep learning inference. standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside fpga lookup tables (luts). our work is motivated by the idea that the luts in an fpga can be used to implement a much greater variety of functions than this. in this paper, we propose a novel approach to training neural networks for fpga deployment using multivariate polynomials as the basic building block. our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the luts with minimal overhead. we show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements. we demonstrate the effectiveness of this approach in three tasks: network intrusion detection, jet identification at the cern large hadron collider, and handwritten digit recognition using the mnist dataset.",10.1109/icfpt59805.2023.00012,2023-09-05,2023-11-06,"['marta andronic', 'george a. constantinides']"
2309.02351,exact inference for continuous-time gaussian process dynamics,cs.lg stat.ml,"physical systems can often be described via a continuous-time dynamical system. in practice, the true system is often unknown and has to be learned from measurement data. since data is typically collected in discrete time, e.g. by sensors, most methods in gaussian process (gp) dynamics model learning are trained on one-step ahead predictions. this can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties have to be conserved. thus, we aim for a gp model of the true continuous-time dynamics. higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. many higher-order integrators require dynamics evaluations at intermediate time steps making exact gp inference intractable. in previous work, this problem is often tackled by approximating the gp posterior with variational inference. however, exact gp inference is preferable in many scenarios, e.g. due to its mathematical guarantees. in order to make direct inference tractable, we propose to leverage multistep and taylor integrators. we demonstrate how to derive flexible inference schemes for these types of integrators. further, we derive tailored sampling schemes that allow to draw consistent dynamics functions from the learned posterior. this is crucial to sample consistent predictions from the dynamics model. we demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system.",,2023-09-05,2024-01-29,"['katharina ensinger', 'nicholas tagliapietra', 'sebastian ziesche', 'sebastian trimpe']"
2309.03494,evaluating deep learning-based melanoma classification using   immunohistochemistry and routine histology: a three center study,eess.iv cs.cv stat.ap,"pathologists routinely use immunohistochemical (ihc)-stained tissue slides against melana in addition to hematoxylin and eosin (h&e)-stained slides to improve their accuracy in diagnosing melanomas. the use of diagnostic deep learning (dl)-based support systems for automated examination of tissue morphology and cellular composition has been well studied in standard h&e-stained tissue slides. in contrast, there are few studies that analyze ihc slides using dl. therefore, we investigated the separate and joint performance of resnets trained on melana and corresponding h&e-stained slides. the melana classifier achieved an area under receiver operating characteristics curve (auroc) of 0.82 and 0.74 on out of distribution (ood)-datasets, similar to the h&e-based benchmark classification of 0.81 and 0.75, respectively. a combined classifier using melana and h&e achieved aurocs of 0.85 and 0.81 on the ood datasets. dl melana-based assistance systems show the same performance as the benchmark h&e classification and may be improved by multi stain classification to assist pathologists in their clinical routine.",10.1371/journal.pone.0297146,2023-09-07,2023-09-08,"['christoph wies', 'lucas schneider', 'sarah haggenmueller', 'tabea-clara bucher', 'sarah hobelsberger', 'markus v. heppt', 'gerardo ferrara', 'eva i. krieghoff-henning', 'titus j. brinker']"
2309.03561,trinary decision trees for handling missing data,stat.ml cs.lg,"this paper introduces the trinary decision tree, an algorithm designed to improve the handling of missing data in decision tree regressors and classifiers. unlike other approaches, the trinary decision tree does not assume that missing values contain any information about the response. both theoretical calculations on estimator bias and numerical illustrations using real data sets are presented to compare its performance with established algorithms in different missing data scenarios (missing completely at random (mcar), and informative missingness (im)). notably, the trinary tree outperforms its peers in mcar settings, especially when data is only missing out-of-sample, while lacking behind in im settings. a hybrid model, the trinarymia tree, which combines the trinary tree and the missing in attributes (mia) approach, shows robust performance in all types of missingness. despite the potential drawback of slower training speed, the trinary tree offers a promising and more accurate method of handling missing data in decision tree algorithms.",,2023-09-07,2024-01-11,['henning zakrisson']
2309.03842,early warning indicators via latent stochastic dynamical systems,stat.ml cs.lg,"detecting early warning indicators for abrupt dynamical transitions in complex systems or high-dimensional observation data is essential in many real-world applications, such as brain diseases, natural disasters, financial crises, and engineering reliability. to this end, we develop a novel approach: the directed anisotropic diffusion map that captures the latent evolutionary dynamics in the low-dimensional manifold. then three effective warning signals (onsager-machlup indicator, sample entropy indicator, and transition probability indicator) are derived through the latent coordinates and the latent stochastic dynamical systems. to validate our framework, we apply this methodology to authentic electroencephalogram (eeg) data. we find that our early warning indicators are capable of detecting the tipping point during state transition. this framework not only bridges the latent dynamics with real-world data but also shows the potential ability for automatic labeling on complex high-dimensional time series.",,2023-09-07,2023-12-29,"['lingyu feng', 'ting gao', 'wang xiao', 'jinqiao duan']"
2309.04452,postprocessing of ensemble weather forecasts using permutation-invariant   neural networks,stat.ml cs.lg physics.ao-ph,"statistical postprocessing is used to translate ensembles of raw numerical weather forecasts into reliable probabilistic forecast distributions. in this study, we examine the use of permutation-invariant neural networks for this task. in contrast to previous approaches, which often operate on ensemble summary statistics and dismiss details of the ensemble distribution, we propose networks that treat forecast ensembles as a set of unordered member forecasts and learn link functions that are by design invariant to permutations of the member ordering. we evaluate the quality of the obtained forecast distributions in terms of calibration and sharpness and compare the models against classical and neural network-based benchmark methods. in case studies addressing the postprocessing of surface temperature and wind gust forecasts, we demonstrate state-of-the-art prediction quality. to deepen the understanding of the learned inference process, we further propose a permutation-based importance analysis for ensemble-valued predictors, which highlights specific aspects of the ensemble forecast that are considered important by the trained postprocessing models. our results suggest that most of the relevant information is contained in a few ensemble-internal degrees of freedom, which may impact the design of future ensemble forecasting and postprocessing systems.",10.1175/aies-d-23-0070.1,2023-09-08,2024-01-18,"['kevin höhlein', 'benedikt schulz', 'rüdiger westermann', 'sebastian lerch']"
2309.04877,a gentle introduction to gradient-based optimization and variational   inequalities for machine learning,cs.lg stat.ml,"the rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. in these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. we provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. while we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.",,2023-09-09,2024-02-26,"['neha s. wadia', 'yatin dandi', 'michael i. jordan']"
2309.05030,"decolonial ai alignment: openness, vi\'{s}e\d{s}a-dharma, and including   excluded knowledges",cs.cy cs.ai stat.ml,"prior work has explicated the coloniality of artificial intelligence (ai) development and deployment through mechanisms such as extractivism, automation, sociological essentialism, surveillance, and containment. however, that work has not engaged much with alignment: teaching behaviors to a large language model (llm) in line with desired values, and has not considered a mechanism that arises within that process: moral absolutism -- a part of the coloniality of knowledge. colonialism has a history of altering the beliefs and values of colonized peoples; in this paper, i argue that this history is recapitulated in current llm alignment practices and technologies. furthermore, i suggest that ai alignment be decolonialized using three forms of openness: openness of models, openness to society, and openness to excluded knowledges. this suggested approach to decolonial ai alignment uses ideas from the argumentative moral philosophical tradition of hinduism, which has been described as an open-source religion. one concept used is vi\'{s}e\d{s}a-dharma, or particular context-specific notions of right and wrong. at the end of the paper, i provide a suggested reference architecture to work toward the proposed framework.",,2023-09-10,2024-01-22,['kush r. varshney']
2309.05092,adaptive conformal classification with noisy labels,stat.me cs.lg math.st stat.th,"this paper develops novel conformal prediction methods for classification tasks that can automatically adapt to random label contamination in the calibration sample, leading to more informative prediction sets with stronger coverage guarantees compared to state-of-the-art approaches. this is made possible by a precise characterization of the effective coverage inflation (or deflation) suffered by standard conformal inferences in the presence of label contamination, which is then made actionable through new calibration algorithms. our solution is flexible and can leverage different modeling assumptions about the label contamination process, while requiring no knowledge of the underlying data distribution or of the inner workings of the machine-learning classifier. the advantages of the proposed methods are demonstrated through extensive simulations and an application to object classification with the cifar-10h image data set.",,2023-09-10,2024-02-21,"['matteo sesia', 'y. x. rachel wang', 'xin tong']"
2309.06240,calibration in machine learning uncertainty quantification: beyond   consistency to target adaptivity,stat.ml cs.lg physics.chem-ph physics.data-an,"reliable uncertainty quantification (uq) in machine learning (ml) regression tasks is becoming the focus of many studies in materials and chemical science. it is now well understood that average calibration is insufficient, and most studies implement additional methods testing the conditional calibration with respect to uncertainty, i.e. consistency. consistency is assessed mostly by so-called reliability diagrams. there exists however another way beyond average calibration, which is conditional calibration with respect to input features, i.e. adaptivity. in practice, adaptivity is the main concern of the final users of a ml-uq method, seeking for the reliability of predictions and uncertainties for any point in features space. this article aims to show that consistency and adaptivity are complementary validation targets, and that a good consistency does not imply a good adaptivity. adapted validation methods are proposed and illustrated on a representative example.",10.1063/5.0174943,2023-09-12,2023-12-07,['pascal pernot']
2309.06429,efficient inference on high-dimensional linear models with missing   outcomes,stat.me math.st stat.ap stat.th,"this paper is concerned with inference on the regression function of a high-dimensional linear model when outcomes are missing at random. we propose an estimator which combines a lasso pilot estimate of the regression function with a bias correction term based on the weighted residuals of the lasso regression. the weights depend on estimates of the missingness probabilities (propensity scores) and solve a convex optimization program that trades off bias and variance optimally. provided that the propensity scores can be pointwise consistently estimated at in-sample data points, our proposed estimator for the regression function is asymptotically normal and semi-parametrically efficient among all asymptotically linear estimators. furthermore, the proposed estimator keeps its asymptotic properties even if the propensity scores are estimated by modern machine learning techniques. we validate the finite-sample performance of the proposed estimator through comparative simulation studies and the real-world problem of inferring the stellar masses of galaxies in the sloan digital sky survey.",,2023-09-12,2024-02-20,"['yikun zhang', 'alexander giessing', 'yen-chi chen']"
2309.06548,online infinite-dimensional regression: learning linear operators,stat.ml cs.lg,"we consider the problem of learning linear operators under squared loss between two infinite-dimensional hilbert spaces in the online setting. we show that the class of linear operators with uniformly bounded $p$-schatten norm is online learnable for any $p \in [1, \infty)$. on the other hand, we prove an impossibility result by showing that the class of uniformly bounded linear operators with respect to the operator norm is \textit{not} online learnable. moreover, we show a separation between sequential uniform convergence and online learnability by identifying a class of bounded linear operators that is online learnable but uniform convergence does not hold. finally, we prove that the impossibility result and the separation between uniform convergence and learnability also hold in the batch setting.",,2023-09-08,2024-01-24,"['vinod raman', 'unique subedi', 'ambuj tewari']"
2309.06991,unsupervised contrast-consistent ranking with language models,cs.lg cs.cl stat.ml,"language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. for instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank product reviews by sentiment. we compare pairwise, pointwise and listwise prompting techniques to elicit a language model's ranking knowledge. however, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. this motivates us to explore an alternative approach that is inspired by an unsupervised probing method called contrast-consistent search (ccs). the idea is to train a probe guided by a logical constraint: a language model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. we hypothesize that similar constraints apply to ranking tasks where all items are related via consistent, pairwise or listwise comparisons. to this end, we extend the binary ccs method to contrast-consistent ranking (ccr) by adapting existing ranking methods such as the max-margin loss, triplet loss and an ordinal regression objective. across different models and datasets, our results confirm that ccr probing performs better or, at least, on a par with prompting.",,2023-09-13,2024-02-03,"['niklas stoehr', 'pengxiang cheng', 'jing wang', 'daniel preotiuc-pietro', 'rajarshi bhowmik']"
2309.07703,causal entropy and information gain for measuring causal control,cs.lg cs.it math.it stat.ml,"artificial intelligence models and methods commonly lack causal interpretability. despite the advancements in interpretable machine learning (iml) methods, they frequently assign importance to features which lack causal influence on the outcome variable. selecting causally relevant features among those identified as relevant by these methods, or even before model training, would offer a solution. feature selection methods utilizing information theoretical quantities have been successful in identifying statistically relevant features. however, the information theoretical quantities they are based on do not incorporate causality, rendering them unsuitable for such scenarios. to address this challenge, this article proposes information theoretical quantities that incorporate the causal structure of the system, which can be used to evaluate causal importance of features for some given outcome variable. specifically, we introduce causal versions of entropy and mutual information, termed causal entropy and causal information gain, which are designed to assess how much control a feature provides over the outcome variable. these newly defined quantities capture changes in the entropy of a variable resulting from interventions on other variables. fundamental results connecting these quantities to the existence of causal effects are derived. the use of causal information gain in feature selection is demonstrated, highlighting its superiority over standard mutual information in revealing which features provide control over a chosen outcome variable. our investigation paves the way for the development of methods with improved interpretability in domains involving causation.",,2023-09-14,2024-01-26,"['francisco nunes ferreira quialheiro simoes', 'mehdi dastani', 'thijs van ommen']"
2309.08436,chunked attention-based encoder-decoder model for streaming speech   recognition,eess.as cs.sd stat.ml,"we study a streamable attention-based encoder-decoder model in which either the decoder, or both the encoder and decoder, operate on pre-defined, fixed-size windows called chunks. a special end-of-chunk (eoc) symbol advances from one chunk to the next chunk, effectively replacing the conventional end-of-sequence symbol. this modification, while minor, situates our model as equivalent to a transducer model that operates on chunks instead of frames, where eoc corresponds to the blank symbol. we further explore the remaining differences between a standard transducer and our model. additionally, we examine relevant aspects such as long-form speech generalization, beam size, and length normalization. through experiments on librispeech and ted-lium-v2, and by concatenating consecutive sequences for long-form trials, we find that our streamable model maintains competitive performance compared to the non-streamable variant and generalizes very well to long-form speech.",,2023-09-15,2024-01-17,"['mohammad zeineldeen', 'albert zeyer', 'ralf schlüter', 'hermann ney']"
2309.08783,quantifying predictive uncertainty of aphasia severity in stroke   patients with sparse heteroscedastic bayesian high-dimensional regression,stat.me stat.ml,"sparse linear regression methods for high-dimensional data commonly assume that residuals have constant variance, which can be violated in practice. for example, aphasia quotient (aq) is a critical measure of language impairment and informs treatment decisions, but it is challenging to measure in stroke patients. it is of interest to use high-resolution t2 neuroimages of brain damage to predict aq. however, sparse regression models show marked evidence of heteroscedastic error even after transformations are applied. this violation of the homoscedasticity assumption can lead to bias in estimated coefficients, prediction intervals (pi) with improper length, and increased type i errors. bayesian heteroscedastic linear regression models relax the homoscedastic error assumption but can enforce restrictive prior assumptions on parameters, and many are computationally infeasible in the high-dimensional setting. this paper proposes estimating high-dimensional heteroscedastic linear regression models using a heteroscedastic partitioned empirical bayes expectation conditional maximization (h-probe) algorithm. h-probe is a computationally efficient maximum a posteriori estimation approach that requires minimal prior assumptions and can incorporate covariates hypothesized to impact heterogeneity. we apply this method by using high-dimensional neuroimages to predict and provide pis for aq that accurately quantify predictive uncertainty. our analysis demonstrates that h-probe can provide narrower pi widths than standard methods without sacrificing coverage. narrower pis are clinically important for determining the risk of moderate to severe aphasia. additionally, through extensive simulation studies, we exhibit that h-probe results in superior prediction, variable selection, and predictive inference compared to alternative methods.",,2023-09-15,2024-01-22,"['anja zgodic', 'ray bai', 'jiajia zhang', 'yuan wang', 'chris rorden', 'alexander mclain']"
2309.09129,$l^1$ estimation: on the optimality of linear estimators,math.st cs.it math.it stat.ml stat.th,"consider the problem of estimating a random variable $x$ from noisy observations $y = x+ z$, where $z$ is standard normal, under the $l^1$ fidelity criterion. it is well known that the optimal bayesian estimator in this setting is the conditional median. this work shows that the only prior distribution on $x$ that induces linearity in the conditional median is gaussian.   along the way, several other results are presented. in particular, it is demonstrated that if the conditional distribution $p_{x|y=y}$ is symmetric for all $y$, then $x$ must follow a gaussian distribution. additionally, we consider other $l^p$ losses and observe the following phenomenon: for $p \in [1,2]$, gaussian is the only prior distribution that induces a linear optimal bayesian estimator, and for $p \in (2,\infty)$, infinitely many prior distributions on $x$ can induce linearity. finally, extensions are provided to encompass noise models leading to conditional distributions from certain exponential families.",,2023-09-16,2024-01-09,"['leighton p. barnes', 'alex dytso', 'jingbo liu', 'h. vincent poor']"
2309.09222,data-driven modeling and inference for bayesian gaussian process odes   via double normalizing flows,cs.lg stat.ml,"recently, gaussian processes have been used to model the vector field of continuous dynamical systems, referred to as gpodes, which are characterized by a probabilistic ode equation. bayesian inference for these models has been extensively studied and applied in tasks such as time series prediction. however, the use of standard gps with basic kernels like squared exponential kernels has been common in gpode research, limiting the model's ability to represent complex scenarios. to address this limitation, we introduce normalizing flows to reparameterize the ode vector field, resulting in a data-driven prior distribution, thereby increasing flexibility and expressive power. we develop a data-driven variational learning algorithm that utilizes analytically tractable probability density functions of normalizing flows, enabling simultaneous learning and inference of unknown continuous dynamics. additionally, we also apply normalizing flows to the posterior inference of gp odes to resolve the issue of strong mean-field assumptions in posterior inference. by applying normalizing flows in both these ways, our model improves accuracy and uncertainty estimates for bayesian gaussian process odes. we validate the effectiveness of our approach on simulated dynamical systems and real-world human motion data, including time series prediction and missing data recovery tasks. experimental results show that our proposed method effectively captures model uncertainty while improving accuracy.",,2023-09-17,2024-01-02,"['jian xu', 'shian du', 'junmei yang', 'xinghao ding', 'john paisley', 'delu zeng']"
2309.09814,convolutional deep kernel machines,stat.ml cs.lg,"standard infinite-width limits of neural networks sacrifice the ability for intermediate layers to learn representations from data. recent work (a theory of representation learning gives a deep generalisation of kernel methods, yang et al. 2023) modified the neural network gaussian process (nngp) limit of bayesian neural networks so that representation learning is retained. furthermore, they found that applying this modified limit to a deep gaussian process gives a practical learning algorithm which they dubbed the deep kernel machine (dkm). however, they only considered the simplest possible setting: regression in small, fully connected networks with e.g. 10 input features. here, we introduce convolutional deep kernel machines. this required us to develop a novel inter-domain inducing point approximation, as well as introducing and experimentally assessing a number of techniques not previously seen in dkms, including analogues to batch normalisation, different likelihoods, and different types of top-layer. the resulting model trains in roughly 77 gpu hours, achieving around 99% test accuracy on mnist, 72% on cifar-100, and 92.7% on cifar-10, which is sota for kernel methods.",,2023-09-18,2024-02-26,"['edward milsom', 'ben anson', 'laurence aitchison']"
2309.10140,a geometric framework for neural feature learning,cs.lg stat.ml,"we present a novel framework for learning system design based on neural feature extractors. first, we introduce the feature geometry, which unifies statistical dependence and features in the same function space with geometric structures. by applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. we propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. to demonstrate the applications of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.",,2023-09-18,2024-01-23,"['xiangxiang xu', 'lizhong zheng']"
2309.10688,on the different regimes of stochastic gradient descent,cs.lg cond-mat.dis-nn stat.ml,"modern deep networks are trained with stochastic gradient descent (sgd) whose key hyperparameters are the number of data considered at each step or batch size $b$, and the step size or learning rate $\eta$. for small $b$ and large $\eta$, sgd corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the ''temperature'' $t\equiv \eta/b$. yet this description is observed to break down for sufficiently large batches $b\geq b^*$, or simplifies to gradient descent (gd) when the temperature is sufficiently small. understanding where these cross-overs take place remains a central challenge. here, we resolve these questions for a teacher-student perceptron classification model and show empirically that our key predictions still apply to deep networks. specifically, we obtain a phase diagram in the $b$-$\eta$ plane that separates three dynamical phases: (i) a noise-dominated sgd governed by temperature, (ii) a large-first-step-dominated sgd and (iii) gd. these different phases also correspond to different regimes of generalization error. remarkably, our analysis reveals that the batch size $b^*$ separating regimes (i) and (ii) scale with the size $p$ of the training set, with an exponent that characterizes the hardness of the classification problem.",10.1073/pnas.2316301121,2023-09-19,2024-02-27,"['antonio sclocchi', 'matthieu wyart']"
2309.11713,quasi-monte carlo for 3d sliced wasserstein,stat.ml cs.gr cs.lg,"monte carlo (mc) integration has been employed as the standard approximation method for the sliced wasserstein (sw) distance, whose analytical expression involves an intractable expectation. however, mc integration is not optimal in terms of absolute approximation error. to provide a better class of empirical sw, we propose quasi-sliced wasserstein (qsw) approximations that rely on quasi-monte carlo (qmc) methods. for a comprehensive investigation of qmc for sw, we focus on the 3d setting, specifically computing the sw between probability measures in three dimensions. in greater detail, we empirically evaluate various methods to construct qmc point sets on the 3d unit-hypersphere, including the gaussian-based and equal area mappings, generalized spiral points, and optimizing discrepancy energies. furthermore, to obtain an unbiased estimator for stochastic optimization, we extend qsw to randomized quasi-sliced wasserstein (rqsw) by introducing randomness in the discussed point sets. theoretically, we prove the asymptotic convergence of qsw and the unbiasedness of rqsw. finally, we conduct experiments on various 3d tasks, such as point-cloud comparison, point-cloud interpolation, image style transfer, and training deep point-cloud autoencoders, to demonstrate the favorable performance of the proposed qsw and rqsw variants.",,2023-09-20,2024-02-16,"['khai nguyen', 'nicola bariletto', 'nhat ho']"
2309.13211,interpretable structural model error discovery from sparse assimilation   increments using spectral bias-reduced neural networks: a quasi-geostrophic   turbulence test case,physics.comp-ph physics.ao-ph physics.data-an stat.co,"earth system models suffer from various structural and parametric errors in their representation of nonlinear, multi-scale processes, leading to uncertainties in their long-term projections. the effects of many of these errors (particularly those due to fast physics) can be quantified in short-term simulations, e.g., as differences between the predicted and observed states (analysis increments). with the increase in the availability of high-quality observations and simulations, learning nudging from these increments to correct model errors has become an active research area. however, most studies focus on using neural networks, which while powerful, are hard to interpret, are data-hungry, and poorly generalize out-of-distribution. here, we show the capabilities of model error discovery with interpretability and data assimilation (medida), a general, data-efficient framework that uses sparsity-promoting equation-discovery techniques to learn model errors from analysis increments. using two-layer quasi-geostrophic turbulence as the test case, medida is shown to successfully discover various linear and nonlinear structural/parametric errors when full observations are available. discovery from spatially sparse observations is found to require highly accurate interpolation schemes. while nns have shown success as interpolators in recent studies, here, they are found inadequate due to their inability to accurately represent small scales, a phenomenon known as spectral bias. we show that a general remedy, adding a random fourier feature layer to the nn, resolves this issue enabling medida to successfully discover model errors from sparse observations. these promising results suggest that with further development, medida could be scaled up to models of the earth system and real observations.",10.1029/2023ms004033,2023-09-22,2024-02-15,"['rambod mojgani', 'ashesh chattopadhyay', 'pedram hassanzadeh']"
2309.13349,speeding-up evolutionary algorithms to solve black-box optimization   problems,cs.ne stat.ml,"population-based evolutionary algorithms are often considered when approaching computationally expensive black-box optimization problems. they employ a selection mechanism to choose the best solutions from a given population after comparing their objective values, which are then used to generate the next population. this iterative process explores the solution space efficiently, leading to improved solutions over time. however, these algorithms require a large number of evaluations to provide a quality solution, which might be computationally expensive when the evaluation cost is high. in some cases, it is possible to replace the original objective function with a less accurate approximation of lower cost. this introduces a trade-off between the evaluation cost and its accuracy.   in this paper, we propose a technique capable of choosing an appropriate approximate function cost during the execution of the optimization algorithm. the proposal finds the minimum evaluation cost at which the solutions are still properly ranked, and consequently, more evaluations can be computed in the same amount of time with minimal accuracy loss. an experimental section on four very different problems reveals that the proposed approach can reach the same objective value in less than half of the time in certain cases.",,2023-09-23,2024-01-29,"['judith echevarrieta', 'etor arza', 'aritz pérez']"
2309.13598,on the posterior distribution in denoising: application to uncertainty   quantification,cs.cv cs.lg stat.ml,"denoisers play a central role in many applications, from noise suppression in low-grade imaging sensors, to empowering score-based generative models. the latter category of methods makes use of tweedie's formula, which links the posterior mean in gaussian denoising (\ie the minimum mse denoiser) with the score of the data distribution. here, we derive a fundamental relation between the higher-order central moments of the posterior distribution, and the higher-order derivatives of the posterior mean. we harness this result for uncertainty quantification of pre-trained denoisers. particularly, we show how to efficiently compute the principal components of the posterior distribution for any desired region of an image, as well as to approximate the full marginal distribution along those (or any other) one-dimensional directions. our method is fast and memory-efficient, as it does not explicitly compute or store the high-order moment tensors and it requires no training or fine tuning of the denoiser. code and examples are available on the project webpage in https://hilamanor.github.io/gaussiandenoisingposterior/ .",,2023-09-24,2024-02-19,"['hila manor', 'tomer michaeli']"
2309.13850,statistical perspective of top-k sparse softmax gating mixture of   experts,stat.ml cs.lg,"top-k sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. the main challenge comes from the structure of the top-k sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. by focusing on a gaussian mixture of experts, we establish theoretical results on the effects of the top-k sparse softmax gating function on both density and parameter estimations. our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. when the true number of experts $k_{\ast}$ is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. however, when $k_{\ast}$ becomes unknown and the true model is over-specified by a gaussian mixture of $k$ experts where $k > k_{\ast}$, our findings suggest that the number of experts selected from the top-k sparse softmax gating function must exceed the total cardinality of a certain number of voronoi cells associated with the true parameters to guarantee the convergence of the density estimation. moreover, while the density estimation rate remains parametric under this setting, the parameter estimation rates become substantially slow due to an intrinsic interaction between the softmax gating and expert functions.",,2023-09-24,2024-02-23,"['huy nguyen', 'pedram akbarian', 'fanqi yan', 'nhat ho']"
2309.13915,sample complexity of neural policy mirror descent for policy   optimization on low-dimensional manifolds,cs.lg stat.ml,"policy gradient methods equipped with deep neural networks have achieved great success in solving high-dimensional reinforcement learning (rl) problems. however, current analyses cannot explain why they are resistant to the curse of dimensionality. in this work, we study the sample complexity of the neural policy mirror descent (npmd) algorithm with deep convolutional neural networks (cnn). motivated by the empirical observation that many high-dimensional environments have state spaces possessing low-dimensional structures, such as those taking images as states, we consider the state space to be a $d$-dimensional manifold embedded in the $d$-dimensional euclidean space with intrinsic dimension $d\ll d$. we show that in each iteration of npmd, both the value function and the policy can be well approximated by cnns. the approximation errors are controlled by the size of the networks, and the smoothness of the previous networks can be inherited. as a result, by properly choosing the network size and hyperparameters, npmd can find an $\epsilon$-optimal policy with $\widetilde{o}(\epsilon^{-\frac{d}{\alpha}-2})$ samples in expectation, where $\alpha\in(0,1]$ indicates the smoothness of environment. compared to previous work, our result exhibits that npmd can leverage the low-dimensional structure of state space to escape from the curse of dimensionality, explaining the efficacy of deep policy gradient algorithms.",10.48550/arxiv.2309.13915,2023-09-25,2024-01-14,"['zhenghao xu', 'xiang ji', 'minshuo chen', 'mengdi wang', 'tuo zhao']"
2309.16044,improving adaptive online learning using refined discretization,cs.lg stat.ml,"we study unconstrained online linear optimization with lipschitz losses. motivated by the pursuit of instance optimality, we propose a new algorithm that simultaneously achieves ($i$) the adagrad-style second order gradient adaptivity; and ($ii$) the comparator norm adaptivity also known as ""parameter freeness"" in the literature. in particular,   - our algorithm does not employ the impractical doubling trick, and does not require an a priori estimate of the time-uniform lipschitz constant;   - the associated regret bound has the optimal $o(\sqrt{v_t})$ dependence on the gradient variance $v_t$, without the typical logarithmic multiplicative factor;   - the leading constant in the regret bound is ""almost"" optimal.   central to these results is a continuous time approach to online learning. we first show that the aimed simultaneous adaptivity can be achieved fairly easily in a continuous time analogue of the problem, where the environment is modeled by an arbitrary continuous semimartingale. then, our key innovation is a new discretization argument that preserves such adaptivity in the discrete time adversarial setting. this refines a non-gradient-adaptive discretization argument from (harvey et al., 2023), both algorithmically and analytically, which could be of independent interest.",,2023-09-27,2024-02-22,"['zhiyu zhang', 'heng yang', 'ashok cutkosky', 'ioannis ch. paschalidis']"
2309.16076,bayesian cram\'er-rao bound estimation with score-based models,stat.ml math.st stat.th,"the bayesian cram\'er-rao bound (crb) provides a lower bound on the error of any bayesian estimator under mild regularity conditions. it can be used to benchmark the performance of estimators, and provides a principled design metric for guiding system design and optimization. however, the bayesian crb depends on the prior distribution, which is often unknown for many problems of interest. this work develops a new data-driven estimator for the bayesian crb using score matching, a statistical estimation technique, to model the prior distribution. the performance of the estimator is analyzed in both the classical parametric modeling regime and the neural network modeling regime. in both settings, we develop novel non-asymptotic bounds on the score matching error and our bayesian crb estimator. our proofs build on results from empirical process theory, including classical bounds and recently introduced techniques for characterizing neural networks, to address the challenges of bounding the score matching error. the performance of the estimator is illustrated empirically on a denoising problem example with a gaussian mixture prior.",,2023-09-27,,"['evan scope crafts', 'xianyang zhang', 'bo zhao']"
2309.16597,transfer learning for bayesian optimization on heterogeneous search   spaces,cs.lg cs.ai stat.ml,"bayesian optimization (bo) is a popular black-box function optimization method, which makes sequential decisions based on a bayesian model, typically a gaussian process (gp), of the function. to ensure the quality of the model, transfer learning approaches have been developed to automatically design gp priors by learning from observations on ""training"" functions. these training functions are typically required to have the same domain as the ""test"" function (black-box function to be optimized). in this paper, we introduce mphd, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical gps. mphd can be seamlessly integrated with bo to transfer knowledge across heterogeneous search spaces. our theoretical and empirical results demonstrate the validity of mphd and its superior performance on challenging black-box function optimization tasks.",,2023-09-28,2024-02-13,"['zhou fan', 'xinran han', 'zi wang']"
2309.16598,cross-prediction-powered inference,stat.ml cs.lg stat.me,"while reliable data-driven decision-making hinges on high-quality labeled data, the acquisition of quality labels often involves laborious human annotations or slow and expensive scientific measurements. machine learning is becoming an appealing alternative as sophisticated predictive techniques are being used to quickly and cheaply produce large amounts of predicted labels; e.g., predicted protein structures are used to supplement experimentally derived structures, predictions of socioeconomic indicators from satellite imagery are used to supplement accurate survey data, and so on. since predictions are imperfect and potentially biased, this practice brings into question the validity of downstream inferences. we introduce cross-prediction: a method for valid inference powered by machine learning. with a small labeled dataset and a large unlabeled dataset, cross-prediction imputes the missing labels via machine learning and applies a form of debiasing to remedy the prediction inaccuracies. the resulting inferences achieve the desired error probability and are more powerful than those that only leverage the labeled data. closely related is the recent proposal of prediction-powered inference, which assumes that a good pre-trained model is already available. we show that cross-prediction is consistently more powerful than an adaptation of prediction-powered inference in which a fraction of the labeled data is split off and used to train the model. finally, we observe that cross-prediction gives more stable conclusions than its competitors; its confidence intervals typically have significantly lower variability.",,2023-09-28,2024-02-28,"['tijana zrnic', 'emmanuel j. candès']"
2309.16746,implicit gaussian process representation of vector fields over arbitrary   latent manifolds,cs.lg cs.ms physics.data-an q-bio.qm stat.ml,"gaussian processes (gps) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. recent works have extended gps to model scalar and vector quantities distributed over non-euclidean domains, including smooth manifolds appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. however, these approaches assume that the manifold underlying the data is known, limiting their practical utility. we introduce rvgp, a generalisation of gps for learning vector signals over latent riemannian manifolds. our method uses positional encoding with eigenfunctions of the connection laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. we demonstrate that rvgp possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. furthermore, we use rvgp to reconstruct high-density neural dynamics derived from low-density eeg recordings in healthy individuals and alzheimer's patients. we show that vector field singularities are important disease markers and that their reconstruction leads to a comparable classification accuracy of disease states to high-density recordings. thus, our method overcomes a significant practical limitation in experimental and clinical applications.",,2023-09-28,2024-01-17,"['robert l. peach', 'matteo vinao-carl', 'nir grossman', 'michael david', 'emma mallas', 'david sharp', 'paresh a. malhotra', 'pierre vandergheynst', 'adam gosztolai']"
2309.16779,intriguing properties of generative classifiers,cs.cv cs.ai cs.lg q-bio.nc stat.ml,"what is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? we build on recent advances in generative modeling that turn text-to-image models into classifiers. this allows us to study their behavior and to compare them against discriminative models and human psychophysical data. we report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.",,2023-09-28,2024-02-14,"['priyank jaini', 'kevin clark', 'robert geirhos']"
2309.16965,controlling continuous relaxation for combinatorial optimization,stat.ml cs.lg stat.co stat.me,"motivated by developments in machine learning technologies, unsupervised learning (ul)-based solvers for co problems have recently been proposed. these solvers train a neural network that outputs a solution by optimizing the co objective directly. ul-based solvers have several advantages over traditional methods. however, various studies have shown that these solvers underperform compared to greedy algorithms for complex co problems. in addition, these solvers employ a continuous relaxation strategy; thus, post-learning rounding from the continuous space back to the original discrete space is required, undermining the robustness of the results. to address these problems, we propose the continuous relaxation annealing (cra) strategy. the cra introduces a penalty term to control the continuity and discreteness of the relaxed variables and eliminate local optima. in addition, the cra implements an annealing process for the penalty term that initially prioritizes continuous solutions and progressively transitions towards discreet solutions until the relaxed variables become nearly discrete, eliminating the artificial rounding. experimental results demonstrate that the cra significantly enhances the ul-based solvers, outperforming both existing ul-based solvers and greedy algorithms for complex co problems.",,2023-09-29,2024-02-03,['yuma ichikawa']
2309.17016,efficient agnostic learning with average smoothness,cs.lg math.st stat.ml stat.th,"we study distribution-free nonparametric regression following a notion of average smoothness initiated by ashlagi et al. (2021), which measures the ""effective"" smoothness of a function with respect to an arbitrary unknown underlying distribution. while the recent work of hanneke et al. (2023) established tight uniform convergence bounds for average-smooth functions in the realizable case and provided a computationally efficient realizable learning algorithm, both of these results currently lack analogs in the general agnostic (i.e. noisy) case.   in this work, we fully close these gaps. first, we provide a distribution-free uniform convergence bound for average-smoothness classes in the agnostic setting. second, we match the derived sample complexity with a computationally efficient agnostic learning algorithm. our results, which are stated in terms of the intrinsic geometry of the data and hold over any totally bounded metric space, show that the guarantees recently obtained for realizable learning of average-smooth functions transfer to the agnostic setting. at the heart of our proof, we establish the uniform convergence rate of a function class in terms of its bracketing entropy, which may be of independent interest.",,2023-09-29,2024-02-13,"['steve hanneke', 'aryeh kontorovich', 'guy kornowski']"
2309.17371,adversarial imitation learning from visual observations using latent   information,cs.lg cs.sy eess.sy stat.ml,"we focus on the problem of imitation learning from visual observations, where the learning agent has access to videos of experts as its sole learning source. the challenges of this framework include the absence of expert actions and the partial observability of the environment, as the ground-truth states can only be inferred from pixels. to tackle this problem, we first conduct a theoretical analysis of imitation learning in partially observable environments. we establish upper bounds on the suboptimality of the learning agent with respect to the divergence between the expert and the agent latent state-transition distributions. motivated by this analysis, we introduce an algorithm called latent adversarial imitation from observations, which combines off-policy adversarial imitation techniques with a learned latent representation of the agent's state from sequences of observations. in experiments on high-dimensional continuous robotic tasks, we show that our algorithm matches state-of-the-art performance while providing significant computational advantages. additionally, we show how our method can be used to improve the efficiency of reinforcement learning from pixels by leveraging expert videos. to ensure reproducibility, we provide free access to our code.",,2023-09-29,2024-01-23,"['vittorio giammarino', 'james queeney', 'ioannis ch. paschalidis']"
2310.00027,out-of-domain unlabeled data improves generalization,stat.ml cs.lg,"we propose a novel framework for incorporating unlabeled data into semi-supervised classification problems, where scenarios involving the minimization of either i) adversarially robust or ii) non-robust loss functions have been considered. notably, we allow the unlabeled samples to deviate slightly (in total variation sense) from the in-domain distribution. the core idea behind our framework is to combine distributionally robust optimization (dro) with self-supervised training. as a result, we also leverage efficient polynomial-time algorithms for the training stage. from a theoretical standpoint, we apply our framework on the classification problem of a mixture of two gaussians in $\mathbb{r}^d$, where in addition to the $m$ independent and labeled samples from the true distribution, a set of $n$ (usually with $n\gg m$) out of domain and unlabeled samples are given as well. using only the labeled data, it is known that the generalization error can be bounded by $\propto\left(d/m\right)^{1/2}$. however, using our method on both isotropic and non-isotropic gaussian mixture models, one can derive a new set of analytically explicit and non-asymptotic bounds which show substantial improvement on the generalization error compared to erm. our results underscore two significant insights: 1) out-of-domain samples, even when unlabeled, can be harnessed to narrow the generalization gap, provided that the true data distribution adheres to a form of the ``cluster assumption"", and 2) the semi-supervised learning paradigm can be regarded as a special case of our framework when there are no distributional shifts. we validate our claims through experiments conducted on a variety of synthetic and real-world datasets.",,2023-09-28,2024-02-15,"['amir hossein saberi', 'amir najafi', 'alireza heidari', 'mohammad hosein movasaghinia', 'abolfazl motahari', 'babak h. khalaj']"
2310.00386,order-preserving gflownets,cs.lg cs.ai stat.ml,"generative flow networks (gflownets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. however, gflownets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly accessible, in the case of multi-objective optimization (moo) tasks for example. moreover, to prioritize identifying high-reward candidates, the conventional practice is to raise the reward to a higher exponent, the optimal choice of which may vary across different environments. to address these issues, we propose order-preserving gflownets (op-gfns), which sample with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates, thus eliminating the need for an explicit formulation of the reward function. we theoretically prove that the training process of op-gfns gradually sparsifies the learned reward landscape in single-objective maximization tasks. the sparsification concentrates on candidates of a higher hierarchy in the ordering, ensuring exploration at the beginning and exploitation towards the end of the training. we demonstrate op-gfn's state-of-the-art performance in single-objective maximization (totally ordered) and multi-objective pareto front approximation (partially ordered) tasks, including synthetic datasets, molecule generation, and neural architecture search.",,2023-09-30,2024-02-25,"['yihang chen', 'lukas mauch']"
2310.00692,a theoretical analysis of noise geometry in stochastic gradient descent,cs.lg stat.ml,"in this paper, we provide a theoretical study of noise geometry for minibatch stochastic gradient descent (sgd), a phenomenon where noise aligns favorably with the geometry of local landscape. we propose two metrics, derived from analyzing how noise influences the loss and subspace projection dynamics, to quantify the alignment strength. we show that for (over-parameterized) linear models and two-layer nonlinear networks, when measured by these metrics, the alignment can be provably guaranteed under conditions independent of the degree of over-parameterization. to showcase the utility of our noise geometry characterizations, we present a refined analysis of the mechanism by which sgd escapes from sharp minima. we reveal that unlike gradient descent (gd), which escapes along the sharpest directions, sgd tends to escape from flatter directions and cyclical learning rates can exploit this sgd characteristic to navigate more effectively towards flatter regions. lastly, extensive experiments are provided to support our theoretical findings.",,2023-10-01,2024-02-01,"['mingze wang', 'lei wu']"
2310.00806,bayesian design principles for frequentist sequential learning,cs.lg math.oc math.st stat.th,"we develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified bayesian principles. we propose a novel optimization approach to generate ""algorithmic beliefs"" at each round, and use bayesian posteriors to make decisions. the optimization objective to create ""algorithmic beliefs,"" which we term ""algorithmic information ratio,"" represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. to the best of our knowledge, this is the first systematical approach to make bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. moreover, the algorithms are simple and often efficient to implement. as a major application, we present a novel algorithm for multi-armed bandits that achieves the ""best-of-all-worlds"" empirical performance in the stochastic, adversarial, and non-stationary environments. and we illustrate how these principles can be used in linear bandits, bandit convex optimization, and reinforcement learning.",,2023-10-01,2024-02-08,"['yunbei xu', 'assaf zeevi']"
2310.01105,energy-guided continuous entropic barycenter estimation for general   costs,cs.lg stat.ml,"optimal transport (ot) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. in short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given ot discrepancies. we propose a novel algorithm for approximating the continuous entropic ot (eot) barycenter for arbitrary ot cost functions. our approach is built upon the dual reformulation of the eot problem based on weak ot, which has recently gained the attention of the ml community. beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seemlessly interconnects with the energy-based models (ebms) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. for validation, we consider several low-dimensional scenarios and image-space setups, including non-euclidean cost functions. furthermore, we investigate the practical task of learning the barycenter on an image manifold generated by a pretrained generative model, opening up new directions for real-world applications.",,2023-10-02,2024-02-06,"['alexander kolesov', 'petr mokrov', 'igor udovichenko', 'milena gazdieva', 'gudmund pammer', 'anastasis kratsios', 'evgeny burnaev', 'alexander korotin']"
2310.01107,ground-a-video: zero-shot grounded video editing using text-to-image   diffusion models,cs.cv cs.ai cs.lg stat.ml,"recent endeavors in video editing have showcased promising results in single-attribute editing or style transfer tasks, either by training text-to-video (t2v) models on text-video data or adopting training-free methods. however, when confronted with the complexities of multi-attribute editing scenarios, they exhibit shortcomings such as omitting or overlooking intended attribute changes, modifying the wrong elements of the input video, and failing to preserve regions of the input video that should remain intact. to address this, here we present a novel grounding-guided video-to-video translation framework called ground-a-video for multi-attribute video editing. ground-a-video attains temporally consistent multi-attribute editing of input videos in a training-free manner without aforementioned shortcomings. central to our method is the introduction of cross-frame gated attention which incorporates groundings information into the latent representations in a temporally consistent fashion, along with modulated cross-attention and optical flow guided inverted latents smoothing. extensive experiments and applications demonstrate that ground-a-video's zero-shot capacity outperforms other baseline methods in terms of edit-accuracy and frame consistency. further results and code are available at our project page (http://ground-a-video.github.io).",,2023-10-02,2024-02-24,"['hyeonho jeong', 'jong chul ye']"
2310.01202,unified uncertainty calibration,stat.ml cs.lg,"to build robust, fair, and safe ai systems, we would like our classifiers to say ``i don't know'' when facing test examples that are difficult or fall outside of the training classes.the ubiquitous strategy to predict under uncertainty is the simplistic \emph{reject-or-classify} rule: abstain from prediction if epistemic uncertainty is high, classify otherwise.unfortunately, this recipe does not allow different sources of uncertainty to communicate with each other, produces miscalibrated predictions, and it does not allow to correct for misspecifications in our uncertainty estimates. to address these three issues, we introduce \emph{unified uncertainty calibration (u2c)}, a holistic framework to combine aleatoric and epistemic uncertainties. u2c enables a clean learning-theoretical analysis of uncertainty estimation, and outperforms reject-or-classify across a variety of imagenet benchmarks. our code is available at: https://github.com/facebookresearch/unifieduncertaintycalibration",,2023-10-02,2024-01-18,"['kamalika chaudhuri', 'david lopez-paz']"
2310.01236,mirror diffusion models for constrained and watermarked generation,stat.ml cs.cv cs.lg,"modern successes of diffusion models in learning complex, high-dimensional data distributions are attributed, in part, to their capability to construct diffusion processes with analytic transition kernels and score functions. the tractability results in a simulation-free framework with stable regression losses, from which reversed, generative processes can be learned at scale. however, when data is confined to a constrained set as opposed to a standard euclidean space, these desirable characteristics appear to be lost based on prior attempts. in this work, we propose mirror diffusion models (mdm), a new class of diffusion models that generate data on convex constrained sets without losing any tractability. this is achieved by learning diffusion processes in a dual space constructed from a mirror map, which, crucially, is a standard euclidean space. we derive efficient computation of mirror maps for popular constrained sets, such as simplices and $\ell_2$-balls, showing significantly improved performance of mdm over existing methods. for safety and privacy purposes, we also explore constrained sets as a new mechanism to embed invisible but quantitative information (i.e., watermarks) in generated data, for which mdm serves as a compelling approach. our work brings new algorithmic opportunities for learning tractable diffusion on complex domains. our code is available at https://github.com/ghliu/mdm",,2023-10-02,2024-02-29,"['guan-horng liu', 'tianrong chen', 'evangelos a. theodorou', 'molei tao']"
2310.01262,non-exchangeable conformal risk control,cs.lg stat.ml,"split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. while the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. in parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best $f_1$-score or minimizing the false negative rate in expectation. in this paper, we leverage and extend these two lines of work by proposing non-exchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. our framework is flexible, makes very few assumptions, and allows weighting the data based on its relevance for a given test example; a careful choice of weights may result on tighter bounds, making our framework useful in the presence of change points, time series, or other forms of distribution drift. experiments with both synthetic and real world data show the usefulness of our method.",,2023-10-02,2024-01-26,"['antónio farinhas', 'chrysoula zerva', 'dennis ulmer', 'andré f. t. martins']"
2310.02258,a neural scaling law from lottery ticket ensembling,cs.lg cs.ai physics.data-an stat.ml,"neural scaling laws (nsl) refer to the phenomenon where model performance improves with scale. sharma & kaplan analyzed nsl using approximation theory and predict that mse losses decay as $n^{-\alpha}$, $\alpha=4/d$, where $n$ is the number of model parameters, and $d$ is the intrinsic input dimension. although their theory works well for some cases (e.g., relu networks), we surprisingly find that a simple 1d problem $y=x^2$ manifests a different scaling law ($\alpha=1$) from their predictions ($\alpha=4$). we opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more ""lottery tickets"", which are ensembled to reduce the variance of outputs. we support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. we attribute the $n^{-1}$ scaling law to the ""central limit theorem"" of lottery tickets. finally, we discuss its potential implications for large language models and statistical physics-type theories of learning.",,2023-10-03,2024-02-01,"['ziming liu', 'max tegmark']"
2310.02823,learning to scale logits for temperature-conditional gflownets,cs.lg stat.ml,"gflownets are probabilistic models that sequentially generate compositional structures through a stochastic policy. among gflownets, temperature-conditional gflownets can introduce temperature-based controllability for exploration and exploitation. we propose \textit{logit-scaling gflownets} (logit-gfn), a novel architectural design that greatly accelerates the training of temperature-conditional gflownets. it is based on the idea that previously proposed approaches introduced numerical challenges in the deep network training, since different temperatures may give rise to very different gradient profiles as well as magnitudes of the policy's logits. we find that the challenge is greatly reduced if a learned function of the temperature is used to scale the policy's logits directly. also, using logit-gfn, gflownets can be improved by having better generalization capabilities in offline learning and mode discovery capabilities in online learning, which is empirically verified in various biological and chemical tasks. our code is available at \url{https://github.com/dbsxodud-11/logit-gfn}",,2023-10-04,2024-02-04,"['minsu kim', 'joohwan ko', 'taeyoung yun', 'dinghuai zhang', 'ling pan', 'woochang kim', 'jinkyoo park', 'emmanuel bengio', 'yoshua bengio']"
2310.02984,scaling laws for associative memories,stat.ml cs.ai cs.cl cs.lg cs.ne,"learning arguably involves the discovery and memorization of abstract rules. the aim of this paper is to study associative memory mechanisms. our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. we derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. we provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.",,2023-10-04,2024-02-20,"['vivien cabannes', 'elvis dohmatob', 'alberto bietti']"
2310.03253,molecule design by latent prompt transformer,cs.lg q-bio.bm stat.ml,"this paper proposes a latent prompt transformer model for solving challenging optimization problems such as molecule design, where the goal is to find molecules with optimal values of a target chemical or biological property that can be computed by an existing software. our proposed model consists of three components. (1) a latent vector whose prior distribution is modeled by a unet transformation of a gaussian white noise vector. (2) a molecule generation model that generates the string-based representation of molecule conditional on the latent vector in (1). we adopt the causal transformer model that takes the latent vector in (1) as prompt. (3) a property prediction model that predicts the value of the target property of a molecule based on a non-linear regression on the latent vector in (1). we call the proposed model the latent prompt transformer model. after initial training of the model on existing molecules and their property values, we then gradually shift the model distribution towards the region that supports desired values of the target property for the purpose of molecule design. our experiments show that our proposed model achieves state of the art performances on several benchmark molecule design tasks.",,2023-10-04,2024-02-05,"['deqian kong', 'yuhao huang', 'jianwen xie', 'ying nian wu']"
2310.03258,assessing electricity service unfairness with transfer counterfactual   learning,cs.lg stat.me,"energy justice is a growing area of interest in interdisciplinary energy research. however, identifying systematic biases in the energy sector remains challenging due to confounding variables, intricate heterogeneity in counterfactual effects, and limited data availability. first, this paper demonstrates how one can evaluate counterfactual unfairness in a power system by analyzing the average causal effect of a specific protected attribute. subsequently, we use subgroup analysis to handle model heterogeneity and introduce a novel method for estimating counterfactual unfairness based on transfer learning, which helps to alleviate the data scarcity in each subgroup. in our numerical analysis, we apply our method to a unique large-scale customer-level power outage data set and investigate the counterfactual effect of demographic factors, such as income and age of the population, on power outage durations. our results indicate that low-income and elderly-populated areas consistently experience longer power outages under both daily and post-disaster operations, and such discrimination is exacerbated under severe conditions. these findings suggest a widespread, systematic issue of injustice in the power service systems and emphasize the necessity for focused interventions in disadvantaged communities.",,2023-10-04,2024-01-24,"['song wei', 'xiangrui kong', 'alinson santos xavier', 'shixiang zhu', 'yao xie', 'feng qiu']"
2310.03298,a latent variable approach for non-hierarchical multi-fidelity adaptive   sampling,stat.ml cs.lg,"multi-fidelity (mf) methods are gaining popularity for enhancing surrogate modeling and design optimization by incorporating data from various low-fidelity (lf) models. while most existing mf methods assume a fixed dataset, adaptive sampling methods that dynamically allocate resources among fidelity models can achieve higher efficiency in the exploring and exploiting the design space. however, most existing mf methods rely on the hierarchical assumption of fidelity levels or fail to capture the intercorrelation between multiple fidelity levels and utilize it to quantify the value of the future samples and navigate the adaptive sampling. to address this hurdle, we propose a framework hinged on a latent embedding for different fidelity models and the associated pre-posterior analysis to explicitly utilize their correlation for adaptive sampling. in this framework, each infill sampling iteration includes two steps: we first identify the location of interest with the greatest potential improvement using the high-fidelity (hf) model, then we search for the next sample across all fidelity levels that maximize the improvement per unit cost at the location identified in the first step. this is made possible by a single latent variable gaussian process (lvgp) model that maps different fidelity models into an interpretable latent space to capture their correlations without assuming hierarchical fidelity levels. the lvgp enables us to assess how lf sampling candidates will affect hf response with pre-posterior analysis and determine the next sample with the best benefit-to-cost ratio. through test cases, we demonstrate that the proposed method outperforms the benchmark methods in both mf global fitting (gf) and bayesian optimization (bo) problems in convergence rate and robustness. moreover, the method offers the flexibility to switch between gf and bo by simply changing the acquisition function.",10.1016/j.cma.2024.116773,2023-10-04,2024-01-21,"['yi-ping chen', 'liwei wang', 'yigitcan comlek', 'wei chen']"
2310.03945,on wasserstein distances for affine transformations of random vectors,stat.ml cs.lg,"we expound on some known lower bounds of the quadratic wasserstein distance between random vectors in $\mathbb{r}^n$ with an emphasis on affine transformations that have been used in manifold learning of data in wasserstein space. in particular, we give concrete lower bounds for rotated copies of random vectors in $\mathbb{r}^2$ by computing the bures metric between the covariance matrices. we also derive upper bounds for compositions of affine maps which yield a fruitful variety of diffeomorphisms applied to an initial data measure. we apply these bounds to various distributions including those lying on a 1-dimensional manifold in $\mathbb{r}^2$ and illustrate the quality of the bounds. finally, we give a framework for mimicking handwritten digit or alphabet datasets that can be applied in a manifold learning framework.",,2023-10-05,2024-02-07,"['keaton hamm', 'andrzej korzeniowski']"
2310.04458,simultaneous dimensionality reduction: a data efficient approach for   multimodal representations learning,stat.ml physics.data-an,"we explore two primary classes of approaches to dimensionality reduction (dr): independent dimensionality reduction (idr) and simultaneous dimensionality reduction (sdr). in idr methods, of which principal components analysis is a paradigmatic example, each modality is compressed independently, striving to retain as much variation within each modality as possible. in contrast, in sdr, one simultaneously compresses the modalities to maximize the covariation between the reduced descriptions while paying less attention to how much individual variation is preserved. paradigmatic examples include partial least squares and canonical correlations analysis. even though these dr methods are a staple of statistics, their relative accuracy and data set size requirements are poorly understood. we introduce a generative linear model to synthesize multimodal data with known variance and covariance structures to examine these questions. we assess the accuracy of the reconstruction of the covariance structures as a function of the number of samples, signal-to-noise ratio, and the number of varying and covarying signals in the data. using numerical experiments, we demonstrate that linear sdr methods consistently outperform linear idr methods and yield higher-quality, more succinct reduced-dimensional representations with smaller datasets. remarkably, regularized cca can identify low-dimensional weak covarying structures even when the number of samples is much smaller than the dimensionality of the data, which is a regime challenging for all dimensionality reduction methods. our work corroborates and explains previous observations in the literature that sdr can be more effective in detecting covariation patterns in data. these findings suggest that sdr should be preferred to idr in real-world data analysis when detecting covariation is more important than preserving variation.",,2023-10-05,2024-02-06,"['eslam abdelaleem', 'ahmed roman', 'k. michael martini', 'ilya nemenman']"
2310.04861,uncovering hidden geometry in transformers via disentangling position   and context,cs.lg cs.ai stat.ml,"transformers are widely used to extract semantic meanings from input tokens, yet they usually operate as black-box models. in this paper, we present a simple yet informative decomposition of hidden states (or embeddings) of trained transformers into interpretable components. for any layer, embedding vectors of input sequence samples are represented by a tensor $\boldsymbol{h} \in \mathbb{r}^{c \times t \times d}$. given embedding vector $\boldsymbol{h}_{c,t} \in \mathbb{r}^d$ at sequence position $t \le t$ in a sequence (or context) $c \le c$, extracting the mean effects yields the decomposition \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] where $\boldsymbol{\mu}$ is the global mean vector, $\mathbf{pos}_t$ and $\mathbf{ctx}_c$ are the mean vectors across contexts and across positions respectively, and $\mathbf{resid}_{c,t}$ is the residual vector. for popular transformer architectures and diverse text datasets, empirically we find pervasive mathematical structure: (1) $(\mathbf{pos}_t)_{t}$ forms a low-dimensional, continuous, and often spiral shape across layers, (2) $(\mathbf{ctx}_c)_c$ shows clear cluster structure that falls into context topics, and (3) $(\mathbf{pos}_t)_{t}$ and $(\mathbf{ctx}_c)_c$ are mutually nearly orthogonal. we argue that smoothness is pervasive and beneficial to transformers trained on languages, and our decomposition leads to improved model interpretability.",,2023-10-07,2024-02-03,"['jiajun song', 'yiqiao zhong']"
2310.05495,on the convergence of federated averaging under partial participation   for over-parameterized neural networks,cs.lg stat.ml,"federated learning (fl) is a widely employed distributed paradigm for collaboratively training machine learning models from multiple clients without sharing local data. in practice, fl encounters challenges in dealing with partial client participation due to the limited bandwidth, intermittent connection and strict synchronized delay. simultaneously, there exist few theoretical convergence guarantees in this practical setting, especially when associated with the non-convex optimization of neural networks. to bridge this gap, we focus on the training problem of federated averaging (fedavg) method for two canonical models: a deep linear network and a two-layer relu network. under the over-parameterized assumption, we provably show that fedavg converges to a global minimum at a linear rate $\mathcal{o}\left((1-\frac{min_{i \in [t]}|s_i|}{n^2})^t\right)$ after $t$ iterations, where $n$ is the number of clients and $|s_i|$ is the number of the participated clients in the $i$-th iteration. experimental evaluations confirm our theoretical results.",,2023-10-09,2024-02-02,"['xin liu', 'wei li', 'dazhi zhan', 'yu pan', 'xin ma', 'yu ding', 'zhisong pan']"
2310.05518,on double descent in reinforcement learning with lstd and random   features,cs.lg cs.ai stat.ml,"temporal difference (td) algorithms are widely used in deep reinforcement learning (rl). their performance is heavily influenced by the size of the neural network. while in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in rl is much less clear. in this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. we identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. furthermore, we observe a double descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. leveraging random features and the lazy training regime, we study the regularized least-square temporal difference (lstd) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. we derive deterministic limits of both the empirical and the true mean-squared bellman error (msbe) that feature correction terms responsible for the double descent. correction terms vanish when the $l_2$-regularization is increased or the number of unvisited states goes to zero. numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.",,2023-10-09,2024-02-18,"['david brellmann', 'eloïse berthier', 'david filliat', 'goran frehse']"
2310.05842,robust angular synchronization via directed graph neural networks,cs.lg math.oc stat.ml,"the angular synchronization problem aims to accurately estimate (up to a constant additive phase) a set of unknown angles $\theta_1, \dots, \theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets $\theta_i-\theta_j \;\mbox{mod} \; 2\pi.$ applications include, for example, sensor network localization, phase retrieval, and distributed clock synchronization. an extension of the problem to the heterogeneous setting (dubbed $k$-synchronization) is to estimate $k$ groups of angles simultaneously, given noisy observations (with unknown group assignment) from each group. existing methods for angular synchronization usually perform poorly in high-noise regimes, which are common in applications. in this paper, we leverage neural networks for the angular synchronization problem, and its heterogeneous extension, by proposing gnnsync, a theoretically-grounded end-to-end trainable framework using directed graph neural networks. in addition, new loss functions are devised to encode synchronization objectives. experimental results on extensive data sets demonstrate that gnnsync attains competitive, and often superior, performance against a comprehensive set of baselines for the angular synchronization problem and its extension, validating the robustness of gnnsync even at high noise levels.",,2023-10-09,2024-02-12,"['yixuan he', 'gesine reinert', 'david wipf', 'mihai cucuringu']"
2310.06112,theoretical analysis of robust overfitting for wide dnns: an ntk   approach,cs.lg stat.ml,"adversarial training (at) is a canonical method for enhancing the robustness of deep neural networks (dnns). however, recent studies empirically demonstrated that it suffers from robust overfitting, i.e., a long time at can be detrimental to the robustness of dnns. this paper presents a theoretical explanation of robust overfitting for dnns. specifically, we non-trivially extend the neural tangent kernel (ntk) theory to at and prove that an adversarially trained wide dnn can be well approximated by a linearized dnn. moreover, for squared loss, closed-form at dynamics for the linearized dnn can be derived, which reveals a new at degeneration phenomenon: a long-term at will result in a wide dnn degenerates to that obtained without at and thus cause robust overfitting. based on our theoretical results, we further design a method namely adv-ntk, the first at algorithm for infinite-width dnns. experiments on real-world datasets show that adv-ntk can help infinite-width dnns enhance comparable robustness to that of their finite-width counterparts, which in turn justifies our theoretical findings. the code is available at https://github.com/fshp971/adv-ntk.",,2023-10-09,2024-02-04,"['shaopeng fu', 'di wang']"
2310.06312,discovering mixtures of structural causal models from time series data,cs.lg stat.ml,"discovering causal relationships from time series data is significant in fields such as finance, climate science, and neuroscience. however, contemporary techniques rely on the simplifying assumption that data originates from the same causal model, while in practice, data is heterogeneous and can stem from different causal models. in this work, we relax this assumption and perform causal discovery from time series data originating from a mixture of causal models. we propose a general variational inference-based framework called mcd to infer the underlying causal models as well as the mixing probability of each sample. our approach employs an end-to-end training process that maximizes an evidence-lower bound for the data likelihood. we present two variants: mcd-linear for linear relationships and independent noise, and mcd-nonlinear for nonlinear causal relationships and history-dependent noise. we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks through extensive experimentation on synthetic and real-world datasets, particularly when the data emanates from diverse underlying causal graphs. theoretically, we prove the identifiability of such a model under some mild assumptions.",,2023-10-10,2024-02-07,"['sumanth varambally', 'yi-an ma', 'rose yu']"
2310.06333,learning bounded-degree polytrees with known skeleton,cs.lg cs.ds math.pr math.st stat.ml stat.th,"we establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of bayesian networks, a widely-studied type of graphical model. recently, bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured bayesian networks, i.e., 1-polytrees. we extend their results by providing an efficient algorithm which learns $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known. we complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight.",,2023-10-10,2024-01-21,"['davin choo', 'joy qiping yang', 'arnab bhattacharyya', 'clément l. canonne']"
2310.06823,neco: neural collapse based out-of-distribution detection,stat.ml cs.ai cs.cv cs.lg,"detecting out-of-distribution (ood) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. we hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences ood data. to benefit from this interplay, we introduce neco, a novel post-hoc method for ood detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify ood data. our extensive experiments demonstrate that neco achieves state-of-the-art results on both small and large-scale ood detection tasks while exhibiting strong generalization capabilities across different network architectures. furthermore, we provide a theoretical explanation for the effectiveness of our method in ood detection. code is available at https://gitlab.com/drti/neco",,2023-10-10,2024-02-27,"['mouïn ben ammar', 'nacim belkhir', 'sebastian popescu', 'antoine manzanera', 'gianni franchi']"
2310.07132,risk assessment and statistical significance in the age of foundation   models,cs.lg math.st q-fin.rm stat.ml stat.th,"we propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. we show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. inspired by portfolio optimization and selection theory in mathematical finance, we define a metrics portfolio for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. the statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit theorems instantiated in practice via a bootstrap variance estimate. we use our framework to compare various large language models regarding risks related to drifting from instructions and outputting toxic content.",,2023-10-10,2024-01-09,"['apoorva nitsure', 'youssef mroueh', 'mattia rigotti', 'kristjan greenewald', 'brian belgodere', 'mikhail yurochkin', 'jiri navratil', 'igor melnyk', 'jerret ross']"
2310.07665,deep backtracking counterfactuals for causally compliant explanations,cs.ai cs.lg stat.ml,"counterfactuals answer questions of what would have been observed under altered circumstances and can therefore offer valuable insights. whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative where all causal laws are kept intact. in the present work, we introduce a practical method called deep backtracking counterfactuals (deepbc) for computing backtracking counterfactuals in structural causal models that consist of deep generative components. we propose two distinct versions of our method--one utilizing langevin monte carlo sampling and the other employing constrained optimization--to generate counterfactuals for high-dimensional data. as a special case, our formulation reduces to methods in the field of counterfactual explanations. compared to these, our approach represents a causally compliant, versatile and modular alternative. we demonstrate these properties experimentally on a modified version of mnist and celeba.",,2023-10-11,2024-02-09,"['klaus-rudolf kladny', 'julius von kügelgen', 'bernhard schölkopf', 'michael muehlebach']"
2310.07852,on the computational complexity of private high-dimensional model   selection,stat.ml cs.lg stat.co stat.me,"we consider the problem of model selection in a high-dimensional sparse linear regression model under privacy constraints. we propose a differentially private best subset selection method with strong utility properties by adopting the well-known exponential mechanism for selecting the best model. we propose an efficient metropolis-hastings algorithm and establish that it enjoys polynomial mixing time to its stationary distribution. furthermore, we also establish approximate differential privacy for the final estimates of the metropolis-hastings random walk using its mixing property. finally, we perform some illustrative experiments that show the strong utility of our algorithm.",,2023-10-11,2024-02-05,"['saptarshi roy', 'zehua wang', 'ambuj tewari']"
2310.07891,a theory of non-linear feature learning with one gradient step in   two-layer neural networks,stat.ml cs.lg,"feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. it is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. however, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. we show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. we further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. by precisely analyzing the improvement in the training and test errors, we demonstrate that these non-linear features can enhance learning.",,2023-10-11,2024-02-03,"['behrad moniri', 'donghwan lee', 'hamed hassani', 'edgar dobriban']"
2310.07953,enhancing sample quality through minimum energy importance weights,stat.me stat.co,"importance sampling is a powerful tool for correcting the distributional mismatch in many statistical and machine learning problems, but in practice its performance is limited by the usage of simple proposals whose importance weights can be computed analytically. to address this limitation, liu and lee (2017) proposed a black-box importance sampling (bbis) algorithm that computes the importance weights for arbitrary simulated samples by minimizing the kernelized stein discrepancy. however, this requires knowing the score function of the target distribution, which is not easy to compute for many bayesian problems. hence, in this paper we propose another novel bbis algorithm using minimum energy design, bbis-med, that requires only the unnormalized density function, which can be utilized as a post-processing step to improve the quality of markov chain monte carlo samples. we demonstrate the effectiveness and wide applicability of our proposed bbis-med algorithm on extensive simulations and a real-world bayesian model calibration problem where the score function cannot be derived analytically.",,2023-10-11,2023-12-31,"['chaofan huang', 'v. roshan joseph']"
2310.07958,towards causal deep learning for vulnerability detection,cs.se cs.cr cs.lg stat.me,"deep learning vulnerability detection has shown promising results in recent years. however, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (ood) data, e.g., applying a trained model to unseen projects in real world. we hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. when the perturbed and ood datasets no longer have the same spurious features, the model prediction fails. to address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. our approach causalvul consists of two phases. first, we designed novel perturbations to discover spurious features that the model may use to make predictions. second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to systematically remove the use of spurious features and thus promote causal based prediction. our results show that causalvul consistently improved the model accuracy, robustness and ood performance for all the state-of-the-art models and datasets we experimented. to the best of our knowledge, this is the first work that introduces do calculus based causal learning to software engineering models and shows it's indeed useful for improving the model accuracy, robustness and generalization. our replication package is located at https://figshare.com/s/0ffda320dcb96c249ef2.",10.1145/3597503.3639170,2023-10-11,2024-01-14,"['md mahbubur rahman', 'ira ceka', 'chengzhi mao', 'saikat chakraborty', 'baishakhi ray', 'wei le']"
2310.08331,dealing with uncertainty: balancing exploration and exploitation in deep   recurrent reinforcement learning,stat.ml cs.lg,"incomplete knowledge of the environment leads an agent to make decisions under uncertainty. one of the major dilemmas in reinforcement learning (rl) where an autonomous agent has to balance two contrasting needs in making its decisions is: exploiting the current knowledge of the environment to maximize the cumulative reward as well as exploring actions that allow improving the knowledge of the environment, hopefully leading to higher reward values (exploration-exploitation trade-off). concurrently, another relevant issue regards the full observability of the states, which may not be assumed in all applications. for instance, when 2d images are considered as input in an rl approach used for finding the best actions within a 3d simulation environment. in this work, we address these issues by deploying and testing several techniques to balance exploration and exploitation trade-off on partially observable systems for predicting steering wheels in autonomous driving scenarios. more precisely, the final aim is to investigate the effects of using both adaptive and deterministic exploration strategies coupled with a deep recurrent q-network. additionally, we adapted and evaluated the impact of a modified quadratic loss function to improve the learning phase of the underlying convolutional recurrent neural network. we show that adaptive methods better approximate the trade-off between exploration and exploitation and, in general, softmax and max-boltzmann strategies outperform epsilon-greedy techniques.",,2023-10-12,2024-02-20,"['valentina zangirolami', 'matteo borrotti']"
2310.08337,neural diffusion models,cs.lg stat.ml,"diffusion models have shown remarkable performance on many generative tasks. despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. in contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. in this paper, we present neural diffusion models (ndms), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. we show how to optimise ndms using a variational bound in a simulation-free setting. moreover, we derive a time-continuous formulation of ndms, which allows fast and reliable inference using off-the-shelf numerical ode and sde solvers. finally, we demonstrate the utility of ndms with learnable transformations through experiments on standard image generation benchmarks, including cifar-10, downsampled versions of imagenet and celeba-hq. ndms outperform conventional diffusion models in terms of likelihood and produce high-quality samples.",,2023-10-12,2024-02-26,"['grigory bartosh', 'dmitry vetrov', 'christian a. naesseth']"
2310.08833,optimal sample complexity for average reward markov decision processes,cs.lg math.oc stat.ml,"we resolve the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic markov decision process (mdp), assuming a generative model. in this context, the existing literature provides a sample complexity upper bound of $\widetilde o(|s||a|t_{\text{mix}}^2 \epsilon^{-2})$ and a lower bound of $\omega(|s||a|t_{\text{mix}} \epsilon^{-2})$. in these expressions, $|s|$ and $|a|$ denote the cardinalities of the state and action spaces respectively, $t_{\text{mix}}$ serves as a uniform upper limit for the total variation mixing times, and $\epsilon$ signifies the error tolerance. therefore, a notable gap of $t_{\text{mix}}$ still remains to be bridged. our primary contribution is the development of an estimator for the optimal policy of average reward mdps with a sample complexity of $\widetilde o(|s||a|t_{\text{mix}}\epsilon^{-2})$. this marks the first algorithm and analysis to reach the literature's lower bound. our new algorithm draws inspiration from ideas in li et al. (2020), jin and sidford (2021), and wang et al. (2023). additionally, we conduct numerical experiments to validate our theoretical findings.",,2023-10-12,2024-02-12,"['shengbo wang', 'jose blanchet', 'peter glynn']"
2310.09639,dpzero: private fine-tuning of language models without backpropagation,cs.lg cs.cr math.oc stat.ml,"the widespread practice of fine-tuning large language models (llms) on domain-specific data faces two major challenges in memory and privacy. first, as the size of llms continues to grow, the memory demands of gradient-based training methods via backpropagation become prohibitively high. second, given the tendency of llms to memorize training data, it is important to protect potentially sensitive information in the fine-tuning data from being regurgitated. zeroth-order methods, which rely solely on forward passes, substantially reduce memory consumption during training. however, directly combining them with standard differentially private gradient descent suffers from growing model size. to bridge this gap, we introduce dpzero, a novel private zeroth-order algorithm with nearly dimension-independent rates. the memory efficiency of dpzero is demonstrated in privately fine-tuning roberta on six downstream tasks.",,2023-10-14,2024-02-14,"['liang zhang', 'bingcong li', 'kiran koshy thekumparampil', 'sewoong oh', 'niao he']"
2310.10107,regret analysis of the posterior sampling-based learning algorithm for   episodic pomdps,cs.lg cs.ai cs.sy eess.sy stat.ml,"learning in pomdps is known to be significantly harder than mdps. in this paper, we consider online learning problem for episodic pomdps with unknown transition and observation models. we propose a posterior sampling-based reinforcement learning algorithm for pomdps (ps4pomdps), which is much simpler and more implementable compared to state-of-the-art optimism-based online learning algorithms for pomdps. we show that the bayesian regret of the proposed algorithm scales as the square root of the number of episodes, matching the lower bound, and is polynomial in the other parameters. in a general setting, its regret scales exponentially in the horizon length $h$, and we show that this is inevitable by providing a lower bound. however, when the pomdp is undercomplete and weakly revealing (an assumption common in recent literature), we establish a polynomial bayesian regret bound. we also propose a posterior sampling algorithm for multi-agent pomdps, and show it too has sublinear regret.",,2023-10-16,2024-02-01,"['dengwang tang', 'rahul jain', 'ashutosh nayyar', 'pierluigi nuzzo']"
2310.10143,an empirical study of self-supervised learning with wasserstein distance,stat.ml cs.lg,"in this study, we delve into the problem of self-supervised learning (ssl) utilizing the 1-wasserstein distance on a tree structure (a.k.a., tree-wasserstein distance (twd)), where twd is defined as the l1 distance between two tree-embedded vectors. in ssl methods, the cosine similarity is often utilized as an objective function; however, it has not been well studied when utilizing the wasserstein distance. training the wasserstein distance is numerically challenging. thus, this study empirically investigates a strategy for optimizing the ssl with the wasserstein distance and finds a stable training procedure. more specifically, we evaluate the combination of two types of twd (total variation and clustertree) and several probability models, including the softmax function, the arcface probability model, and simplicial embedding. we propose a simple yet effective jeffrey divergence-based regularization method to stabilize optimization. through empirical experiments on stl10, cifar10, cifar100, and svhn, we find that a simple combination of the softmax function and twd can obtain significantly lower results than the standard simclr. moreover, a simple combination of twd and simsiam fails to train the model. we find that the model performance depends on the combination of twd and probability model, and that the jeffrey divergence regularization helps in model training. finally, we show that the appropriate combination of the twd and probability model outperforms cosine similarity-based representation learning.",,2023-10-16,2024-02-05,"['makoto yamada', 'yuki takezawa', 'guillaume houry', 'kira michaela dusterwald', 'deborah sulem', 'han zhao', 'yao-hung hubert tsai']"
2310.10407,ensemble methods for testing a global null,stat.me,"testing a global null is a canonical problem in statistics and has a wide range of applications. in view of the fact that no uniformly most powerful test exists, prior and/or domain knowledge are commonly used to focus on a certain class of alternatives to improve the testing power. however, it is generally challenging to develop tests that are particularly powerful against a certain class of alternatives. in this paper, motivated by the success of ensemble learning methods for prediction or classification, we propose an ensemble framework for testing that mimics the spirit of random forests to deal with the challenges. our ensemble testing framework aggregates a collection of weak base tests to form a final ensemble test that maintains strong and robust power for global nulls. we apply the framework to four problems about global testing in different classes of alternatives arising from whole genome sequencing (wgs) association studies. specific ensemble tests are proposed for each of these problems, and their theoretical optimality is established in terms of bahadur efficiency. extensive simulations and an analysis of a real wgs dataset are conducted to demonstrate the type i error control and/or power gain of the proposed ensemble tests.",10.1093/jrsssb/qkad131,2023-10-16,,"['yaowu liu', 'zhonghua liu', 'xihong lin']"
2310.10434,equivariant matrix function neural networks,stat.ml cond-mat.mtrl-sci cs.lg physics.chem-ph,"graph neural networks (gnns), especially message-passing neural networks (mpnns), have emerged as powerful architectures for learning on graphs in diverse applications. however, mpnns face challenges when modeling non-local interactions in graphs such as large conjugated molecules, and social networks due to oversmoothing and oversquashing. although spectral gnns and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack generalizability, or fail to capture detailed structural relationships or symmetries in the data. to address these concerns, we introduce matrix function neural networks (mfns), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size. the mfn architecture achieves stateof-the-art performance in standard graph benchmarks, such as the zinc and tu datasets, and is able to capture intricate non-local interactions in quantum systems, paving the way to new state-of-the-art force fields.",,2023-10-16,2024-01-30,"['ilyes batatia', 'lars l. schaaf', 'huajie chen', 'gábor csányi', 'christoph ortner', 'felix a. faber']"
2310.10534,comparing comparators in generalization bounds,cs.lg cs.it math.it math.st stat.ml stat.th,"we derive generic information-theoretic and pac-bayesian generalization bounds involving an arbitrary convex comparator function, which measures the discrepancy between the training and population loss. the bounds hold under the assumption that the cumulant-generating function (cgf) of the comparator is upper-bounded by the corresponding cgf within a family of bounding distributions. we show that the tightest possible bound is obtained with the comparator being the convex conjugate of the cgf of the bounding distribution, also known as the cram\'er function. this conclusion applies more broadly to generalization bounds with a similar structure. this confirms the near-optimality of known bounds for bounded and sub-gaussian losses and leads to novel bounds under other bounding distributions.",,2023-10-16,2024-02-21,"['fredrik hellström', 'benjamin guedj']"
2310.10556,sample complexity of preference-based nonparametric off-policy   evaluation with deep networks,cs.lg stat.ml,"a recently popular approach to solving reinforcement learning is with data from human preferences. in fact, human preference data are now used with classic reinforcement learning algorithms such as actor-critic methods, which involve evaluating an intermediate policy over a reward learned from human preference data with distribution shift, known as off-policy evaluation (ope). such algorithm includes (i) learning reward function from human preference dataset, and (ii) learning expected cumulative reward of a target policy. despite the huge empirical success, existing ope methods with preference data often lack theoretical understanding and rely heavily on heuristics. in this paper, we study the sample efficiency of ope with human preference and establish a statistical guarantee for it. specifically, we approach ope by learning the value function by fitted-q-evaluation with a deep neural network. by appropriately selecting the size of a relu network, we show that one can leverage any low-dimensional manifold structure in the markov decision process and obtain a sample-efficient estimator without suffering from the curse of high data ambient dimensionality. under the assumption of high reward smoothness, our results \textit{almost align with the classical ope results with observable reward data}. to the best of our knowledge, this is the first result that establishes a \textit{provably efficient} guarantee for off-policy evaluation with rlhf.",,2023-10-16,2024-02-26,"['zihao li', 'xiang ji', 'minshuo chen', 'mengdi wang']"
2310.11122,sensitivity-aware amortized bayesian inference,stat.ml cs.lg stat.me,"sensitivity analyses reveal the influence of various modeling choices on the outcomes of statistical analyses. while theoretically appealing, they are overwhelmingly inefficient for complex bayesian models. in this work, we propose sensitivity-aware amortized bayesian inference (sa-abi), a multifaceted approach to efficiently integrate sensitivity analyses into simulation-based inference with neural networks. first, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. second, we leverage the rapid inference of neural networks to assess sensitivity to data perturbations and preprocessing steps. in contrast to most other bayesian approaches, both steps circumvent the costly bottleneck of refitting the model for each choice of likelihood, prior, or data set. finally, we propose to use deep ensembles to detect sensitivity arising from unreliable approximation (e.g., due to model misspecification). we demonstrate the effectiveness of our method in applied modeling problems, ranging from disease outbreak dynamics and global warming thresholds to human decision-making. our results support sensitivity-aware inference as a default choice for amortized bayesian workflows, automatically providing modelers with insights into otherwise hidden dimensions.",,2023-10-17,2024-02-19,"['lasse elsemüller', 'hans olischläger', 'marvin schmitt', 'paul-christian bürkner', 'ullrich köthe', 'stefan t. radev']"
2310.11531,efficient online learning with offline datasets for infinite horizon   mdps: a bayesian approach,cs.lg cs.ai cs.sy eess.sy stat.ml,"in this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. we assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. we show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. we establish an upper bound on regret of the exact informed psrl algorithm that scales as $\tilde{o}(\sqrt{t})$. this requires a novel prior-dependent regret analysis of bayesian online learning algorithms for the infinite horizon setting. we then propose the informed rlsvi algorithm to efficiently approximate the ipsrl algorithm.",,2023-10-17,2024-02-01,"['dengwang tang', 'rahul jain', 'botao hao', 'zheng wen']"
2310.12115,mmd-based variable importance for distributional random forest,stat.ml cs.lg stat.me,"distributional random forest (drf) is a flexible forest-based method to estimate the full conditional distribution of a multivariate output of interest given input variables. in this article, we introduce a variable importance algorithm for drfs, based on the well-established drop and relearn principle and mmd distance. while traditional importance measures only detect variables with an influence on the output mean, our algorithm detects variables impacting the output distribution more generally. we show that the introduced importance measure is consistent, exhibits high empirical performance on both real and simulated data, and outperforms competitors. in particular, our algorithm is highly efficient to select variables through recursive feature elimination, and can therefore provide small sets of variables to build accurate estimates of conditional output distributions.",,2023-10-18,2024-02-14,"['clément bénard', 'jeffrey näf', 'julie josse']"
2310.12447,constrained reweighting of distributions: an optimal transport approach,stat.ml cs.lg,"we commonly encounter the problem of identifying an optimally weight adjusted version of the empirical distribution of observed data, adhering to predefined constraints on the weights. such constraints often manifest as restrictions on the moments, tail behaviour, shapes, number of modes, etc., of the resulting weight adjusted empirical distribution. in this article, we substantially enhance the flexibility of such methodology by introducing a nonparametrically imbued distributional constraints on the weights, and developing a general framework leveraging the maximum entropy principle and tools from optimal transport. the key idea is to ensure that the maximum entropy weight adjusted empirical distribution of the observed data is close to a pre-specified probability distribution in terms of the optimal transport metric while allowing for subtle departures. the versatility of the framework is demonstrated in the context of three disparate applications where data re-weighting is warranted to satisfy side constraints on the optimization problem at the heart of the statistical task: namely, portfolio allocation, semi-parametric inference for complex surveys, and ensuring algorithmic fairness in machine learning algorithms.",,2023-10-18,2024-01-16,"['abhisek chakraborty', 'anirban bhattacharya', 'debdeep pati']"
2310.12595,meta-learning with hierarchical models based on similarity of causal   mechanisms,cs.lg stat.ml,"in this work the goal is to generalise to new data in a non-iid setting where datasets from related tasks are observed, each generated by a different causal mechanism, and the test dataset comes from the same task distribution. this setup is motivated by personalised medicine, where a patient is a task and complex diseases are heterogeneous across patients in cause and progression. the difficulty is that there usually is not enough data in one task to identify the causal mechanism, and unless the mechanisms are the same, pooling data across tasks, which meta-learning does one way or the other, may lead to worse predictors when the test setting may be uncontrollably different. in this paper we introduce to meta-learning, formulated as bayesian hierarchical modelling, a proxy measure of similarity of the causal mechanisms of tasks, by learning a suitable embedding of the tasks from the whole data set. this embedding is used as auxiliary data for assessing which tasks should be pooled in the hierarchical model. we show that such pooling improves predictions in three health-related case studies, and by sensitivity analyses on simulated data that the method aids generalisability by utilising interventional data to identify tasks with similar causal mechanisms for pooling, even in limited data settings.",,2023-10-19,2024-02-15,"['sophie wharrie', 'samuel kaski']"
2310.12781,conditional density estimations from privacy-protected data,stat.ml cs.lg stat.co,"many modern statistical analysis and machine learning applications require training models on sensitive user data. differential privacy provides a formal guarantee that individual-level information about users does not leak. in this framework, randomized algorithms inject calibrated noise into the confidential data, resulting in privacy-protected datasets or queries. however, restricting access to only privatized data during statistical analysis makes it computationally challenging to make valid inferences on the parameters underlying the confidential data. in this work, we propose simulation-based inference methods from privacy-protected datasets. in addition to sequential monte carlo approximate bayesian computation, we use neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. we illustrate our methods on discrete time-series data under an infectious disease model and with ordinary linear regression models. illustrating the privacy-utility trade-off, our experiments and analysis demonstrate the necessity and feasibility of designing valid statistical inference procedures to correct for biases introduced by the privacy-protection mechanisms.",,2023-10-19,2023-12-30,"['yifei xiong', 'nianqiao p. ju', 'sanguo zhang']"
2310.12934,generative flow networks as entropy-regularized rl,cs.lg stat.ml,"the recently proposed generative flow networks (gflownets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. gflownets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (rl). our work extends the connection between rl and gflownets to a general case. we demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized rl problem with a specific reward and regularizer structure. furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft rl algorithms to gflownet training across several probabilistic modeling tasks. contrary to previously reported results, we show that entropic rl approaches can be competitive against established gflownet training methods. this perspective opens a direct path for integrating rl principles into the realm of generative flow networks.",,2023-10-19,2024-02-25,"['daniil tiapkin', 'nikita morozov', 'alexey naumov', 'dmitry vetrov']"
2310.13164,almost equivariance via lie algebra convolutions,cs.lg stat.ml,"recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. analysis of the built-in equivariance of existing neural network architectures, as well as the study of building models that explicitly ""bake in"" equivariance, have become significant research areas in their own right. however, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. while strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances. in such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform. therefore, in this work we study a closely related topic, that of almost equivariance. we provide a definition of almost equivariance and give a practical method for encoding almost equivariance in models by appealing to the lie algebra of a lie group. specifically, we define lie algebra convolutions and demonstrate that they offer several benefits over lie group convolutions, including being well-defined for non-compact lie groups having non-surjective exponential map. from there, we demonstrate connections between the notions of equivariance and isometry and those of almost equivariance and almost isometry. we prove two existence theorems, one showing the existence of almost isometries within bounded distance of isometries of a manifold, and another showing the converse for hilbert spaces. we extend these theorems to prove the existence of almost equivariant manifold embeddings within bounded distance of fully equivariant embedding functions, subject to certain constraints on the group action and the function class. finally, we demonstrate the validity of our approach by benchmarking against datasets in fully equivariant and almost equivariant settings.",,2023-10-19,2024-02-23,['daniel mcneela']
2310.13769,compositional deep probabilistic models of dna encoded libraries,q-bio.qm stat.ml,"dna-encoded library (del) has proven to be a powerful tool that utilizes combinatorially constructed small molecules to facilitate highly-efficient screening assays. these selection experiments, involving multiple stages of washing, elution, and identification of potent binders via unique dna barcodes, often generate complex data. this complexity can potentially mask the underlying signals, necessitating the application of computational tools such as machine learning to uncover valuable insights. we introduce a compositional deep probabilistic model of del data, del-compose, which decomposes molecular representations into their mono-synthon, di-synthon, and tri-synthon building blocks and capitalizes on the inherent hierarchical structure of these molecules by modeling latent reactions between embedded synthons. additionally, we investigate methods to improve the observation models for del count data such as integrating covariate factors to more effectively account for data noise. across two popular public benchmark datasets (ca-ix and hrp), our model demonstrates strong performance compared to count baselines, enriches the correct pharmacophores, and offers valuable insights via its intrinsic interpretable structure, thereby providing a robust tool for the analysis of del data.",,2023-10-20,2024-02-13,"['benson chen', 'mohammad m. sultan', 'theofanis karaletsos']"
2310.13786,fundamental limits of membership inference attacks on machine learning   models,stat.ml cs.ai cs.lg,"membership inference attacks (mia) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. this article provides theoretical guarantees by exploring the fundamental statistical limitations associated with mias on machine learning models. more precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. we then deduce that in a very general regression setting with overfitting algorithms, attacks may have a high probability of success. finally, we investigate several situations for which we provide bounds on this quantity of interest. our results enable us to deduce the accuracy of potential attacks based on the number of samples and other structural parameters of learning models. in certain instances, these parameters can be directly estimated from the dataset.",,2023-10-20,2024-01-31,"['eric aubinais', 'elisabeth gassiat', 'pablo piantanida']"
2310.14720,extended deep adaptive input normalization for preprocessing time series   data for neural networks,cs.lg stat.ml,"data preprocessing is a crucial part of any machine learning pipeline, and it can have a significant impact on both performance and training efficiency. this is especially evident when using deep neural networks for time series prediction and classification: real-world time series data often exhibit irregularities such as multi-modality, skewness and outliers, and the model performance can degrade rapidly if these characteristics are not adequately addressed. in this work, we propose the edain (extended deep adaptive input normalization) layer, a novel adaptive neural layer that learns how to appropriately normalize irregular time series data for a given task in an end-to-end fashion, instead of using a fixed normalization scheme. this is achieved by optimizing its unknown parameters simultaneously with the deep neural network using back-propagation. our experiments, conducted using synthetic data, a credit default prediction dataset, and a large-scale limit order book benchmark dataset, demonstrate the superior performance of the edain layer when compared to conventional normalization methods and existing adaptive time series preprocessing layers.",,2023-10-23,2024-02-29,"['marcus a. k. september', 'francesco sanna passino', 'leonie goldmann', 'anton hinel']"
2310.14763,externally valid policy evaluation combining trial and observational   data,stat.me cs.lg stat.ml,"randomized trials are widely considered as the gold standard for evaluating the effects of decision policies. trial data is, however, drawn from a population which may differ from the intended target population and this raises a problem of external validity (aka. generalizability). in this paper we seek to use trial data to draw valid inferences about the outcome of a policy on the target population. additional covariate data from the target population is used to model the sampling of individuals in the trial study. we develop a method that yields certifiably valid trial-based policy evaluations under any specified range of model miscalibrations. the method is nonparametric and the validity is assured even with finite samples. the certified policy evaluations are illustrated using both simulated and real data.",,2023-10-23,2024-02-22,"['sofia ek', 'dave zachariah']"
2310.14901,series of hessian-vector products for tractable saddle-free newton   optimisation of neural networks,cs.lg stat.ml,"despite their popularity in the field of continuous optimisation, second-order quasi-newton methods are challenging to apply in machine learning, as the hessian matrix is intractably large. this computational burden is exacerbated by the need to address non-convexity, for instance by modifying the hessian's eigenvalues as in saddle-free newton methods. we propose an optimisation algorithm which addresses both of these concerns - to our knowledge, the first efficiently-scalable optimisation algorithm to asymptotically use the exact inverse hessian with absolute-value eigenvalues. our method frames the problem as a series which principally square-roots and inverts the squared hessian, then uses it to precondition a gradient vector, all without explicitly computing or eigendecomposing the hessian. a truncation of this infinite series provides a new optimisation algorithm which is scalable and comparable to other first- and second-order optimisation methods in both runtime and optimisation performance. we demonstrate this in a variety of settings, including a resnet-18 trained on cifar-10.",,2023-10-23,2024-02-27,"['elre t. oldewage', 'ross m. clarke', 'josé miguel hernández-lobato']"
2310.14968,bayesian active learning in the presence of nuisance parameters,cs.lg stat.ml,"in many settings, such as scientific inference, optimization, and transfer learning, the learner has a well-defined objective, which can be treated as estimation of a target parameter, and no intrinsic interest in characterizing the entire data-generating process. usually, the learner must also contend with additional sources of uncertainty or variables -- with nuisance parameters. bayesian active learning, or sequential optimal experimental design, can straightforwardly accommodate the presence of nuisance parameters, and so is a natural active learning framework for such problems. however, the introduction of nuisance parameters can lead to bias in the bayesian learner's estimate of the target parameters, a phenomenon we refer to as negative interference. we characterize the threat of negative interference and how it fundamentally changes the nature of the bayesian active learner's task. we show that the extent of negative interference can be extremely large, and that accurate estimation of the nuisance parameters is critical to reducing it. the bayesian active learner is confronted with a dilemma: whether to spend a finite acquisition budget in pursuit of estimation of the target or of the nuisance parameters. our setting encompasses bayesian transfer learning as a special case, and our results shed light on the phenomenon of negative transfer between learning environments.",,2023-10-23,2024-02-13,"['sabina j. sloman', 'ayush bharti', 'julien martinelli', 'samuel kaski']"
2310.15330,towards the theory of unsupervised federated learning: non-asymptotic   analysis of federated em algorithms,stat.ml cs.lg,"while supervised federated learning approaches have enjoyed significant success, the domain of unsupervised federated learning remains relatively underexplored. several federated em algorithms have gained popularity in practice, however, their theoretical foundations are often lacking. in this paper, we first introduce a federated gradient em algorithm (fedgrem) designed for the unsupervised learning of mixture models, which supplements the existing federated em algorithms by considering task heterogeneity and potential adversarial attacks. we present a comprehensive finite-sample theory that holds for general mixture models, then apply this general theory on specific statistical models to characterize the explicit estimation error of model parameters and mixture proportions. our theory elucidates when and how fedgrem outperforms local single-task learning with insights extending to existing federated em algorithms. this bridges the gap between their practical success and theoretical understanding. our simulation results validate our theory, and demonstrate fedgrem's superiority over existing unsupervised federated learning benchmarks.",,2023-10-23,2024-02-05,"['ye tian', 'haolei weng', 'yang feng']"
2310.15351,random exploration in bayesian optimization: order-optimal regret and   computational efficiency,cs.lg stat.ml,"we consider bayesian optimization using gaussian process models, also referred to as kernel-based bandit optimization. we study the methodology of exploring the domain using random samples drawn from a distribution. we show that this random exploration approach achieves the optimal error rates. our analysis is based on novel concentration bounds in an infinite dimensional hilbert space established in this work, which may be of independent interest. we further develop an algorithm based on random exploration with domain shrinking and establish its order-optimal regret guarantees under both noise-free and noisy settings. in the noise-free setting, our analysis closes the existing gap in regret performance and thereby resolves a colt open problem. the proposed algorithm also enjoys a computational advantage over prevailing methods due to the random exploration that obviates the expensive optimization of a non-convex acquisition function for choosing the query points at each iteration.",,2023-10-23,2024-02-02,"['sudeep salgia', 'sattar vakili', 'qing zhao']"
2310.15450,general identifiability and achievability for causal representation   learning,cs.lg stat.ml,"this paper focuses on causal representation learning (crl) under a general nonparametric latent causal model and a general transformation model that maps the latent data to the observational data. it establishes identifiability and achievability results using two hard uncoupled interventions per node in the latent causal graph. notably, one does not know which pair of intervention environments have the same node intervened (hence, uncoupled). for identifiability, the paper establishes that perfect recovery of the latent causal model and variables is guaranteed under uncoupled interventions. for achievability, an algorithm is designed that uses observational and interventional data and recovers the latent causal model and variables with provable guarantees. this algorithm leverages score variations across different environments to estimate the inverse of the transformer and, subsequently, the latent variables. the analysis, additionally, recovers the identifiability result for two hard coupled interventions, that is when metadata about the pair of environments that have the same node intervened is known. this paper also shows that when observational data is available, additional faithfulness assumptions that are adopted by the existing literature are unnecessary.",,2023-10-23,2024-02-14,"['burak varıcı', 'emre acartürk', 'karthikeyan shanmugam', 'ali tajer']"
2310.15627,contextual directed acyclic graphs,stat.ml cs.lg,"estimating the structure of directed acyclic graphs (dags) from observational data remains a significant challenge in machine learning. most research in this area concentrates on learning a single dag for the entire population. this paper considers an alternative setting where the graph structure varies across individuals based on available ""contextual"" features. we tackle this contextual dag problem via a neural network that maps the contextual features to a dag, represented as a weighted adjacency matrix. the neural network is equipped with a novel projection layer that ensures the output matrices are sparse and satisfy a recently developed characterization of acyclicity. we devise a scalable computational framework for learning contextual dags and provide a convergence guarantee and an analytical gradient for backpropagating through the projection layer. our experiments suggest that the new approach can recover the true context-specific graph where existing approaches fail.",,2023-10-24,2024-02-20,"['ryan thompson', 'edwin v. bonilla', 'robert kohn']"
2310.16221,hierarchical randomized smoothing,cs.lg cs.ai cs.cv stat.ml,"real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). as a solution, we introduce hierarchical randomized smoothing: we partially smooth objects by adding random noise only on a randomly selected subset of their entities. by adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. we initialize hierarchical smoothing using different noising distributions, yielding novel robustness certificates for discrete and continuous domains. we experimentally demonstrate the importance of hierarchical smoothing in image and node classification, where it yields superior robustness-accuracy trade-offs. overall, hierarchical smoothing is an important contribution towards models that are both - certifiably robust to perturbations and accurate.",,2023-10-24,2024-01-15,"['yan scholten', 'jan schuchardt', 'aleksandar bojchevski', 'stephan günnemann']"
2310.16441,grokking in linear estimators -- a solvable model that groks without   understanding,stat.ml cond-mat.dis-nn cs.lg math-ph math.mp,"grokking is the intriguing phenomenon where a model learns to generalize long after it has fit the training data. we show both analytically and numerically that grokking can surprisingly occur in linear networks performing linear tasks in a simple teacher-student setup with gaussian inputs. in this setting, the full training dynamics is derived in terms of the training and generalization data covariance matrix. we present exact predictions on how the grokking time depends on input and output dimensionality, train sample size, regularization, and network initialization. we demonstrate that the sharp increase in generalization accuracy may not imply a transition from ""memorization"" to ""understanding"", but can simply be an artifact of the accuracy measure. we provide empirical verification for our calculations, along with preliminary results indicating that some predictions also hold for deeper networks, with non-linear activations.",,2023-10-25,,"['noam levi', 'alon beck', 'yohai bar-sinai']"
2310.16834,discrete diffusion modeling by estimating the ratios of the data   distribution,stat.ml cs.cl cs.lg,"despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. in this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. experimentally, we test our score entropy discrete diffusion models (sedd) on standard language modeling tasks. for comparable model sizes, sedd beats existing language diffusion paradigms (reducing perplexity by $25$-$75$\%) and is competitive with autoregressive models, in particular outperforming gpt-2. furthermore, compared to autoregressive mdoels, sedd generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\times$ better generative perplexity than un-annealed gpt-2), can trade compute and quality (similar quality with $32\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).",,2023-10-25,2024-02-20,"['aaron lou', 'chenlin meng', 'stefano ermon']"
2310.17168,learning an inventory control policy with general inventory arrival   dynamics,cs.lg stat.ml,"in this paper we address the problem of learning and backtesting inventory control policies in the presence of general arrival dynamics -- which we term as a quantity-over-time arrivals model (qot). we also allow for order quantities to be modified as a post-processing step to meet vendor constraints such as order minimum and batch size constraints -- a common practice in real supply chains. to the best of our knowledge this is the first work to handle either arbitrary arrival dynamics or an arbitrary downstream post-processing of order quantities. building upon recent work (madeka et al., 2022) we similarly formulate the periodic review inventory control problem as an exogenous decision process, where most of the state is outside the control of the agent. madeka et al., 2022 show how to construct a simulator that replays historic data to solve this class of problem. in our case, we incorporate a deep generative model for the arrivals process as part of the history replay. by formulating the problem as an exogenous decision process, we can apply results from madeka et al., 2022 to obtain a reduction to supervised learning. via simulation studies we show that this approach yields statistically significant improvements in profitability over production baselines. using data from a real-world a/b test, we show that gen-qot generalizes well to off-policy data and that the resulting buying policy outperforms traditional inventory management systems in real world settings.",,2023-10-26,2024-01-21,"['sohrab andaz', 'carson eisenach', 'dhruv madeka', 'kari torkkola', 'randy jia', 'dean foster', 'sham kakade']"
2310.17273,looping in the human collaborative and explainable bayesian optimization,cs.lg cs.hc stat.ml,"like many optimizers, bayesian optimization often falls short of gaining user trust due to opacity. while attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. we relax these assumptions and propose a more balanced human-ai partnership with our collaborative and explainable bayesian optimization (coexbo) framework. instead of explicitly requiring a user to provide a knowledge model, coexbo employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. coexbo explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. furthermore, coexbo offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to a vanilla bayesian optimization. we validate coexbo's efficacy through human-ai teaming experiments in lithium-ion battery design, highlighting substantial improvements over conventional methods. code is available https://github.com/ma921/coexbo.",,2023-10-26,2024-02-29,"['masaki adachi', 'brady planden', 'david a. howey', 'michael a. osborne', 'sebastian orbell', 'natalia ares', 'krikamol muandet', 'siu lun chau']"
2310.17496,tackling interference induced by data training loops in a/b tests: a   weighted training approach,stat.me cs.lg econ.em,"in modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. however, these data training loops can introduce interference in a/b tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. to address these challenges, we introduce a novel approach called weighted training. this approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. we demonstrate that this approach achieves the least variance among all estimators that do not cause shifts in the training distributions. through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods.",,2023-10-26,2024-02-03,['nian si']
2310.17759,optimal guarantees for algorithmic reproducibility and gradient   complexity in convex optimization,cs.lg math.oc stat.ml,"algorithmic reproducibility measures the deviation in outputs of machine learning algorithms upon minor changes in the training process. previous work suggests that first-order methods would need to trade-off convergence rate (gradient complexity) for better reproducibility. in this work, we challenge this perception and demonstrate that both optimal reproducibility and near-optimal convergence guarantees can be achieved for smooth convex minimization and smooth convex-concave minimax problems under various error-prone oracle settings. particularly, given the inexact initialization oracle, our regularization-based algorithms achieve the best of both worlds - optimal reproducibility and near-optimal gradient complexity - for minimization and minimax optimization. with the inexact gradient oracle, the near-optimal guarantees also hold for minimax optimization. additionally, with the stochastic gradient oracle, we show that stochastic gradient descent ascent is optimal in terms of both reproducibility and gradient complexity. we believe our results contribute to an enhanced understanding of the reproducibility-convergence trade-off in the context of convex optimization.",,2023-10-26,2024-01-09,"['liang zhang', 'junchi yang', 'amin karbasi', 'niao he']"
2310.17816,local discovery by partitioning: polynomial-time causal discovery around   exposure-outcome pairs,stat.ml cs.lg stat.me,"causal discovery is crucial for causal inference in observational studies: it can enable the identification of valid adjustment sets (vas) for unbiased effect estimation. however, global causal discovery is notoriously hard in the nonparametric setting, with exponential time and sample complexity in the worst case. to address this, we propose local discovery by partitioning (ldp), a novel nonparametric local discovery algorithm that is tailored for downstream inference tasks while avoiding the pretreatment assumption. ldp is a constraint-based procedure that partitions variables into subsets defined solely by their causal relation to an exposure-outcome pair. further, ldp returns a vas for the exposure-outcome pair under causal insufficiency and mild sufficient conditions. total independence tests is worst-case quadratic in variable count. asymptotic theoretical guarantees are numerically validated on synthetic graphs. adjustment sets from ldp yield less biased and more precise average treatment effect estimates than baseline discovery algorithms, with ldp outperforming on confounder recall, runtime, and test count for vas discovery. further, ldp ran at least 1300x faster than baselines on a benchmark.",,2023-10-25,2024-02-12,"['jacqueline maasch', 'weishen pan', 'shantanu gupta', 'volodymyr kuleshov', 'kyra gan', 'fei wang']"
2310.18027,bayesian prognostic covariate adjustment with additive mixture priors,stat.me stat.ap stat.ml,"effective and rapid decision-making from randomized controlled trials (rcts) requires unbiased and precise treatment effect inferences. two strategies to address this requirement are to adjust for covariates that are highly correlated with the outcome, and to leverage historical control information via bayes' theorem. we propose a new bayesian prognostic covariate adjustment methodology, referred to as bayesian procova, that combines these two strategies. covariate adjustment in bayesian procova is based on generative artificial intelligence (ai) algorithms that construct a digital twin generator (dtg) for rct participants. the dtg is trained on historical control data and yields a digital twin (dt) probability distribution for each rct participant's outcome under the control treatment. the expectation of the dt distribution, referred to as the prognostic score, defines the covariate for adjustment. historical control information is leveraged via an additive mixture prior with two components: an informative prior probability distribution specified based on historical control data, and a weakly informative prior distribution. the mixture weight determines the extent to which posterior inferences are drawn from the informative component, versus the weakly informative component. this weight has a prior distribution as well, and so the entire additive mixture prior is completely pre-specifiable without involving any rct information. we establish an efficient gibbs algorithm for sampling from the posterior distribution, and derive closed-form expressions for the posterior mean and variance of the treatment effect parameter conditional on the weight, in bayesian procova. we evaluate efficiency gains of bayesian procova via its bias control and variance reduction compared to frequentist procova in simulation studies that encompass different discrepancies. these gains translate to smaller rcts.",,2023-10-27,2024-02-28,"['alyssa m. vanderbeek', 'arman sabbaghi', 'jon r. walsh', 'charles k. fisher']"
2310.18212,robustness of algorithms for causal structure learning to hyperparameter   choice,cs.lg stat.me,"hyperparameters play a critical role in machine learning. hyperparameter tuning can make the difference between state-of-the-art and poor prediction performance for any algorithm, but it is particularly challenging for structure learning due to its unsupervised nature. as a result, hyperparameter tuning is often neglected in favour of using the default values provided by a particular implementation of an algorithm. while there have been numerous studies on performance evaluation of causal discovery algorithms, how hyperparameters affect individual algorithms, as well as the choice of the best algorithm for a specific problem, has not been studied in depth before. this work addresses this gap by investigating the influence of hyperparameters on causal structure learning tasks. specifically, we perform an empirical evaluation of hyperparameter selection for some seminal learning algorithms on datasets of varying levels of complexity. we find that, while the choice of algorithm remains crucial to obtaining state-of-the-art performance, hyperparameter selection in ensemble settings strongly influences the choice of algorithm, in that a poor choice of hyperparameters can lead to analysts using algorithms which do not give state-of-the-art performance for their data.",,2023-10-27,2024-02-20,"['damian machlanski', 'spyridon samothrakis', 'paul clarke']"
2310.18304,a stability principle for learning under non-stationarity,cs.lg cs.ai math.oc stat.ml,"we develop a versatile framework for statistical learning in non-stationary environments. in each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. our theory showcases the adaptability of this approach to unknown non-stationarity. the regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or lipschitz only. at the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.",,2023-10-27,2024-01-22,"['chengpiao huang', 'kaizheng wang']"
2310.18306,supervised and penalized baseline correction,stat.ml cs.lg eess.sp,"spectroscopic measurements can show distorted spectral shapes arising from a mixture of absorbing and scattering contributions. these distortions (or baselines) often manifest themselves as non-constant offsets or low-frequency oscillations. as a result, these baselines can adversely affect analytical and quantitative results. baseline correction is an umbrella term where one applies pre-processing methods to obtain baseline spectra (the unwanted distortions) and then remove the distortions by differencing. however, current state-of-the art baseline correction methods do not utilize analyte concentrations even if they are available, or even if they contribute significantly to the observed spectral variability. we examine a class of state-of-the-art methods (penalized baseline correction) and modify them such that they can accommodate a priori analyte concentrations such that prediction can be enhanced. performance will be assessed on two near infra-red data sets across both classical penalized baseline correction methods (without analyte information) and modified penalized baseline correction methods (leveraging analyte information).",,2023-10-27,2024-02-25,"['erik andries', 'ramin nikzad-langerodi']"
2310.18449,conditional generative representation for black-box optimization with   implicit constraints,stat.ml cs.ce cs.lg,"black-box optimization (bbo) has become increasingly relevant for tackling complex decision-making problems, especially in public policy domains such as police districting. however, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. this paper introduces a novel bbo framework, termed as the conditional and generative black-box optimization (cagebo). this approach leverages a conditional variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a simplified, constraint-free latent space. the cagebo efficiently handles the implicit constraints often found in public policy applications, allowing for optimization in the latent space while evaluating objectives in the original space. we validate our method through a case study on large-scale police districting problems in atlanta, georgia. our results reveal that our cagebo offers notable improvements in performance and efficiency compared to the baselines.",,2023-10-27,2024-02-02,"['wenqian xing', 'jungho lee', 'chong liu', 'shixiang zhu']"
2310.18884,simple and asymmetric graph contrastive learning without augmentations,cs.lg stat.ml,"graph contrastive learning (gcl) has shown superior performance in representation learning in graph-structured data. despite their success, most existing gcl methods rely on prefabricated graph augmentation and homophily assumptions. thus, they fail to generalize well to heterophilic graphs where connected nodes may have different class labels and dissimilar features. in this paper, we study the problem of conducting contrastive learning on homophilic and heterophilic graphs. we find that we can achieve promising performance simply by considering an asymmetric view of the neighboring nodes. the resulting simple algorithm, asymmetric contrastive learning for graphs (graphacl), is easy to implement and does not rely on graph augmentations and homophily assumptions. we provide theoretical and empirical evidence that graphacl can capture one-hop local neighborhood information and two-hop monophily similarity, which are both important for modeling heterophilic graphs. experimental results show that the simple graphacl significantly outperforms state-of-the-art graph contrastive learning and self-supervised learning methods on homophilic and heterophilic graphs. the code of graphacl is available at https://github.com/tengxiao1/graphacl.",,2023-10-28,2024-02-24,"['teng xiao', 'huaisheng zhu', 'zhengyu chen', 'suhang wang']"
2310.19043,differentially private permutation tests: applications to kernel methods,math.st cs.cr cs.lg stat.me stat.ml stat.th,"recent years have witnessed growing concerns about the privacy of sensitive data. in response to these concerns, differential privacy has emerged as a rigorous framework for privacy protection, gaining widespread recognition in both academic and industrial circles. while substantial progress has been made in private data analysis, existing methods often suffer from impracticality or a significant loss of statistical efficiency. this paper aims to alleviate these concerns in the context of hypothesis testing by introducing differentially private permutation tests. the proposed framework extends classical non-private permutation tests to private settings, maintaining both finite-sample validity and differential privacy in a rigorous manner. the power of the proposed test depends on the choice of a test statistic, and we establish general conditions for consistency and non-asymptotic uniform power. to demonstrate the utility and practicality of our framework, we focus on reproducing kernel-based test statistics and introduce differentially private kernel tests for two-sample and independence testing: dpmmd and dphsic. the proposed kernel tests are straightforward to implement, applicable to various types of data, and attain minimax optimal power across different privacy regimes. our empirical evaluations further highlight their competitive power under various synthetic and real-world scenarios, emphasizing their practical value. the code is publicly available to facilitate the implementation of our framework.",,2023-10-29,2024-01-07,"['ilmun kim', 'antonin schrab']"
2310.19064,apple tasting: combinatorial dimensions and minimax rates,cs.lg stat.ml,"in online binary classification under \emph{apple tasting} feedback, the learner only observes the true label if it predicts ``1"". first studied by \cite{helmbold2000apple}, we revisit this classical partial-feedback setting and study online learnability from a combinatorial perspective. we show that the littlestone dimension continues to provide a tight quantitative characterization of apple tasting in the agnostic setting, closing an open question posed by \cite{helmbold2000apple}. in addition, we give a new combinatorial parameter, called the effective width, that tightly quantifies the minimax expected mistakes in the realizable setting. as a corollary, we use the effective width to establish a \emph{trichotomy} of the minimax expected number of mistakes in the realizable setting. in particular, we show that in the realizable setting, the expected number of mistakes of any learner, under apple tasting feedback, can be $\theta(1), \theta(\sqrt{t})$, or $\theta(t)$. this is in contrast to the full-information realizable setting where only $\theta(1)$ and $\theta(t)$ are possible.",,2023-10-29,2024-02-09,"['vinod raman', 'unique subedi', 'ananth raman', 'ambuj tewari']"
2310.19192,emergence of grid-like representations by training recurrent networks   with conformal normalization,q-bio.nc cs.lg stat.ml,"grid cells in the entorhinal cortex of mammalian brains exhibit striking hexagon grid firing patterns in their response maps as the animal (e.g., a rat) navigates in a 2d open environment. in this paper, we study the emergence of the hexagon grid patterns of grid cells based on a general recurrent neural network (rnn) model that captures the navigation process. the responses of grid cells collectively form a high dimensional vector, representing the 2d self-position of the agent. as the agent moves, the vector is transformed by an rnn that takes the velocity of the agent as input. we propose a simple yet general conformal normalization of the input velocity of the rnn, so that the local displacement of the position vector in the high-dimensional neural space is proportional to the local displacement of the agent in the 2d physical space, regardless of the direction of the input velocity. we apply this mechanism to both a linear rnn and nonlinear rnns. theoretically, we provide an understanding that explains the connection between conformal normalization and the emergence of hexagon grid patterns. empirically, we conduct extensive experiments to verify that conformal normalization is crucial for the emergence of hexagon grid patterns, across various types of rnns. the learned patterns share similar profiles to biological grid cells, and the topological properties of the patterns also align with our theoretical understanding.",,2023-10-29,2024-02-19,"['dehong xu', 'ruiqi gao', 'wen-hao zhang', 'xue-xin wei', 'ying nian wu']"
2310.19253,flow-based distributionally robust optimization,cs.lg stat.me stat.ml,"we present a computationally efficient framework, called $\texttt{flowdro}$, for solving flow-based distributionally robust optimization (dro) problems with wasserstein uncertainty sets while aiming to find continuous worst-case distribution (also called the least favorable distribution, lfd) and sample from it. the requirement for lfd to be continuous is so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. to tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models and continuous-time invertible transport maps between the data distribution and the target distribution and develop a wasserstein proximal gradient flow type algorithm. in theory, we establish the equivalence of the solution by optimal transport map to the original formulation, as well as the dual form of the problem through wasserstein calculus and brenier theorem. in practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. we demonstrate its usage in adversarial learning, distributionally robust hypothesis testing, and a new mechanism for data-driven distribution perturbation differential privacy, where the proposed method gives strong empirical performance on high-dimensional real data.",,2023-10-29,2024-02-24,"['chen xu', 'jonghyeok lee', 'xiuyuan cheng', 'yao xie']"
2310.19273,the memory perturbation equation: understanding model's sensitivity to   data,cs.lg cs.ai stat.ml,"understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. to simplify such issues, we present the memory-perturbation equation (mpe) which relates model's sensitivity to perturbation in its training data. derived using bayesian principles, the mpe unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. the proposed equation is expected to be useful for future research on robust and adaptive learning.",,2023-10-30,2024-01-16,"['peter nickl', 'lu xu', 'dharmesh tailor', 'thomas möllenhoff', 'mohammad emtiyaz khan']"
2310.19390,implicit manifold gaussian process regression,stat.ml cs.lg,"gaussian process regression is widely used because of its ability to provide well-calibrated uncertainty estimates and handle small or sparse datasets. however, it struggles with high-dimensional data. one possible way to scale this technique to higher dimensions is to leverage the implicit low-dimensional manifold upon which the data actually lies, as postulated by the manifold hypothesis. prior work ordinarily requires the manifold structure to be explicitly provided though, i.e. given by a mesh or be known to be one of the well-known manifolds like the sphere. in contrast, in this paper we propose a gaussian process regression technique capable of inferring implicit structure directly from data (labeled and unlabeled) in a fully differentiable way. for the resulting model, we discuss its convergence to the mat\'ern gaussian process on the assumed manifold. our technique scales up to hundreds of thousands of data points, and may improve the predictive performance and calibration of the standard gaussian process regression in high-dimensional settings.",,2023-10-30,2024-02-01,"['bernardo fichera', 'viacheslav borovitskiy', 'andreas krause', 'aude billard']"
2310.19491,generator identification for linear sdes with additive and   multiplicative noise,math.st cs.lg stat.ml stat.th,"in this paper, we present conditions for identifying the generator of a linear stochastic differential equation (sde) from the distribution of its solution process with a given fixed initial state. these identifiability conditions are crucial in causal inference using linear sdes as they enable the identification of the post-intervention distributions from its observational distribution. specifically, we derive a sufficient and necessary condition for identifying the generator of linear sdes with additive noise, as well as a sufficient condition for identifying the generator of linear sdes with multiplicative noise. we show that the conditions derived for both types of sdes are generic. moreover, we offer geometric interpretations of the derived identifiability conditions to enhance their understanding. to validate our theoretical results, we perform a series of simulations, which support and substantiate the established findings.",,2023-10-30,2024-01-21,"['yuanyuan wang', 'xi geng', 'wei huang', 'biwei huang', 'mingming gong']"
2310.19608,on feynman--kac training of partial bayesian neural networks,cs.lg stat.ml,"recently, partial bayesian neural networks (pbnns), which only consider a subset of the parameters to be stochastic, were shown to perform competitively with full bayesian neural networks. however, pbnns are often multi-modal in the latent variable space and thus challenging to approximate with parametric models. to address this problem, we propose an efficient sampling-based training strategy, wherein the training of a pbnn is formulated as simulating a feynman--kac model. we then describe variations of sequential monte carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution of this model at a tractable computational cost. using various synthetic and real-world datasets we show that our proposed training scheme outperforms the state of the art in terms of predictive performance.",,2023-10-30,2024-02-27,"['zheng zhao', 'sebastian mair', 'thomas b. schön', 'jens sjölund']"
2310.19683,an online bootstrap for time series,stat.ml cs.lg stat.co stat.me,"resampling methods such as the bootstrap have proven invaluable in the field of machine learning. however, the applicability of traditional bootstrap methods is limited when dealing with large streams of dependent data, such as time series or spatially correlated observations. in this paper, we propose a novel bootstrap method that is designed to account for data dependencies and can be executed online, making it particularly suitable for real-time applications. this method is based on an autoregressive sequence of increasingly dependent resampling weights. we prove the theoretical validity of the proposed bootstrap scheme under general conditions. we demonstrate the effectiveness of our approach through extensive simulations and show that it provides reliable uncertainty quantification even in the presence of complex data dependencies. our work bridges the gap between classical resampling techniques and the demands of modern data analysis, providing a valuable tool for researchers and practitioners in dynamic, data-rich environments.",,2023-10-30,2024-02-26,"['nicolai palm', 'thomas nagler']"
2310.19789,diffenc: variational diffusion with a learned encoder,cs.lg cs.cv stat.ml,"diffusion models may be viewed as hierarchical variational autoencoders (vaes) with two improvements: parameter sharing for the conditional distributions in the generative process and efficient computation of the loss as independent terms over the hierarchy. we consider two changes to the diffusion model that retain these advantages while adding flexibility to the model. firstly, we introduce a data- and depth-dependent mean function in the diffusion process, which leads to a modified diffusion loss. our proposed framework, diffenc, achieves a statistically significant improvement in likelihood on cifar-10. secondly, we let the ratio of the noise variance of the reverse encoder process and the generative process be a free weight parameter rather than being fixed to 1. this leads to theoretical insights: for a finite depth hierarchy, the evidence lower bound (elbo) can be used as an objective for a weighted diffusion loss approach and for optimizing the noise schedule specifically for inference. for the infinite-depth hierarchy, on the other hand, the weight parameter has to be 1 to have a well-defined elbo.",,2023-10-30,2024-02-08,"['beatrix m. g. nielsen', 'anders christensen', 'andrea dittadi', 'ole winther']"
2310.20007,improved bayesian regret bounds for thompson sampling in reinforcement   learning,stat.ml cs.ai cs.lg,"in this paper, we prove the first bayesian regret bounds for thompson sampling in reinforcement learning in a multitude of settings. we simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. this leads to an upper bound of order $\widetilde{o}(h\sqrt{d_{l_1}t})$ in the time inhomogeneous reinforcement learning problem where $h$ is the episode length and $d_{l_1}$ is the kolmogorov $l_1-$dimension of the space of environments. we then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.",,2023-10-30,2024-02-06,"['ahmadreza moradipari', 'mohammad pedramfar', 'modjtaba shokrian zini', 'vaneet aggarwal']"
2310.20285,accelerating generalized linear models by trading off computation for   uncertainty,cs.lg stat.ml,"bayesian generalized linear models (glms) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. however, exact inference in glms is prohibitively expensive for large datasets, thus requiring approximations in practice. the resulting approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. in this work, we introduce a family of iterative methods that explicitly model this error. they are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for glms. as we demonstrate on a realistically large classification problem, our method significantly accelerates training compared to competitive baselines by trading off reduced computation for increased uncertainty.",,2023-10-31,2024-02-07,"['lukas tatzel', 'jonathan wenger', 'frank schneider', 'philipp hennig']"
2310.20708,unexpected improvements to expected improvement for bayesian   optimization,cs.lg cs.na math.na stat.ml,"expected improvement (ei) is arguably the most popular acquisition function in bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. notably, ei and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. this difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. herein, we propose logei, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. we demonstrate that numerical pathologies manifest themselves in ""classic"" analytic ei, expected hypervolume improvement (ehvi), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. our empirical results show that members of the logei family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature.",,2023-10-31,2024-01-18,"['sebastian ament', 'samuel daulton', 'david eriksson', 'maximilian balandat', 'eytan bakshy']"
2311.00109,fairwasp: fast and optimal fair wasserstein pre-processing,cs.lg stat.ml,"recent years have seen a surge of machine learning approaches aimed at reducing disparities in model outputs across different subgroups. in many settings, training data may be used in multiple downstream applications by different users, which means it may be most effective to intervene on the training data itself. in this work, we present fairwasp, a novel pre-processing approach designed to reduce disparities in classification datasets without modifying the original data. fairwasp returns sample-level weights such that the reweighted dataset minimizes the wasserstein distance to the original dataset while satisfying (an empirical version of) demographic parity, a popular fairness criterion. we show theoretically that integer weights are optimal, which means our method can be equivalently understood as duplicating or eliminating samples. fairwasp can therefore be used to construct datasets which can be fed into any classification method, not just methods which accept sample weights. our work is based on reformulating the pre-processing task as a large-scale mixed-integer program (mip), for which we propose a highly efficient algorithm based on the cutting plane method. experiments demonstrate that our proposed optimization algorithm significantly outperforms state-of-the-art commercial solvers in solving both the mip and its linear program relaxation. further experiments highlight the competitive performance of fairwasp in reducing disparities while preserving accuracy in downstream classification settings.",,2023-10-31,2024-02-08,"['zikai xiong', 'niccolò dalmasso', 'alan mishler', 'vamsi k. potluru', 'tucker balch', 'manuela veloso']"
2311.00541,an embedded diachronic sense change model with a case study from ancient   greek,cs.cl stat.me,"word meanings change over time, and word senses evolve, emerge or die out in the process. for ancient languages, where the corpora are often small and sparse, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. gasc (genre-aware semantic change) and disc (diachronic sense change) are existing generative models that have been used to analyse sense change for target words from an ancient greek text corpus, using unsupervised learning without the help of any pre-training. these models represent the senses of a given target word such as ""kosmos"" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. the models are fitted using markov chain monte carlo (mcmc) methods to measure temporal changes in these representations. in this paper, we introduce edisc, an embedded disc model, which combines word embeddings with disc to provide superior model performance. we show empirically that edisc offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with mcmc methods. we also discuss the challenges of fitting these models.",,2023-11-01,2024-01-11,"['schyan zafar', 'geoff k. nicholls']"
2311.00636,kronecker-factored approximate curvature for modern neural network   architectures,cs.lg stat.ml,"the core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\textit{weight-sharing}$. kronecker-factored approximate curvature (k-fac), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. however, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. in this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of k-fac -- $\textit{expand}$ and $\textit{reduce}$. we show that they are exact for deep linear networks with weight-sharing in their respective setting. notably, k-fac-reduce is generally faster than k-fac-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a wide resnet. finally, we observe little difference between these two k-fac variations when using them to train both a graph neural network and a vision transformer. however, both variations are able to reach a fixed validation metric target in $50$-$75\%$ of the number of steps of a first-order reference run, which translates into a comparable improvement in wall-clock time. this highlights the potential of applying k-fac to modern neural network architectures.",,2023-11-01,2024-01-11,"['runa eschenhagen', 'alexander immer', 'richard e. turner', 'frank schneider', 'philipp hennig']"
2311.00983,optimizing inventory routing: a decision-focused learning approach using   neural networks,cs.lg cs.ai cs.sy eess.sy math.oc stat.ml,"inventory routing problem (irp) is a crucial challenge in supply chain management as it involves optimizing efficient route selection while considering the uncertainty of inventory demand planning. to solve irps, usually a two-stage approach is employed, where demand is predicted using machine learning techniques first, and then an optimization algorithm is used to minimize routing costs. our experiment shows machine learning models fall short of achieving perfect accuracy because inventory levels are influenced by the dynamic business environment, which, in turn, affects the optimization problem in the next stage, resulting in sub-optimal decisions. in this paper, we formulate and propose a decision-focused learning-based approach to solving real-world irps. this approach directly integrates inventory prediction and routing optimization within an end-to-end system potentially ensuring a robust supply chain strategy.",,2023-11-02,,"['md shafikul islam', 'azmine toushik wasi']"
2311.01139,add and thin: diffusion for temporal point processes,cs.lg stat.ml,"autoregressive neural networks within the temporal point process (tpp) framework have become the standard for modeling continuous-time event data. even though these models can expressively capture event sequences in a one-step-ahead fashion, they are inherently limited for long-term forecasting applications due to the accumulation of errors caused by their sequential nature. to overcome these limitations, we derive add-thin, a principled probabilistic denoising diffusion model for tpps that operates on entire event sequences. unlike existing diffusion approaches, add-thin naturally handles data with discrete and continuous components. in experiments on synthetic and real-world datasets, our model matches the state-of-the-art tpp models in density estimation and strongly outperforms them in forecasting.",,2023-11-02,2024-02-20,"['david lüdke', 'marin biloš', 'oleksandr shchur', 'marten lienen', 'stephan günnemann']"
2311.01356,upper and lower bounds for the lipschitz constant of random neural   networks,stat.ml cs.lg math.pr,"empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. the worst-case robustness against these so-called adversarial examples can be quantified by the lipschitz constant of the neural network. in this paper, we study upper and lower bounds for the lipschitz constant of random relu neural networks. specifically, we assume that the weights and biases follow a generalization of the he initialization, where general symmetric distributions for the biases are permitted. for shallow neural networks, we characterize the lipschitz constant up to an absolute numerical constant. for deep networks with fixed depth and sufficiently large width, our established upper bound is larger than the lower bound by a factor that is logarithmic in the width.",,2023-11-02,2024-01-18,"['paul geuchen', 'thomas heindl', 'dominik stöger', 'felix voigtlaender']"
2311.01644,should under-parameterized student networks copy or average teacher   weights?,cs.lg cs.ne stat.ml,"any continuous function $f^*$ can be approximated arbitrarily well by a neural network with sufficiently many neurons $k$. we consider the case when $f^*$ itself is a neural network with one hidden layer and $k$ neurons. approximating $f^*$ with a neural network with $n< k$ neurons can thus be seen as fitting an under-parameterized ""student"" network with $n$ neurons to a ""teacher"" network with $k$ neurons. as the student has fewer neurons than the teacher, it is unclear, whether each of the $n$ student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. for shallow neural networks with erf activation function and for the standard gaussian input distribution, we prove that ""copy-average"" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. moreover, the optimum among such configurations is reached when $n-1$ student neurons each copy one teacher neuron and the $n$-th student neuron averages the remaining $k-n+1$ teacher neurons. for the student network with $n=1$ neuron, we provide additionally a closed-form solution of the non-trivial critical point(s) for commonly used activation functions through solving an equivalent constrained optimization problem. empirically, we find for the erf activation function that gradient flow converges either to the optimal copy-average critical point or to another point where each student neuron approximately copies a different teacher neuron. finally, we find similar results for the relu activation function, suggesting that the optimal solution of underparameterized networks has a universal structure.",,2023-11-02,2024-01-15,"['berfin şimşek', 'amire bendjeddou', 'wulfram gerstner', 'johanni brea']"
2311.01771,efficient generalized low-rank tensor contextual bandits,cs.lg stat.ml,"in this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. to this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus can be represented by a tensor. in this formulation, the reward is determined through a generalized linear function applied to the inner product of the action's feature tensor and a fixed but unknown parameter tensor with a low tubal rank. to effectively achieve the trade-off between exploration and exploitation, we introduce a novel algorithm called ""generalized low-rank tensor exploration subspace then refine"" (g-lowtestr). this algorithm first collects raw data to explore the intrinsic low-rank tensor subspace information embedded in the decision-making scenario, and then converts the original problem into an almost lower-dimensional generalized linear contextual bandits problem. rigorous theoretical analysis shows that the regret bound of g-lowtestr is superior to those in vectorization and matricization cases. we conduct a series of simulations and real data experiments to further highlight the effectiveness of g-lowtestr, leveraging its ability to capitalize on the low-rank tensor structure for enhanced learning.",,2023-11-03,2024-01-17,"['qianxin yi', 'yiyang yang', 'shaojie tang', 'jiapeng liu', 'yao wang']"
2311.01797,on the generalization properties of diffusion models,cs.lg stat.ml,"diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. this work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. we establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($o(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progressively increasing distances between modes. this precisely elucidates the adverse effect of ""modes shift"" in ground truths on the model generalization. moreover, these estimates are not solely theoretical constructs but have also been confirmed through numerical simulations. our findings contribute to the rigorous understanding of diffusion models' generalization properties and provide insights that may guide practical applications.",,2023-11-03,2024-01-12,"['puheng li', 'zhong li', 'huishuai zhang', 'jiang bian']"
2311.02467,individualized policy evaluation and learning under clustered network   interference,stat.me cs.lg econ.em,"while there now exists a large literature on policy evaluation and learning, much of prior work assumes that the treatment assignment of one unit does not affect the outcome of another unit. unfortunately, ignoring interference may lead to biased policy evaluation and ineffective learned policies. for example, treating influential individuals who have many friends can generate positive spillover effects, thereby improving the overall performance of an individualized treatment rule (itr). we consider the problem of evaluating and learning an optimal itr under clustered network interference (also known as partial interference) where clusters of units are sampled from a population and units may influence one another within each cluster. unlike previous methods that impose strong restrictions on spillover effects, the proposed methodology only assumes a semiparametric structural model where each unit's outcome is an additive function of individual treatments within the cluster. under this model, we propose an estimator that can be used to evaluate the empirical performance of an itr. we show that this estimator is substantially more efficient than the standard inverse probability weighting estimator, which does not impose any assumption about spillover effects. we derive the finite-sample regret bound for a learned itr, showing that the use of our efficient evaluation estimator leads to the improved performance of learned policies. finally, we conduct simulation and empirical studies to illustrate the advantages of the proposed methodology.",,2023-11-04,2024-02-04,"['yi zhang', 'kosuke imai']"
2311.02516,forward $\chi^2$ divergence based variational importance sampling,cs.lg stat.co stat.ml,"maximizing the log-likelihood is a crucial aspect of learning latent variable models, and variational inference (vi) stands as the commonly adopted method. however, vi can encounter challenges in achieving a high log-likelihood when dealing with complicated posterior distributions. in response to this limitation, we introduce a novel variational importance sampling (vis) approach that directly estimates and maximizes the log-likelihood. vis leverages the optimal proposal distribution, achieved by minimizing the forward $\chi^2$ divergence, to enhance log-likelihood estimation. we apply vis to various popular latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. results demonstrate that our approach consistently outperforms state-of-the-art baselines, both in terms of log-likelihood and model parameter estimation.",,2023-11-04,2024-02-02,"['chengrui li', 'yule wang', 'weihan li', 'anqi wu']"
2311.02794,modelling cellular perturbations with the sparse additive mechanism   shift variational autoencoder,stat.ml cs.ai cs.lg q-bio.qm,"generative models of observations under interventions have been a vibrant topic of interest across machine learning and the sciences in recent years. for example, in drug discovery, there is a need to model the effects of diverse interventions on cells in order to characterize unknown biological mechanisms of action. we propose the sparse additive mechanism shift variational autoencoder, sams-vae, to combine compositionality, disentanglement, and interpretability for perturbation models. sams-vae models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects. crucially, sams-vae sparsifies these global latent variables for individual perturbations to identify disentangled, perturbation-specific latent subspaces that are flexibly composable. we evaluate sams-vae both quantitatively and qualitatively on a range of tasks using two popular single cell sequencing datasets. in order to measure perturbation-specific model-properties, we also introduce a framework for evaluation of perturbation models based on average treatment effects with links to posterior predictive checks. sams-vae outperforms comparable models in terms of generalization across in-distribution and out-of-distribution tasks, including a combinatorial reasoning task under resource paucity, and yields interpretable latent structures which correlate strongly to known biological mechanisms. our results suggest sams-vae is an interesting addition to the modeling toolkit for machine learning-driven scientific discovery.",,2023-11-05,2024-01-15,"['michael bereket', 'theofanis karaletsos']"
2311.03242,approximating langevin monte carlo with resnet-like neural network   architectures,cs.lg math.pr math.st stat.ml stat.th,"we sample from a given target distribution by constructing a neural network which maps samples from a simple reference, e.g. the standard normal distribution, to samples from the target. to that end, we propose using a neural network architecture inspired by the langevin monte carlo (lmc) algorithm. based on lmc perturbation results, we show approximation rates of the proposed architecture for smooth, log-concave target distributions measured in the wasserstein-$2$ distance. the analysis heavily relies on the notion of sub-gaussianity of the intermediate measures of the perturbed lmc process. in particular, we derive bounds on the growth of the intermediate variance proxies under different assumptions on the perturbations. moreover, we propose an architecture similar to deep residual neural networks and derive expressivity results for approximating the sample to target distribution map.",,2023-11-06,2024-01-22,"['charles miranda', 'janina schütte', 'david sommer', 'martin eigel']"
2311.03490,"analytics, have some humility: a statistical view of fourth-down   decision making",stat.ap,"the standard mathematical approach to fourth-down decision making in american football is to make the decision that maximizes estimated win probability. win probability estimates arise from machine learning models fit from historical data. these models, however, are overfit high-variance estimators, exacerbated by the highly correlated nature of football play-by-play data. thus, it is imperative to knit uncertainty quantification into the fourth-down decision procedure; we do so using bootstrapping. we find that uncertainty in the estimated optimal fourth-down decision is far greater than that currently expressed by sports analysts in popular sports media. our contribution is a major advance in fourth-down strategic decision making: far fewer fourth-down decisions are as obvious as analysts claim.",,2023-11-06,2024-01-19,"['ryan s. brill', 'ronald yurko', 'abraham j. wyner']"
2311.04041,hilbert's projective metric for functions of bounded growth and   exponential convergence of sinkhorn's algorithm,math.pr math.oc stat.ml,"motivated by the entropic optimal transport problem in unbounded settings, we study versions of hilbert's projective metric for spaces of integrable functions of bounded growth. these versions of hilbert's metric originate from cones which are relaxations of the cone of all non-negative functions, in the sense that they include all functions having non-negative integral values when multiplied with certain test functions. we show that kernel integral operators are contractions with respect to suitable specifications of such metrics even for kernels which are not bounded away from zero, provided that the decay to zero of the kernel is controlled. as an application to entropic optimal transport, we show exponential convergence of sinkhorn's algorithm in settings where the marginal distributions have sufficiently light tails compared to the growth of the cost function.",,2023-11-07,2024-01-17,['stephan eckstein']
2311.04153,"kernel-, mean- and noise-marginalised gaussian processes for exoplanet   transits and $h_0$ inference",astro-ph.co astro-ph.ep astro-ph.im cs.lg stat.ml,"using a fully bayesian approach, gaussian process regression is extended to include marginalisation over the kernel choice and kernel hyperparameters. in addition, bayesian model comparison via the evidence enables direct kernel comparison. the calculation of the joint posterior was implemented with a transdimensional sampler which simultaneously samples over the discrete kernel choice and their hyperparameters by embedding these in a higher-dimensional space, from which samples are taken using nested sampling. kernel recovery and mean function inference were explored on synthetic data from exoplanet transit light curve simulations. subsequently, the method was extended to marginalisation over mean functions and noise models and applied to the inference of the present-day hubble parameter, $h_0$, from real measurements of the hubble parameter as a function of redshift, derived from the cosmologically model-independent cosmic chronometer and $\lambda$cdm-dependent baryon acoustic oscillation observations. the inferred $h_0$ values from the cosmic chronometers, baryon acoustic oscillations and combined datasets are $h_0= 66 \pm 6\, \mathrm{km}\,\mathrm{s}^{-1}\,\mathrm{mpc}^{-1}$, $h_0= 67 \pm 10\, \mathrm{km}\,\mathrm{s}^{-1}\,\mathrm{mpc}^{-1}$ and $h_0= 69 \pm 6\, \mathrm{km}\,\mathrm{s}^{-1}\,\mathrm{mpc}^{-1}$, respectively. the kernel posterior of the cosmic chronometers dataset prefers a non-stationary linear kernel. finally, the datasets are shown to be not in tension with $\ln r=12.17\pm 0.02$.",10.1093/mnras/stae087,2023-11-07,2024-02-12,"['namu kroupa', 'david yallup', 'will handley', 'michael hobson']"
2311.04696,quantification and cross-fitting inference of asymmetric relations under   generative exposure mapping models,stat.me math.st stat.th,"in many practical studies, learning directionality between a pair of variables is of great interest while notoriously hard when their underlying relation is nonlinear. this paper presents a method that examines asymmetry in exposure-outcome pairs when a priori assumptions about their relative ordering are unavailable. our approach utilizes a framework of generative exposure mapping (gem) to study asymmetric relations in continuous exposure-outcome pairs, through which we can capture distributional asymmetries with no prefixed variable ordering. we propose a coefficient of asymmetry to quantify relational asymmetry using shannon's entropy analytics as well as statistical estimation and inference for such an estimand of directionality. large-sample theoretical guarantees are established for cross-fitting inference techniques. the proposed methodology is extended to allow both measured confounders and contamination in outcome measurements, which is extensively evaluated through extensive simulation studies and real data applications.",,2023-11-08,2024-02-12,"['soumik purkayastha', 'peter x. -k. song']"
2311.05436,fair coresets via optimal transport,stat.ml cs.cy cs.lg,"data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. at the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. current approaches create fair synthetic representative samples by optimizing local properties relative to the original samples, but their effect on downstream learning processes has yet to be explored. in this work, we present fair wasserstein coresets (fwc), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. fwc minimizes the wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. we show that an unconstrained version of fwc is equivalent to lloyd's algorithm for k-medians and k-means clustering. experiments conducted on both synthetic and real datasets show that fwc: (i) achieves a competitive fairness-performance tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (gpt-3.5 and gpt-4).",,2023-11-09,2024-02-08,"['zikai xiong', 'niccolò dalmasso', 'shubham sharma', 'freddy lecue', 'daniele magazzeni', 'vamsi k. potluru', 'tucker balch', 'manuela veloso']"
2311.05672,conditional optimal transport on function spaces,math.oc math.pr stat.co stat.ml,"we present a systematic study of conditional triangular transport maps in function spaces from the perspective of optimal transportation and with a view towards amortized bayesian inference. more specifically, we develop a theory of constrained optimal transport problems that describe block-triangular monge maps that characterize conditional measures along with their kantorovich relaxations. this generalizes the theory of optimal triangular transport to separable infinite-dimensional function spaces with general cost functions. we further tailor our results to the case of bayesian inference problems and obtain regularity estimates on the conditioning maps from the prior to the posterior. finally, we present numerical experiments that demonstrate the computational applicability of our theoretical results for amortized and likelihood-free inference of functional parameters.",,2023-11-09,2024-02-06,"['bamdad hosseini', 'alexander w. hsu', 'amirhossein taghvaei']"
2311.06130,high-dimensional mixed-categorical gaussian processes with application   to multidisciplinary design optimization for a green aircraft,math.oc cs.ai stat.ml,"recently, there has been a growing interest in mixed-categorical metamodels based on gaussian process (gp) for bayesian optimization. in this context, different approaches can be used to build the mixed-categorical gp. many of these approaches involve a high number of hyperparameters; in fact, the more general and precise the strategy used to build the gp, the greater the number of hyperparameters to estimate. this paper introduces an innovative dimension reduction algorithm that relies on partial least squares regression to reduce the number of hyperparameters used to build a mixed-variable gp. our goal is to generalize classical dimension reduction techniques commonly used within gp (for continuous inputs) to handle mixed-categorical inputs. the good potential of the proposed method is demonstrated in both structural and multidisciplinary application contexts. the targeted applications include the analysis of a cantilever beam as well as the optimization of a green aircraft, resulting in a significant 439-kilogram reduction in fuel consumption during a single mission.",,2023-11-10,2024-02-02,"['paul saves', 'youssef diouane', 'nathalie bartoli', 'thierry lefebvre', 'joseph morlier']"
2311.06397,predicting stock price of construction companies using weighted ensemble   learning,stat.ap,"modeling the behavior of stock price data has always been one of the challengeous applications of artificial intelligence (ai) and machine learning (ml) due to its high complexity and dependence on various conditions. recent studies show that this will be difficult to do with just one learning model. the problem can be more complex for companies of construction section, due to the dependency of their behavior on more conditions. this study aims to provide a hybrid model for improving the accuracy of prediction for stock price index of companies in construction section. the contribution of this paper can be considered as follows: first, a combination of several prediction models is used to predict stock price, so that learning models can cover each other's error. in this research, an ensemble model based on artificial neural network (ann), gaussian process regression (gpr) and classification and regression tree (cart) is presented for predicting stock price index. second, the optimization technique is used to determine the effect of each learning model on the prediction result. for this purpose, first all three mentioned algorithms process the data simultaneously and perform the prediction operation. then, using the cuckoo search (cs) algorithm, the output weight of each algorithm is determined as a coefficient. finally, using the ensemble technique, these results are combined and the final output is generated through weighted averaging on optimal coefficients. the results showed that using cs optimization in the proposed ensemble system is highly effective in reducing prediction error. comparing the evaluation results of the proposed system with similar algorithms, indicates that our model is more accurate and can be useful for predicting stock price index in real-world scenarios.",,2023-11-10,2024-02-19,['xinyuan song']
2311.06748,how do minimum-norm shallow denoisers look in function space?,stat.ml cs.lg,"neural network (nn) denoisers are an essential building block in many common tasks, ranging from image reconstruction to image generation. however, the success of these models is not well understood from a theoretical perspective. in this paper, we aim to characterize the functions realized by shallow relu nn denoisers -- in the common theoretical setting of interpolation (i.e., zero training loss) with a minimal representation cost (i.e., minimal $\ell^2$ norm weights). first, for univariate data, we derive a closed form for the nn denoiser function, find it is contractive toward the clean data points, and prove it generalizes better than the empirical mmse estimator at a low noise level. next, for multivariate data, we find the nn denoiser functions in a closed form under various geometric assumptions on the training data: data contained in a low-dimensional subspace, data contained in a union of one-sided rays, or several types of simplexes. these functions decompose into a sum of simple rank-one piecewise linear interpolations aligned with edges and/or faces connecting training samples. we empirically verify this alignment phenomenon on synthetic data and real images.",,2023-11-12,2024-01-16,"['chen zeno', 'greg ongie', 'yaniv blumenfeld', 'nir weinberger', 'daniel soudry']"
2311.07454,discrete nonparametric causal discovery under latent class confounding,cs.lg cs.cc math.st stat.th,"directed acyclic graphs are used to model the causal structure of a system. ``causal discovery'' describes the problem of learning this structure from data. when data is an aggregate from multiple sources (populations or environments), global confounding obscures conditional independence properties that drive many causal discovery algorithms. this setting is sometimes known as a mixture model or a latent class. while some modern methods for causal discovery are able to work around unobserved confounding in specific cases, the only known ways to deal with a global confounder involve parametric assumptions. that are unsuitable for discrete distributions.focusing on discrete and non-parametric observed variables, we demonstrate that causal discovery can still be identifiable under bounded latent classes. the feasibility of this problem is governed by a trade-off between the cardinality of the global confounder, the cardinalities of the observed variables, and the sparsity of the causal structure.",,2023-11-13,2024-02-13,"['bijan mazaheri', 'spencer gordon', 'yuval rabani', 'leonard schulman']"
2311.07511,uncertainty estimation in satellite precipitation interpolation with   machine learning,stat.ml cs.lg physics.ao-ph stat.ap stat.me,"merging satellite and gauge data with machine learning produces high-resolution precipitation datasets, but uncertainty estimates are often missing. we address this gap by benchmarking six algorithms, mostly novel for this task, for quantifying predictive uncertainty in spatial interpolation. on 15 years of monthly data over the contiguous united states (conus), we compared quantile regression (qr), quantile regression forests (qrf), generalized random forests (grf), gradient boosting machines (gbm), light gradient boosting machines (lightgbm), and quantile regression neural networks (qrnn). their ability to issue predictive precipitation quantiles at nine quantile levels (0.025, 0.050, 0.100, 0.250, 0.500, 0.750, 0.900, 0.950, 0.975), approximating the full probability distribution, was evaluated using quantile scoring functions and the quantile scoring rule. feature importance analysis revealed satellite precipitation (persiann (precipitation estimation from remotely sensed information using artificial neural networks) and imerg (integrated multi-satellite retrievals) datasets) as the most informative predictor, followed by gauge elevation and distance to satellite grid points. compared to qr, lightgbm showed improved performance with respect to the quantile scoring rule by 11.10%, followed by qrf (7.96%), grf (7.44%), gbm (4.64%) and qrnn (1.73%). notably, lightgbm outperformed all random forest variants, the current standard in spatial interpolation with machine learning. to conclude, we propose a suite of machine learning algorithms for estimating uncertainty in interpolating spatial data, supported with a formal evaluation framework based on scoring functions and scoring rules.",,2023-11-13,2024-02-26,"['georgia papacharalampous', 'hristos tyralis', 'nikolaos doulamis', 'anastasios doulamis']"
2311.07972,residual importance weighted transfer learning for high-dimensional   linear regression,stat.me,"transfer learning is an emerging paradigm for leveraging multiple sources to improve the statistical inference on a single target. in this paper, we propose a novel approach named residual importance weighted transfer learning (riw-tl) for high-dimensional linear models built on penalized likelihood. compared to existing methods such as trans-lasso that selects sources in an all-in-all-out manner, riw-tl includes samples via importance weighting and thus may permit more effective sample use. to determine the weights, remarkably riw-tl only requires the knowledge of one-dimensional densities dependent on residuals, thus overcoming the curse of dimensionality of having to estimate high-dimensional densities in naive importance weighting. we show that the oracle riw-tl provides a faster rate than its competitors and develop a cross-fitting procedure to estimate this oracle. we discuss variants of riw-tl by adopting different choices for residual weighting. the theoretical properties of riw-tl and its variants are established and compared with those of lasso and trans-lasso. extensive simulation and a real data analysis confirm its advantages.",,2023-11-14,2024-01-03,"['junlong zhao', 'shengbin zheng', 'chenlei leng']"
2311.08149,modeling complex disease trajectories using deep generative models with   semi-supervised latent processes,cs.lg stat.ml,"in this paper, we propose a deep generative time series approach using latent temporal processes for modeling and holistically analyzing complex disease trajectories. we aim to find meaningful temporal latent representations of an underlying generative process that explain the observed disease trajectories in an interpretable and comprehensive way. to enhance the interpretability of these latent temporal processes, we develop a semi-supervised approach for disentangling the latent space using established medical concepts. by combining the generative approach with medical knowledge, we leverage the ability to discover novel aspects of the disease while integrating medical concepts into the model. we show that the learned temporal latent processes can be utilized for further data analysis and clinical hypothesis testing, including finding similar patients and clustering the disease into new sub-types. moreover, our method enables personalized online monitoring and prediction of multivariate time series including uncertainty quantification. we demonstrate the effectiveness of our approach in modeling systemic sclerosis, showcasing the potential of our machine learning model to capture complex disease trajectories and acquire new medical knowledge.",,2023-11-14,2024-01-29,"['cécile trottet', 'manuel schürch', 'ahmed allam', 'imon barua', 'liubov petelytska', 'oliver distler', 'anna-maria hoffmann-vold', 'michael krauthammer', 'the eustar collaborators']"
2311.08168,time-uniform confidence spheres for means of random vectors,math.st cs.it math.it stat.me stat.ml stat.th,"we derive and study time-uniform confidence spheres -- confidence sphere sequences (csss) -- which contain the mean of random vectors with high probability simultaneously across all sample sizes. inspired by the original work of catoni and giulini, we unify and extend their analysis to cover both the sequential setting and to handle a variety of distributional assumptions. our results include an empirical-bernstein css for bounded random vectors (resulting in a novel empirical-bernstein confidence interval with asymptotic width scaling proportionally to the true unknown variance), csss for sub-$\psi$ random vectors (which includes sub-gamma, sub-poisson, and sub-exponential), and csss for heavy-tailed random vectors (two moments only). finally, we provide two csss that are robust to contamination by huber noise. the first is a robust version of our empirical-bernstein css, and the second extends recent work in the univariate setting to heavy-tailed multivariate distributions.",,2023-11-14,2024-02-28,"['ben chugg', 'hongjian wang', 'aaditya ramdas']"
2311.08527,inferring the long-term causal effects of long-term treatments from   short-term experiments,stat.ap stat.me,"we study inference on the long-term causal effect of a continual exposure to a novel intervention, which we term a long-term treatment, based on an experiment involving only short-term observations. key examples include the long-term health effects of regularly-taken medicine or of environmental hazards and the long-term effects on users of changes to an online platform. this stands in contrast to short-term treatments or ``shocks,"" whose long-term effect can reasonably be mediated by short-term observations, enabling the use of surrogate methods. long-term treatments by definition have direct effects on long-term outcomes via continual exposure, so surrogacy conditions cannot reasonably hold. we connect the problem with offline reinforcement learning, leveraging doubly-robust estimators to estimate long-term causal effects for long-term treatments and construct confidence intervals.",,2023-11-14,2024-02-06,"['allen tran', 'aurélien bibaut', 'nathan kallus']"
2311.09018,on the foundation of distributionally robust reinforcement learning,cs.lg cs.sy eess.sy math.oc stat.ml,"motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (drrl). this is accomplished through a comprehensive modeling framework centered around distributionally robust markov decision processes (drmdps). this framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. by unifying and extending existing formulations, we rigorously construct drmdps that embraces various modeling attributes for both the decision maker and the adversary. these attributes include adaptability granularity, exploring history-dependent, markov, and markov time-homogeneous decision maker and adversary dynamics. additionally, we delve into the flexibility of shifts induced by the adversary, examining sa and s-rectangularity. within this drmdp framework, we investigate conditions for the existence or absence of the dynamic programming principle (dpp). from an algorithmic standpoint, the existence of dpp holds significant implications, as the vast majority of existing data and computationally efficiency rl algorithms are reliant on the dpp. to study its existence, we comprehensively examine combinations of controller and adversary attributes, providing streamlined proofs grounded in a unified methodology. we also offer counterexamples for settings in which a dpp with full generality is absent.",,2023-11-15,2024-01-19,"['shengbo wang', 'nian si', 'jose blanchet', 'zhengyuan zhou']"
2311.09200,are normalizing flows the key to unlocking the exponential mechanism? a   path through the accuracy-privacy ceiling constraining differentially private   ml,stat.ml cs.ai cs.cr cs.lg math.pr,"the state of the art and de facto standard for differentially private machine learning (ml) is differentially private stochastic gradient descent (dpsgd). yet, the method is inherently wasteful. by adding noise to every gradient, it diminishes the overall privacy with every gradient step. despite 15 years of fruitful research advancing the composition theorems, sub-sampling methods, and implementation techniques, adequate accuracy and privacy is often unattainable with current private ml methods. meanwhile, the exponential mechanism (expm), designed for private optimization, has been historically sidelined from privately training modern ml algorithms primarily because expm requires sampling from a historically intractable density. despite the recent discovery of normalizing flow models (nfs), expressive deep networks for approximating intractable distributions, expm remains in the background. our position is that leveraging nfs to circumvent historic obstructions of expm is a potentially transformational solution for differentially private ml worth attention. we introduce a new training method, expm+nf, as a potential alternative to dpsgd, and we provide experiment with logistic regression and a modern deep learning model to test whether training via expm+nf is viable with ""good"" privacy parameters. under the assumption that the nf output distribution is the expm distribution, we are able to achieve $\varepsilon$ a low as $1\mathrm{e}{-3}$ -- three orders of magnitude stronger privacy with similar accuracy. this work outlines a new avenue for advancing differentially private ml, namely discovering nf approximation guarantees. code to be provided after review.",,2023-11-15,2024-02-02,"['robert a. bridges', 'vandy j. tombs', 'christopher b. stanley']"
2311.10246,surprisal driven $k$-nn for robust and interpretable nonparametric   learning,cs.lg cs.ai stat.ml,"nonparametric learning is a fundamental concept in machine learning that aims to capture complex patterns and relationships in data without making strong assumptions about the underlying data distribution. owing to simplicity and familiarity, one of the most well-known algorithms under this paradigm is the $k$-nearest neighbors ($k$-nn) algorithm. driven by the usage of machine learning in safety-critical applications, in this work, we shed new light on the traditional nearest neighbors algorithm from the perspective of information theory and propose a robust and interpretable framework for tasks such as classification, regression, density estimation, and anomaly detection using a single model. we can determine data point weights as well as feature contributions by calculating the conditional entropy for adding a feature without the need for explicit model training. this allows us to compute feature contributions by providing detailed data point influence weights with perfect attribution and can be used to query counterfactuals. instead of using a traditional distance measure which needs to be scaled and contextualized, we use a novel formulation of $\textit{surprisal}$ (amount of information required to explain the difference between the observed and expected result). finally, our work showcases the architecture's versatility by achieving state-of-the-art results in classification and anomaly detection, while also attaining competitive results for regression across a statistically significant number of datasets.",,2023-11-16,2024-02-02,"['amartya banerjee', 'christopher j. hazard', 'jacob beel', 'cade mack', 'jack xia', 'michael resnick', 'will goddin']"
2311.10590,edugym: an environment and notebook suite for reinforcement learning   education,cs.lg cs.ai cs.cy stat.ml,"due to the empirical success of reinforcement learning, an increasing number of students study the subject. however, from our practical teaching experience, we see students entering the field (bachelor, master and early phd) often struggle. on the one hand, textbooks and (online) lectures provide the fundamentals, but students find it hard to translate between equations and code. on the other hand, public codebases do provide practical examples, but the implemented algorithms tend to be complex, and the underlying test environments contain multiple reinforcement learning challenges at once. although this is realistic from a research perspective, it often hinders educational conceptual understanding. to solve this issue we introduce edugym, a set of educational reinforcement learning environments and associated interactive notebooks tailored for education. each edugym environment is specifically designed to illustrate a certain aspect/challenge of reinforcement learning (e.g., exploration, partial observability, stochasticity, etc.), while the associated interactive notebook explains the challenge and its possible solution approaches, connecting equations and code in a single document. an evaluation among rl students and researchers shows 86% of them think edugym is a useful tool for reinforcement learning education. all notebooks are available from https://www.edugym.org/, while the full software package can be installed from https://github.com/rlg-leiden/edugym.",,2023-11-17,2024-02-22,"['thomas m. moerland', 'matthias müller-brockhausen', 'zhao yang', 'andrius bernatavicius', 'koen ponse', 'tom kouwenhoven', 'andreas sauter', 'michiel van der meer', 'bram renting', 'aske plaat']"
2311.10900,a powerful rank-based correction to multiple testing under positive   dependency,stat.me math.st stat.ml stat.th,"we develop a novel multiple hypothesis testing correction with family-wise error rate (fwer) control that efficiently exploits positive dependencies between potentially correlated statistical hypothesis tests. our proposed algorithm $\texttt{max-rank}$ is conceptually straight-forward, relying on the use of a $\max$-operator in the rank domain of computed test statistics. we compare our approach to the frequently employed bonferroni correction, theoretically and empirically demonstrating its superiority over bonferroni in the case of existing positive dependency, and its equivalence otherwise. our advantage over bonferroni increases as the number of tests rises, and we maintain high statistical power whilst ensuring fwer control. we specifically frame our algorithm in the context of parallel permutation testing, a scenario that arises in our primary application of conformal prediction, a recently popularized approach for quantifying uncertainty in complex predictive settings.",,2023-11-17,2024-01-25,"['alexander timans', 'christoph-nikolas straehle', 'kaspar sakmann', 'eric nalisnick']"
2311.11463,designing monitoring strategies for deployed machine learning   algorithms: navigating performativity through a causal lens,cs.lg stat.ml,"after a machine learning (ml)-based system is deployed, monitoring its performance is important to ensure the safety and effectiveness of the algorithm over time. when an ml algorithm interacts with its environment, the algorithm can affect the data-generating mechanism and be a major source of bias when evaluating its standalone performance, an issue known as performativity. although prior work has shown how to validate models in the presence of performativity using causal inference techniques, there has been little work on how to monitor models in the presence of performativity. unlike the setting of model validation, there is much less agreement on which performance metrics to monitor. different monitoring criteria impact how interpretable the resulting test statistic is, what assumptions are needed for identifiability, and the speed of detection. when this choice is further coupled with the decision to use observational versus interventional data, ml deployment teams are faced with a multitude of monitoring options. the aim of this work is to highlight the relatively under-appreciated complexity of designing a monitoring strategy and how causal reasoning can provide a systematic framework for choosing between these options. as a motivating example, we consider an ml-based risk prediction algorithm for predicting unplanned readmissions. bringing together tools from causal inference and statistical process control, we consider six monitoring procedures (three candidate monitoring criteria and two data sources) and investigate their operating characteristics in simulation studies. results from this case study emphasize the seemingly simple (and obvious) fact that not all monitoring systems are created equal, which has real-world impacts on the design and documentation of ml monitoring systems.",,2023-11-19,2024-02-26,"['jean feng', 'adarsh subbaswamy', 'alexej gossmann', 'harvineet singh', 'berkman sahiner', 'mi-ok kim', 'gene pennello', 'nicholas petrick', 'romain pirracchio', 'fan xia']"
2311.12244,efficient reinforcement learning from partial observability,cs.lg cs.ai stat.ml,"in most real-world reinforcement learning applications, state information is only partially observable, which breaks the markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. partially observable markov decision processes (pomdps), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. to address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. we provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinforcement learning towards more practical applications.",,2023-11-20,2024-02-11,"['hongming zhang', 'tongzheng ren', 'chenjun xiao', 'dale schuurmans', 'bo dai']"
2311.12267,learning causal representations from general environments:   identifiability and intrinsic ambiguity,cs.lg cs.ai econ.em stat.ap stat.ml,"we study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. in this work, we provide the first identifiability results based on data that stem from general environments. we show that for linear causal models, while the causal graph can be fully recovered, the latent variables are only identified up to the surrounded-node ambiguity (sna) \citep{varici2023score}. we provide a counterpart of our guarantee, showing that sna is basically unavoidable in our setting. we also propose an algorithm, \texttt{lingcrel} which provably recovers the ground-truth model up to sna, and we demonstrate its effectiveness via numerical experiments. finally, we consider general non-parametric causal models and show that the same identification barrier holds when assuming access to groups of soft single-node interventions.",,2023-11-20,2024-02-03,"['jikai jin', 'vasilis syrgkanis']"
2311.13584,on diffusion-based generative models and their error bounds: the   log-concave case with full convergence estimates,cs.lg math.oc math.pr stat.ml,"we provide full theoretical guarantees for the convergence behaviour of diffusion-based generative models under the assumption of strongly log-concave data distributions while our approximating class of functions used for score estimation is made of lipschitz continuous functions. we demonstrate via a motivating example, sampling from a gaussian distribution with unknown mean, the powerfulness of our approach. in this case, explicit estimates are provided for the associated optimization problem, i.e. score approximation, while these are combined with the corresponding sampling estimates. as a result, we obtain the best known upper bound estimates in terms of key quantities of interest, such as the dimension and rates of convergence, for the wasserstein-2 distance between the data distribution (gaussian with unknown mean) and our sampling algorithm.   beyond the motivating example and in order to allow for the use of a diverse range of stochastic optimizers, we present our results using an $l^2$-accurate score estimation assumption, which crucially is formed under an expectation with respect to the stochastic optimizer and our novel auxiliary process that uses only known information. this approach yields the best known convergence rate for our sampling algorithm.",,2023-11-22,2024-02-17,"['stefano bruno', 'ying zhang', 'dong-young lim', 'ömer deniz akyildiz', 'sotirios sabanis']"
2311.13594,labeling neural representations with inverse recognition,cs.lg cs.ai stat.ml,"deep neural networks (dnns) demonstrate remarkable capabilities in learning complex hierarchical data representations, but the nature of these representations remains largely unknown. existing global explainability methods, such as network dissection, face limitations such as reliance on segmentation masks, lack of statistical significance testing, and high computational demands. we propose inverse recognition (invert), a scalable approach for connecting learned representations with human-understandable concepts by leveraging their capacity to discriminate between these concepts. in contrast to prior work, invert is capable of handling diverse types of neurons, exhibits less computational complexity, and does not rely on the availability of segmentation masks. moreover, invert provides an interpretable metric assessing the alignment between the representation and its corresponding explanation and delivering a measure of statistical significance. we demonstrate the applicability of invert in various scenarios, including the identification of representations affected by spurious correlations, and the interpretation of the hierarchical structure of decision-making within the models.",,2023-11-22,2024-01-18,"['kirill bykov', 'laura kopf', 'shinichi nakajima', 'marius kloft', 'marina m. -c. höhne']"
2311.13845,touring sampling with pushforward maps,cs.lg cs.ai stat.ml,"the number of sampling methods could be daunting for a practitioner looking to cast powerful machine learning methods to their specific problem. this paper takes a theoretical stance to review and organize many sampling approaches in the ``generative modeling'' setting, where one wants to generate new data that are similar to some training examples. by revealing links between existing methods, it might prove useful to overcome some of the current challenges in sampling with diffusion models, such as long inference time due to diffusion simulation, or the lack of diversity in generated samples.",,2023-11-23,2024-02-20,"['vivien cabannes', 'charles arnal']"
2311.14079,empirical comparison between cross-validation and mutation-validation in   model selection,cs.lg stat.ml,"mutation validation (mv) is a recently proposed approach for model selection, garnering significant interest due to its unique characteristics and potential benefits compared to the widely used cross-validation (cv) method. in this study, we empirically compared mv and $k$-fold cv using benchmark and real-world datasets. by employing bayesian tests, we compared generalization estimates yielding three posterior probabilities: practical equivalence, cv superiority, and mv superiority. we also evaluated the differences in the capacity of the selected models and computational efficiency. we found that both mv and cv select models with practically equivalent generalization performance across various machine learning algorithms and the majority of benchmark datasets. mv exhibited advantages in terms of selecting simpler models and lower computational costs. however, in some cases mv selected overly simplistic models leading to underfitting and showed instability in hyperparameter selection. these limitations of mv became more evident in the evaluation of a real-world neuroscientific task of predicting sex at birth using brain functional connectivity.",,2023-11-23,2024-02-15,"['jinyang yu', 'sami hamdan', 'leonard sasse', 'abigail morrison', 'kaustubh r. patil']"
2311.14212,annotation sensitivity: training data collection methods affect model   performance,stat.ml cs.cl cs.lg stat.me,"when training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. this study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. we introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. we collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. we then fine-tune bert models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. we find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model performance, 3) model predictions, and 4) model learning curves. our results emphasize the crucial role played by the annotation instrument which has received little attention in the machine learning literature. we call for additional research into how and why the instrument impacts the annotations to inform the development of best practices in instrument design.",,2023-11-23,2024-01-22,"['christoph kern', 'stephanie eckman', 'jacob beck', 'rob chew', 'bolei ma', 'frauke kreuter']"
2311.14220,assumption-lean and data-adaptive post-prediction inference,stat.me cs.lg stat.ml,"a primary challenge facing modern scientific research is the limited availability of gold-standard data which can be both costly and labor-intensive to obtain. with the rapid development of machine learning (ml), scientists have relied on ml algorithms to predict these gold-standard outcomes with easily obtained covariates. however, these predicted outcomes are often used directly in subsequent statistical analyses, ignoring imprecision and heterogeneity introduced by the prediction procedure. this will likely result in false positive findings and invalid scientific conclusions. in this work, we introduce an assumption-lean and data-adaptive post-prediction inference (pop-inf) procedure that allows valid and powerful inference based on ml-predicted outcomes. its ""assumption-lean"" property guarantees reliable statistical inference without assumptions on the ml-prediction, for a wide range of statistical quantities. its ""data-adaptive'"" feature guarantees an efficiency gain over existing post-prediction inference methods, regardless of the accuracy of ml-prediction. we demonstrate the superiority and applicability of our method through simulations and large-scale genomic data.",,2023-11-23,2024-02-06,"['jiacheng miao', 'xinran miao', 'yixuan wu', 'jiwei zhao', 'qiongshi lu']"
2311.14645,a general framework for user-guided bayesian optimization,cs.lg stat.ml,"the optimization of expensive-to-evaluate black-box functions is prevalent in various scientific disciplines. bayesian optimization is an automatic, general and sample-efficient method to solve these problems with minimal knowledge of the underlying function dynamics. however, the ability of bayesian optimization to incorporate prior knowledge or beliefs about the function at hand in order to accelerate the optimization is limited, which reduces its appeal for knowledgeable practitioners with tight budgets. to allow domain experts to customize the optimization routine, we propose colabo, the first bayesian-principled framework for incorporating prior beliefs beyond the typical kernel structure, such as the likely location of the optimizer or the optimal value. the generality of colabo makes it applicable across different monte carlo acquisition functions and types of user beliefs. we empirically demonstrate colabo's ability to substantially accelerate optimization when the prior information is accurate, and to retain approximately default performance when it is misleading.",,2023-11-24,2024-02-17,"['carl hvarfner', 'frank hutter', 'luigi nardi']"
2311.14649,learning in deep factor graphs with gaussian belief propagation,cs.lg stat.ml,"we propose an approach to do learning in gaussian factor graphs. we treat all relevant quantities (inputs, outputs, parameters, latents) as random variables in a graphical model, and view both training and prediction as inference problems with different observed nodes. our experiments show that these problems can be efficiently solved with belief propagation (bp), whose updates are inherently local, presenting exciting opportunities for distributed and asynchronous training. our approach can be scaled to deep networks and provides a natural means to do continual learning: use the bp-estimated parameter marginals of the current task as parameter priors for the next. on a video denoising task we demonstrate the benefit of learnable parameters over a classical factor graph approach and we show encouraging performance of deep factor graphs for continual image classification.",,2023-11-24,2024-02-28,"['seth nabarro', 'mark van der wilk', 'andrew j davison']"
2311.14652,one pass streaming algorithm for super long token attention   approximation in sublinear space,cs.lg cs.cl stat.ml,"attention computation takes both the time complexity of $o(n^2)$ and the space complexity of $o(n^2)$ simultaneously, which makes deploying large language models (llms) in streaming applications that involve long contexts requiring substantial computational resources. in recent openai devday (nov 6, 2023), openai released a new model that is able to support a 128k-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128k ($n \gg 2^d$). considering a single-layer self-attention with query, key, and value matrices $q, k, v \in \mathbb{r}^{n \times d}$, the polynomial method approximates the attention output $t \in \mathbb{r}^{n \times d}$. it accomplishes this by constructing $u_1, u_2 \in \mathbb{r}^{n \times t}$ to expedite attention ${\sf attn}(q, k, v)$ computation within $n^{1+o(1)}$ time executions. despite this, computing the approximated attention matrix $u_1u_2^\top \in \mathbb{r}^{n \times n}$ still necessitates $o(n^2)$ space, leading to significant memory usage. in response to these challenges, we introduce a new algorithm that only reads one pass of the data in a streaming fashion. this method employs sublinear space $o(n)$ to store three sketch matrices, alleviating the need for exact $k, v$ storage. notably, our algorithm exhibits exceptional memory-efficient performance with super-long tokens. as the token length $n$ increases, our error guarantee diminishes while the memory usage remains nearly constant. this unique attribute underscores the potential of our technique in efficiently handling llms in streaming applications.",,2023-11-24,2024-02-05,"['raghav addanki', 'chenyang li', 'zhao song', 'chiwun yang']"
2311.14828,deep latent force models: ode-based process convolutions for bayesian   deep learning,stat.ml cs.lg,"modelling the behaviour of highly nonlinear dynamical systems with robust uncertainty quantification is a challenging task which typically requires approaches specifically designed to address the problem at hand. we introduce a domain-agnostic model to address this issue termed the deep latent force model (dlfm), a deep gaussian process with physics-informed kernels at each layer, derived from ordinary differential equations using the framework of process convolutions. two distinct formulations of the dlfm are presented which utilise weight-space and variational inducing points-based gaussian process approximations, both of which are amenable to doubly stochastic variational inference. we present empirical evidence of the capability of the dlfm to capture the dynamics present in highly nonlinear real-world multi-output time series data. additionally, we find that the dlfm is capable of achieving comparable performance to a range of non-physics-informed probabilistic models on benchmark univariate regression tasks. we also empirically assess the negative impact of the inducing points framework on the extrapolation capabilities of lfm-based models.",,2023-11-24,2024-01-24,"['thomas baldwin-mcdonald', 'mauricio a. álvarez']"
2311.15051,large catapults in momentum gradient descent with warmup: an empirical   study,cs.lg math.oc stat.ml,"although gradient descent with momentum is widely used in modern deep learning, a concrete understanding of its effects on the training trajectory still remains elusive. in this work, we empirically show that momentum gradient descent with a large learning rate and learning rate warmup displays large catapults, driving the iterates towards flatter minima than those found by gradient descent. we then provide empirical evidence and theoretical intuition that the large catapult is caused by momentum ""amplifying"" the self-stabilization effect (damian et al., 2023).b.1",,2023-11-25,2024-01-08,"['prin phunyaphibarn', 'junghyun lee', 'bohan wang', 'huishuai zhang', 'chulhee yun']"
2311.15654,event detection in time series: universal deep learning approach,stat.ml cs.lg,"event detection in time series is a challenging task due to the prevalence of imbalanced datasets, rare events, and time interval-defined events. traditional supervised deep learning methods primarily employ binary classification, where each time step is assigned a binary label indicating the presence or absence of an event. however, these methods struggle to handle these specific scenarios effectively. to address these limitations, we propose a novel supervised regression-based deep learning approach that offers several advantages over classification-based methods. our approach, with a limited number of parameters, can effectively handle various types of events within a unified framework, including rare events and imbalanced datasets. we provide theoretical justifications for its universality and precision and demonstrate its superior performance across diverse domains, particularly for rare events and imbalanced datasets.",,2023-11-27,2023-12-29,"['menouar azib', 'benjamin renard', 'philippe garnier', 'vincent génot', 'nicolas andré']"
2311.15878,policy learning with distributional welfare,stat.me econ.em math.st stat.ml stat.th,"in this paper, we explore optimal treatment allocation policies that target distributional welfare. most literature on treatment choice has considered utilitarian welfare based on the conditional average treatment effect (ate). while average welfare is intuitive, it may yield undesirable allocations especially when individuals are heterogeneous (e.g., with outliers) - the very reason individualized treatments were introduced in the first place. this observation motivates us to propose an optimal policy that allocates the treatment based on the conditional quantile of individual treatment effects (qote). depending on the choice of the quantile probability, this criterion can accommodate a policymaker who is either prudent or negligent. the challenge of identifying the qote lies in its requirement for knowledge of the joint distribution of the counterfactual outcomes, which is generally hard to recover even with experimental data. therefore, we introduce minimax policies that are robust to model uncertainty. a range of identifying assumptions can be used to yield more informative policies. for both stochastic and deterministic policies, we establish the asymptotic bound on the regret of implementing the proposed policies. in simulations and two empirical applications, we compare optimal decisions based on the qote with decisions based on other criteria. the framework can be generalized to any setting where welfare is defined as a functional of the joint distribution of the potential outcomes.",,2023-11-27,2024-01-30,"['yifan cui', 'sukjin han']"
2311.16054,metric space magnitude for evaluating the diversity of latent   representations,cs.lg math.gt stat.ml,"the magnitude of a metric space is a recently-established invariant, providing a measure of the 'effective size' of a space across multiple scales while also capturing numerous geometrical properties. we develop a family of magnitude-based measures of the intrinsic diversity of latent representations, formalising a novel notion of dissimilarity between magnitude functions of finite metric spaces. our measures are provably stable under perturbations of the data, can be efficiently calculated, and enable a rigorous multi-scale comparison of latent representations. we show the utility and superior performance of our measures in an experimental suite that comprises different domains and tasks, including the evaluation of diversity, the detection of mode collapse, and the evaluation of generative models for text, image, and graph data.",,2023-11-27,2024-02-03,"['katharina limbeck', 'rayna andreeva', 'rik sarkar', 'bastian rieck']"
2311.16856,attentional graph neural networks for robust massive network   localization,cs.lg eess.sp stat.ml,"in recent years, graph neural networks (gnns) have emerged as a prominent tool for classification tasks in machine learning. however, their application in regression tasks remains underexplored. to tap the potential of gnns in regression, this paper integrates gnns with attention mechanism, a technique that revolutionized sequential learning tasks with its adaptability and robustness, to tackle a challenging nonlinear regression problem: network localization. we first introduce a novel network localization method based on graph convolutional network (gcn), which exhibits exceptional precision even under severe non-line-of-sight (nlos) conditions, thereby diminishing the need for laborious offline calibration or nlos identification. we further propose an attentional graph neural network (agnn) model, aimed at improving the limited flexibility and mitigating the high sensitivity to the hyperparameter of the gcn-based method. the agnn comprises two crucial modules, each designed with distinct attention architectures to address specific issues associated with the gcn-based method, rendering it more practical in real-world scenarios. experimental results substantiate the efficacy of our proposed gcn-based method and agnn model, as well as the enhancements of agnn model. additionally, we delve into the performance improvements of agnn model by analyzing it from the perspectives of dynamic attention and computational complexity.",,2023-11-28,2024-02-14,"['wenzhong yan', 'juntao wang', 'feng yin', 'yang tian', 'abdelhak m. zoubir']"
2311.16877,imputation using training labels and classification via label imputation,cs.lg stat.ml,"missing data is a common problem in practical settings. various imputation methods have been developed to deal with missing data. however, even though the label is usually available in the training data, the common practice of imputation usually only relies on the input and ignores the label. in this work, we illustrate how stacking the label into the input can significantly improve the imputation of the input. in addition, we propose a classification strategy that initializes the predicted test label with missing values and stacks the label with the input for imputation. this allows imputing the label and the input at the same time. also, the technique is capable of handling data training with missing labels without any prior imputation and is applicable to continuous, categorical, or mixed-type data. experiments show promising results in terms of accuracy.",,2023-11-28,2024-01-28,"['thu nguyen', 'tuan l. vo', 'pål halvorsen', 'michael a. riegler']"
2311.17287,utilizing model residuals to identify rental properties of interest: the   price anomaly score (pas) and its application to real-time data in manhattan,cs.lg stat.ap,"understanding whether a property is priced fairly hinders buyers and sellers since they usually do not have an objective viewpoint of the price distribution for the overall market of their interest. drawing from data collected of all possible available properties for rent in manhattan as of september 2023, this paper aims to strengthen our understanding of model residuals; specifically on machine learning models which generalize for a majority of the distribution of a well-proportioned dataset. most models generally perceive deviations from predicted values as mere inaccuracies, however this paper proposes a different vantage point: when generalizing to at least 75\% of the data-set, the remaining deviations reveal significant insights. to harness these insights, we introduce the price anomaly score (pas), a metric capable of capturing boundaries between irregularly predicted prices. by combining relative pricing discrepancies with statistical significance, the price anomaly score (pas) offers a multifaceted view of rental valuations. this metric allows experts to identify overpriced or underpriced properties within a dataset by aggregating pas values, then fine-tuning upper and lower boundaries to any threshold to set indicators of choice.",10.18517/ijods.4.2.97-106.2023,2023-11-28,,"['youssef sultan', 'jackson c. rafter', 'huyen t. nguyen']"
2311.17539,analyzing sharpness-aware minimization under overparameterization,cs.lg math.oc stat.ml,"training an overparameterized neural network can yield minimizers of different generalization capabilities despite the same level of training loss. with evidence that suggests a correlation between sharpness of minima and their generalization errors, increasing efforts have been made to develop an optimization method to explicitly find flat minima as more generalizable solutions. however, this sharpness-aware minimization (sam) strategy has not been studied much yet as to whether and how it is affected by overparameterization.   in this work, we analyze sam under overparameterization of varying degrees and present both empirical and theoretical results that indicate a critical influence of overparameterization on sam. specifically, we conduct extensive numerical experiments across various domains, and show that there exists a consistent trend that sam continues to benefit from increasing overparameterization. we also discover compelling cases where the effect of overparameterization is more pronounced or even diminished along with a series of ablation studies. on the theoretical side, we use standard techniques in optimization and prove that sam can achieve a linear rate of convergence under overparameterization in a stochastic setting. we also show that overparameterization can improve generalization of sam based on an analysis of two-layer networks, and further, that the linearly stable minima found by sam have more uniform hessian moments compared to sgd.",,2023-11-29,2024-02-05,"['sungbin shin', 'dongyeop lee', 'maksym andriushchenko', 'namhoon lee']"
2311.17563,efficient computation of sparse and robust maximum association   estimators,stat.co stat.ml,"although robust statistical estimators are less affected by outlying observations, their computation is usually more challenging. this is particularly the case in high-dimensional sparse settings. the availability of new optimization procedures, mainly developed in the computer science domain, offers new possibilities for the field of robust statistics. this paper investigates how such procedures can be used for robust sparse association estimators. the problem can be split into a robust estimation step followed by an optimization for the remaining decoupled, (bi-)convex problem. a combination of the augmented lagrangian algorithm and adaptive gradient descent is implemented to also include suitable constraints for inducing sparsity. we provide results concerning the precision of the algorithm and show the advantages over existing algorithms in this context. high-dimensional empirical examples underline the usefulness of this procedure. extensions to other robust sparse estimators are possible.",,2023-11-29,2024-02-19,"['pia pfeiffer', 'andreas alfons', 'peter filzmoser']"
2311.17795,marginal laplacian score,cs.lg stat.ml,"high-dimensional imbalanced data poses a machine learning challenge. in the absence of sufficient or high-quality labels, unsupervised feature selection methods are crucial for the success of subsequent algorithms. therefore, we introduce a marginal laplacian score (mls), a modification of the well known laplacian score (ls) tailored to better address imbalanced data. we introduce an assumption that the minority class or anomalous appear more frequently in the margin of the features. consequently, mls aims to preserve the local structure of the dataset's margin. we propose its integration into modern feature selection methods that utilize the laplacian score. we integrate the mls algorithm into the differentiable unsupervised feature selection (dufs), resulting in dufs-mls. the proposed methods demonstrate robust and improved performance on synthetic and public datasets.",,2023-11-29,2024-02-02,"['guy hay', 'ohad volk']"
2311.17816,fixed point actions from convolutional neural networks,hep-lat cs.lg hep-ph stat.ml,"lattice gauge-equivariant convolutional neural networks (l-cnns) can be used to form arbitrarily shaped wilson loops and can approximate any gauge-covariant or gauge-invariant function on the lattice. here we use l-cnns to describe fixed point (fp) actions which are based on renormalization group transformations. fp actions are classically perfect, i.e., they have no lattice artifacts on classical gauge-field configurations satisfying the equations of motion, and therefore possess scale invariant instanton solutions. fp actions are tree-level symanzik-improved to all orders in the lattice spacing and can produce physical predictions with very small lattice artifacts even on coarse lattices. we find that l-cnns are much more accurate at parametrizing the fp action compared to older approaches. they may therefore provide a way to circumvent critical slowing down and topological freezing towards the continuum limit.",,2023-11-29,,"['kieran holland', 'andreas ipp', 'david i. müller', 'urs wenger']"
2311.18048,an interventional perspective on identifiability in gaussian lti systems   with independent component analysis,cs.lg cs.ce cs.sy eess.sy stat.me,"we investigate the relationship between system identification and intervention design in dynamical systems. while previous research demonstrated how identifiable representation learning methods, such as independent component analysis (ica), can reveal cause-effect relationships, it relied on a passive perspective without considering how to collect data. our work shows that in gaussian linear time-invariant (lti) systems, the system parameters can be identified by introducing diverse intervention signals in a multi-environment setting. by harnessing appropriate diversity assumptions motivated by the ica literature, our findings connect experiment design and representational identifiability in dynamical systems. we corroborate our findings on synthetic and (simulated) physical data. additionally, we show that hidden markov models, in general, and (gaussian) lti systems, in particular, fulfil a generalization of the causal de finetti theorem with continuous parameters.",,2023-11-29,2024-02-16,"['goutham rajendran', 'patrik reizinger', 'wieland brendel', 'pradeep ravikumar']"
2311.18460,causal fairness under unobserved confounding: a neural sensitivity   framework,cs.lg cs.ai cs.cy stat.me,"fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. in this work, we analyze the sensitivity of causal fairness to unobserved confounding. our contributions are three-fold. first, we derive bounds for causal fairness metrics under different sources of unobserved confounding. this enables practitioners to examine the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. third, we demonstrate the effectiveness of our framework in a series of experiments, including a real-world case study about predicting prison sentences. to the best of our knowledge, ours is the first work to study causal fairness under unobserved confounding. to this end, our work is of direct practical value as a refutation strategy to ensure the fairness of predictions in high-stakes applications.",,2023-11-30,2024-02-27,"['maresa schröder', 'dennis frauen', 'stefan feuerriegel']"
2311.18672,a comparison between invariant and equivariant classical and quantum   graph neural networks,quant-ph cs.lg hep-ph stat.ml,"machine learning algorithms are heavily relied on to understand the vast amounts of data from high-energy particle collisions at the cern large hadron collider (lhc). the data from such collision events can naturally be represented with graph structures. therefore, deep geometric methods, such as graph neural networks (gnns), have been leveraged for various data analysis tasks in high-energy physics. one typical task is jet tagging, where jets are viewed as point clouds with distinct features and edge connections between their constituent particles. the increasing size and complexity of the lhc particle datasets, as well as the computational models used for their analysis, greatly motivate the development of alternative fast and efficient computational paradigms such as quantum computation. in addition, to enhance the validity and robustness of deep networks, one can leverage the fundamental symmetries present in the data through the use of invariant inputs and equivariant layers. in this paper, we perform a fair and comprehensive comparison between classical graph neural networks (gnns) and equivariant graph neural networks (egnns) and their quantum counterparts: quantum graph neural networks (qgnns) and equivariant quantum graph neural networks (eqgnn). the four architectures were benchmarked on a binary classification task to classify the parton-level particle initiating the jet. based on their auc scores, the quantum networks were shown to outperform the classical networks. however, seeing the computational advantage of the quantum networks in practice may have to wait for the further development of quantum technology and its associated apis.",,2023-11-30,2024-01-16,"['roy t. forestano', 'marçal comajoan cara', 'gopal ramesh dahale', 'zhongtian dong', 'sergei gleyzer', 'daniel justice', 'kyoungchul kong', 'tom magorsch', 'konstantin t. matchev', 'katia matcheva', 'eyup b. unlu']"
2311.18826,geometry-aware normalizing wasserstein flows for optimal causal   inference,cs.lg stat.ml,"this paper presents a groundbreaking approach to causal inference by integrating continuous normalizing flows (cnfs) with parametric submodels, enhancing their geometric sensitivity and improving upon traditional targeted maximum likelihood estimation (tmle). our method employs cnfs to refine tmle, optimizing the cram\'er-rao bound and transitioning from a predefined distribution $p_0$ to a data-driven distribution $p_1$. we innovate further by embedding wasserstein gradient flows within fokker-planck equations, thus imposing geometric structures that boost the robustness of cnfs, particularly in optimal transport theory.   our approach addresses the disparity between sample and population distributions, a critical factor in parameter estimation bias. we leverage optimal transport and wasserstein gradient flows to develop causal inference methodologies with minimal variance in finite-sample settings, outperforming traditional methods like tmle and aipw. this novel framework, centered on wasserstein gradient flows, minimizes variance in efficient influence functions under distribution $p_t$. preliminary experiments showcase our method's superiority, yielding lower mean-squared errors compared to standard flows, thereby demonstrating the potential of geometry-aware normalizing wasserstein flows in advancing statistical modeling and inference.",,2023-11-30,2024-02-01,['kaiwen hou']
2312.00054,is inverse reinforcement learning harder than standard reinforcement   learning? a theoretical perspective,stat.ml cs.ai cs.lg,"inverse reinforcement learning (irl) -- the problem of learning reward functions from demonstrations of an \emph{expert policy} -- plays a critical role in developing intelligent systems. while widely used in applications, theoretical understandings of irl present unique challenges and remain less developed compared with standard rl. for example, it remains open how to do irl efficiently in standard \emph{offline} settings with pre-collected data, where states are obtained from a \emph{behavior policy} (which could be the expert policy itself), and actions are sampled from the expert policy.   this paper provides the first line of results for efficient irl in vanilla offline and online settings using polynomial samples and runtime. our algorithms and analyses seamlessly adapt the pessimism principle commonly used in offline rl, and achieve irl guarantees in stronger metrics than considered in existing work. we provide lower bounds showing that our sample complexities are nearly optimal. as an application, we also show that the learned rewards can \emph{transfer} to another target mdp with suitable guarantees when the target mdp satisfies certain similarity assumptions with the original (source) mdp.",,2023-11-28,2024-02-10,"['lei zhao', 'mengdi wang', 'yu bai']"
2312.01046,bagged regularized $k$-distances for anomaly detection,stat.ml cs.lg math.st stat.th,"we consider the paradigm of unsupervised anomaly detection, which involves the identification of anomalies within a dataset in the absence of labeled examples. though distance-based methods are top-performing for unsupervised anomaly detection, they suffer heavily from the sensitivity to the choice of the number of the nearest neighbors. in this paper, we propose a new distance-based algorithm called bagged regularized $k$-distances for anomaly detection (brdad) converting the unsupervised anomaly detection problem into a convex optimization problem. our brdad algorithm selects the weights by minimizing the surrogate risk, i.e., the finite sample bound of the empirical risk of the bagged weighted $k$-distances for density estimation (bwdde). this approach enables us to successfully address the sensitivity challenge of the hyperparameter choice in distance-based algorithms. moreover, when dealing with large-scale datasets, the efficiency issues can be addressed by the incorporated bagging technique in our brdad algorithm. on the theoretical side, we establish fast convergence rates of the auc regret of our algorithm and demonstrate that the bagging technique significantly reduces the computational complexity. on the practical side, we conduct numerical experiments on anomaly detection benchmarks to illustrate the insensitivity of parameter selection of our algorithm compared with other state-of-the-art distance-based methods. moreover, promising improvements are brought by applying the bagging technique in our algorithm on real-world datasets.",,2023-12-02,2024-02-13,"['yuchao cai', 'yuheng ma', 'hanfang yang', 'hanyuan hang']"
2312.01127,symmetric mean-field langevin dynamics for distributional minimax   problems,math.oc stat.ml,"in this paper, we extend mean-field langevin dynamics to minimax optimization over probability distributions for the first time with symmetric and provably convergent updates. we propose mean-field langevin averaged gradient (mfl-ag), a single-loop algorithm that implements gradient descent ascent in the distribution spaces with a novel weighted averaging, and establish average-iterate convergence to the mixed nash equilibrium. we also study both time and particle discretization regimes and prove a new uniform-in-time propagation of chaos result which accounts for the dependency of the particle interactions on all previous distributions. furthermore, we propose mean-field langevin anchored best response (mfl-abr), a symmetric double-loop algorithm based on best response dynamics with linear last-iterate convergence. finally, we study applications to zero-sum markov games and conduct simulations demonstrating long-term optimality.",,2023-12-02,2024-02-16,"['juno kim', 'kakei yamamoto', 'kazusato oko', 'zhuoran yang', 'taiji suzuki']"
2312.01187,sassl: enhancing self-supervised learning via neural style transfer,cs.cv cs.lg stat.ml,"existing data augmentation in self-supervised learning, while diverse, fails to preserve the inherent structure of natural images. this results in distorted augmented samples with compromised semantic information, ultimately impacting downstream performance. to overcome this, we propose sassl: style augmentations for self supervised learning, a novel augmentation technique based on neural style transfer. sassl decouples semantic and stylistic attributes in images and applies transformations exclusively to the style while preserving content, generating diverse samples that better retain semantics. our technique boosts top-1 classification accuracy on imagenet by up to 2$\%$ compared to established self-supervised methods like moco, simclr, and byol, while achieving superior transfer learning performance across various datasets.",,2023-12-02,2024-02-03,"['renan a. rojas-gomez', 'karan singhal', 'ali etemad', 'alex bijamov', 'warren r. morningstar', 'philip andrew mansfield']"
2312.01210,when accurate prediction models yield harmful self-fulfilling prophecies,stat.me cs.lg stat.ml,"objective: prediction models are popular in medical research and practice. by predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. many prediction models are deployed for decision support based on their prediction accuracy in validation studies. we investigate whether this is a safe and valid approach.   materials and methods: we show that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. these models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model.   results: our main result is a formal characterization of a set of such prediction models. next we show that models that are well calibrated before and after deployment are useless for decision making as they made no change in the data distribution.   discussion: our results point to the need to revise standard practices for validation, deployment and evaluation of prediction models that are used in medical decisions.   conclusion: outcome prediction models can yield harmful self-fulfilling prophecies when used for decision making, a new perspective on prediction model development, deployment and monitoring is needed.",,2023-12-02,2024-02-08,"['wouter a. c. van amsterdam', 'nan van geloven', 'jesse h. krijthe', 'rajesh ranganath', 'giovanni ciná']"
2312.01518,analyzing state-level longevity trends with the u.s. mortality database,stat.ap,"we investigate state-level age-specific mortality trends based on the united states mortality database (usmdb) published by the human mortality database. in tandem with looking at the longevity experience across the 51 states, we also consider a collection of socio-demographic, economic and educational covariates that correlate with mortality trends. to obtain smoothed mortality surfaces for each state, we implement the machine learning framework of multi-output gaussian process regression (huynh & ludkovski 2021) on targeted groupings of 3-6 states. our detailed exploratory analysis shows that the mortality experience is highly inhomogeneous across states in terms of respective age structures. we moreover document multiple divergent trends between best and worst states, between females and males, and between younger and older ages. the comparisons across the 50+ fitted models offer opportunities for rich insights about drivers of mortality in the u.s. and are visualized through numerous figures and an online interactive dashboard.",,2023-12-03,2024-01-03,"['mike ludkovski', 'doris padilla']"
2312.01520,entropy and the kullback-leibler divergence for bayesian networks:   computational complexity and efficient implementation,cs.ai cs.lg stat.co stat.ml,"bayesian networks (bns) are a foundational model in machine learning and causal inference. their graphical structure can handle high-dimensional problems, divide them into a sparse collection of smaller ones, underlies judea pearl's causality, and determines their explainability and interpretability. despite their popularity, there are almost no resources in the literature on how to compute shannon's entropy and the kullback-leibler (kl) divergence for bns under their most common distributional assumptions. in this paper, we provide computationally efficient algorithms for both by leveraging bns' graphical structure, and we illustrate them with a complete set of numerical examples. in the process, we show it is possible to reduce the computational complexity of kl from cubic to quadratic for gaussian bns.",,2023-11-29,2024-01-15,['marco scutari']
2312.01735,weighted q-learning for optimal dynamic treatment regimes with mnar   covariates,stat.me,"dynamic treatment regimes (dtrs) formalize medical decision-making as a sequence of rules for different stages, mapping patient-level information to recommended treatments. in practice, estimating an optimal dtr using observational data from electronic medical record (emr) databases can be complicated by covariates that are missing not at random (mnar) due to informative monitoring of patients. since complete case analysis can result in consistent estimation of outcome model parameters under the assumption of outcome-independent missingness, q-learning is a natural approach to accommodating mnar covariates. however, the backward induction algorithm used in q-learning can introduce challenges, as mnar covariates at later stages can result in mnar pseudo-outcomes at earlier stages, leading to suboptimal dtrs, even if the longitudinal outcome variables are fully observed. to address this unique missing data problem in dtr settings, we propose two weighted q-learning approaches where inverse probability weights for missingness of the pseudo-outcomes are obtained through estimating equations with valid nonresponse instrumental variables or sensitivity analysis. asymptotic properties of the weighted q-learning estimators are derived and the finite-sample performance of the proposed methods is evaluated and compared with alternative methods through extensive simulation studies. using emr data from the medical information mart for intensive care database, we apply the proposed methods to investigate the optimal fluid strategy for sepsis patients in intensive care units.",,2023-12-04,2024-02-22,"['jian sun', 'li su', 'bo fu']"
2312.01925,coefficient shape alignment in multivariate functional regression,stat.me stat.ml,"in multivariate functional data analysis, different functional covariates can be homogeneous. the hidden homogeneity structure is informative about the connectivity or association of different covariates. the covariates with pronounced homogeneity can be analyzed jointly within the same group, which gives rise to a way of parsimoniously modeling multivariate functional data. in this paper, a novel grouped multivariate functional regression model with a new regularization approach termed ""coefficient shape alignment"" is developed to tackle the potential homogeneity of different functional covariates. the modeling procedure includes two main steps: first detect the unknown grouping structure with the new regularization approach to aggregate covariates into disjoint groups; and then the grouped multivariate functional regression model is established based on the detected grouping structure. in this new grouped model, the coefficient functions of covariates in the same homogeneous group share the same shape invariant to scaling. the new regularization approach builds on penalizing the discrepancy of coefficient shape. the consistency property of the detected grouping structure is thoroughly investigated, and the conditions that guarantee uncovering the underlying true grouping structure are developed. the asymptotic properties of the model estimates are also developed. extensive simulation studies are conducted to investigate the finite-sample properties of the developed methods. the practical utility of the proposed methods is illustrated in the real data analysis on sugar quality evaluation. this work provides a novel means for analyzing the underlying homogeneity of functional covariates and developing parsimonious model structures for multivariate functional data.",,2023-12-04,2024-01-15,"['shuhao jiao', 'ngai-hang chan']"
2312.02119,tree of attacks: jailbreaking black-box llms automatically,cs.lg cs.ai cs.cl cs.cr stat.ml,"while large language models (llms) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. in this work, we present tree of attacks with pruning (tap), an automated method for generating jailbreaks that only requires black-box access to the target llm. tap utilizes an llm to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. crucially, before sending prompts to the target, tap assesses them and prunes the ones unlikely to result in jailbreaks. using tree-of-thought reasoning allows tap to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. in empirical evaluations, we observe that tap generates prompts that jailbreak state-of-the-art llms (including gpt4 and gpt4-turbo) for more than 80% of the prompts using only a small number of queries. interestingly, tap is also capable of jailbreaking llms protected by state-of-the-art guardrails, e.g., llamaguard. this significantly improves upon the previous state-of-the-art black-box method for generating jailbreaks.",,2023-12-04,2024-02-21,"['anay mehrotra', 'manolis zampetakis', 'paul kassianik', 'blaine nelson', 'hyrum anderson', 'yaron singer', 'amin karbasi']"
2312.02246,conditional variational diffusion models,cs.cv cs.ai cs.lg stat.ml,"inverse problems aim to determine parameters from observations, a crucial task in engineering and science. lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. fine-tuning this schedule for specific applications is crucial but time-costly and does not guarantee an optimal result. we propose a novel approach for learning the schedule as part of the training process. our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. this approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior results to previous methods and fine-tuned diffusion models. we conclude that fine-tuning the schedule by experimentation should be avoided because it can be learned during training in a stable way that yields better results.",,2023-12-04,2024-01-23,"['gabriel della maggiora', 'luis alberto croquevielle', 'nikita deshpande', 'harry horsley', 'thomas heinis', 'artur yakimovich']"
2312.02708,"provable adversarial robustness for group equivariant tasks: graphs,   point clouds, molecules, and more",cs.lg cs.cr stat.ml,"a machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. however, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. in such tasks, even perturbations with large norm do not necessarily change an input's semantic content. furthermore, there are perturbations for which a model's prediction explicitly needs to change. for the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. we then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. certification methods are, however, unavailable for many models, such as those with continuous equivariances. we close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. we additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.",,2023-12-05,2024-01-15,"['jan schuchardt', 'yan scholten', 'stephan günnemann']"
2312.02828,"convergence rates for stochastic approximation: biased noise with   unbounded variance, and applications",stat.ml cs.lg math.oc math.pr,"the stochastic approximation (sa) algorithm introduced by robbins and monro in 1951 has been a standard method for solving equations of the form $\mathbf{f}({\boldsymbol {\theta}}) = \mathbf{0}$, when only noisy measurements of $\mathbf{f}(\cdot)$ are available. if $\mathbf{f}({\boldsymbol {\theta}}) = \nabla j({\boldsymbol {\theta}})$ for some function $j(\cdot)$, then sa can also be used to find a stationary point of $j(\cdot)$. at each time $t$, the current guess ${\boldsymbol {\theta}}_t$ is updated to ${\boldsymbol {\theta}}_{t+1}$ using a noisy measurement of the form $\mathbf{f}({\boldsymbol {\theta}}_t) + {\boldsymbol {\xi}}_{t+1}$. in much of the literature, it is assumed that the error term ${\boldsymbol {\xi}}_{t+1}$ has zero conditional mean, and/or that its conditional variance is bounded as a function of $t$ (though not necessarily with respect to ${\boldsymbol {\theta}}_t$). over the years, sa has been applied to a variety of areas, out of which the focus in this paper is on convex and nonconvex optimization. as it turns out, in these applications, the above-mentioned assumptions on the measurement error do not always hold. in zero-order methods, the error neither has zero mean nor bounded conditional variance. in the present paper, we extend sa theory to encompass errors with nonzero conditional mean and/or unbounded conditional variance. in addition, we derive estimates for the rate of convergence of the algorithm, and compute the ``optimal step size sequences'' to maximize the estimated rate of convergence.",,2023-12-05,2024-01-09,"['rajeeva l. karandikar', 'm. vidyasagar']"
2312.02946,calibrating dimension reduction hyperparameters in the presence of noise,stat.ap stat.ml,"the goal of dimension reduction tools is to construct a low-dimensional representation of high-dimensional data. these tools are employed for a variety of reasons such as noise reduction, visualization, and to lower computational costs. however, there is a fundamental issue that is highly discussed in other modeling problems, but almost entirely ignored in the dimension reduction literature: overfitting. if we interpret data as a combination of signal and noise, prior works judge dimension reduction techniques on their ability to capture the entirety of the data, i.e. both the signal and the noise. in the context of other modeling problems, techniques such as feature-selection, cross-validation, and regularization are employed to combat overfitting, but no such precautions are taken when performing dimension reduction. in this paper, we present a framework that models dimension reduction problems in the presence of noise and use this framework to explore the role perplexity and number of neighbors play in overfitting data when applying t-sne and umap. more specifically, we show previously recommended values for perplexity and number of neighbors are too small and tend to overfit the noise. we also present a workflow others may use to calibrate hyperparameters in the presence of noise.",,2023-12-05,2024-01-30,"['justin lin', 'julia fukuyama']"
2312.02959,detecting algorithmic bias in medical ai-models,stat.ml cs.cy cs.lg stat.ap,"with the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. this paper presents an innovative framework for detecting areas of algorithmic bias in medical-ai decision support systems. our approach efficiently identifies potential biases in medical-ai models, specifically in the context of sepsis prediction, by employing the classification and regression trees (cart) algorithm. we verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. the effectiveness of the concept is further validated by experiments using electronic medical records from grady memorial hospital in atlanta, georgia. these tests demonstrate the practical implementation of our strategy in a clinical environment, where it can function as a vital instrument for guaranteeing fairness and equity in ai-based medical decisions.",,2023-12-05,2024-02-29,"['jeffrey smith', 'andre holder', 'rishikesan kamaleswaran', 'yao xie']"
2312.03006,multi-weight ranking for multi-criteria decision making,cs.ai cs.lg math.st stat.th,"cone distribution functions from statistics are turned into multi-criteria decision making tools. it is demonstrated that this procedure can be considered as an upgrade of the weighted sum scalarization insofar as it absorbs a whole collection of weighted sum scalarizations at once instead of fixing a particular one in advance. as examples show, this type of scalarization--in contrast to a pure weighted sum scalarization-is also able to detect ``non-convex"" parts of the pareto frontier. situations are characterized in which different types of rank reversal occur, and it is explained why this might even be useful for analyzing the ranking procedure. the ranking functions are then extended to sets providing unary indicators for set preferences which establishes, for the first time, the link between set optimization methods and set-based multi-objective optimization. a potential application in machine learning is outlined.",,2023-12-04,2024-01-14,"['andreas h hamel', 'daniel kostner']"
2312.03262,low-cost high-power membership inference attacks,stat.ml cs.cr cs.lg,"membership inference attacks (mia) aim to detect if a particular data point was used in training a machine learning model. recent strong attacks have high computational costs and inconsistent performance under varying conditions, rendering them unreliable for practical privacy risk assessment. we design a novel, efficient, and robust membership inference attack (rmia) which accurately differentiates between population data and training data of a model, with minimal computational overhead. we achieve this by a more accurate modeling of the null hypothesis setting in our likelihood ratio tests, and effectively leveraging both reference models and reference data samples from the population. our algorithm exhibits superior test power (true-positive rate) compared to prior methods, throughout the tpr-fpr curve including at extremely low false-positive rates (as low as 0). under computation constraints, where only a limited number of pre-trained reference models (as few as 1) are available, and also when we vary other elements of the attack, our method performs exceptionally well, unlike some prior attacks that approach random guessing. rmia outperforms the prior work in all configurations of the attack setup. rmia lays the algorithmic groundwork for practical yet accurate and reliable privacy risk analysis in machine learning.",,2023-12-05,2024-02-26,"['sajjad zarifzadeh', 'philippe liu', 'reza shokri']"
2312.03311,on the nystrom approximation for preconditioning in kernel machines,stat.ml cs.lg,"kernel methods are a popular class of nonlinear predictive models in machine learning. scalable algorithms for learning kernel models need to be iterative in nature, but convergence can be slow due to poor conditioning. spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models. however computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets. a nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications. in this paper we analyze the trade-offs of using such an approximated preconditioner. specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the nystrom-based approximated preconditioner to accelerate gradient descent nearly as well as the exact preconditioner, while also reducing the computational and storage overheads.",,2023-12-06,2024-01-24,"['amirhesam abedsoltan', 'parthe pandit', 'luis rademacher', 'mikhail belkin']"
2312.03871,hidden yet quantifiable: a lower bound for confounding strength using   randomized trials,stat.ml cs.lg,"in the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new treatments in clinical practice. yet, unobserved confounding can significantly compromise causal conclusions drawn from non-randomized data. we propose a novel strategy that leverages randomized trials to quantify unobserved confounding. first, we design a statistical test to detect unobserved confounding with strength above a given threshold. then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. we evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world setting.",,2023-12-06,2024-02-21,"['piersilvio de bartolomeis', 'javier abad', 'konstantin donhauser', 'fanny yang']"
2312.04027,the sample complexity of multi-distribution learning,cs.lg cs.ai cs.ds stat.ml,"multi-distribution learning generalizes the classic pac learning to handle data coming from multiple distributions. given a set of $k$ data distributions and a hypothesis class of vc dimension $d$, the goal is to learn a hypothesis that minimizes the maximum population loss over $k$ distributions, up to $\epsilon$ additive error. in this paper, we settle the sample complexity of multi-distribution learning by giving an algorithm of sample complexity $\widetilde{o}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. this matches the lower bound up to sub-polynomial factor and resolves the colt 2023 open problem of awasthi, haghtalab and zhao [ahz23].",,2023-12-06,2024-01-28,['binghui peng']
2312.04501,graph metanetworks for processing diverse neural architectures,cs.lg cs.ai stat.ml,"neural networks efficiently encode learned information within their parameters. consequently, many tasks can be unified by treating neural networks themselves as input data. when doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. however, those works developed architectures tailored to specific networks such as mlps and cnns without normalization layers, and generalizing such architectures to other types of networks can be challenging. in this work, we overcome these challenges by building new metanetworks - neural networks that take weights from other neural networks as input. put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. our approach, graph metanetworks (gmns), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, resnet blocks, and group-equivariant linear layers. we prove that gmns are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. we validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.",,2023-12-07,2023-12-29,"['derek lim', 'haggai maron', 'marc t. law', 'jonathan lorraine', 'james lucas']"
2312.04722,sensitivity analysis in the presence of intrinsic stochasticity for   discrete fracture network simulations,stat.ap stat.ml,"large-scale discrete fracture network (dfn) simulators are standard fare for studies involving the sub-surface transport of particles since direct observation of real world underground fracture networks is generally infeasible. while these simulators have seen numerous successes over several engineering applications, estimations on quantities of interest (qoi) - such as breakthrough time of particles reaching the edge of the system - suffer from a two distinct types of uncertainty. a run of a dfn simulator requires several parameter values to be set that dictate the placement and size of fractures, the density of fractures, and the overall permeability of the system; uncertainty on the proper parameter choices will lead to some amount of uncertainty in the qoi, called epistemic uncertainty. furthermore, since dfn simulators rely on stochastic processes to place fractures and govern flow, understanding how this randomness affects the qoi requires several runs of the simulator at distinct random seeds. the uncertainty in the qoi attributed to different realizations (i.e. different seeds) of the same random process leads to a second type of uncertainty, called aleatoric uncertainty. in this paper, we perform a sensitivity analysis, which directly attributes the uncertainty observed in the qoi to the epistemic uncertainty from each input parameter and to the aleatoric uncertainty. we make several design choices to handle an observed heteroskedasticity in dfn simulators, where the aleatoric uncertainty changes for different inputs, since the quality makes several standard statistical methods inadmissible. beyond the specific takeaways on which input variables affect uncertainty the most for dfn simulators, a major contribution of this paper is the introduction of a statistically rigorous workflow for characterizing the uncertainty in dfn flow simulations that exhibit heteroskedasticity.",,2023-12-07,2024-01-04,"['alexander c. murph', 'justin d. strait', 'kelly r. moran', 'jeffrey d. hyman', 'hari s. viswanathan', 'philip h. stauffer']"
2312.05134,optimal multi-distribution learning,cs.lg stat.ml,"multi-distribution learning (mdl), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. achieving data-efficient mdl necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. however, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. focusing on a hypothesis class of vapnik-chervonenkis (vc) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. our algorithmic ideas and theory have been further extended to accommodate rademacher classes. the proposed algorithms are oracle-efficient, which access the hypothesis class solely through an empirical risk minimization oracle. additionally, we establish the necessity of randomization, unveiling a large sample size barrier when only deterministic hypotheses are permitted. these findings successfully resolve three open problems presented in colt 2023 (i.e., awasthi et al., (2023, problem 1, 3 and 4)).",,2023-12-08,2024-01-20,"['zihan zhang', 'wenhao zhan', 'yuxin chen', 'simon s. du', 'jason d. lee']"
2312.05440,consistency models for scalable and fast simulation-based inference,cs.lg cs.ai stat.ml,"simulation-based inference (sbi) is constantly in search of more expressive algorithms for accurately inferring the parameters of complex models from noisy data. we present consistency models for neural posterior estimation (cmpe), a new free-form conditional sampler for scalable, fast, and amortized sbi with generative neural networks. cmpe combines the advantages of normalizing flows and flow matching methods into a single generative architecture: it essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem. our empirical evaluation demonstrates that cmpe not only outperforms current state-of-the-art algorithms on three hard low-dimensional problems but also achieves competitive performance in a high-dimensional bayesian denoising experiment and in estimating a computationally demanding multi-scale model of tumor spheroid growth.",,2023-12-08,2024-02-27,"['marvin schmitt', 'valentin pratz', 'ullrich köthe', 'paul-christian bürkner', 'stefan t radev']"
2312.05910,ensemble kalman filtering meets gaussian process ssm for non-mean-field   and online inference,cs.lg eess.sp stat.ml,"the gaussian process state-space models (gpssms) represent a versatile class of data-driven nonlinear dynamical system models. however, the presence of numerous latent variables in gpssm incurs unresolved issues for existing variational inference approaches, particularly under the more realistic non-mean-field (nmf) assumption, including extensive training effort, compromised inference accuracy, and infeasibility for online applications, among others. in this paper, we tackle these challenges by incorporating the ensemble kalman filter (enkf), a well-established model-based filtering technique, into the nmf variational inference framework to approximate the posterior distribution of the latent states. this novel marriage between enkf and gpssm not only eliminates the need for extensive parameterization in learning variational distributions, but also enables an interpretable, closed-form approximation of the evidence lower bound (elbo). moreover, owing to the streamlined parameterization via the enkf, the new gpssm model can be easily accommodated in online learning applications. we demonstrate that the resulting enkf-aided online algorithm embodies a principled objective function by ensuring data-fitting accuracy while incorporating model regularizations to mitigate overfitting. we also provide detailed analysis and fresh insights for the proposed algorithms. comprehensive evaluation across diverse real and synthetic datasets corroborates the superior learning and inference performance of our enkf-aided variational inference algorithms compared to existing methods.",,2023-12-10,2024-01-14,"['zhidi lin', 'yiyong sun', 'feng yin', 'alexandre hoang thiéry']"
2312.05974,learning the causal structure of networked dynamical systems under   latent nodes and structured noise,cs.lg stat.me,"this paper considers learning the hidden causal network of a linear networked dynamical system (nds) from the time series data at some of its nodes -- partial observability. the dynamics of the nds are driven by colored noise that generates spurious associations across pairs of nodes, rendering the problem much harder. to address the challenge of noise correlation and partial observability, we assign to each pair of nodes a feature vector computed from the time series data of observed nodes. the feature embedding is engineered to yield structural consistency: there exists an affine hyperplane that consistently partitions the set of features, separating the feature vectors corresponding to connected pairs of nodes from those corresponding to disconnected pairs. the causal inference problem is thus addressed via clustering the designed features. we demonstrate with simple baseline supervised methods the competitive performance of the proposed causal inference mechanism under broad connectivity regimes and noise correlation levels, including a real world network. further, we devise novel technical guarantees of structural consistency for linear nds under the considered regime.",,2023-12-10,2024-02-12,"['augusto santos', 'diogo rente', 'rui seabra', 'josé m. f. moura']"
2312.06499,taco: targeted concept removal in output embeddings for nlp via   information theory and explainability,cs.cl stat.ml,"the fairness of natural language processing (nlp) models has emerged as a crucial concern. information theory indicates that to achieve fairness, a model should not be able to predict sensitive variables, such as gender, ethnicity, and age. however, information related to these variables often appears implicitly in language, posing a challenge in identifying and mitigating biases effectively. to tackle this issue, we present a novel approach that operates at the embedding level of an nlp model, independent of the specific architecture. our method leverages insights from recent advances in xai techniques and employs an embedding transformation to eliminate implicit information from a selected variable. by directly manipulating the embeddings in the final layer, our approach enables a seamless integration into existing models without requiring significant modifications or retraining. in evaluation, we show that the proposed post-hoc approach significantly reduces gender-related associations in nlp models while preserving the overall performance and functionality of the models. an implementation of our method is available: https://github.com/fanny-jourdan/taco",,2023-12-11,2024-01-11,"['fanny jourdan', 'louis béthune', 'agustin picard', 'laurent risser', 'nicholas asher']"
2312.06658,mean estimation in the add-remove model of differential privacy,cs.ds cs.cr cs.it math.it stat.ml,"differential privacy is often studied under two different models of neighboring datasets: the add-remove model and the swap model. while the swap model is frequently used in the academic literature to simplify analysis, many practical applications rely on the more conservative add-remove model, where obtaining tight results can be difficult. here, we study the problem of one-dimensional mean estimation under the add-remove model. we propose a new algorithm and show that it is min-max optimal, achieving the best possible constant in the leading term of the mean squared error for all $\epsilon$, and that this constant is the same as the optimal algorithm under the swap model. these results show that the add-remove and swap models give nearly identical errors for mean estimation, even though the add-remove model cannot treat the size of the dataset as public information. we also demonstrate empirically that our proposed algorithm yields at least a factor of two improvement in mean squared error over algorithms frequently used in practice. one of our main technical contributions is a new hour-glass mechanism, which might be of independent interest in other scenarios.",,2023-12-11,2024-02-19,"['alex kulesza', 'ananda theertha suresh', 'yuyan wang']"
2312.07177,fast meta-analytic approximations for relational event models:   applications to data streams and multilevel data,stat.me,"large relational-event history data stemming from large networks are becoming increasingly available due to recent technological developments (e.g. digital communication, online databases, etc). this opens many new doors to learning about complex interaction behavior between actors in temporal social networks. the relational event model has become the gold standard for relational event history analysis. currently, however, the main bottleneck to fit relational events models is of computational nature in the form of memory storage limitations and computational complexity. relational event models are therefore mainly used for relatively small data sets while larger, more interesting datasets, including multilevel data structures and relational event data streams, cannot be analyzed on standard desktop computers. this paper addresses this problem by developing approximation algorithms based on meta-analysis methods that can fit relational event models significantly faster while avoiding the computational issues. in particular, meta-analytic approximations are proposed for analyzing streams of relational event data and multilevel relational event data and potentially of combinations thereof. the accuracy and the statistical properties of the methods are assessed using numerical simulations. furthermore, real-world data are used to illustrate the potential of the methodology to study social interaction behavior in an organizational network and interaction behavior among political actors. the algorithms are implemented in a publicly available r package 'remx'.",,2023-12-12,2024-02-27,['fabio vieira roger leenders joris mulder']
2312.07281,safe multi-task bayesian optimization,cs.lg cs.sy eess.sy stat.ml,"bayesian optimization has become a powerful tool for safe online optimization of systems, due to its high sample efficiency and noise robustness. for further speed-up reduced physical models of the system can be incorporated into the optimization to accelerate the process, since the models are able to offer an approximation of the actual system, and sampling from them is significantly cheaper. the similarity between model and reality is represented by additional hyperparameters and learned within the optimization process. safety is an important criteria for online optimization methods like bayesian optimization, which has been addressed by recent literature, which provide safety guarantees under the assumption of known hyperparameters. however, in practice this is not applicable. therefore, we extend the robust gaussian process uniform error bounds to meet the multi-task setting, which involves the calculation of a confidence region from the hyperparameter posterior distribution utilizing markov chain monte carlo methods. then, using the robust safety bounds, bayesian optimization is applied to safely optimize the system while incorporating measurements of the models. simulations show that the optimization can be significantly accelerated compared to other state-of-the-art safe bayesian optimization methods depending on the fidelity of the models.",,2023-12-12,2024-01-08,"['jannis o. lübsen', 'christian hespe', 'annika eichler']"
2312.07930,towards optimal statistical watermarking,cs.lg cs.cl cs.cr cs.it math.it stat.ml,"we study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-offs between the type i error and type ii error. we characterize the uniformly most powerful (ump) watermark in the general hypothesis testing setting and the minimax type ii error in the model-agnostic setting. in the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small type i and type ii errors. our rate of $\theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. moreover, we formulate the robust watermarking problem where the user is allowed to perform a class of perturbations on the generated texts, and characterize the optimal type ii error of robust ump tests via a linear programming problem. to the best of our knowledge, this is the first systematic statistical treatment on the watermarking problem with near-optimal rates in the i.i.d. setting, which might be of interest for future works.",,2023-12-13,2024-02-06,"['baihe huang', 'hanlin zhu', 'banghua zhu', 'kannan ramchandran', 'michael i. jordan', 'jason d. lee', 'jiantao jiao']"
2312.08823,fast sampling from constrained spaces using the metropolis-adjusted   mirror langevin algorithm,stat.co cs.ds cs.lg math.st stat.ml stat.th,"we propose a new method called the metropolis-adjusted mirror langevin algorithm for approximate sampling from distributions whose support is a compact and convex set. this algorithm adds an accept-reject filter to the markov chain induced by a single step of the mirror langevin algorithm (zhang et al., 2020), which is a basic discretisation of the mirror langevin dynamics. due to the inclusion of this filter, our method is unbiased relative to the target, while known discretisations of the mirror langevin dynamics including the mirror langevin algorithm have an asymptotic bias. for this algorithm, we also give upper bounds for the number of iterations taken to mix to a constrained distribution whose potential is relatively smooth, convex, and lipschitz continuous with respect to a self-concordant mirror function. as a consequence of the reversibility of the markov chain induced by the inclusion of the metropolis-hastings filter, we obtain an exponentially better dependence on the error tolerance for approximate constrained sampling. we also present numerical experiments that corroborate our theoretical findings.",,2023-12-14,2024-02-09,"['vishwak srinivasan', 'andre wibisono', 'ashia wilson']"
2312.09033,using surprise index for competency assessment in autonomous   decision-making,cs.ro stat.ml,"this paper considers the problem of evaluating an autonomous system's competency in performing a task, particularly when working in dynamic and uncertain environments. the inherent opacity of machine learning models, from the perspective of the user, often described as a `black box', poses a challenge. to overcome this, we propose using a measure called the surprise index, which leverages available measurement data to quantify whether the dynamic system performs as expected. we show that the surprise index can be computed in closed form for dynamic systems when observed evidence in a probabilistic model if the joint distribution for that evidence follows a multivariate gaussian marginal distribution. we then apply it to a nonlinear spacecraft maneuver problem, where actions are chosen by a reinforcement learning agent and show it can indicate how well the trajectory follows the required orbit.",,2023-12-14,2024-01-10,"['akash ratheesh', 'ofer dagan', 'nisar r. ahmed', 'jay mcmahon']"
2312.09817,calibrated one round federated learning with bayesian inference in the   predictive space,cs.lg stat.ml,"federated learning (fl) involves training a model over a dataset distributed among clients, with the constraint that each client's dataset is localized and possibly heterogeneous. in fl, small and noisy datasets are common, highlighting the need for well-calibrated models that represent the uncertainty of predictions. the closest fl techniques to achieving such goals are the bayesian fl methods which collect parameter samples from local posteriors, and aggregate them to approximate the global posterior. to improve scalability for larger models, one common bayesian approach is to approximate the global predictive posterior by multiplying local predictive posteriors. in this work, we demonstrate that this method gives systematically overconfident predictions, and we remedy this by proposing $\beta$-predictive bayes, a bayesian fl algorithm that interpolates between a mixture and product of the predictive posteriors, using a tunable parameter $\beta$. this parameter is tuned to improve the global ensemble's calibration, before it is distilled to a single model. our method is evaluated on a variety of regression and classification datasets to demonstrate its superiority in calibration to other baselines, even as data heterogeneity increases. code available at https://github.com/hasanmohsin/betapredbayesfl",,2023-12-15,2024-01-09,"['mohsin hasan', 'guojun zhang', 'kaiyang guo', 'xi chen', 'pascal poupart']"
2312.11456,iterative preference learning from human feedback: bridging theory and   practice for rlhf under kl-constraint,cs.lg cs.ai stat.ml,"this paper studies the theoretical framework of the alignment process of generative models with reinforcement learning from human feedback (rlhf). we consider a standard mathematical formulation, the reverse-kl regularized contextual bandit for rlhf. despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. we investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.   moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel rlhf algorithms. this includes an iterative version of the direct preference optimization (dpo) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. our empirical evaluations on real-world alignment experiment of large language model demonstrate that these proposed methods significantly surpass existing strong baselines, such as dpo and rejection sampling optimization (rso), showcasing the connections between solid theoretical foundations and their powerful practical implementations.",,2023-12-18,2024-02-20,"['wei xiong', 'hanze dong', 'chenlu ye', 'ziqi wang', 'han zhong', 'heng ji', 'nan jiang', 'tong zhang']"
2312.12137,best arm identification with fixed budget: a large deviation perspective,cs.lg stat.ml,"we consider the problem of identifying the best arm in stochastic multi-armed bandits (mabs) using a fixed sampling budget. characterizing the minimal instance-specific error probability for this problem constitutes one of the important remaining open problems in mabs. when arms are selected using a static sampling strategy, the error probability decays exponentially with the number of samples at a rate that can be explicitly derived via large deviation techniques. analyzing the performance of algorithms with adaptive sampling strategies is however much more challenging. in this paper, we establish a connection between the large deviation principle (ldp) satisfied by the empirical proportions of arm draws and that satisfied by the empirical arm rewards. this connection holds for any adaptive algorithm, and is leveraged (i) to improve error probability upper bounds of some existing algorithms, such as the celebrated \sr (successive rejects) algorithm \citep{audibert2010best}, and (ii) to devise and analyze new algorithms. in particular, we present \sred (continuous rejects), a truly adaptive algorithm that can reject arms in {\it any} round based on the observed empirical gaps between the rewards of various arms. applying our large deviation results, we prove that \sred enjoys better performance guarantees than existing algorithms, including \sr. extensive numerical experiments confirm this observation.",,2023-12-19,2024-02-19,"['po-an wang', 'ruo-chun tzeng', 'alexandre proutiere']"
2312.12616,online variational sequential monte carlo,stat.ml cs.lg,"being the most classical generative model for serial data, state-space models (ssm) are fundamental in ai and statistical machine learning. in ssm, any form of parameter learning or latent state inference typically involves the computation of complex latent-state posteriors. in this work, we build upon the variational sequential monte carlo (vsmc) method, which provides computationally efficient and accurate model parameter estimation and bayesian latent-state inference by combining particle methods and variational inference. while standard vsmc operates in the offline mode, by re-processing repeatedly a given batch of data, we distribute the approximation of the gradient of the vsmc surrogate elbo in time using stochastic approximation, allowing for online learning in the presence of streams of data. this results in an algorithm, online vsmc, that is capable of performing efficiently, entirely on-the-fly, both parameter estimation and particle proposal adaptation. in addition, we provide rigorous theoretical results describing the algorithm's convergence properties as the number of data tends to infinity as well as numerical illustrations of its excellent convergence properties and usefulness also in batch-processing settings.",,2023-12-19,2024-02-02,"['alessandro mastrototaro', 'jimmy olsson']"
2312.12839,comparing machine learning algorithms by union-free generic depth,cs.lg stat.ml,"we propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. despite intensive studies in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. we introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. concretely, we provide two examples of classifier comparisons on samples of standard benchmark data sets. our results demonstrate promisingly the wide variety of different analysis approaches based on ufg methods. furthermore, the examples outline that our approach differs substantially from existing benchmarking approaches, and thus adds a new perspective to the vivid debate on classifier comparison.",,2023-12-20,2024-02-21,"['hannah blocher', 'georg schollmeyer', 'malte nalenz', 'christoph jansen']"
2312.12937,robust loss functions for training decision trees with noisy labels,cs.lg stat.ml,"we consider training decision trees using noisily labeled data, focusing on loss functions that can lead to robust learning algorithms. our contributions are threefold. first, we offer novel theoretical insights on the robustness of many existing loss functions in the context of decision tree learning. we show that some of the losses belong to a class of what we call conservative losses, and the conservative losses lead to an early stopping behavior during training and noise-tolerant predictions during testing. second, we introduce a framework for constructing robust loss functions, called distribution losses. these losses apply percentile-based penalties based on an assumed margin distribution, and they naturally allow adapting to different noise rates via a robustness parameter. in particular, we introduce a new loss called the negative exponential loss, which leads to an efficient greedy impurity-reduction learning algorithm. lastly, our experiments on multiple datasets and noise settings validate our theoretical insight and the effectiveness of our adaptive negative exponential loss.",,2023-12-20,2024-01-22,"['jonathan wilton', 'nan ye']"
2312.13152,neural stochastic differential equations with change points: a   generative adversarial approach,cs.lg stat.ml,"stochastic differential equations (sdes) have been widely used to model real world random phenomena. existing works mainly focus on the case where the time series is modeled by a single sde, which might be restrictive for modeling time series with distributional shift. in this work, we propose a change point detection algorithm for time series modeled as neural sdes. given a time series dataset, the proposed method jointly learns the unknown change points and the parameters of distinct neural sde models corresponding to each change point. specifically, the sdes are learned under the framework of generative adversarial networks (gans) and the change points are detected based on the output of the gan discriminator in a forward pass. at each step of the proposed algorithm, the change points and the sde model parameters are updated in an alternating fashion. numerical results on both synthetic and real datasets are provided to validate the performance of our algorithm in comparison to classical change point detection benchmarks, standard gan-based neural sdes, and other state-of-the-art deep generative models for time series data.",,2023-12-20,2024-01-22,"['zhongchang sun', 'yousef el-laham', 'svitlana vyetrenko']"
2312.14040,balancing specialization and adaptation in a transforming scientific   landscape,cs.si physics.soc-ph stat.ap,"how scientists navigate between the need to capitalize on their prior knowledge by specializing, and the urge to adapt to evolving research opportunities? drawing on from diverse perspectives on adaptation, in particular from institutional change and cultural evolution, this paper proposes an unsupervised bayesian model of the evolution of scientists' research portfolios in response to transformations in their field. the model relies on scientific abstracts and authorship data to evaluate the influence of intellectual, social, and institutional resources on scientists' trajectories within a cohort of $2\,195$ high-energy physicists between 2000 and 2019. using optimal transport, the reallocation of research efforts is shown to be shaped by learning costs, thus enhancing the utility of the scientific capital disseminated among scientists. two dimensions of social capital, namely ``diversity'' and ``power'', have opposite effects on the magnitude of change in scientists' research interests: while ``diversity'' disrupts and expands research interests, ``power'' stabilizes physicists' research agendas -- as does institutional stability. social capital plays a more crucial role in shifts between cognitively distant research areas. overall, this contribution provides new approaches for understanding and modeling collective adaptation.",,2023-12-21,2024-02-08,['lucas gautheron']
2312.14549,a machine learning approach based on survival analysis for ibnr   frequencies in non-life reserving,stat.me stat.ap,"we introduce new approaches for forecasting ibnr (incurred but not reported) frequencies by leveraging individual claims data, which includes accident date, reporting delay, and possibly additional features for every reported claim. a key element of our proposal involves computing development factors, which may be influenced by both the accident date and other features. these development factors serve as the basis for predictions. while we assume close to continuous observations of accident date and reporting delay, the development factors can be expressed at any level of granularity, such as months, quarters, or year and predictions across different granularity levels exhibit coherence. the calculation of development factors relies on the estimation of a hazard function in reverse development time, and we present three distinct methods for estimating this function: the cox proportional hazard model, a feed-forward neural network, and xgboost (extreme gradient boosting). in all three cases, estimation is based on the same partial likelihood that accommodates left truncation and ties in the data. while the first case is a semi-parametric model that assumes in parts a log linear structure, the two machine learning approaches only assume that the baseline and the other factors are multiplicatively separable. through an extensive simulation study and real-world data application, our approach demonstrates promising results. this paper comes with an accompanying r-package, $\texttt{resurv}$, which can be accessed at \url{https://github.com/edhofman/resurv}",,2023-12-22,2024-01-02,"['munir hiabu', 'emil hofman', 'gabriele pittarello']"
2312.14698,time-changed normalizing flows for accurate sde modeling,cs.lg stat.ml,"the generative paradigm has become increasingly important in machine learning and deep learning models. among popular generative models are normalizing flows, which enable exact likelihood estimation by transforming a base distribution through diffeomorphic transformations. extending the normalizing flow framework to handle time-indexed flows gave dynamic normalizing flows, a powerful tool to model time series, stochastic processes, and neural stochastic differential equations (sdes). in this work, we propose a novel variant of dynamic normalizing flows, a time changed normalizing flow (tcnf), based on time deformation of a brownian motion which constitutes a versatile and extensive family of gaussian processes. this approach enables us to effectively model some sdes, that cannot be modeled otherwise, including standard ones such as the well-known ornstein-uhlenbeck process, and generalizes prior methodologies, leading to improved results and better inference and prediction capability.",,2023-12-22,2024-01-15,"['naoufal el bekri', 'lucas drumetz', 'franck vermet']"
2312.14848,isolated pulsar population synthesis with simulation-based inference,astro-ph.he astro-ph.im cs.lg stat.ml,"we combine pulsar population synthesis with simulation-based inference to constrain the magneto-rotational properties of isolated galactic radio pulsars. we first develop a flexible framework to model neutron-star birth properties and evolution, focusing on their dynamical, rotational and magnetic characteristics. in particular, we sample initial magnetic-field strengths, $b$, and spin periods, $p$, from log-normal distributions and capture the late-time magnetic-field decay with a power law. each log-normal is described by a mean, $\mu_{\log b}, \mu_{\log p}$, and standard deviation, $\sigma_{\log b}, \sigma_{\log p}$, while the power law is characterized by the index, $a_{\rm late}$, resulting in five free parameters. we subsequently model the stars' radio emission and observational biases to mimic detections with three radio surveys, and produce a large database of synthetic $p$-$\dot{p}$ diagrams by varying our input parameters. we then follow a simulation-based inference approach that focuses on neural posterior estimation and employ this database to train deep neural networks to directly infer the posterior distributions of the five model parameters. after successfully validating these individual neural density estimators on simulated data, we use an ensemble of networks to infer the posterior distributions for the observed pulsar population. we obtain $\mu_{\log b} = 13.10^{+0.08}_{-0.10}$, $\sigma_{\log b} = 0.45^{+0.05}_{-0.05}$ and $\mu_{\log p} = -1.00^{+0.26}_{-0.21}$, $\sigma_{\log p} = 0.38^{+0.33}_{-0.18}$ for the log-normal distributions, and $a_{\rm late} = -1.80^{+0.65}_{-0.61}$ for the power law at $95\%$ credible interval. our approach represents a crucial step towards robust statistical inference for complex population-synthesis frameworks and forms the basis for future multi-wavelength analyses of galactic pulsars.",,2023-12-22,,"['vanessa graber', 'michele ronchi', 'celsa pardo-araujo', 'nanda rea']"
2312.14886,sample path regularity of gaussian processes from the covariance kernel,cs.lg math.pr math.st stat.ml stat.th,"gaussian processes (gps) are the most common formalism for defining probability distributions over spaces of functions. while applications of gps are myriad, a comprehensive understanding of gp sample paths, i.e. the function spaces over which they define a probability measure, is lacking. in practice, gps are not constructed through a probability measure, but instead through a mean function and a covariance kernel. in this paper we provide necessary and sufficient conditions on the covariance kernel for the sample paths of the corresponding gp to attain a given regularity. we use the framework of h\""older regularity as it grants particularly straightforward conditions, which simplify further in the cases of stationary and isotropic gps. we then demonstrate that our results allow for novel and unusually tight characterisations of the sample path regularities of the gps commonly used in machine learning applications, such as the mat\'ern gps.",,2023-12-22,2024-02-16,"['nathaël da costa', 'marvin pförtner', 'lancelot da costa', 'philipp hennig']"
2312.14922,"learning from higher-order statistics, efficiently: hypothesis tests,   random features, and neural networks",stat.ml cond-mat.stat-mech cs.lg,"neural networks excel at discovering statistical patterns in high-dimensional data sets. in practice, higher-order cumulants, which quantify the non-gaussian correlations between three or more variables, are particularly important for the performance of neural networks. but how efficient are neural networks at extracting features from higher-order cumulants? we study this question in the spiked cumulant model, where the statistician needs to recover a privileged direction or ""spike"" from the order-$p\ge 4$ cumulants of $d$-dimensional inputs. we first characterise the fundamental statistical and computational limits of recovering the spike by analysing the number of samples $n$ required to strongly distinguish between inputs from the spiked cumulant model and isotropic gaussian inputs. we find that statistical distinguishability requires $n\gtrsim d$ samples, while distinguishing the two distributions in polynomial time requires $n \gtrsim d^2$ samples for a wide class of algorithms, i.e. those covered by the low-degree conjecture. these results suggest the existence of a wide statistical-to-computational gap in this problem. numerical experiments show that neural networks learn to distinguish the two distributions with quadratic sample complexity, while ""lazy"" methods like random features are not better than random guessing in this regime. our results show that neural networks extract information from higher-order correlations in the spiked cumulant model efficiently, and reveal a large gap in the amount of data required by neural networks and random features to learn from higher-order cumulants.",,2023-12-22,2024-02-19,"['eszter székely', 'lorenzo bardone', 'federica gerace', 'sebastian goldt']"
2312.15148,personalized federated learning with attention-based client selection,cs.lg cs.it eess.sp math.it stat.ml,"personalized federated learning (pfl) relies on collective data knowledge to build customized models. however, non-iid data between clients poses significant challenges, as collaborating with clients who have diverse data distributions can harm local model performance, especially with limited training data. to address this issue, we propose fedacs, a new pfl algorithm with an attention-based client selection mechanism. fedacs integrates an attention mechanism to enhance collaboration among clients with similar data distributions and mitigate the data scarcity issue. it prioritizes and allocates resources based on data similarity. we further establish the theoretical convergence behavior of fedacs. experiments on cifar10 and fmnist validate fedacs's superiority, showcasing its potential to advance personalized federated learning. by tackling non-iid data challenges and data scarcity, fedacs offers promising advances in the field of personalized federated learning.",,2023-12-22,,"['zihan chen', 'jundong li', 'cong shen']"
2312.15282,causal forecasting for pricing,stat.ml cs.lg,"this paper proposes a novel method for demand forecasting in a pricing context. here, modeling the causal relationship between price as an input variable to demand is crucial because retailers aim to set prices in a (profit) optimal manner in a downstream decision making problem. our methods bring together the double machine learning methodology for causal inference and state-of-the-art transformer-based forecasting models. in extensive empirical experiments, we show on the one hand that our method estimates the causal effect better in a fully controlled setting via synthetic, yet realistic data. on the other hand, we demonstrate on real-world data that our method outperforms forecasting methods in off-policy settings (i.e., when there's a change in the pricing policy) while only slightly trailing in the on-policy setting.",,2023-12-23,2024-01-30,"['douglas schultz', 'johannes stephan', 'julian sieber', 'trudie yeh', 'manuel kunz', 'patrick doupe', 'tim januschowski']"
2312.15433,best-of-both-worlds algorithms for linear contextual bandits,cs.lg stat.ml,"we study best-of-both-worlds algorithms for $k$-armed linear contextual bandits. our algorithms deliver near-optimal regret bounds in both the adversarial and stochastic regimes, without prior knowledge about the environment. in the stochastic regime, we achieve the polylogarithmic rate $\frac{(dk)^2\mathrm{poly}\log(dkt)}{\delta_{\min}}$, where $\delta_{\min}$ is the minimum suboptimality gap over the $d$-dimensional context space. in the adversarial regime, we obtain either the first-order $\widetilde{o}(dk\sqrt{l^*})$ bound, or the second-order $\widetilde{o}(dk\sqrt{\lambda^*})$ bound, where $l^*$ is the cumulative loss of the best action and $\lambda^*$ is a notion of the cumulative second moment for the losses incurred by the algorithm. moreover, we develop an algorithm based on ftrl with shannon entropy regularizer that does not require the knowledge of the inverse of the covariance matrix, and achieves a polylogarithmic regret in the stochastic regime while obtaining $\widetilde{o}\big(dk\sqrt{t}\big)$ regret bounds in the adversarial regime.",,2023-12-24,2024-02-19,"['yuko kuroki', 'alberto rumi', 'taira tsuchiya', 'fabio vitale', 'nicolò cesa-bianchi']"
2312.15474,a conservative approach for few-shot transfer in off-dynamics   reinforcement learning,cs.lg stat.ml,"off-dynamics reinforcement learning (odrl) seeks to transfer a policy from a source environment to a target environment characterized by distinct yet similar dynamics. in this context, traditional rl agents depend excessively on the dynamics of the source environment, resulting in the discovery of policies that excel in this environment but fail to provide reasonable performance in the target one. in the few-shot framework, a limited number of transitions from the target environment are introduced to facilitate a more effective transfer. addressing this challenge, we propose an innovative approach inspired by recent advancements in imitation learning and conservative rl algorithms. the proposed method introduces a penalty to regulate the trajectories generated by the source-trained policy. we evaluate our method across various environments representing diverse off-dynamics conditions, where access to the target environment is extremely limited. these experiments include high-dimensional systems relevant to real-world applications. across most tested scenarios, our proposed method demonstrates performance improvements compared to existing baselines.",,2023-12-24,2024-02-21,"['paul daoudi', 'christophe prieur', 'bogdan robu', 'merwan barlier', 'ludovic dos santos']"
2312.15551,leveraging public representations for private transfer learning,cs.lg cs.cr stat.ml,"motivated by the recent empirical success of incorporating public data into differentially private learning, we theoretically investigate how a shared representation learned from public data can improve private learning. we explore two common scenarios of transfer learning for linear regression, both of which assume the public and private tasks (regression vectors) share a low-rank subspace in a high-dimensional space. in the first single-task transfer scenario, the goal is to learn a single model shared across all users, each corresponding to a row in a dataset. we provide matching upper and lower bounds showing that our algorithm achieves the optimal excess risk within a natural class of algorithms that search for the linear model within the given subspace estimate. in the second scenario of multitask model personalization, we show that with sufficient public data, users can avoid private coordination, as purely local learning within the given subspace achieves the same utility. taken together, our results help to characterize the benefits of public data across common regimes of private transfer learning.",,2023-12-24,2024-01-16,"['pratiksha thaker', 'amrith setlur', 'zhiwei steven wu', 'virginia smith']"
2312.15995,generalization in kernel regression under realistic assumptions,cs.lg cs.ai stat.ml,"it is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise. many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression. however, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup. this work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings. specifically, we provide rigorous bounds that hold for common kernels and for any amount of regularization, noise, any input dimension, and any number of samples. furthermore, we provide relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest. these reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the kernel provides it with an implicit form of regularization, enabling good generalization. when applied to common kernels, our results imply benign overfitting in high input dimensions, nearly tempered overfitting in fixed dimensions, and explicit convergence rates for regularized regression. as a by-product, we obtain time-dependent bounds for neural networks trained in the kernel regime.",,2023-12-26,2024-02-20,"['daniel barzilai', 'ohad shamir']"
2312.16043,an extended asymmetric sigmoid with perceptron (sigtron) for imbalanced   linear classification,cs.lg cs.ai cs.cv cs.ne stat.ml,"this article presents a new polynomial parameterized sigmoid called sigtron, which is an extended asymmetric sigmoid with perceptron, and its companion convex model called sigtron-imbalanced classification (sic) model that employs a virtual sigtron-induced convex loss function. in contrast to the conventional $\pi$-weighted cost-sensitive learning model, the sic model does not have an external $\pi$-weight on the loss function but has internal parameters in the virtual sigtron-induced loss function. as a consequence, when the given training dataset is close to the well-balanced condition, we show that the proposed sic model is more adaptive to variations of the dataset, such as the inconsistency of the scale-class-imbalance ratio between the training and test datasets. this adaptation is achieved by creating a skewed hyperplane equation. additionally, we present a quasi-newton optimization(l-bfgs) framework for the virtual convex loss by developing an interval-based bisection line search. empirically, we have observed that the proposed approach outperforms $\pi$-weighted convex focal loss and balanced classifier liblinear(logistic regression, svm, and l2svm) in terms of test classification accuracy with $51$ two-class and $67$ multi-class datasets. in binary classification problems, where the scale-class-imbalance ratio of the training dataset is not significant but the inconsistency exists, a group of sic models with the best test accuracy for each dataset (top$1$) outperforms libsvm(c-svc with rbf kernel), a well-known kernel-based classifier.",,2023-12-26,2024-02-04,['hyenkyun woo']
2312.16160,symmpi: predictive inference for data with group symmetries,stat.me cs.lg math.st stat.ml stat.th,"quantifying the uncertainty of predictions is a core problem in modern statistics. methods for predictive inference have been developed under a variety of assumptions, often -- for instance, in standard conformal prediction -- relying on the invariance of the distribution of the data under special groups of transformations such as permutation groups. moreover, many existing methods for predictive inference aim to predict unobserved outcomes in sequences of feature-outcome observations. meanwhile, there is interest in predictive inference under more general observation models (e.g., for partially observed features) and for data satisfying more general distributional symmetries (e.g., rotationally invariant or coordinate-independent observations in physics). here we propose symmpi, a methodology for predictive inference when data distributions have general group symmetries in arbitrary observation models. our methods leverage the novel notion of distributional equivariant transformations, which process the data while preserving their distributional invariances. we show that symmpi has valid coverage under distributional invariance and characterize its performance under distribution shift, recovering recent results as special cases. we apply symmpi to predict unobserved values associated to vertices in a network, where the distribution is unchanged under relabelings that keep the network structure unchanged. in several simulations in a two-layer hierarchical model, and in an empirical data analysis example, symmpi performs favorably compared to existing methods.",,2023-12-26,2023-12-28,"['edgar dobriban', 'mengxin yu']"
2312.16307,incentive-aware synthetic control: accurate counterfactual estimation   via incentivized exploration,econ.em cs.gt cs.lg stat.me,"we consider the setting of synthetic control methods (scms), a canonical approach used to estimate the treatment effect on the treated in a panel data setting. we shed light on a frequently overlooked but ubiquitous assumption made in scms of ""overlap"": a treated unit can be written as some combination -- typically, convex or linear combination -- of the units that remain under control. we show that if units select their own interventions, and there is sufficiently large heterogeneity between units that prefer different interventions, overlap will not hold. we address this issue by proposing a framework which incentivizes units with different preferences to take interventions they would not normally consider. specifically, leveraging tools from information design and online learning, we propose a scm that incentivizes exploration in panel data settings by providing incentive-compatible intervention recommendations to units. we establish this estimator obtains valid counterfactual estimates without the need for an a priori overlap assumption. we extend our results to the setting of synthetic interventions, where the goal is to produce counterfactual outcomes under all interventions, not just control. finally, we provide two hypothesis tests for determining whether unit overlap holds for a given panel dataset.",,2023-12-26,2024-02-13,"['daniel ngo', 'keegan harris', 'anish agarwal', 'vasilis syrgkanis', 'zhiwei steven wu']"
2312.16341,harnessing the power of federated learning in federated contextual   bandits,stat.ml cs.it cs.lg cs.ma math.it,"federated learning (fl) has demonstrated great potential in revolutionizing distributed machine learning, and tremendous efforts have been made to extend it beyond the original focus on supervised learning. among many directions, federated contextual bandits (fcb), a pivotal integration of fl and sequential decision-making, has garnered significant attention in recent years. despite substantial progress, existing fcb approaches have largely employed their tailored fl components, often deviating from the canonical fl framework. consequently, even renowned algorithms like fedavg remain under-utilized in fcb, let alone other fl advancements. motivated by this disconnection, this work takes one step towards building a tighter relationship between the canonical fl study and the investigations on fcb. in particular, a novel fcb design, termed fedigw, is proposed to leverage a regression-based cb algorithm, i.e., inverse gap weighting. compared with existing fcb approaches, the proposed fedigw design can better harness the entire spectrum of fl innovations, which is concretely reflected as (1) flexible incorporation of (both existing and forthcoming) fl protocols; (2) modularized plug-in of fl analyses in performance guarantees; (3) seamless integration of fl appendages (such as personalization, robustness, and privacy). we substantiate these claims through rigorous theoretical analyses and empirical evaluations.",,2023-12-26,,"['chengshuai shi', 'ruida zhou', 'kun yang', 'cong shen']"
2312.16360,mean-field underdamped langevin dynamics and its spacetime   discretization,stat.co math.oc math.st stat.ml stat.th,"we propose a new method called the n-particle underdamped langevin algorithm for optimizing a special class of non-linear functionals defined over the space of probability measures. examples of problems with this formulation include training mean-field neural networks, maximum mean discrepancy minimization and kernel stein discrepancy minimization. our algorithm is based on a novel spacetime discretization of the mean-field underdamped langevin dynamics, for which we provide a new, fast mixing guarantee. in addition, we demonstrate that our algorithm converges globally in total variation distance, bridging the theoretical gap between the dynamics and its practical implementation.",,2023-12-26,2024-02-06,"['qiang fu', 'ashia wilson']"
2312.16819,hidden minima in two-layer relu networks,cs.lg math.oc stat.ml,"the optimization problem associated to fitting two-layer relu networks having $d$~inputs, $k$~neurons, and labels generated by a target network, is considered. two types of infinite families of spurious minima, giving one minimum per $d$, were recently found. the loss at minima belonging to the first type converges to zero as $d$ increases. in the second type, the loss remains bounded away from zero. that being so, how may one avoid minima belonging to the latter type? fortunately, such minima are never detected by standard optimization methods. motivated by questions concerning the nature of this phenomenon, we develop methods to study distinctive analytic properties of hidden minima.   by existing analyses, the hessian spectrum of both types agree modulo $o(d^{-1/2})$-terms -- not promising. thus, rather, our investigation proceeds by studying curves along which the loss is minimized or maximized, generally referred to as tangency arcs. we prove that apparently far removed group representation-theoretic considerations concerning the arrangement of subspaces invariant to the action of subgroups of $s_d$, the symmetry group over $d$ symbols, relative to ones fixed by the action yield a precise description of all finitely many admissible types of tangency arcs. the general results used for the loss function reveal that arcs emanating from hidden minima differ, characteristically, by their structure and symmetry, precisely on account of the $o(d^{-1/2})$-eigenvalue terms absent in previous work, indicating in particular the subtlety of the analysis. the theoretical results, stated and proved for o-minimal structures, show that the set comprising all tangency arcs is topologically sufficiently tame to enable a numerical construction of tangency arcs and so compare how minima, both types, are positioned relative to adjacent critical points.",,2023-12-27,2024-02-19,['yossi arjevani']
2312.16887,automatic scoring of cognition drawings: assessing the quality of   machine-based scores against a gold standard,stat.ap cs.cv cs.lg stat.me,"figure drawing is often used as part of dementia screening protocols. the survey of health aging and retirement in europe (share) has adopted three drawing tests from addenbrooke's cognitive examination iii as part of its questionnaire module on cognition. while the drawings are usually scored by trained clinicians, share uses the face-to-face interviewers who conduct the interviews to score the drawings during fieldwork. this may pose a risk to data quality, as interviewers may be less consistent in their scoring and more likely to make errors due to their lack of clinical training. this paper therefore reports a first proof of concept and evaluates the feasibility of automating scoring using deep learning. we train several different convolutional neural network (cnn) models using about 2,000 drawings from the 8th wave of the share panel in germany and the corresponding interviewer scores, as well as self-developed 'gold standard' scores. the results suggest that this approach is indeed feasible. compared to training on interviewer scores, models trained on the gold standard data improve prediction accuracy by about 10 percentage points. the best performing model, convnext base, achieves an accuracy of about 85%, which is 5 percentage points higher than the accuracy of the interviewers. while this is a promising result, the models still struggle to score partially correct drawings, which are also problematic for interviewers. this suggests that more and better training data is needed to achieve production-level prediction accuracy. we therefore discuss possible next steps to improve the quality and quantity of training examples.",,2023-12-28,2023-12-29,"['arne bethmann', 'marina aoki', 'charlotte hunsicker', 'claudia weileder']"
2312.17093,lite: a stable framework for lattice-integrated embedding of topological   descriptors,math.at cs.cg stat.ml,"in this paper, we introduce a new family of descriptors for persistence diagrams. our approach transforms these diagrams into elements of a finite-dimensional vector space using functionals based on the discrete measures they induce. while our focus is primarily on identity and frequency-based transformations, we do not restrict our approach exclusively to this types of techniques. we term this family of transformations as lite (lattice integrated topological embedding) and prove stability for some members of this family against the 1-$kantorovitch$-$rubinstein$ metric, ensuring its responsiveness to subtle data variations. extensive comparative analysis reveals that our descriptor performs competitively with the current state-of-art from the topological data analysis literature, and often surpasses, the existing methods. this research not only introduces an innovative perspective for data scientists but also critiques the current trajectory of literature on methodologies for vectorizing diagrams. it establishes a foundation for future progress in applying persistence diagrams to data analysis and machine learning under a more simple and effective lens.",,2023-12-28,2024-01-05,"['michael etienne van huffel', 'matteo palo']"
2312.17122,large language model for causal decision making,cs.cl cs.ai stat.ml,"large language models (llms) have shown their success in language understanding and reasoning on general topics. however, their capability to inference based on user-specified structured data and knowledge in corpus-rare concepts like causal decision-making is still limited. in this work, we explore the possibility of fine-tuning an open-sourced llm into llm4causal, which can identify the causal task, execute a corresponding function, and interpret its numerical results based on users' queries and the provided dataset. meanwhile, we propose a data generation process for more controllable gpt prompting and present two instruction-tuning datasets: (1) causal-retrieval-bench for causal problem identification and input parameter extraction for causal function calling and (2) causal-interpret-bench for in-context causal interpretation. with three case studies, we showed that llm4causal can deliver end-to-end solutions for causal problems and provide easy-to-understand answers. numerical studies also reveal that it has a remarkable ability to identify the correct causal task given a query.",,2023-12-28,2023-12-29,"['haitao jiang', 'lin ge', 'yuhe gao', 'jianian wang', 'rui song']"
2312.17173,non-vacuous generalization bounds for large language models,stat.ml cs.lg,"modern language models can contain billions of parameters, raising the question of whether they can generalize beyond the training data or simply regurgitate their training corpora. we provide the first non-vacuous generalization bounds for pretrained large language models (llms), indicating that language models are capable of discovering regularities that generalize to unseen data. in particular, we derive a compression bound that is valid for the unbounded log-likelihood loss using prediction smoothing, and we extend the bound to handle subsampling, accelerating bound computation on massive datasets. to achieve the extreme level of compression required for non-vacuous generalization bounds, we devise sublora, a low-dimensional non-linear parameterization. using this approach, we find that larger models have better generalization bounds and are more compressible than smaller models.",,2023-12-28,2024-02-12,"['sanae lotfi', 'marc finzi', 'yilun kuang', 'tim g. j. rudner', 'micah goldblum', 'andrew gordon wilson']"
2312.17306,gradient flossing: improving gradient descent through dynamic control of   jacobians,cs.lg cs.ai nlin.cd q-bio.nc stat.ml,"training recurrent neural networks (rnns) remains a challenge due to the instability of gradients across long time horizons, which can lead to exploding and vanishing gradients. recent research has linked these problems to the values of lyapunov exponents for the forward-dynamics, which describe the growth or shrinkage of infinitesimal perturbations. here, we propose gradient flossing, a novel approach to tackling gradient instability by pushing lyapunov exponents of the forward dynamics toward zero during learning. we achieve this by regularizing lyapunov exponents through backpropagation using differentiable linear algebra. this enables us to ""floss"" the gradients, stabilizing them and thus improving network training. we demonstrate that gradient flossing controls not only the gradient norm but also the condition number of the long-term jacobian, facilitating multidimensional error feedback propagation. we find that applying gradient flossing prior to training enhances both the success rate and convergence speed for tasks involving long time horizons. for challenging tasks, we show that gradient flossing during training can further increase the time horizon that can be bridged by backpropagation through time. moreover, we demonstrate the effectiveness of our approach on various rnn architectures and tasks of variable temporal complexity. additionally, we provide a simple implementation of our gradient flossing algorithm that can be used in practice. our results indicate that gradient flossing via regularizing lyapunov exponents can significantly enhance the effectiveness of rnn training and mitigate the exploding and vanishing gradient problem.",,2023-12-28,,['rainer engelken']
2312.17346,stanhop: sparse tandem hopfield model for memory-enhanced time series   prediction,cs.lg cs.ai cs.cv cs.ne stat.ml,"we present stanhop-net (sparse tandem hopfield network) for multivariate time series prediction with memory-enhanced capabilities. at the heart of our approach is stanhop, a novel hopfield-based neural network block, which sparsely learns and stores both temporal and cross-series representations in a data-dependent fashion. in essence, stanhop sequentially learn temporal representation and cross-series representation using two tandem sparse hopfield layers. in addition, stanhop incorporates two additional external memory modules: a plug-and-play module and a tune-and-play module for train-less and task-aware memory-enhancements, respectively. they allow stanhop-net to swiftly respond to certain sudden events. methodologically, we construct the stanhop-net by stacking stanhop blocks in a hierarchical fashion, enabling multi-resolution feature extraction with resolution-specific sparsity. theoretically, we introduce a sparse extension of the modern hopfield model (generalized sparse modern hopfield model) and show that it endows a tighter memory retrieval error compared to the dense counterpart without sacrificing memory capacity. empirically, we validate the efficacy of our framework on both synthetic and real-world settings.",,2023-12-28,,"['dennis wu', 'jerry yao-chieh hu', 'weijian li', 'bo-yu chen', 'han liu']"
2312.17348,a randomized algorithm to solve reduced rank operator regression,cs.lg cs.na math.na stat.ml,"we present and analyze an algorithm designed for addressing vector-valued regression problems involving possibly infinite-dimensional input and output spaces. the algorithm is a randomized adaptation of reduced rank regression, a technique to optimally learn a low-rank vector-valued function (i.e. an operator) between sampled data via regularized empirical risk minimization with rank constraints. we propose gaussian sketching techniques both for the primal and dual optimization objectives, yielding randomized reduced rank regression (r4) estimators that are efficient and accurate. for each of our r4 algorithms we prove that the resulting regularized empirical risk is, in expectation w.r.t. randomness of a sketch, arbitrarily close to the optimal value when hyper-parameteres are properly tuned. numerical expreriments illustrate the tightness of our bounds and show advantages in two distinct scenarios: (i) solving a vector-valued regression problem using synthetic and large-scale neuroscience datasets, and (ii) regressing the koopman operator of a nonlinear stochastic dynamical system.",,2023-12-28,,"['giacomo turri', 'vladimir kostic', 'pietro novelli', 'massimiliano pontil']"
2312.17363,a comparison of full information maximum likelihood and machine learning   missing data analytical methods in growth curve modeling,stat.ap,"missing data are inevitable in longitudinal studies. traditional methods, such as the full information maximum likelihood (fiml), are commonly used to handle ignorable missing data. however, they may lead to biased model estimation due to missing not at random data that often appear in longitudinal studies. recently, machine learning methods, such as random forests (rf) and k-nearest neighbors (knn) imputation methods, have been proposed to cope with missing values. although machine learning imputation methods have been gaining popularity, few studies have investigated the tenability and utility of these methods in longitudinal research. through monte carlo simulations, this study evaluates and compares the performance of traditional and machine learning approaches (fiml, rf, and knn) in growth curve modeling. the effects of sample size, the rate of missingness, and the missing data mechanism on model estimation are investigated. results indicate that fiml is a better choice than the two machine learning imputation methods in terms of model estimation accuracy and efficiency.",,2023-12-28,,"['dandan tang', 'xin tong']"
2312.17404,parameter optimization with conscious allocation (poca),cs.lg stat.ml,"the performance of modern machine learning algorithms depends upon the selection of a set of hyperparameters. common examples of hyperparameters are learning rate and the number of layers in a dense neural network. auto-ml is a branch of optimization that has produced important contributions in this area. within auto-ml, hyperband-based approaches, which eliminate poorly-performing configurations after evaluating them at low budgets, are among the most effective. however, the performance of these algorithms strongly depends on how effectively they allocate the computational budget to various hyperparameter configurations. we present the new parameter optimization with conscious allocation (poca), a hyperband-based algorithm that adaptively allocates the inputted budget to the hyperparameter configurations it generates following a bayesian sampling scheme. we compare poca to its nearest competitor at optimizing the hyperparameters of an artificial toy function and a deep neural network and find that poca finds strong configurations faster in both settings.",,2023-12-28,,"['joshua inman', 'tanmay khandait', 'giulia pedrielli', 'lalitha sankar']"
2312.17411,generative posterior networks for approximately bayesian epistemic   uncertainty estimation,cs.lg stat.ml,"in many real-world problems, there is a limited set of training data, but an abundance of unlabeled data. we propose a new method, generative posterior networks (gpns), that uses unlabeled data to estimate epistemic uncertainty in high-dimensional problems. a gpn is a generative model that, given a prior distribution over functions, approximates the posterior distribution directly by regularizing the network towards samples from the prior. we prove theoretically that our method indeed approximates the bayesian posterior and show empirically that it improves epistemic uncertainty estimation and scalability over competing methods.",,2023-12-28,,"['melrose roderick', 'felix berkenkamp', 'fatemeh sheikholeslami', 'zico kolter']"
2312.17463,out of the ordinary: spectrally adapting regression for covariate shift,cs.lg stat.ml,"designing deep neural network classifiers that perform robustly on distributions differing from the available training data is an active area of machine learning research. however, out-of-distribution generalization for regression-the analogous problem for modeling continuous targets-remains relatively unexplored. to tackle this problem, we return to first principles and analyze how the closed-form solution for ordinary least squares (ols) regression is sensitive to covariate shift. we characterize the out-of-distribution risk of the ols model in terms of the eigenspectrum decomposition of the source and target data. we then use this insight to propose a method for adapting the weights of the last layer of a pre-trained neural regression model to perform better on input data originating from a different distribution. we demonstrate how this lightweight spectral adaptation procedure can improve out-of-distribution performance for synthetic and real-world datasets.",,2023-12-28,,"['benjamin eyre', 'elliot creager', 'david madras', 'vardan papyan', 'richard zemel']"
2312.17553,a fully automated pipeline using swin transformers for deep   learning-based blood segmentation on head ct scans after aneurysmal   subarachnoid hemorrhage,cs.cv stat.ml,"background: accurate volumetric assessment of spontaneous subarachnoid hemorrhage (sah) is a labor-intensive task performed with current manual and semiautomatic methods that might be relevant for its clinical and prognostic implications. in the present research, we sought to develop and validate an artificial intelligence-driven, fully automated blood segmentation tool for sah patients via noncontrast computed tomography (ncct) scans employing a transformer-based swin unetr architecture. methods: we retrospectively analyzed ncct scans from patients with confirmed aneurysmal subarachnoid hemorrhage (asah) utilizing the swin unetr for segmentation. the performance of the proposed method was evaluated against manually segmented ground truth data using metrics such as dice score, intersection over union (iou), the volumetric similarity index (vsi), the symmetric average surface distance (sasd), and sensitivity and specificity. a validation cohort from an external institution was included to test the generalizability of the model. results: the model demonstrated high accuracy with robust performance metrics across the internal and external validation cohorts. notably, it achieved high dice coefficient (0.873), iou (0.810), vsi (0.840), sensitivity (0.821) and specificity (0.996) values and a low sasd (1.866), suggesting proficiency in segmenting blood in sah patients. the model's efficiency was reflected in its processing speed, indicating potential for real-time applications. conclusions: our swin unetr-based model offers significant advances in the automated segmentation of blood after asah on ncct images. despite the computational intensity, the model operates effectively on standard hardware with a user-friendly interface, facilitating broader clinical adoption. further validation across diverse datasets is warranted to confirm its clinical reliability.",,2023-12-29,,"['sergio garcia garcia', 'santiago cepeda', 'ignacio arrese', 'rosario sarabia']"
2312.17584,interpretable and explainable machine learning methods for predictive   process monitoring: a systematic literature review,cs.lg cs.ai stat.ml,"this paper presents a systematic literature review (slr) on the explainability and interpretability of machine learning (ml) models within the context of predictive process mining, using the prisma framework. given the rapid advancement of artificial intelligence (ai) and ml systems, understanding the ""black-box"" nature of these technologies has become increasingly critical. focusing specifically on the domain of process mining, this paper delves into the challenges of interpreting ml models trained with complex business process data. we differentiate between intrinsically interpretable models and those that require post-hoc explanation techniques, providing a comprehensive overview of the current methodologies and their applications across various application domains. through a rigorous bibliographic analysis, this research offers a detailed synthesis of the state of explainability and interpretability in predictive process mining, identifying key trends, challenges, and future directions. our findings aim to equip researchers and practitioners with a deeper understanding of how to develop and implement more trustworthy, transparent, and effective intelligent systems for predictive process analytics.",,2023-12-29,,"['nijat mehdiyev', 'maxim majlatow', 'peter fettke']"
2312.17666,user strategization and trustworthy algorithms,cs.cy cs.gt cs.lg stat.ml,"many human-facing algorithms -- including those that power recommender systems or hiring decision tools -- are trained on data provided by their users. the developers of these algorithms commonly adopt the assumption that the data generating process is exogenous: that is, how a user reacts to a given prompt (e.g., a recommendation or hiring suggestion) depends on the prompt and not on the algorithm that generated it. for example, the assumption that a person's behavior follows a ground-truth distribution is an exogeneity assumption. in practice, when algorithms interact with humans, this assumption rarely holds because users can be strategic. recent studies document, for example, tiktok users changing their scrolling behavior after learning that tiktok uses it to curate their feed, and uber drivers changing how they accept and cancel rides in response to changes in uber's algorithm.   our work studies the implications of this strategic behavior by modeling the interactions between a user and their data-driven platform as a repeated, two-player game. we first find that user strategization can actually help platforms in the short term. we then show that it corrupts platforms' data and ultimately hurts their ability to make counterfactual decisions. we connect this phenomenon to user trust, and show that designing trustworthy algorithms can go hand in hand with accurate estimation. finally, we provide a formalization of trustworthiness that inspires potential interventions.",,2023-12-29,,"['sarah h. cen', 'andrew ilyas', 'aleksander madry']"
2312.17701,density estimation using the perceptron,math.st stat.th,"we propose a new density estimation algorithm. given $n$ i.i.d. samples from a distribution belonging to a class of densities on $\mathbb{r}^d$, our estimator outputs any density in the class whose ''perceptron discrepancy'' with the empirical distribution is at most $o(\sqrt{d/n})$. the perceptron discrepancy between two distributions is defined as the largest difference in mass that they place on any halfspace of $\mathbb{r}^d$. it is shown that this estimator achieves expected total variation distance to the truth that is almost minimax optimal over the class of densities with bounded sobolev norm and gaussian mixtures. this suggests that regularity of the prior distribution could be an explanation for the efficiency of the ubiquitous step in machine learning that replaces optimization over large function spaces with simpler parametric classes (e.g. in the discriminators of gans).   we generalize the above to show that replacing the ''perceptron discrepancy'' with the generalized energy distance of sz\'ekeley-rizzo further improves total variation loss. the generalized energy distance between empirical distributions is easily computable and differentiable, thus making it especially useful for fitting generative models. to the best of our knowledge, it is the first example of a distance with such properties for which there are minimax statistical guarantees.",,2023-12-29,2024-02-20,"['patrik róbert gerber', 'tianze jiang', 'yury polyanskiy', 'rui sun']"
2401.00122,salsa: sequential approximate leverage-score algorithm with application   in analyzing big time series data,stat.ml cs.lg,"we develop a new efficient sequential approximate leverage score algorithm, salsa, using methods from randomized numerical linear algebra (randnla) for large matrices. we demonstrate that, with high probability, the accuracy of salsa's approximations is within $(1 + o({\varepsilon}))$ of the true leverage scores. in addition, we show that the theoretical computational complexity and numerical accuracy of salsa surpass existing approximations. these theoretical results are subsequently utilized to develop an efficient algorithm, named lsarma, for fitting an appropriate arma model to large-scale time series data. our proposed algorithm is, with high probability, guaranteed to find the maximum likelihood estimates of the parameters for the true underlying arma model. furthermore, it has a worst-case running time that significantly improves those of the state-of-the-art alternatives in big data regimes. empirical results on large-scale data strongly support these theoretical results and underscore the efficacy of our new approach.",,2023-12-29,,"['ali eshragh', 'luke yerbury', 'asef nazari', 'fred roosta', 'michael w. mahoney']"
2401.00276,second-order uncertainty quantification: variance-based measures,cs.lg stat.ml,"uncertainty quantification is a critical aspect of machine learning models, providing important insights into the reliability of predictions and aiding the decision-making process in real-world applications. this paper proposes a novel way to use variance-based measures to quantify uncertainty on the basis of second-order distributions in classification problems. a distinctive feature of the measures is the ability to reason about uncertainties on a class-based level, which is useful in situations where nuanced decision-making is required. recalling some properties from the literature, we highlight that the variance-based measures satisfy important (axiomatic) properties. in addition to this axiomatic approach, we present empirical results showing the measures to be effective and competitive to commonly used entropy-based measures.",,2023-12-30,,"['yusuf sale', 'paul hofman', 'lisa wimmer', 'eyke hüllermeier', 'thomas nagler']"
2401.00490,kernel density estimation for multiclass quantification,cs.lg stat.ml,"several disciplines, like the social sciences, epidemiology, sentiment analysis, or market research, are interested in knowing the distribution of the classes in a population rather than the individual labels of the members thereof. quantification is the supervised machine learning task concerned with obtaining accurate predictors of class prevalence, and to do so particularly in the presence of label shift. the distribution-matching (dm) approaches represent one of the most important families among the quantification methods that have been proposed in the literature so far. current dm approaches model the involved populations by means of histograms of posterior probabilities. in this paper, we argue that their application to the multiclass setting is suboptimal since the histograms become class-specific, thus missing the opportunity to model inter-class information that may exist in the data. we propose a new representation mechanism based on multivariate densities that we model via kernel density estimation (kde). the experiments we have carried out show our method, dubbed kdey, yields superior quantification performance with respect to previous dm approaches. we also investigate the kde-based representation within the maximum likelihood framework and show kdey often shows superior performance with respect to the expectation-maximization method for quantification, arguably the strongest contender in the quantification arena to date.",,2023-12-31,2024-01-02,"['alejandro moreo', 'pablo gonzález', 'juan josé del coz']"
2401.00510,smoothness estimation for whittle-mat\'ern processes on closed   riemannian manifolds,math.st stat.th,"the family of mat\'ern kernels are often used in spatial statistics, function approximation and gaussian process methods in machine learning. one reason for their popularity is the presence of a smoothness parameter that controls, for example, optimal error bounds for kriging and posterior contraction rates in gaussian process regression. on closed riemannian manifolds, we show that the smoothness parameter can be consistently estimated from the maximizer(s) of the gaussian likelihood when the underlying data are from point evaluations of a gaussian process and, perhaps surprisingly, even when the data comprise evaluations of a non-gaussian process. the points at which the process is observed need not have any particular spatial structure beyond quasi-uniformity. our methods are based on results from approximation theory for the sobolev scale of hilbert spaces. moreover, we generalize a well-known equivalence of measures phenomenon related to mat\'ern kernels to the non-gaussian case by using kakutani's theorem.",,2023-12-31,,"['moritz korte-stapff', 'toni karvonen', 'eric moulines']"
2401.00593,"simplicity bias, algorithmic probability, and the random logistic map",cs.it math.ds math.it nlin.cd stat.ml,"simplicity bias is an intriguing phenomenon prevalent in various input-output maps, characterized by a preference for simpler, more regular, or symmetric outputs. notably, these maps typically feature high-probability outputs with simple patterns, whereas complex patterns are exponentially less probable. this bias has been extensively examined and attributed to principles derived from algorithmic information theory and algorithmic probability. in a significant advancement, it has been demonstrated that the renowned logistic map $x_{k+1}=\mu x_k(1-x_k)$, and other one-dimensional maps exhibit simplicity bias when conceptualized as input-output systems. building upon this foundational work, our research delves into the manifestations of simplicity bias within the random logistic map, specifically focusing on scenarios involving additive noise. this investigation is driven by the overarching goal of formulating a comprehensive theory for the prediction and analysis of time series.our primary contributions are multifaceted. we discover that simplicity bias is observable in the random logistic map for specific ranges of $\mu$ and noise magnitudes. additionally, we find that this bias persists even with the introduction of small measurement noise, though it diminishes as noise levels increase. our studies also revisit the phenomenon of noise-induced chaos, particularly when $\mu=3.83$, revealing its characteristics through complexity-probability plots. intriguingly, we employ the logistic map to underscore a paradoxical aspect of data analysis: more data adhering to a consistent trend can occasionally lead to reduced confidence in extrapolation predictions, challenging conventional wisdom.we propose that adopting a probability-complexity perspective in analyzing dynamical systems could significantly enrich statistical learning theories related to series prediction.",10.13140/rg.2.2.29746.79048,2023-12-31,,"['boumediene hamzi', 'kamaludin dingle']"
2401.00611,a compact representation for bayesian neural networks by removing   permutation symmetry,stat.ml cs.ai cs.lg,"bayesian neural networks (bnns) are a principled approach to modeling predictive uncertainties in deep learning, which are important in safety-critical applications. since exact bayesian inference over the weights in a bnn is intractable, various approximate inference methods exist, among which sampling methods such as hamiltonian monte carlo (hmc) are often considered the gold standard. while hmc provides high-quality samples, it lacks interpretable summary statistics because its sample mean and variance is meaningless in neural networks due to permutation symmetry. in this paper, we first show that the role of permutations can be meaningfully quantified by a number of transpositions metric. we then show that the recently proposed rebasin method allows us to summarize hmc samples into a compact representation that provides a meaningful explicit uncertainty estimate for each weight in a neural network, thus unifying sampling methods with variational inference. we show that this compact representation allows us to compare trained bnns directly in weight space across sampling methods and variational inference, and to efficiently prune neural networks trained without explicit bayesian frameworks by exploiting uncertainty estimates from hmc.",,2023-12-31,,"['tim z. xiao', 'weiyang liu', 'robert bamler']"
2401.00691,stochastic gradient descent for additive nonparametric regression,stat.ml cs.lg,"this paper introduces an iterative algorithm for training additive models that enjoys favorable memory storage and computational requirements. the algorithm can be viewed as the functional counterpart of stochastic gradient descent, applied to the coefficients of a truncated basis expansion of the component functions. we show that the resulting estimator satisfies an oracle inequality that allows for model mis-specification. in the well-specified setting, by choosing the learning rate carefully across three distinct stages of training, we demonstrate that its risk is minimax optimal in terms of the dependence on the dimensionality of the data and the size of the training sample. we further illustrate the computational benefits by comparing the approach with traditional backfitting on two real-world datasets.",,2024-01-01,2024-02-13,"['xin chen', 'jason m. klusowski']"
2401.00773,unsupervised outlier detection using random subspace and subsampling   ensembles of dirichlet process mixtures,cs.lg cs.ai stat.ml,"probabilistic mixture models are acknowledged as a valuable tool for unsupervised outlier detection owing to their interpretability and intuitive grounding in statistical principles. within this framework, dirichlet process mixture models emerge as a compelling alternative to conventional finite mixture models for both clustering and outlier detection tasks. however, despite their evident advantages, the widespread adoption of dirichlet process mixture models in unsupervised outlier detection has been hampered by challenges related to computational inefficiency and sensitivity to outliers during the construction of detectors. to tackle these challenges, we propose a novel outlier detection method based on ensembles of dirichlet process gaussian mixtures. the proposed method is a fully unsupervised algorithm that capitalizes on random subspace and subsampling ensembles, not only ensuring efficient computation but also enhancing the robustness of the resulting outlier detector. moreover, the proposed method leverages variational inference for dirichlet process mixtures to ensure efficient and fast computation. empirical studies with benchmark datasets demonstrate that our method outperforms existing approaches for unsupervised outlier detection.",,2024-01-01,2024-01-13,"['dongwook kim', 'juyeon park', 'hee cheol chung', 'seonghyun jeong']"
2401.00781,inferring heterogeneous treatment effects of crashes on highway traffic:   a doubly robust causal machine learning approach,cs.lg stat.ml,"highway traffic crashes exert a considerable impact on both transportation systems and the economy. in this context, accurate and dependable emergency responses are crucial for effective traffic management. however, the influence of crashes on traffic status varies across diverse factors and may be biased due to selection bias. therefore, there arises a necessity to accurately estimate the heterogeneous causal effects of crashes, thereby providing essential insights to facilitate individual-level emergency decision-making. this paper proposes a novel causal machine learning framework to estimate the causal effect of different types of crashes on highway speed. the neyman-rubin causal model (rcm) is employed to formulate this problem from a causal perspective. the conditional shapley value index (csvi) is proposed based on causal graph theory to filter adverse variables, and the structural causal model (scm) is then adopted to define the statistical estimand for causal effects. the treatment effects are estimated by doubly robust learning (drl) methods, which combine doubly robust causal inference with classification and regression machine learning models. experimental results from 4815 crashes on highway interstate 5 in washington state reveal the heterogeneous treatment effects of crashes at varying distances and durations. the rear-end crashes cause more severe congestion and longer durations than other types of crashes, and the sideswipe crashes have the longest delayed impact. additionally, the findings show that rear-end crashes affect traffic greater at night, while crash to objects has the most significant influence during peak hours. statistical hypothesis tests, error metrics based on matched ""counterfactual outcomes"", and sensitive analyses are employed for assessment, and the results validate the accuracy and effectiveness of our method.",,2024-01-01,,"['shuang li', 'ziyuan pu', 'zhiyong cui', 'seunghyeon lee', 'xiucheng guo', 'dong ngoduy']"
2401.00800,factor importance ranking and selection using total indices,stat.me stat.ml,"factor importance measures the impact of each feature on output prediction accuracy. many existing works focus on the model-based importance, but an important feature in one learning algorithm may hold little significance in another model. hence, a factor importance measure ought to characterize the feature's predictive potential without relying on a specific prediction algorithm. such algorithm-agnostic importance is termed as intrinsic importance in williamson et al. (2023), but their estimator again requires model fitting. to bypass the modeling step, we present the equivalence between predictiveness potential and total sobol' indices from global sensitivity analysis, and introduce a novel consistent estimator that can be directly estimated from noisy data. integrating with forward selection and backward elimination gives rise to first, factor importance ranking and selection using total (sobol') indices. extensive simulations are provided to demonstrate the effectiveness of first on regression and binary classification problems, and a clear advantage over the state-of-the-art methods.",,2024-01-01,2024-01-11,"['chaofan huang', 'v. roshan joseph']"
2401.00828,multi-lattice sampling of quantum field theories via neural   operator-based flows,cs.lg hep-lat stat.ml,"we consider the problem of sampling discrete field configurations $\phi$ from the boltzmann distribution $[d\phi] z^{-1} e^{-s[\phi]}$, where $s$ is the lattice-discretization of the continuous euclidean action $\mathcal s$ of some quantum field theory. since such densities arise as the approximation of the underlying functional density $[\mathcal d\phi(x)] \mathcal z^{-1} e^{-\mathcal s[\phi(x)]}$, we frame the task as an instance of operator learning. in particular, we propose to approximate a time-dependent operator $\mathcal v_t$ whose time integral provides a mapping between the functional distributions of the free theory $[\mathcal d\phi(x)] \mathcal z_0^{-1} e^{-\mathcal s_{0}[\phi(x)]}$ and of the target theory $[\mathcal d\phi(x)]\mathcal z^{-1}e^{-\mathcal s[\phi(x)]}$. whenever a particular lattice is chosen, the operator $\mathcal v_t$ can be discretized to a finite dimensional, time-dependent vector field $v_t$ which in turn induces a continuous normalizing flow between finite dimensional distributions over the chosen lattice. this flow can then be trained to be a diffeormorphism between the discretized free and target theories $[d\phi] z_0^{-1} e^{-s_{0}[\phi]}$, $[d\phi] z^{-1}e^{-s[\phi]}$. we run experiments on the $\phi^4$-theory to explore to what extent such operator-based flow architectures generalize to lattice sizes they were not trained on and show that pretraining on smaller lattices can lead to speedup over training only a target lattice size.",,2024-01-01,2024-01-17,"['bálint máté', 'françois fleuret']"
2401.00953,families of costs with zero and nonnegative mtw tensor in optimal   transport,math.ap cs.it cs.lg math.it stat.ml,"we compute explicitly the mtw tensor (or cross curvature) for the optimal transport problem on $\mathbb{r}^n$ with a cost function of form $\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$, where $\mathsf{u}$ is a scalar function with inverse $\mathsf{s}$, $x^{\ft}y$ is a nondegenerate bilinear pairing of vectors $x, y$ belonging to an open subset of $\mathbb{r}^n$. the condition that the mtw-tensor vanishes on null vectors under the kim-mccann metric is a fourth-order nonlinear ode, which could be reduced to a linear ode of the form $\mathsf{s}^{(2)} - s\mathsf{s}^{(1)} + p\mathsf{s} = 0$ with constant coefficients $p$ and $s$. the resulting inverse functions include {\it lambert} and {\it generalized inverse hyperbolic\slash trigonometric} functions. the square euclidean metric and $\log$-type costs are equivalent to instances of these solutions. the optimal map for the family is also explicit. for cost functions of a similar form on a hyperboloid model of the hyperbolic space and unit sphere, we also express this tensor in terms of algebraic expressions in derivatives of $\mathsf{s}$ using the gauss-codazzi equation, obtaining new families of strictly regular costs for these manifolds, including new families of {\it power function costs}. we analyze the $\sinh$-type hyperbolic cost, providing examples of $\mathsf{c}$-convex functions and divergence.",,2024-01-01,,['du nguyen']
2401.00972,robust meta-model for predicting the need for blood transfusion in   non-traumatic icu patients,cs.lg cs.cy stat.ap,"objective: blood transfusions, crucial in managing anemia and coagulopathy in icu settings, require accurate prediction for effective resource allocation and patient risk assessment. however, existing clinical decision support systems have primarily targeted a particular patient demographic with unique medical conditions and focused on a single type of blood transfusion. this study aims to develop an advanced machine learning-based model to predict the probability of transfusion necessity over the next 24 hours for a diverse range of non-traumatic icu patients.   methods: we conducted a retrospective cohort study on 72,072 adult non-traumatic icu patients admitted to a high-volume us metropolitan academic hospital between 2016 and 2020. we developed a meta-learner and various machine learning models to serve as predictors, training them annually with four-year data and evaluating on the fifth, unseen year, iteratively over five years.   results: the experimental results revealed that the meta-model surpasses the other models in different development scenarios. it achieved notable performance metrics, including an area under the receiver operating characteristic (auroc) curve of 0.97, an accuracy rate of 0.93, and an f1-score of 0.89 in the best scenario.   conclusion: this study pioneers the use of machine learning models for predicting blood transfusion needs in a diverse cohort of critically ill patients. the findings of this evaluation confirm that our model not only predicts transfusion requirements effectively but also identifies key biomarkers for making transfusion decisions.",,2024-01-01,,"['alireza rafiei', 'ronald moore', 'tilendra choudhary', 'curtis marshall', 'geoffrey smith', 'john d. roback', 'ravi m. patel', 'cassandra d. josephson', 'rishikesan kamaleswaran']"
2401.00981,machine learning classification of alzheimer's disease stages using   cerebrospinal fluid biomarkers alone,cs.lg q-bio.qm stat.ap,"early diagnosis of alzheimer's disease is a challenge because the existing methodologies do not identify the patients in their preclinical stage, which can last up to a decade prior to the onset of clinical symptoms. several research studies demonstrate the potential of cerebrospinal fluid biomarkers, amyloid beta 1-42, t-tau, and p-tau, in early diagnosis of alzheimer's disease stages. in this work, we used machine learning models to classify different stages of alzheimer's disease based on the cerebrospinal fluid biomarker levels alone. an electronic health record of patients from the national alzheimer's coordinating centre database was analyzed and the patients were subdivided based on mini-mental state scores and clinical dementia ratings. statistical and correlation analyses were performed to identify significant differences between the alzheimer's stages. afterward, machine learning classifiers including k-nearest neighbors, ensemble boosted tree, ensemble bagged tree, support vector machine, logistic regression, and naive bayes classifiers were employed to classify the alzheimer's disease stages. the results demonstrate that ensemble boosted tree (84.4%) and logistic regression (73.4%) provide the highest accuracy for binary classification, while ensemble bagged tree (75.4%) demonstrates better accuracy for multiclassification. the findings from this research are expected to help clinicians in making an informed decision regarding the early diagnosis of alzheimer's from the cerebrospinal fluid biomarkers alone, monitoring of the disease progression, and implementation of appropriate intervention measures.",,2024-01-01,,"['vivek kumar tiwari', 'premananda indic', 'shawana tabassum']"
2401.00987,inverting estimating equations for causal inference on quantiles,stat.me math.st stat.ml stat.th,"the causal inference literature frequently focuses on estimating the mean of the potential outcome, whereas the quantiles of the potential outcome may carry important additional information. we propose a universal approach, based on the inverse estimating equations, to generalize a wide class of causal inference solutions from estimating the mean of the potential outcome to its quantiles. we assume that an identifying moment function is available to identify the mean of the threshold-transformed potential outcome, based on which a convenient construction of the estimating equation of quantiles of potential outcome is proposed. in addition, we also give a general construction of the efficient influence functions of the mean and quantiles of potential outcomes, and identify their connection. we motivate estimators for the quantile estimands with the efficient influence function, and develop their asymptotic properties when either parametric models or data-adaptive machine learners are used to estimate the nuisance functions. a broad implication of our results is that one can rework the existing result for mean causal estimands to facilitate causal inference on quantiles, rather than starting from scratch. our results are illustrated by several examples.",,2024-01-01,,"['chao cheng', 'fan li']"
2401.01047,sharp analysis of power iteration for tensor pca,cs.lg cs.na math.na stat.ml,"we investigate the power iteration algorithm for the tensor pca model introduced in richard and montanari (2014). previous work studying the properties of tensor power iteration is either limited to a constant number of iterations, or requires a non-trivial data-independent initialization. in this paper, we move beyond these limitations and analyze the dynamics of randomly initialized tensor power iteration up to polynomially many steps. our contributions are threefold: first, we establish sharp bounds on the number of iterations required for power method to converge to the planted signal, for a broad range of the signal-to-noise ratios. second, our analysis reveals that the actual algorithmic threshold for power iteration is smaller than the one conjectured in literature by a polylog(n) factor, where n is the ambient dimension. finally, we propose a simple and effective stopping criterion for power iteration, which provably outputs a solution that is highly correlated with the true signal. extensive numerical experiments verify our theoretical results.",,2024-01-02,,"['yuchen wu', 'kangjie zhou']"
2401.01048,pac-bayesian domain adaptation bounds for multi-view learning,cs.lg stat.ml,"this paper presents a series of new results for domain adaptation in the multi-view learning setting. the incorporation of multiple views in the domain adaptation was paid little attention in the previous studies. in this way, we propose an analysis of generalization bounds with pac-bayesian theory to consolidate the two paradigms, which are currently treated separately. firstly, building on previous work by germain et al., we adapt the distance between distribution proposed by germain et al. for domain adaptation with the concept of multi-view learning. thus, we introduce a novel distance that is tailored for the multi-view domain adaptation setting. then, we give pac-bayesian bounds for estimating the introduced divergence. finally, we compare the different new bounds with the previous studies.",,2024-01-02,,"['mehdi hennequin', 'khalid benabdeslem', 'haytham elghazel']"
2401.01148,pac-bayes-chernoff bounds for unbounded losses,stat.ml cs.lg,"we introduce a new pac-bayes oracle bound for unbounded losses. this result can be understood as a pac-bayesian version of the cram\'er-chernoff bound. the proof technique relies on controlling the tails of certain random variables involving the cram\'er transform of the loss. we highlight several applications of the main theorem. first, we show that our result naturally allows exact optimization of the free parameter on many pac-bayes bounds. second, we recover and generalize previous results. finally, we show that our approach allows working with richer assumptions that result in more informative and potentially tighter bounds. in this direction, we provide a general bound under a new ``model-dependent bounded cgf"" assumption from which we obtain bounds based on parameter norms and log-sobolev inequalities. all these bounds can be minimized to obtain novel posteriors.",,2024-01-02,2024-02-06,"['ioar casado', 'luis a. ortega', 'andrés r. masegosa', 'aritz pérez']"
2401.01242,encoding binary events from continuous time series in rooted trees using   contrastive learning,cs.lg cs.ai cs.si stat.ml,"broadband infrastructure owners do not always know how their customers are connected in the local networks, which are structured as rooted trees. a recent study is able to infer the topology of a local network using discrete time series data from the leaves of the tree (customers). in this study we propose a contrastive approach for learning a binary event encoder from continuous time series data. as a preliminary result, we show that our approach has some potential in learning a valuable encoder.",,2024-01-02,,"['tobias engelhardt rasmussen', 'siv sørensen']"
2401.01294,efficient sparse least absolute deviation regression with differential   privacy,stat.ml cs.lg stat.me,"in recent years, privacy-preserving machine learning algorithms have attracted increasing attention because of their important applications in many scientific fields. however, in the literature, most privacy-preserving algorithms demand learning objectives to be strongly convex and lipschitz smooth, which thus cannot cover a wide class of robust loss functions (e.g., quantile/least absolute loss). in this work, we aim to develop a fast privacy-preserving learning solution for a sparse robust regression problem. our learning loss consists of a robust least absolute loss and an $\ell_1$ sparse penalty term. to fast solve the non-smooth loss under a given privacy budget, we develop a fast robust and privacy-preserving estimation (frappe) algorithm for least absolute deviation regression. our algorithm achieves a fast estimation by reformulating the sparse lad problem as a penalized least square estimation problem and adopts a three-stage noise injection to guarantee the $(\epsilon,\delta)$-differential privacy. we show that our algorithm can achieve better privacy and statistical accuracy trade-off compared with the state-of-the-art privacy-preserving regression algorithms. in the end, we conduct experiments to verify the efficiency of our proposed frappe algorithm.",10.1109/tifs.2023.3349054,2024-01-02,,"['weidong liu', 'xiaojun mao', 'xiaofei zhang', 'xin zhang']"
2401.01335,self-play fine-tuning converts weak language models to strong language   models,cs.lg cs.ai cs.cl stat.ml,"harnessing the power of human-annotated data through supervised fine-tuning (sft) is pivotal for advancing large language models (llms). in this paper, we delve into the prospect of growing a strong llm out of a weak one without the need for acquiring additional human-annotated data. we propose a new fine-tuning method called self-play fine-tuning (spin), which starts from a supervised fine-tuned model. at the heart of spin lies a self-play mechanism, where the llm refines its capability by playing against instances of itself. more specifically, the llm generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. our method progressively elevates the llm from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for sft. theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the llm policy aligns with the target data distribution. empirically, we evaluate our method on several benchmark datasets including the huggingface open llm leaderboard, mt-bench, and datasets from big-bench. our results show that spin can significantly improve the llm's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (dpo) supplemented with extra gpt-4 preference data. this sheds light on the promise of self-play, enabling the achievement of human-level performance in llms without the need for expert opponents. codes are available at https://github.com/uclaml/spin.",,2024-01-02,2024-02-12,"['zixiang chen', 'yihe deng', 'huizhuo yuan', 'kaixuan ji', 'quanquan gu']"
2401.01404,scalable network reconstruction in subquadratic time,cs.ds cs.lg physics.data-an stat.co stat.ml,"network reconstruction consists in determining the unobserved pairwise couplings between $n$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. a major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $o(n^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $o(n)$. here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $o(n^{3/2}\log n)$, but with a more typical log-linear complexity of $o(n\log^2n)$. our algorithm relies on a stochastic second neighbor search that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search. in practice, our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline, allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges.",,2024-01-02,2024-02-12,['tiago p. peixoto']
2401.01426,modular learning of deep causal generative models for high-dimensional   causal inference,cs.lg cs.ai cs.it math.it stat.me stat.ml,"pearl's causal hierarchy establishes a clear separation between observational, interventional, and counterfactual questions. researchers proposed sound and complete algorithms to compute identifiable causal queries at a given level of the hierarchy using the causal structure and data from the lower levels of the hierarchy. however, most of these algorithms assume that we can accurately estimate the probability distribution of the data, which is an impractical assumption for high-dimensional variables such as images. on the other hand, modern generative deep learning architectures can be trained to learn how to accurately sample from such high-dimensional distributions. especially with the recent rise of foundation models for images, it is desirable to leverage pre-trained models to answer causal queries with such high-dimensional data. to address this, we propose a sequential training algorithm that, given the causal structure and a pre-trained conditional generative model, can train a deep causal generative model, which utilizes the pre-trained model and can provably sample from identifiable interventional and counterfactual distributions. our algorithm, called modular-dcm, uses adversarial training to learn the network weights, and to the best of our knowledge, is the first algorithm that can make use of pre-trained models and provably sample from any identifiable causal query in the presence of latent confounders with high-dimensional data. we demonstrate the utility of our algorithm using semi-synthetic and real-world datasets containing images as variables in the causal structure.",,2024-01-02,,"['md musfiqur rahman', 'murat kocaoglu']"
2401.01460,point cloud classification via deep set linearized optimal transport,cs.lg stat.ml,"we introduce deep set linearized optimal transport, an algorithm designed for the efficient simultaneous embedding of point clouds into an $l^2-$space. this embedding preserves specific low-dimensional structures within the wasserstein space while constructing a classifier to distinguish between various classes of point clouds. our approach is motivated by the observation that $l^2-$distances between optimal transport maps for distinct point clouds, originating from a shared fixed reference distribution, provide an approximation of the wasserstein-2 distance between these point clouds, under certain assumptions. to learn approximations of these transport maps, we employ input convex neural networks (icnns) and establish that, under specific conditions, euclidean distances between samples from these icnns closely mirror wasserstein-2 distances between the true distributions. additionally, we train a discriminator network that attaches weights these samples and creates a permutation invariant classifier to differentiate between different classes of point clouds. we showcase the advantages of our algorithm over the standard deep set approach through experiments on a flow cytometry dataset with a limited number of labeled point clouds.",,2024-01-02,,"['scott mahan', 'caroline moosmüller', 'alexander cloninger']"
2401.01645,model averaging and double machine learning,econ.em stat.ml,"this paper discusses pairing double/debiased machine learning (ddml) with stacking, a model averaging method for combining multiple candidate learners, to estimate structural parameters. we introduce two new stacking approaches for ddml: short-stacking exploits the cross-fitting step of ddml to substantially reduce the computational burden and pooled stacking enforces common stacking weights over cross-fitting folds. using calibrated simulation studies and two applications estimating gender gaps in citations and wages, we show that ddml with stacking is more robust to partially unknown functional forms than common alternative approaches based on single pre-selected learners. we provide stata and r software implementing our proposals.",,2024-01-03,,"['achim ahrens', 'christian b. hansen', 'mark e. schaffer', 'thomas wiemann']"
2401.01789,deep learning the hurst parameter of linear fractional processes and   assessing its reliability,stat.ml cs.ai cs.lg,"this research explores the reliability of deep learning, specifically long short-term memory (lstm) networks, for estimating the hurst parameter in fractional stochastic processes. the study focuses on three types of processes: fractional brownian motion (fbm), fractional ornstein-uhlenbeck (fou) process, and linear fractional stable motions (lfsm). the work involves a fast generation of extensive datasets for fbm and fou to train the lstm network on a large volume of data in a feasible time. the study analyses the accuracy of the lstm network's hurst parameter estimation regarding various performance measures like rmse, mae, mre, and quantiles of the absolute and relative errors. it finds that lstm outperforms the traditional statistical methods in the case of fbm and fou processes; however, it has limited accuracy on lfsm processes. the research also delves into the implications of training length and valuation sequence length on the lstm's performance. the methodology is applied by estimating the hurst parameter in li-ion battery degradation data and obtaining confidence bounds for the estimation. the study concludes that while deep learning methods show promise in parameter estimation of fractional processes, their effectiveness is contingent on the process type and the quality of training data.",,2024-01-03,,"['dániel boros', 'bálint csanády', 'iván ivkovic', 'lóránt nagy', 'andrás lukács', 'lászló márkus']"
2401.01804,efficient computation of confidence sets using classification on   equidistributed grids,econ.em stat.ml,"economic models produce moment inequalities, which can be used to form tests of the true parameters. confidence sets (cs) of the true parameters are derived by inverting these tests. however, they often lack analytical expressions, necessitating a grid search to obtain the cs numerically by retaining the grid points that pass the test. when the statistic is not asymptotically pivotal, constructing the critical value for each grid point in the parameter space adds to the computational burden. in this paper, we convert the computational issue into a classification problem by using a support vector machine (svm) classifier. its decision function provides a faster and more systematic way of dividing the parameter space into two regions: inside vs. outside of the confidence set. we label those points in the cs as 1 and those outside as -1. researchers can train the svm classifier on a grid of manageable size and use it to determine whether points on denser grids are in the cs or not. we establish certain conditions for the grid so that there is a tuning that allows us to asymptotically reproduce the test in the cs. this means that in the limit, a point is classified as belonging to the confidence set if and only if it is labeled as 1 by the svm.",,2024-01-03,,['lujie zhou']
2401.01812,robust learning of staged tree models: a case study in evaluating   transport services,stat.ap,"staged trees are a relatively recent class of probabilistic graphical models that extend bayesian networks to formally and graphically account for non-symmetric patterns of dependence. machine learning algorithms to learn them from data have been implemented in various pieces of software. however, to date, methods to assess the robustness and validity of the learned, non-symmetric relationships are not available. here, we introduce validation techniques tailored to staged tree models based on non-parametric bootstrap resampling methods and investigate their use in practical applications. in particular, we focus on the evaluation of transport services using large-scale survey data. in these types of applications, data from heterogeneous sources must be collated together. staged trees provide a natural framework for this integration of data and its analysis. for the thorough evaluation of transport services, we further implement novel what-if sensitivity analyses for staged trees and their visualization using software.",,2024-01-03,,"['manuele leonelli', 'gherardo varando']"
2401.01857,optimal cross-learning for contextual bandits with unknown context   distributions,cs.lg stat.ml,"we consider the problem of designing contextual bandit algorithms in the ``cross-learning'' setting of balseiro et al., where the learner observes the loss for the action they play in all possible contexts, not just the context of the current round. we specifically consider the setting where losses are chosen adversarially and contexts are sampled i.i.d. from an unknown distribution. in this setting, we resolve an open problem of balseiro et al. by providing an efficient algorithm with a nearly tight (up to logarithmic factors) regret bound of $\widetilde{o}(\sqrt{tk})$, independent of the number of contexts. as a consequence, we obtain the first nearly tight regret bounds for the problems of learning to bid in first-price auctions (under unknown value distributions) and sleeping bandits with a stochastic action set.   at the core of our algorithm is a novel technique for coordinating the execution of a learning algorithm over multiple epochs in such a way to remove correlations between estimation of the unknown distribution and the actions played by the algorithm. this technique may be of independent interest for other learning problems involving estimation of an unknown context distribution.",,2024-01-03,,"['jon schneider', 'julian zimmert']"
2401.01869,on the hardness of learning under symmetries,cs.lg cs.ds math.st stat.ml stat.th,"we study the problem of learning equivariant neural networks via gradient descent. the incorporation of known symmetries (""equivariance"") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. however, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (csq) model, a framework encompassing gradient descent. in this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? we answer this question in the negative. in particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension. therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard.",,2024-01-03,,"['bobak t. kiani', 'thien le', 'hannah lawrence', 'stefanie jegelka', 'melanie weber']"
2401.01969,a comparative study with traditional and transfer learning-enhanced   machine learning algorithms for geotechnical characterisation of coal spoil,stat.ap,"the characterisation of materials is a prerequisite for evaluating and predicting the stability of mining waste dumps. over the past three decades, the bhp mitsubishi alliance coal framework has been a cornerstone in australian coal mines for characterising waste dump materials. however, its reliance on subjective human observations has introduced potential inaccuracies and subjectivity into the process. in response to these limitations, this study proposes an innovative approach to classify coal spoil attributes by remotely acquiring images through phones/tablets. automated image-based classification relies on feature extraction and a substantial amount of data. nevertheless, the inherent complexity of geological factors contributing to the formation of both rare and dominant materials leads to imbalanced data. recognising the need for classification mechanisms to overcome these challenges in spoil classification, the study explores and compares the use of convolutional neural networks, hybrid deep learning, and traditional techniques. among the twelve models evaluated in this study, the resnet18-k nearest neighbour model emerges as a powerful tool in geotechnical characterisation. however, it is essential to address issues of interpretability and adaptability to diverse datasets. as this study evolves, the field of geotechnical characterisation of spoil can anticipate the development of more robust methods in the future.",,2024-01-03,,"['sureka thiruchittampalam', 'kuruparan shanmugalingam', 'bikram p banarjee', 'nancy f glenn', 'simit raval']"
2401.01977,conformal causal inference for cluster randomized trials: model-robust   inference without asymptotic approximations,stat.me,"in the analysis of cluster randomized trials, two typical features are that individuals within a cluster are correlated and that the total number of clusters can sometimes be limited. while model-robust treatment effect estimators have been recently developed, their asymptotic theory requires the number of clusters to approach infinity, and one often has to empirically assess the applicability of those methods in finite samples. to address this challenge, we propose a conformal causal inference framework that achieves the target coverage probability of treatment effects in finite samples without the need for asymptotic approximations. meanwhile, we prove that this framework is compatible with arbitrary working models, including machine learning algorithms leveraging baseline covariates, possesses robustness against arbitrary misspecification of working models, and accommodates a variety of within-cluster correlations. under this framework, we offer efficient algorithms to make inferences on treatment effects at both the cluster and individual levels, applicable to user-specified covariate subgroups and two types of test data. finally, we demonstrate our methods via simulations and a real data application based on a cluster randomized trial for treating chronic pain.",,2024-01-03,,"['bingkai wang', 'fan li', 'mengxin yu']"
2401.01988,hierarchical clustering in ${\lambda}$cdm cosmologies via persistence   energy,astro-ph.co cs.cg math.at stat.ml,"in this research, we investigate the structural evolution of the cosmic web, employing advanced methodologies from topological data analysis. our approach involves leveraging lite, an innovative method from recent literature that embeds persistence diagrams into elements of vector spaces. utilizing this methodology, we analyze three quintessential cosmic structures: clusters, filaments, and voids. a central discovery is the correlation between \textit{persistence energy} and redshift values, linking persistent homology with cosmic evolution and providing insights into the dynamics of cosmic structures.",,2024-01-03,2024-01-08,"['michael etienne van huffel', 'leonardo aldo alejandro barberi', 'tobias sagis']"
2401.02058,neural collapse for cross-entropy class-imbalanced learning with   unconstrained relu feature model,cs.lg stat.ml,"the current paradigm of training deep neural networks for classification tasks includes minimizing the empirical risk that pushes the training loss value towards zero, even after the training error has been vanished. in this terminal phase of training, it has been observed that the last-layer features collapse to their class-means and these class-means converge to the vertices of a simplex equiangular tight frame (etf). this phenomenon is termed as neural collapse (nc). to theoretically understand this phenomenon, recent works employ a simplified unconstrained feature model to prove that nc emerges at the global solutions of the training problem. however, when the training dataset is class-imbalanced, some nc properties will no longer be true. for example, the class-means geometry will skew away from the simplex etf when the loss converges. in this paper, we generalize nc to imbalanced regime for cross-entropy loss under the unconstrained relu feature model. we prove that, while the within-class features collapse property still holds in this setting, the class-means will converge to a structure consisting of orthogonal vectors with different lengths. furthermore, we find that the classifier weights are aligned to the scaled and centered class-means with scaling factors depend on the number of training samples of each class, which generalizes nc in the class-balanced setting. we empirically prove our results through experiments on practical architectures and dataset.",,2024-01-03,,"['hien dang', 'tho tran', 'tan nguyen', 'nhat ho']"
2401.02062,"u-trustworthy models.reliability, competence, and confidence in   decision-making",stat.ml cs.lg,"with growing concerns regarding bias and discrimination in predictive models, the ai community has increasingly focused on assessing ai system trustworthiness. conventionally, trustworthy ai literature relies on the probabilistic framework and calibration as prerequisites for trustworthiness. in this work, we depart from this viewpoint by proposing a novel trust framework inspired by the philosophy literature on trust. we present a precise mathematical definition of trustworthiness, termed $\mathcal{u}$-trustworthiness, specifically tailored for a subset of tasks aimed at maximizing a utility function. we argue that a model's $\mathcal{u}$-trustworthiness is contingent upon its ability to maximize bayes utility within this task subset. our first set of results challenges the probabilistic framework by demonstrating its potential to favor less trustworthy models and introduce the risk of misleading trustworthiness assessments. within the context of $\mathcal{u}$-trustworthiness, we prove that properly-ranked models are inherently $\mathcal{u}$-trustworthy. furthermore, we advocate for the adoption of the auc metric as the preferred measure of trustworthiness. by offering both theoretical guarantees and experimental validation, auc enables robust evaluation of trustworthiness, thereby enhancing model selection and hyperparameter tuning to yield more trustworthy outcomes.",,2024-01-03,,"['ritwik vashistha', 'arya farahi']"
2401.02080,energy based diffusion generator for efficient sampling of boltzmann   distributions,cs.lg stat.co stat.ml,"we introduce a novel sampler called the energy based diffusion generator for generating samples from arbitrary target distributions. the sampling model employs a structure similar to a variational autoencoder, utilizing a decoder to transform latent variables from a simple distribution into random variables approximating the target distribution, and we design an encoder based on the diffusion model. leveraging the powerful modeling capacity of the diffusion model for complex distributions, we can obtain an accurate variational estimate of the kullback-leibler divergence between the distributions of the generated samples and the target. moreover, we propose a decoder based on generalized hamiltonian dynamics to further enhance sampling performance. through empirical evaluation, we demonstrate the effectiveness of our method across various complex distribution functions, showcasing its superiority compared to existing methods.",,2024-01-04,,"['yan wang', 'ling guo', 'hao wu', 'tao zhou']"
2401.02203,robust bilinear factor analysis based on the matrix-variate $t$   distribution,stat.ml cs.lg,"factor analysis based on multivariate $t$ distribution ($t$fa) is a useful robust tool for extracting common factors on heavy-tailed or contaminated data. however, $t$fa is only applicable to vector data. when $t$fa is applied to matrix data, it is common to first vectorize the matrix observations. this introduces two challenges for $t$fa: (i) the inherent matrix structure of the data is broken, and (ii) robustness may be lost, as vectorized matrix data typically results in a high data dimension, which could easily lead to the breakdown of $t$fa. to address these issues, starting from the intrinsic matrix structure of matrix data, a novel robust factor analysis model, namely bilinear factor analysis built on the matrix-variate $t$ distribution ($t$bfa), is proposed in this paper. the novelty is that it is capable to simultaneously extract common factors for both row and column variables of interest on heavy-tailed or contaminated matrix data. two efficient algorithms for maximum likelihood estimation of $t$bfa are developed. closed-form expression for the fisher information matrix to calculate the accuracy of parameter estimates are derived. empirical studies are conducted to understand the proposed $t$bfa model and compare with related competitors. the results demonstrate the superiority and practicality of $t$bfa. importantly, $t$bfa exhibits a significantly higher breakdown point than $t$fa, making it more suitable for matrix data.",,2024-01-04,,"['xuan ma', 'jianhua zhao', 'changchun shang', 'fen jiang', 'philip l. h. yu']"
2401.02325,a robust quantile huber loss with interpretable parameter adjustment in   distributional reinforcement learning,cs.lg stat.ml,"distributional reinforcement learning (rl) estimates return distribution mainly by learning quantile values via minimizing the quantile huber loss function, entailing a threshold parameter often selected heuristically or via hyperparameter search, which may not generalize well and can be suboptimal. this paper introduces a generalized quantile huber loss function derived from wasserstein distance (wd) calculation between gaussian distributions, capturing noise in predicted (current) and target (bellman-updated) quantile values. compared to the classical quantile huber loss, this innovative loss function enhances robustness against outliers. notably, the classical huber loss function can be seen as an approximation of our proposed loss, enabling parameter adjustment by approximating the amount of noise in the data during the learning process. empirical tests on atari games, a common application in distributional rl, and a recent hedging strategy using distributional rl, validate the effectiveness of our proposed loss function and its potential for parameter adjustments in distributional rl. the implementation of the proposed loss function is available here.",,2024-01-04,2024-01-07,"['parvin malekzadeh', 'konstantinos n. plataniotis', 'zissis poulos', 'zeyu wang']"
2401.02349,a survey analyzing generalization in deep reinforcement learning,cs.lg cs.ai stat.ml,"reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. while deep reinforcement learning policies are currently being deployed in many different fields from medical applications to self driving vehicles, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. in this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and generalization capabilities. furthermore, we will formalize and unify the diverse solution approaches to increase generalization, and overcome overfitting in state-action value functions. we believe our study can provide a compact systematic unified analysis for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with improved generalization abilities.",,2024-01-04,,['ezgi korkmaz']
2401.02413,simulation-based inference with quantile regression,stat.ml cs.lg,"we present neural quantile estimation (nqe), a novel simulation-based inference (sbi) method based on conditional quantile regression. nqe autoregressively learns individual one dimensional quantiles for each posterior dimension, conditioned on the data and previous posterior dimensions. posterior samples are obtained by interpolating the predicted quantiles using monotonic cubic hermite spline, with specific treatment for the tail behavior and multi-modal distributions. we introduce an alternative definition for the bayesian credible region using the local cumulative density function (cdf), offering substantially faster evaluation than the traditional highest posterior density region (hpdr). in case of limited simulation budget and/or known model misspecification, a post-processing broadening step can be integrated into nqe to ensure the unbiasedness of the posterior estimation with negligible additional computational cost. we demonstrate that the proposed nqe method achieves state-of-the-art performance on a variety of benchmark problems.",,2024-01-04,,['he jia']
2401.02431,execution time budget assignment for mixed criticality systems,cs.pf cs.dc stat.ml,in this paper we propose to quantify execution time variability of programs using statistical dispersion parameters. we show how the execution time variability can be exploited in mixed criticality real-time systems. we propose a heuristic to compute the execution time budget to be allocated to each low criticality real-time task according to its execution time variability. we show using experiments and simulations that the proposed heuristic reduces the probability of exceeding the allocated budget compared to algorithms which do not take into account the execution time variability parameter.,,2023-11-14,2024-01-09,"['mohamed amine khelassi', 'yasmina abdeddaïm']"
2401.02520,structured matrix learning under arbitrary entrywise dependence and   estimation of markov transition kernel,stat.ml cs.lg math.st stat.th,"the problem of structured matrix estimation has been studied mostly under strong noise dependence assumptions. this paper considers a general framework of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come from any joint distribution with arbitrary dependence across entries. we propose an incoherent-constrained least-square estimator and prove its tightness both in the sense of deterministic lower bound and matching minimax risks under various noise distributions. to attain this, we establish a novel result asserting that the difference between two arbitrary low-rank incoherent matrices must spread energy out across its entries, in other words cannot be too sparse, which sheds light on the structure of incoherent low-rank matrices and may be of independent interest. we then showcase the applications of our framework to several important statistical machine learning problems. in the problem of estimating a structured markov transition kernel, the proposed method achieves the minimax optimality and the result can be extended to estimating the conditional mean operator, a crucial component in reinforcement learning. the applications to multitask regression and structured covariance estimation are also presented. we propose an alternating minimization algorithm to approximately solve the potentially hard optimization problem. numerical results corroborate the effectiveness of our method which typically converges in a few steps.",,2024-01-04,,"['jinhang chai', 'jianqing fan']"
2401.02544,hyperparameter estimation for sparse bayesian learning models,cs.lg stat.co,"sparse bayesian learning (sbl) models are extensively used in signal processing and machine learning for promoting sparsity through hierarchical priors. the hyperparameters in sbl models are crucial for the model's performance, but they are often difficult to estimate due to the non-convexity and the high-dimensionality of the associated objective function. this paper presents a comprehensive framework for hyperparameter estimation in sbl models, encompassing well-known algorithms such as the expectation-maximization (em), mackay, and convex bounding (cb) algorithms. these algorithms are cohesively interpreted within an alternating minimization and linearization (aml) paradigm, distinguished by their unique linearized surrogate functions. additionally, a novel algorithm within the aml framework is introduced, showing enhanced efficiency, especially under low signal noise ratios. this is further improved by a new alternating minimization and quadratic approximation (amq) paradigm, which includes a proximal regularization term. the paper substantiates these advancements with thorough convergence analysis and numerical experiments, demonstrating the algorithm's effectiveness in various noise conditions and signal-to-noise ratios.",,2024-01-04,,"['feng yu', 'lixin shen', 'guohui song']"
2401.02549,quantitative technology forecasting: a review of trend extrapolation   methods,cs.ai stat.ap,"quantitative technology forecasting uses quantitative methods to understand and project technological changes. it is a broad field encompassing many different techniques and has been applied to a vast range of technologies. a widely used approach in this field is trend extrapolation. based on the publications available to us, there has been little or no attempt made to systematically review the empirical evidence on quantitative trend extrapolation techniques. this study attempts to close this gap by conducting a systematic review of technology forecasting literature addressing the application of quantitative trend extrapolation techniques. we identified 25 studies relevant to the objective of this research and classified the techniques used in the studies into different categories, among which growth curves and time series methods were shown to remain popular over the past decade, while newer methods, such as machine learning-based hybrid models, have emerged in recent years. as more effort and evidence are needed to determine if hybrid models are superior to traditional methods, we expect to see a growing trend in the development and application of hybrid models to technology forecasting.",10.1142/s0219877023300021,2024-01-04,,"['peng-hung tsai', 'daniel berleant', 'richard s. segall', 'hyacinthe aboudja', 'venkata jaipal r. batthula', 'sheela duggirala', 'michael howell']"
2401.02592,guaranteed nonconvex factorization approach for tensor train recovery,stat.ml cs.lg eess.sp math.oc,"in this paper, we provide the first convergence guarantee for the factorization approach. specifically, to avoid the scaling ambiguity and to facilitate theoretical analysis, we optimize over the so-called left-orthogonal tt format which enforces orthonormality among most of the factors. to ensure the orthonormal structure, we utilize the riemannian gradient descent (rgd) for optimizing those factors over the stiefel manifold. we first delve into the tt factorization problem and establish the local linear convergence of rgd. notably, the rate of convergence only experiences a linear decline as the tensor order increases. we then study the sensing problem that aims to recover a tt format tensor from linear measurements. assuming the sensing operator satisfies the restricted isometry property (rip), we show that with a proper initialization, which could be obtained through spectral initialization, rgd also converges to the ground-truth tensor at a linear rate. furthermore, we expand our analysis to encompass scenarios involving gaussian noise in the measurements. we prove that rgd can reliably recover the ground truth at a linear rate, with the recovery error exhibiting only polynomial growth in relation to the tensor order. we conduct various experiments to validate our theoretical findings.",,2024-01-04,,"['zhen qin', 'michael b. wakin', 'zhihui zhu']"
2401.02630,model-agnostic interpretation framework in machine learning: a   comparative study in nba sports,cs.lg stat.ap,"the field of machine learning has seen tremendous progress in recent years, with deep learning models delivering exceptional performance across a range of tasks. however, these models often come at the cost of interpretability, as they operate as opaque ""black boxes"" that obscure the rationale behind their decisions. this lack of transparency can limit understanding of the models' underlying principles and impede their deployment in sensitive domains, such as healthcare or finance. to address this challenge, our research team has proposed an innovative framework designed to reconcile the trade-off between model performance and interpretability. our approach is centered around modular operations on high-dimensional data, which enable end-to-end processing while preserving interpretability. by fusing diverse interpretability techniques and modularized data processing, our framework sheds light on the decision-making processes of complex models without compromising their performance. we have extensively tested our framework and validated its superior efficacy in achieving a harmonious balance between computational efficiency and interpretability. our approach addresses a critical need in contemporary machine learning applications by providing unprecedented insights into the inner workings of complex models, fostering trust, transparency, and accountability in their deployment across diverse domains.",,2024-01-04,,['shun liu']
2401.02650,improving sample efficiency of high dimensional bayesian optimization   with mcmc,cs.lg stat.ml,"sequential optimization methods are often confronted with the curse of dimensionality in high-dimensional spaces. current approaches under the gaussian process framework are still burdened by the computational complexity of tracking gaussian process posteriors and need to partition the optimization problem into small regions to ensure exploration or assume an underlying low-dimensional structure. with the idea of transiting the candidate points towards more promising positions, we propose a new method based on markov chain monte carlo to efficiently sample from an approximated posterior. we provide theoretical guarantees of its convergence in the gaussian process thompson sampling setting. we also show experimentally that both the metropolis-hastings and the langevin dynamics version of our algorithm outperform state-of-the-art methods in high-dimensional sequential optimization and reinforcement learning benchmarks.",,2024-01-05,,"['zeji yi', 'yunyue wei', 'chu xin cheng', 'kaibo he', 'yanan sui']"
2401.02708,triplesurv: triplet time-adaptive coordinate loss for survival analysis,cs.lg cs.ai stat.ml,"a core challenge in survival analysis is to model the distribution of censored time-to-event data, where the event of interest may be a death, failure, or occurrence of a specific event. previous studies have showed that ranking and maximum likelihood estimation (mle)loss functions are widely-used for survival analysis. however, ranking loss only focus on the ranking of survival time and does not consider potential effect of samples for exact survival time values. furthermore, the mle is unbounded and easily subject to outliers (e.g., censored data), which may cause poor performance of modeling. to handle the complexities of learning process and exploit valuable survival time values, we propose a time-adaptive coordinate loss function, triplesurv, to achieve adaptive adjustments by introducing the differences in the survival time between sample pairs into the ranking, which can encourage the model to quantitatively rank relative risk of pairs, ultimately enhancing the accuracy of predictions. most importantly, the triplesurv is proficient in quantifying the relative risk between samples by ranking ordering of pairs, and consider the time interval as a trade-off to calibrate the robustness of model over sample distribution. our triplesurv is evaluated on three real-world survival datasets and a public synthetic dataset. the results show that our method outperforms the state-of-the-art methods and exhibits good model performance and robustness on modeling various sophisticated data distributions with different censor rates. our code will be available upon acceptance.",,2024-01-05,,"['liwen zhang', 'lianzhen zhong', 'fan yang', 'di dong', 'hui hui', 'jie tian']"
2401.02735,shared active subspace for multivariate vector-valued functions,stat.me cs.lg stat.ml,"this paper proposes several approaches as baselines to compute a shared active subspace for multivariate vector-valued functions. the goal is to minimize the deviation between the function evaluations on the original space and those on the reconstructed one. this is done either by manipulating the gradients or the symmetric positive (semi-)definite (spd) matrices computed from the gradients of each component function so as to get a single structure common to all component functions. these approaches can be applied to any data irrespective of the underlying distribution unlike the existing vector-valued approach that is constrained to a normal distribution. we test the effectiveness of these methods on five optimization problems. the experiments show that, in general, the spd-level methods are superior to the gradient-level ones, and are close to the vector-valued approach in the case of a normal distribution. interestingly, in most cases it suffices to take the sum of the spd matrices to identify the best shared active subspace.",,2024-01-05,,"['khadija musayeva', 'mickael binois']"
2401.02736,on the numerical reliability of nonsmooth autodiff: a maxpool case study,cs.lg cs.na math.na math.oc stat.ml,"this paper considers the reliability of automatic differentiation (ad) for neural networks involving the nonsmooth maxpool operation. we investigate the behavior of ad across different precision levels (16, 32, 64 bits) and convolutional architectures (lenet, vgg, and resnet) on various datasets (mnist, cifar10, svhn, and imagenet). although ad can be incorrect, recent research has shown that it coincides with the derivative almost everywhere, even in the presence of nonsmooth operations (such as maxpool and relu). on the other hand, in practice, ad operates with floating-point numbers (not real numbers), and there is, therefore, a need to explore subsets on which ad can be numerically incorrect. these subsets include a bifurcation zone (where ad is incorrect over reals) and a compensation zone (where ad is incorrect over floating-point numbers but correct over reals). using sgd for the training process, we study the impact of different choices of the nonsmooth jacobian for the maxpool function on the precision of 16 and 32 bits. these findings suggest that nonsmooth maxpool jacobians with lower norms help maintain stable and efficient test accuracy, whereas those with higher norms can result in instability and decreased performance. we also observe that the influence of maxpool's nonsmooth jacobians on learning can be reduced by using batch normalization, adam-like optimizers, or increasing the precision level.",,2024-01-05,,['ryan boustany']
2401.02739,denoising diffusion variational inference: diffusion models as   expressive variational posteriors,cs.lg q-bio.qm stat.ml,"we propose denoising diffusion variational inference (ddvi), an approximate inference algorithm for latent variable models which relies on diffusion models as flexible variational posteriors. specifically, our method introduces an expressive class of approximate posteriors with auxiliary latent variables that perform diffusion in latent space by reversing a user-specified noising process. we fit these models by optimizing a lower bound on the marginal likelihood inspired by the wake-sleep algorithm. our method is easy to implement (it fits a regularized extension of the elbo), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. it increases the expressivity of flow-based methods via non-invertible deep recurrent architectures and avoids the instability of adversarial methods. we use ddvi on a motivating task in biology -- inferring latent ancestry from human genomes -- and we find that it outperforms strong baselines on the thousand genomes dataset.",,2024-01-05,2024-02-19,"['top piriyakulkij', 'yingheng wang', 'volodymyr kuleshov']"
2401.02890,nonlinear functional regression by functional deep neural network with   kernel embedding,stat.ml cs.lg,"with the rapid development of deep learning in various fields of science and technology, such as speech recognition, image classification, and natural language processing, recently it is also widely applied in the functional data analysis (fda) with some empirical success. however, due to the infinite dimensional input, we need a powerful dimension reduction method for functional learning tasks, especially for the nonlinear functional regression. in this paper, based on the idea of smooth kernel integral transformation, we propose a functional deep neural network with an efficient and fully data-dependent dimension reduction method. the architecture of our functional net consists of a kernel embedding step: an integral transformation with a data-dependent smooth kernel; a projection step: a dimension reduction by projection with eigenfunction basis based on the embedding kernel; and finally an expressive deep relu neural network for the prediction. the utilization of smooth kernel embedding enables our functional net to be discretization invariant, efficient, and robust to noisy observations, capable of utilizing information in both input functions and responses data, and have a low requirement on the number of discrete points for an unimpaired generalization performance. we conduct theoretical analysis including approximation error and generalization error analysis, and numerical simulations to verify these advantages of our functional net.",,2024-01-05,,"['zhongjie shi', 'jun fan', 'linhao song', 'ding-xuan zhou', 'johan a. k. suykens']"
2401.02904,class-wise generalization error: an information-theoretic analysis,cs.lg stat.ml,"existing generalization theories of supervised learning typically take a holistic approach and provide bounds for the expected generalization over the whole data distribution, which implicitly assumes that the model generalizes similarly for all the classes. in practice, however, there are significant variations in generalization performance among different classes, which cannot be captured by the existing generalization bounds. in this work, we tackle this problem by theoretically studying the class-generalization error, which quantifies the generalization performance of each individual class. we derive a novel information-theoretic bound for class-generalization error using the kl divergence, and we further obtain several tighter bounds using the conditional mutual information (cmi), which are significantly easier to estimate in practice. we empirically validate our proposed bounds in different neural networks and show that they accurately capture the complex class-generalization error behavior. moreover, we show that the theoretical tools developed in this paper can be applied in several applications beyond this context.",,2024-01-05,,"['firas laakom', 'yuheng bu', 'moncef gabbouj']"
2401.02930,"dagma-dce: interpretable, non-parametric differentiable causal discovery",cs.lg stat.me stat.ml,"we introduce dagma-dce, an interpretable and model-agnostic scheme for differentiable causal discovery. current non- or over-parametric methods in differentiable causal discovery use opaque proxies of ``independence'' to justify the inclusion or exclusion of a causal relationship. we show theoretically and empirically that these proxies may be arbitrarily different than the actual causal strength. juxtaposed to existing differentiable causal discovery algorithms, \textsc{dagma-dce} uses an interpretable measure of causal strength to define weighted adjacency matrices. in a number of simulated datasets, we show our method achieves state-of-the-art level performance. we additionally show that \textsc{dagma-dce} allows for principled thresholding and sparsity penalties by domain-experts. the code for our method is available open-source at https://github.com/danwaxman/dagma-dce, and can easily be adapted to arbitrary differentiable models.",10.1109/ojsp.2024.3351593,2024-01-05,,"['daniel waxman', 'kurt butler', 'petar m. djuric']"
2401.03058,krylov cubic regularized newton: a subspace second-order method with   dimension-free convergence rate,math.oc cs.lg stat.ml,"second-order optimization methods, such as cubic regularized newton methods, are known for their rapid convergence rates; nevertheless, they become impractical in high-dimensional problems due to their substantial memory requirements and computational costs. one promising approach is to execute second-order updates within a lower-dimensional subspace, giving rise to subspace second-order methods. however, the majority of existing subspace second-order methods randomly select subspaces, consequently resulting in slower convergence rates depending on the problem's dimension $d$. in this paper, we introduce a novel subspace cubic regularized newton method that achieves a dimension-independent global convergence rate of ${o}\left(\frac{1}{mk}+\frac{1}{k^2}\right)$ for solving convex optimization problems. here, $m$ represents the subspace dimension, which can be significantly smaller than $d$. instead of adopting a random subspace, our primary innovation involves performing the cubic regularized newton update within the krylov subspace associated with the hessian and the gradient of the objective function. this result marks the first instance of a dimension-independent convergence rate for a subspace second-order method. furthermore, when specific spectral conditions of the hessian are met, our method recovers the convergence rate of a full-dimensional cubic regularized newton method. numerical experiments show our method converges faster than existing random subspace methods, especially for high-dimensional problems.",,2024-01-05,,"['ruichen jiang', 'parameswaran raman', 'shoham sabach', 'aryan mokhtari', 'mingyi hong', 'volkan cevher']"
2401.03101,forecasting hospital discharges for respiratory conditions in costa rica   using climate and pollution data,stat.ap,"respiratory diseases represent one of the most significant economic burdens on healthcare systems worldwide. the variation in the increasing number of cases depends greatly on climatic seasonal effects, socioeconomic factors, and pollution. therefore, understanding these variations and obtaining precise forecasts allows health authorities to make correct decisions regarding the allocation of limited economic and human resources. this study aims to model and forecast weekly hospitalizations due to respiratory conditions in seven regional hospitals in costa rica using four statistical learning techniques (random forest, xgboost, facebook's prophet forecasting model, and an ensemble method combining the above methods), along with 22 climate change indices and aerosol optical depth as an indicator of pollution. models are trained using data from 2000 to 2018 and are evaluated using data from 2019 as testing data. reliable predictions are obtained for each of the seven regional hospitals",,2024-01-05,,"['shu wei chou-chen', 'luis a. barboza']"
2401.03137,spqr: controlling q-ensemble independence with spiked random model for   reinforcement learning,cs.lg cs.ai stat.ml,"alleviating overestimation bias is a critical challenge for deep reinforcement learning to achieve successful performance on more complex tasks or offline datasets containing out-of-distribution data. in order to overcome overestimation bias, ensemble methods for q-learning have been investigated to exploit the diversity of multiple q-functions. since network initialization has been the predominant approach to promote diversity in q-functions, heuristically designed diversity injection methods have been studied in the literature. however, previous studies have not attempted to approach guaranteed independence over an ensemble from a theoretical perspective. by introducing a novel regularization loss for q-ensemble independence based on random matrix theory, we propose spiked wishart q-ensemble independence regularization (spqr) for reinforcement learning. specifically, we modify the intractable hypothesis testing criterion for the q-ensemble independence into a tractable kl divergence between the spectral distribution of the q-ensemble and the target wigner's semicircle distribution. we implement spqr in several online and offline ensemble q-learning algorithms. in the experiments, spqr outperforms the baseline algorithms in both online and offline rl benchmarks.",,2024-01-06,,"['dohyeok lee', 'seungyub han', 'taehyun cho', 'jungwoo lee']"
2401.03206,a robbins--monro sequence that can exploit prior information for faster   convergence,cs.lg cs.na math.na math.oc math.pr math.st stat.me stat.ml stat.th,"we propose a new method to improve the convergence speed of the robbins-monro algorithm by introducing prior information about the target point into the robbins-monro iteration. we achieve the incorporation of prior information without the need of a -- potentially wrong -- regression model, which would also entail additional constraints. we show that this prior-information robbins-monro sequence is convergent for a wide range of prior distributions, even wrong ones, such as gaussian, weighted sum of gaussians, e.g., in a kernel density estimate, as well as bounded arbitrary distribution functions greater than zero. we furthermore analyse the sequence numerically to understand its performance and the influence of parameters. the results demonstrate that the prior-information robbins-monro sequence converges faster than the standard one, especially during the first steps, which are particularly important for applications where the number of function measurements is limited, and when the noise of observing the underlying function is large. we finally propose a rule to select the parameters of the sequence.",,2024-01-06,,"['siwei liu', 'ke ma', 'stephan m. goetz']"
2401.03228,"reflected schr\""odinger bridge for constrained generative modeling",stat.ml cs.lg,"diffusion models have become the go-to method for large-scale generative models in real-world applications. these applications often involve data distributions confined within bounded domains, typically requiring ad-hoc thresholding techniques for boundary enforcement. reflected diffusion models (lou23) aim to enhance generalizability by generating the data distribution through a backward process governed by reflected brownian motion. however, reflected diffusion models may not easily adapt to diverse domains without the derivation of proper diffeomorphic mappings and do not guarantee optimal transport properties. to overcome these limitations, we introduce the reflected schrodinger bridge algorithm: an entropy-regularized optimal transport approach tailored for generating data within diverse bounded domains. we derive elegant reflected forward-backward stochastic differential equations with neumann and robin boundary conditions, extend divergence-based likelihood training to bounded domains, and explore natural connections to entropic optimal transport for the study of approximate linear convergence - a valuable insight for practical training. our algorithm yields robust generative modeling in diverse domains, and its scalability is demonstrated in real-world constrained generative modeling through standard image benchmarks.",,2024-01-06,,"['wei deng', 'yu chen', 'nicole tianjiao yang', 'hengrong du', 'qi feng', 'ricky t. q. chen']"
2401.03248,neuronal temporal filters as normal mode extractors,q-bio.nc cs.sy eess.sy nlin.cd stat.ml,"to generate actions in the face of physiological delays, the brain must predict the future. here we explore how prediction may lie at the core of brain function by considering a neuron predicting the future of a scalar time series input. assuming that the dynamics of the lag vector (a vector composed of several consecutive elements of the time series) are locally linear, normal mode decomposition decomposes the dynamics into independently evolving (eigen-)modes allowing for straightforward prediction. we propose that a neuron learns the top mode and projects its input onto the associated subspace. under this interpretation, the temporal filter of a neuron corresponds to the left eigenvector of a generalized eigenvalue problem. we mathematically analyze the operation of such an algorithm on noisy observations of synthetic data generated by a linear system. interestingly, the shape of the temporal filter varies with the signal-to-noise ratio (snr): a noisy input yields a monophasic filter and a growing snr leads to multiphasic filters with progressively greater number of phases. such variation in the temporal filter with input snr resembles that observed experimentally in biological neurons.",,2024-01-06,,"['siavash golkar', 'jules berman', 'david lipshutz', 'robert mihai haret', 'tim gollisch', 'dmitri b. chklovskii']"
2401.03251,teles: temporal lexeme similarity score to estimate confidence in   end-to-end asr,eess.as cs.lg cs.sd stat.ml,"confidence estimation of predictions from an end-to-end (e2e) automatic speech recognition (asr) model benefits asr's downstream and upstream tasks. class-probability-based confidence scores do not accurately represent the quality of overconfident asr predictions. an ancillary confidence estimation model (cem) calibrates the predictions. state-of-the-art (sota) solutions use binary target scores for cem training. however, the binary labels do not reveal the granular information of predicted words, such as temporal alignment between reference and hypothesis and whether the predicted word is entirely incorrect or contains spelling errors. addressing this issue, we propose a novel temporal-lexeme similarity (teles) confidence score to train cem. to address the data imbalance of target scores while training cem, we use shrinkage loss to focus on hard-to-learn data points and minimise the impact of easily learned data points. we conduct experiments with asr models trained in three languages, namely hindi, tamil, and kannada, with varying training data sizes. experiments show that teles generalises well across domains. to demonstrate the applicability of the proposed method, we formulate a teles-based acquisition (teles-a) function for sampling uncertainty in active learning. we observe a significant reduction in the word error rate (wer) as compared to sota methods.",,2024-01-06,,"['nagarathna ravi', 'thishyan raj t', 'vipul arora']"
2401.03301,"on sample-efficient offline reinforcement learning: data diversity,   posterior sampling, and beyond",cs.lg cs.ai stat.ml,"we seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (rl). further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. in this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline rl and (ii) using this notion to {unify} three distinct classes of offline rl algorithms based on version spaces (vs), regularized optimization (ro), and posterior sampling (ps). we establish that vs-based, ro-based, and ps-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. this result is surprising, given that the prior work suggested an unfavorable sample complexity of the ro-based algorithm compared to the vs-based algorithm, whereas posterior sampling is rarely considered in offline rl due to its explorative nature. notably, our proposed model-free ps-based algorithm for offline rl is {novel}, with sub-optimality bounds that are {frequentist} (i.e., worst-case) in nature.",,2024-01-06,2024-02-06,"['thanh nguyen-tang', 'raman arora']"
2401.03302,realism in action: anomaly-aware diagnosis of brain tumors from medical   images using yolov8 and deit,eess.iv cs.ai cs.cv cs.lg stat.ml,"in the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. this study addresses the issue by leveraging deep learning (dl) techniques to detect and classify brain tumors in challenging situations. the curated data set from the national brain mapping lab (nbml) comprises 81 patients, including 30 tumor cases and 51 normal cases. the detection and classification pipelines are separated into two consecutive tasks. the detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 normal per 1 tumor) to comply with real world scenarios. next, in addition to common evaluation metrics for the testing, we employed a novel performance evaluation method called patient to patient (ptp), focusing on the realistic evaluation of the model. in the detection phase, we fine-tuned a yolov8n detection model to detect the tumor region. subsequent testing and evaluation yielded competitive performance both in common evaluation metrics and ptp metrics. furthermore, using the data efficient image transformer (deit) module, we distilled a vision transformer (vit) model from a fine-tuned resnet152 as a teacher in the classification phase. this approach demonstrates promising strides in reliable tumor detection and classification, offering potential advancements in tumor diagnosis for real-world medical imaging scenarios.",,2024-01-06,2024-01-10,"['seyed mohammad hossein hashemi', 'leila safari', 'amirhossein dadashzade taromi']"
2401.03341,weakly augmented variational autoencoder in time series anomaly   detection,cs.lg stat.ml,"due to their unsupervised training and uncertainty estimation, deep variational autoencoders (vaes) have become powerful tools for reconstruction-based time series anomaly detection (tsad). existing vae-based tsad methods, either statistical or deep, tune meta-priors to estimate the likelihood probability for effectively capturing spatiotemporal dependencies in the data. however, these methods confront the challenge of inherent data scarcity, which is often the case in anomaly detection tasks. such scarcity easily leads to latent holes, discontinuous regions in latent space, resulting in non-robust reconstructions on these discontinuous spaces. we propose a novel generative framework that combines vaes with self-supervised learning (ssl) to address this issue.",,2024-01-06,,"['zhangkai wu', 'longbing cao', 'qi zhang', 'junxian zhou', 'hui chen']"
2401.03350,accurate and scalable estimation of epistemic uncertainty for graph   neural networks,cs.lg stat.ml,"while graph neural networks (gnns) are widely used for node and graph representation learning tasks, the reliability of gnn uncertainty estimates under distribution shifts remains relatively under-explored. indeed, while post-hoc calibration strategies can be used to improve in-distribution calibration, they need not also improve calibration under distribution shift. however, techniques which produce gnns with better intrinsic uncertainty estimates are particularly valuable, as they can always be combined with post-hoc strategies later. therefore, in this work, we propose g-$\delta$uq, a novel training framework designed to improve intrinsic gnn uncertainty estimates. our framework adapts the principle of stochastic data centering to graph data through novel graph anchoring strategies, and is able to support partially stochastic gnns. while, the prevalent wisdom is that fully stochastic networks are necessary to obtain reliable estimates, we find that the functional diversity induced by our anchoring strategies when sampling hypotheses renders this unnecessary and allows us to support g-$\delta$uq on pretrained models. indeed, through extensive evaluation under covariate, concept and graph size shifts, we show that g-$\delta$uq leads to better calibrated gnns for node and graph classification. further, it also improves performance on the uncertainty-based tasks of out-of-distribution detection and generalization gap estimation. overall, our work provides insights into uncertainty estimation for gnns, and demonstrates the utility of g-$\delta$uq in obtaining reliable estimates.",,2024-01-06,,"['puja trivedi', 'mark heimann', 'rushil anirudh', 'danai koutra', 'jayaraman j. thiagarajan']"
2401.03397,predicting the skies: a novel model for flight-level passenger traffic   forecasting,cs.lg cs.ai stat.ap,"accurate prediction of flight-level passenger traffic is of paramount importance in airline operations, influencing key decisions from pricing to route optimization. this study introduces a novel, multimodal deep learning approach to the challenge of predicting flight-level passenger traffic, yielding substantial accuracy improvements compared to traditional models. leveraging an extensive dataset from american airlines, our model ingests historical traffic data, fare closure information, and seasonality attributes specific to each flight. our proposed neural network integrates the strengths of recurrent neural networks (rnn) and convolutional neural networks (cnn), exploiting the temporal patterns and spatial relationships within the data to enhance prediction performance. crucial to the success of our model is a comprehensive data processing strategy. we construct 3d tensors to represent data, apply careful masking strategies to mirror real-world dynamics, and employ data augmentation techniques to enrich the diversity of our training set. the efficacy of our approach is borne out in the results: our model demonstrates an approximate 33\% improvement in mean squared error (mse) compared to traditional benchmarks. this study, therefore, highlights the significant potential of deep learning techniques and meticulous data processing in advancing the field of flight traffic prediction.",,2024-01-07,2024-01-09,"['sina ehsani', 'elina sergeeva', 'wendy murdy', 'benjamin fox']"
2401.03482,uncertainty quantification on clinical trial outcome prediction,cs.lg stat.ml,"the importance of uncertainty quantification is increasingly recognized in the diverse field of machine learning. accurately assessing model prediction uncertainty can help provide deeper understanding and confidence for researchers and practitioners. this is especially critical in medical diagnosis and drug discovery areas, where reliable predictions directly impact research quality and patient health.   in this paper, we proposed incorporating uncertainty quantification into clinical trial outcome predictions. our main goal is to enhance the model's ability to discern nuanced differences, thereby significantly improving its overall performance.   we have adopted a selective classification approach to fulfill our objective, integrating it seamlessly with the hierarchical interaction network (hint), which is at the forefront of clinical trial prediction modeling. selective classification, encompassing a spectrum of methods for uncertainty quantification, empowers the model to withhold decision-making in the face of samples marked by ambiguity or low confidence, thereby amplifying the accuracy of predictions for the instances it chooses to classify. a series of comprehensive experiments demonstrate that incorporating selective classification into clinical trial predictions markedly enhances the model's performance, as evidenced by significant upticks in pivotal metrics such as pr-auc, f1, roc-auc, and overall accuracy.   specifically, the proposed method achieved 32.37\%, 21.43\%, and 13.27\% relative improvement on pr-auc over the base model (hint) in phase i, ii, and iii trial outcome prediction, respectively. when predicting phase iii, our method reaches 0.9022 pr-auc scores.   these findings illustrate the robustness and prospective utility of this strategy within the area of clinical trial predictions, potentially setting a new benchmark in the field.",,2024-01-07,,"['tianyi chen', 'nan hao', 'yingzhou lu', 'capucine van rechem']"
2401.03756,adaptive experimental design for policy learning,cs.lg cs.ai econ.em stat.me stat.ml,"evidence-based targeting has been a topic of growing interest among the practitioners of policy and business. formulating decision-maker's policy learning as a fixed-budget best arm identification (bai) problem with contextual information, we study an optimal adaptive experimental design for policy learning with multiple treatment arms. in the sampling stage, the planner assigns treatment arms adaptively over sequentially arriving experimental units upon observing their contextual information (covariates). after the experiment, the planner recommends an individualized assignment rule to the population. setting the worst-case expected regret as the performance criterion of adaptive sampling and recommended policies, we derive its asymptotic lower bounds, and propose a strategy, adaptive sampling-policy learning strategy (plas), whose leading factor of the regret upper bound aligns with the lower bound as the size of experimental units increases.",,2024-01-08,2024-02-08,"['masahiro kato', 'kyohei okumura', 'takuya ishihara', 'toru kitagawa']"
2401.03820,optimal differentially private pca and estimation for spiked covariance   matrices,math.st cs.it math.it stat.me stat.ml stat.th,"estimating a covariance matrix and its associated principal components is a fundamental problem in contemporary statistics. while optimal estimation procedures have been developed with well-understood properties, the increasing demand for privacy preservation introduces new complexities to this classical problem. in this paper, we study optimal differentially private principal component analysis (pca) and covariance estimation within the spiked covariance model.   we precisely characterize the sensitivity of eigenvalues and eigenvectors under this model and establish the minimax rates of convergence for estimating both the principal components and covariance matrix. these rates hold up to logarithmic factors and encompass general schatten norms, including spectral norm, frobenius norm, and nuclear norm as special cases.   we introduce computationally efficient differentially private estimators and prove their minimax optimality, up to logarithmic factors. additionally, matching minimax lower bounds are established. notably, in comparison with existing literature, our results accommodate a diverging rank, necessitate no eigengap condition between distinct principal components, and remain valid even if the sample size is much smaller than the dimension.",,2024-01-08,,"['t. tony cai', 'dong xia', 'mengyue zha']"
2401.03824,a topological description of loss surfaces based on betti numbers,cs.lg stat.ml,"in the context of deep learning models, attention has recently been paid to studying the surface of the loss function in order to better understand training with methods based on gradient descent. this search for an appropriate description, both analytical and topological, has led to numerous efforts to identify spurious minima and characterize gradient dynamics. our work aims to contribute to this field by providing a topological measure to evaluate loss complexity in the case of multilayer neural networks. we compare deep and shallow architectures with common sigmoidal activation functions by deriving upper and lower bounds on the complexity of their loss function and revealing how that complexity is influenced by the number of hidden units, training models, and the activation function used. additionally, we found that certain variations in the loss function or model architecture, such as adding an $\ell_2$ regularization term or implementing skip connections in a feedforward network, do not affect loss topology in specific cases.",,2024-01-08,,"['maria sofia bucarelli', ""giuseppe alessio d'inverno"", 'monica bianchini', 'franco scarselli', 'fabrizio silvestri']"
2401.03892,sampling in unit time with kernel fisher-rao flow,stat.co cs.lg stat.ml,"we introduce a new mean-field ode and corresponding interacting particle systems (ips) for sampling from an unnormalized target density. the ips are gradient-free, available in closed form, and only require the ability to sample from a reference density and compute the (unnormalized) target-to-reference density ratio. the mean-field ode is obtained by solving a poisson equation for a velocity field that transports samples along the geometric mixture of the two densities, which is the path of a particular fisher-rao gradient flow. we employ a rkhs ansatz for the velocity field, which makes the poisson equation tractable and enables discretization of the resulting mean-field ode over finite samples. the mean-field ode can be additionally be derived from a discrete-time perspective as the limit of successive linearizations of the monge-amp\`ere equations within a framework known as sample-driven optimal transport. we introduce a stochastic variant of our approach and demonstrate empirically that our ips can produce high-quality samples from varied target distributions, outperforming comparable gradient-free particle systems and competitive with gradient-based alternatives.",,2024-01-08,2024-02-06,"['aimee maurais', 'youssef marzouk']"
2401.03893,finite-time decoupled convergence in nonlinear two-time-scale stochastic   approximation,math.oc stat.ml,"in two-time-scale stochastic approximation (sa), two iterates are updated at varying speeds using different step sizes, with each update influencing the other. previous studies in linear two-time-scale sa have found that the convergence rates of the mean-square errors for these updates are dependent solely on their respective step sizes, leading to what is referred to as decoupled convergence. however, the possibility of achieving this decoupled convergence in nonlinear sa remains less understood. our research explores the potential for finite-time decoupled convergence in nonlinear two-time-scale sa. we find that under a weaker lipschitz condition, traditional analyses are insufficient for achieving decoupled convergence. this finding is further numerically supported by a counterexample. but by introducing an additional condition of nested local linearity, we show that decoupled convergence is still feasible, contingent on the appropriate choice of step sizes associated with smoothness parameters. our analysis depends on a refined characterization of the matrix cross term between the two iterates and utilizes fourth-order moments to control higher-order approximation errors induced by the local linearity assumption.",,2024-01-08,,"['yuze han', 'xiang li', 'zhihua zhang']"
2401.03921,design a metric robust to complicated high dimensional noise for   efficient manifold denoising,stat.ml cs.lg stat.ap,"in this manuscript, we propose an efficient manifold denoiser based on landmark diffusion and optimal shrinkage under the complicated high dimensional noise and compact manifold setup. it is flexible to handle several setups, including the high ambient space dimension with a manifold embedding that occupies a subspace of high or low dimensions, and the noise could be colored and dependent. a systematic comparison with other existing algorithms on both simulated and real datasets is provided. this manuscript is mainly algorithmic and we report several existing tools and numerical results. theoretical guarantees and more comparisons will be reported in the official paper of this manuscript.",,2024-01-08,,['hau-tieng wu']
2401.03923,a non-asymptotic distributional theory of approximate message passing   for sparse and robust regression,math.st cs.it cs.lg eess.sp math.it stat.ml stat.th,"characterizing the distribution of high-dimensional statistical estimators is a challenging task, due to the breakdown of classical asymptotic theory in high dimension. this paper makes progress towards this by developing non-asymptotic distributional characterizations for approximate message passing (amp) -- a family of iterative algorithms that prove effective as both fast estimators and powerful theoretical machinery -- for both sparse and robust regression. prior amp theory, which focused on high-dimensional asymptotics for the most part, failed to describe the behavior of amp when the number of iterations exceeds $o\big({\log n}/{\log \log n}\big)$ (with $n$ the sample size). we establish the first finite-sample non-asymptotic distributional theory of amp for both sparse and robust regression that accommodates a polynomial number of iterations. our results derive approximate accuracy of gaussian approximation of the amp iterates, which improves upon all prior results and implies enhanced distributional characterizations for both optimally tuned lasso and robust m-estimator.",,2024-01-08,,"['gen li', 'yuting wei']"
2401.04013,weak correlations as the underlying principle for linearization of   gradient-based learning systems,cs.lg cond-mat.stat-mech hep-th math.pr stat.ml,"deep learning models, such as wide neural networks, can be conceptualized as nonlinear dynamical physical systems characterized by a multitude of interacting degrees of freedom. such systems in the infinite limit, tend to exhibit simplified dynamics. this paper delves into gradient descent-based learning algorithms, that display a linear structure in their parameter dynamics, reminiscent of the neural tangent kernel. we establish this apparent linearity arises due to weak correlations between the first and higher-order derivatives of the hypothesis function, concerning the parameters, taken around their initial values. this insight suggests that these weak correlations could be the underlying reason for the observed linearization in such systems. as a case in point, we showcase this weak correlations structure within neural networks in the large width limit. exploiting the relationship between linearity and weak correlations, we derive a bound on deviations from linearity observed during the training trajectory of stochastic gradient descent. to facilitate our proof, we introduce a novel method to characterise the asymptotic behavior of random tensors.",,2024-01-08,,"['ori shem-ur', 'yaron oz']"
2401.04082,improved motif-scaffolding with se(3) flow matching,q-bio.qm cs.lg stat.ml,"protein design often begins with knowledge of a desired function from a motif which motif-scaffolding aims to construct a functional protein around. recently, generative models have achieved breakthrough success in designing scaffolds for a diverse range of motifs. however, the generated scaffolds tend to lack structural diversity, which can hinder success in wet-lab validation. in this work, we extend frameflow, an se(3) flow matching model for protein backbone generation, to perform motif-scaffolding with two complementary approaches. the first is motif amortization, in which frameflow is trained with the motif as input using a data augmentation strategy. the second is motif guidance, which performs scaffolding using an estimate of the conditional score from frameflow, and requires no additional training. both approaches achieve an equivalent or higher success rate than previous state-of-the-art methods, with 2.5 times more structurally diverse scaffolds. code: https://github.com/ microsoft/frame-flow.",,2024-01-08,,"['jason yim', 'andrew campbell', 'emile mathieu', 'andrew y. k. foong', 'michael gastegger', 'josé jiménez-luna', 'sarah lewis', 'victor garcia satorras', 'bastiaan s. veeling', 'frank noé', 'regina barzilay', 'tommi s. jaakkola']"
2401.04265,estimation of subsidiary performance metrics under optimal policies,math.st stat.th,"in policy learning, the goal is typically to optimize a primary performance metric, but other subsidiary metrics often also warrant attention. this paper presents two strategies for evaluating these subsidiary metrics under a policy that is optimal for the primary one. the first relies on a novel margin condition that facilitates wald-type inference. under this and other regularity conditions, we show that the one-step corrected estimator is efficient. despite the utility of this margin condition, it places strong restrictions on how the subsidiary metric behaves for nearly optimal policies, which may not hold in practice. we therefore introduce alternative, two-stage strategies that do not require a margin condition. the first stage constructs a set of candidate policies and the second builds a uniform confidence interval over this set. we provide numerical simulations to evaluate the performance of these methods in different scenarios.",,2024-01-08,,"['zhaoqi li', 'houssam nassif', 'alex luedtke']"
2401.04280,predicting the structure of dynamic graphs,cs.lg cs.si stat.ml,"dynamic graph embeddings, inductive and incremental learning facilitate predictive tasks such as node classification and link prediction. however, predicting the structure of a graph at a future time step from a time series of graphs, allowing for new nodes has not gained much attention. in this paper, we present such an approach. we use time series methods to predict the node degree at future time points and combine it with flux balance analysis -- a linear programming method used in biochemistry -- to obtain the structure of future graphs. furthermore, we explore the predictive graph distribution for different parameter values. we evaluate this method using synthetic and real datasets and demonstrate its utility and applicability.",,2024-01-08,,['sevvandi kandanaarachchi']
2401.04286,universal consistency of wide and deep relu neural networks and minimax   optimal convergence rates for kolmogorov-donoho optimal function classes,stat.ml cs.lg,"in this paper, we prove the universal consistency of wide and deep relu neural network classifiers trained on the logistic loss. we also give sufficient conditions for a class of probability measures for which classifiers based on neural networks achieve minimax optimal rates of convergence. the result applies to a wide range of known function classes. in particular, while most previous works impose explicit smoothness assumptions on the regression function, our framework encompasses more general settings. the proposed neural networks are either the minimizers of the logistic loss or the $0$-$1$ loss. in the former case, they are interpolating classifiers that exhibit a benign overfitting behavior.",,2024-01-08,2024-01-30,"['hyunouk ko', 'xiaoming huo']"
2401.04372,stable generative modeling using diffusion maps,stat.ml cs.lg cs.na math.na stat.co,"we consider the problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. such settings have recently drawn considerable interest in the context of generative modelling. in this paper, we propose a generative model combining diffusion maps and langevin dynamics. diffusion maps are used to approximate the drift term from the available training samples, which is then implemented in a discrete-time langevin sampler to generate new samples. by setting the kernel bandwidth to match the time step size used in the unadjusted langevin algorithm, our method effectively circumvents any stability issues typically associated with time-stepping stiff stochastic differential equations. more precisely, we introduce a novel split-step scheme, ensuring that the generated samples remain within the convex hull of the training samples. our framework can be naturally extended to generate conditional samples. we demonstrate the performance of our proposed scheme through experiments on synthetic datasets with increasing dimensions and on a stochastic subgrid-scale parametrization conditional sampling problem.",,2024-01-09,,"['georg gottwald', 'fengyi li', 'youssef marzouk', 'sebastian reich']"
2401.04535,"semi-supervised deep sobolev regression: estimation, variable selection   and beyond",stat.ml cs.lg,"we propose sdore, a semi-supervised deep sobolev regressor, for the nonparametric estimation of the underlying regression function and its gradient. sdore employs deep neural networks to minimize empirical risk with gradient norm regularization, allowing computation of the gradient norm on unlabeled data. we conduct a comprehensive analysis of the convergence rates of sdore and establish a minimax optimal rate for the regression function. crucially, we also derive a convergence rate for the associated plug-in gradient estimator, even in the presence of significant domain shift. these theoretical findings offer valuable prior guidance for selecting regularization parameters and determining the size of the neural network, while showcasing the provable advantage of leveraging unlabeled data in semi-supervised learning. to the best of our knowledge, sdore is the first provable neural network-based approach that simultaneously estimates the regression function and its gradient, with diverse applications including nonparametric variable selection and inverse problems. the effectiveness of sdore is validated through an extensive range of numerical simulations and real data analysis.",,2024-01-09,,"['zhao ding', 'chenguang duan', 'yuling jiao', 'jerry zhijian yang']"
2401.04553,linear recursive feature machines provably recover low-rank matrices,stat.ml cs.lg,"a fundamental problem in machine learning is to understand how neural networks make accurate predictions, while seemingly bypassing the curse of dimensionality. a possible explanation is that common training algorithms for neural networks implicitly perform dimensionality reduction - a process called feature learning. recent work posited that the effects of feature learning can be elicited from a classical statistical estimator called the average gradient outer product (agop). the authors proposed recursive feature machines (rfms) as an algorithm that explicitly performs feature learning by alternating between (1) reweighting the feature vectors by the agop and (2) learning the prediction function in the transformed space. in this work, we develop the first theoretical guarantees for how rfm performs dimensionality reduction by focusing on the class of overparametrized problems arising in sparse linear regression and low-rank matrix recovery. specifically, we show that rfm restricted to linear models (lin-rfm) generalizes the well-studied iteratively reweighted least squares (irls) algorithm. our results shed light on the connection between feature learning in neural networks and classical sparse recovery algorithms. in addition, we provide an implementation of lin-rfm that scales to matrices with millions of missing entries. our implementation is faster than the standard irls algorithm as it is svd-free. it also outperforms deep linear networks for sparse linear regression and low-rank matrix completion.",,2024-01-09,,"['adityanarayanan radhakrishnan', 'mikhail belkin', 'dmitriy drusvyatskiy']"
2401.04682,mixture of multilayer stochastic block models for multiview clustering,cs.lg math.st stat.ml stat.th,"in this work, we propose an original method for aggregating multiple clustering coming from different sources of information. each partition is encoded by a co-membership matrix between observations. our approach uses a mixture of multilayer stochastic block models (sbm) to group co-membership matrices with similar information into components and to partition observations into different clusters, taking into account their specificities within the components. the identifiability of the model parameters is established and a variational bayesian em algorithm is proposed for the estimation of these parameters. the bayesian framework allows for selecting an optimal number of clusters and components. the proposed approach is compared using synthetic data with consensus clustering and tensor-based algorithms for community detection in large-scale complex networks. finally, the method is utilized to analyze global food trading networks, leading to structures of interest.",,2024-01-09,,"['kylliann de santiago', 'marie szafranski', 'christophe ambroise']"
2401.04691,ai-based mapping of the conservation status of orchid assemblages at   global scale,cs.lg stat.ap,"although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk. we hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales. we introduce a new deep species distribution model trained on 1m occurrences of 14k orchid species to predict their assemblages at global scale and at kilometre resolution. we propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage. we show and analyze the variation of these indicators at world scale and in relation to currently protected areas in sumatra island. global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales. the highest level of threat is found at madagascar and the neighbouring islands. in sumatra, we found good correspondence of protected areas with our indicators, but supplementing current iucn assessments with status predictions results in alarming levels of species threat across the island. recent advances in deep learning enable reliable mapping of the conservation status of species assemblages on a global scale. as an umbrella taxon, orchid family provides a reference for identifying vulnerable ecosystems worldwide, and prioritising conservation actions both at international and local levels.",,2024-01-09,,"['joaquim estopinan', 'maximilien servajean', 'pierre bonnet', 'alexis joly', 'françois munoz']"
2401.04778,generative neural networks for characteristic functions,stat.ml cs.lg stat.me,"in this work, we provide a simulation algorithm to simulate from a (multivariate) characteristic function, which is only accessible in a black-box format. we construct a generative neural network, whose loss function exploits a specific representation of the maximum-mean-discrepancy metric to directly incorporate the targeted characteristic function. the construction is universal in the sense that it is independent of the dimension and that it does not require any assumptions on the given characteristic function. furthermore, finite sample guarantees on the approximation quality in terms of the maximum-mean discrepancy metric are derived. the method is illustrated in a short simulation study.",,2024-01-09,,['florian brück']
2401.04847,on the correctness of the generalized isotonic recursive partitioning   algorithm,stat.ml cs.lg math.oc,"this paper presents an in-depth analysis of the generalized isotonic recursive partitioning (girp) algorithm for fitting isotonic models under separable convex losses, proposed by luss and rosset [j. comput. graph. statist., 23 (2014), pp. 192--201] for differentiable losses and extended by painsky and rosset [ieee trans. pattern anal. mach. intell., 38 (2016), pp. 308-321] for nondifferentiable losses. the girp algorithm poseses an attractive feature that in each step of the algorithm, the intermediate solution satisfies the isotonicity constraint. the paper begins with an example showing that the girp algorithm as described in the literature may fail to produce an isotonic model, suggesting that the existence and uniqueness of the solution to the isotonic regression problem must be carefully addressed. it proceeds with showing that, among possibly many solutions, there indeed exists a solution that can be found by recursive binary partitioning of the set of observed data. a small modification of the girp algorithm suffices to obtain a correct solution and preserve the desired property that all the intermediate solutions are isotonic. this proposed modification includes a proper choice of intermediate solutions and a simplification of the partitioning step from ternary to binary.",,2024-01-09,2024-01-10,"['joong-ho won', 'jihan jung']"
2401.04856,a good score does not lead to a good generative model,cs.lg stat.ml,"score-based generative models (sgms) is one leading method in generative modeling, renowned for their ability to generate high-quality samples from complex, high-dimensional data distributions. the method enjoys empirical success and is supported by rigorous theoretical convergence properties. in particular, it has been shown that sgms can generate samples from a distribution that is close to the ground-truth if the underlying score function is learned well, suggesting the success of sgm as a generative model. we provide a counter-example in this paper. through the sample complexity argument, we provide one specific setting where the score function is learned well. yet, sgms in this setting can only output samples that are gaussian blurring of training data points, mimicking the effects of kernel density estimation. the finding resonates a series of recent finding that reveal that sgms can demonstrate strong memorization effect and fail to generate.",,2024-01-09,2024-01-27,"['sixu li', 'shi chen', 'qin li']"
2401.04874,feature network methods in machine learning and applications,stat.ml cs.lg,"a machine learning (ml) feature network is a graph that connects ml features in learning tasks based on their similarity. this network representation allows us to view feature vectors as functions on the network. by leveraging function operations from fourier analysis and from functional analysis, one can easily generate new and novel features, making use of the graph structure imposed on the feature vectors. such network structures have previously been studied implicitly in image processing and computational biology. we thus describe feature networks as graph structures imposed on feature vectors, and provide applications in machine learning. one application involves graph-based generalizations of convolutional neural networks, involving structured deep learning with hierarchical representations of features that have varying depth or complexity. this extends also to learning algorithms that are able to generate useful new multilevel features. additionally, we discuss the use of feature networks to engineer new features, which can enhance the expressiveness of the model. we give a specific example of a deep tree-structured feature network, where hierarchical connections are formed through feature clustering and feed-forward learning. this results in low learning complexity and computational efficiency. unlike ""standard"" neural features which are limited to modulated (thresholded) linear combinations of adjacent ones, feature networks offer more general feedforward dependencies among features. for example, radial basis functions or graph structure-based dependencies between features can be utilized.",,2024-01-09,,"['xinying mu', 'mark kon']"
2401.04890,"nonparametric partial disentanglement via mechanism sparsity: sparse   actions, interventions and sparse temporal dependencies",stat.ml cs.lg,"this work introduces a novel principle for disentanglement we call mechanism sparsity regularization, which applies when the latent factors of interest depend sparsely on observed auxiliary variables and/or past latent factors. we propose a representation learning method that induces disentanglement by simultaneously learning the latent factors and the sparse causal graphical model that explains them. we develop a nonparametric identifiability theory that formalizes this principle and shows that the latent factors can be recovered by regularizing the learned causal graph to be sparse. more precisely, we show identifiablity up to a novel equivalence relation we call ""consistency"", which allows some latent factors to remain entangled (hence the term partial disentanglement). to describe the structure of this entanglement, we introduce the notions of entanglement graphs and graph preserving functions. we further provide a graphical criterion which guarantees complete disentanglement, that is identifiability up to permutations and element-wise transformations. we demonstrate the scope of the mechanism sparsity principle as well as the assumptions it relies on with several worked out examples. for instance, the framework shows how one can leverage multi-node interventions with unknown targets on the latent factors to disentangle them. we further draw connections between our nonparametric results and the now popular exponential family assumption. lastly, we propose an estimation procedure based on variational autoencoders and a sparsity constraint and demonstrate it on various synthetic datasets. this work is meant to be a significantly extended version of lachapelle et al. (2022).",,2024-01-09,,"['sébastien lachapelle', 'pau rodríguez lópez', 'yash sharma', 'katie everett', 'rémi le priol', 'alexandre lacoste', 'simon lacoste-julien']"
2401.04900,spt: spectral transformer for red giant stars age and mass estimation,astro-ph.sr astro-ph.im cs.lg stat.ml,"the age and mass of red giants are essential for understanding the structure and evolution of the milky way. traditional isochrone methods for these estimations are inherently limited due to overlapping isochrones in the hertzsprung-russell diagram, while asteroseismology, though more precise, requires high-precision, long-term observations. in response to these challenges, we developed a novel framework, spectral transformer (spt), to predict the age and mass of red giants aligned with asteroseismology from their spectra. a key component of spt, the multi-head hadamard self-attention mechanism, designed specifically for spectra, can capture complex relationships across different wavelength. further, we introduced a mahalanobis distance-based loss function to address scale imbalance and interaction mode loss, and incorporated monte carlo dropout for quantitative analysis of prediction uncertainty.trained and tested on 3,880 red giant spectra from lamost, the spt achieved remarkable age and mass estimations with average percentage errors of 17.64% and 6.61%, respectively, and provided uncertainties for each corresponding prediction. the results significantly outperform those of traditional machine learning algorithms and demonstrate a high level of consistency with asteroseismology methods and isochrone fitting techniques. in the future, our work will leverage datasets from the chinese space station telescope and the large synoptic survey telescope to enhance the precision of the model and broaden its applicability in the field of astronomy and astrophysics.",,2024-01-09,,"['mengmeng zhang', 'fan wu', 'yude bu', 'shanshan li', 'zhenping yi', 'meng liu', 'xiaoming kong']"
2401.04933,rethinking test-time likelihood: the likelihood path principle and its   application to ood detection,cs.lg stat.ml,"while likelihood is attractive in theory, its estimates by deep generative models (dgms) are often broken in practice, and perform poorly for out of distribution (ood) detection. various recent works started to consider alternative scores and achieved better performances. however, such recipes do not come with provable guarantees, nor is it clear that their choices extract sufficient information.   we attempt to change this by conducting a case study on variational autoencoders (vaes). first, we introduce the likelihood path (lpath) principle, generalizing the likelihood principle. this narrows the search for informative summary statistics down to the minimal sufficient statistics of vaes' conditional likelihoods. second, introducing new theoretic tools such as nearly essential support, essential distance and co-lipschitzness, we obtain non-asymptotic provable ood detection guarantees for certain distillation of the minimal sufficient statistics. the corresponding lpath algorithm demonstrates sota performances, even using simple and small vaes with poor likelihood estimates. to our best knowledge, this is the first provable unsupervised ood method that delivers excellent empirical results, better than any other vaes based techniques. we use the same model as \cite{xiao2020likelihood}, open sourced from: https://github.com/xavierxiao/likelihood-regret",,2024-01-10,,"['sicong huang', 'jiawei he', 'kry yik chau lui']"
2401.05193,experiment planning with function approximation,cs.lg cs.ai stat.ml,"we study the problem of experiment planning with function approximation in contextual bandit problems. in settings where there is a significant overhead to deploying adaptive algorithms -- for example, when the execution of the data collection policies is required to be distributed, or a human in the loop is needed to implement these policies -- producing in advance a set of policies for data collection is paramount. we study the setting where a large dataset of contexts but not rewards is available and may be used by the learner to design an effective data collection strategy. although when rewards are linear this problem has been well studied, results are still missing for more complex reward models. in this work we propose two experiment planning strategies compatible with function approximation. the first is an eluder planning and sampling procedure that can recover optimality guarantees depending on the eluder dimension of the reward function class. for the second, we show that a uniform sampler achieves competitive optimality rates in the setting where the number of actions is small. we finalize our results introducing a statistical gap fleshing out the fundamental differences between planning and adaptive learning and provide results for planning with model selection.",,2024-01-10,,"['aldo pacchiano', 'jonathan n. lee', 'emma brunskill']"
2401.05233,"taming ""data-hungry"" reinforcement learning? stability in continuous   state-action spaces",cs.lg cs.it cs.sy eess.sy math.it math.oc stat.ml,"we introduce a novel framework for analyzing reinforcement learning (rl) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings. our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the bellman operator and occupation measures. we argue that these properties are satisfied in many continuous state-action markov decision processes, and demonstrate how they arise naturally when using linear function approximation methods. our analysis offers fresh perspectives on the roles of pessimism and optimism in off-line and on-line rl, and highlights the connection between off-line rl and transfer learning.",,2024-01-10,,"['yaqi duan', 'martin j. wainwright']"
2401.05244,reliability analysis of complex systems using subset simulations with   hamiltonian neural networks,stat.ml cs.lg stat.ap stat.co,"we present a new subset simulation approach using hamiltonian neural network-based monte carlo sampling for reliability analysis. the proposed strategy combines the superior sampling of the hamiltonian monte carlo method with computationally efficient gradient evaluations using hamiltonian neural networks. this combination is especially advantageous because the neural network architecture conserves the hamiltonian, which defines the acceptance criteria of the hamiltonian monte carlo sampler. hence, this strategy achieves high acceptance rates at low computational cost. our approach estimates small failure probabilities using subset simulations. however, in low-probability sample regions, the gradient evaluation is particularly challenging. the remarkable accuracy of the proposed strategy is demonstrated on different reliability problems, and its efficiency is compared to the traditional hamiltonian monte carlo method. we note that this approach can reach its limitations for gradient estimations in low-probability regions of complex and high-dimensional distributions. thus, we propose techniques to improve gradient prediction in these particular situations and enable accurate estimations of the probability of failure. the highlight of this study is the reliability analysis of a system whose parameter distributions must be inferred with bayesian inference problems. in such a case, the hamiltonian monte carlo method requires a full model evaluation for each gradient evaluation and, therefore, comes at a very high cost. however, using hamiltonian neural networks in this framework replaces the expensive model evaluation, resulting in tremendous improvements in computational efficiency.",,2024-01-10,,"['denny thaler', 'somayajulu l. n. dhulipala', 'franz bamer', 'bernd markert', 'michael d. shields']"
2401.05330,hierarchical causal models,stat.me stat.ml,"scientists often want to learn about cause and effect from hierarchical data, collected from subunits nested inside units. consider students in schools, cells in patients, or cities in states. in such settings, unit-level variables (e.g. each school's budget) may affect subunit-level variables (e.g. the test scores of each student in each school) and vice versa. to address causal questions with hierarchical data, we propose hierarchical causal models, which extend structural causal models and causal graphical models by adding inner plates. we develop a general graphical identification technique for hierarchical causal models that extends do-calculus. we find many situations in which hierarchical data can enable causal identification even when it would be impossible with non-hierarchical data, that is, if we had only unit-level summaries of subunit-level variables (e.g. the school's average test score, rather than each student's score). we develop estimation techniques for hierarchical causal models, using methods including hierarchical bayesian models. we illustrate our results in simulation and via a reanalysis of the classic ""eight schools"" study.",,2024-01-10,,"['eli n. weinstein', 'david m. blei']"
2401.05388,bayesian ecg reconstruction using denoising diffusion generative models,eess.sp cs.lg stat.ml,"in this work, we propose a denoising diffusion generative model (ddgm) trained with healthy electrocardiogram (ecg) data that focuses on ecg morphology and inter-lead dependence. our results show that this innovative generative model can successfully generate realistic ecg signals. furthermore, we explore the application of recent breakthroughs in solving linear inverse bayesian problems using ddgm. this approach enables the development of several important clinical tools. these include the calculation of corrected qt intervals (qtc), effective noise suppression of ecg signals, recovery of missing ecg leads, and identification of anomalous readings, enabling significant advances in cardiac health monitoring and diagnosis.",,2023-12-18,,"['gabriel v. cardoso', 'lisa bedin', 'josselin duchateau', 'rémi dubois', 'eric moulines']"
2401.05420,holobeam: learning optimal beamforming in far-field holographic   metasurface transceivers,eess.sp cs.lg stat.ml,"holographic metasurface transceivers (hmts) are emerging as cost-effective substitutes to large antenna arrays for beamforming in millimeter and terahertz wave communication. however, to achieve desired channel gains through beamforming in hmt, phase-shifts of a large number of elements need to be appropriately set, which is challenging. also, these optimal phase-shifts depend on the location of the receivers, which could be unknown. in this work, we develop a learning algorithm using a {\it fixed-budget multi-armed bandit framework} to beamform and maximize received signal strength at the receiver for far-field regions. our algorithm, named \algo exploits the parametric form of channel gains of the beams, which can be expressed in terms of two {\it phase-shifting parameters}. even after parameterization, the problem is still challenging as phase-shifting parameters take continuous values. to overcome this, {\it\hb} works with the discrete values of phase-shifting parameters and exploits their unimodal relations with channel gains to learn the optimal values faster. we upper bound the probability of {\it\hb} incorrectly identifying the (discrete) optimal phase-shift parameters in terms of the number of pilots used in learning. we show that this probability decays exponentially with the number of pilot signals. we demonstrate that {\it\hb} outperforms state-of-the-art algorithms through extensive simulations.",,2023-12-29,,"['debamita ghosh', 'manjesh kumar hanawal', 'nikola zlatanova']"
2401.05424,a toolbox for modelling engagement with educational videos,cs.cy cs.ir cs.lg stat.ap,"with the advancement and utility of artificial intelligence (ai), personalising education to a global population could be a cornerstone of new educational systems in the future. this work presents the peekc dataset and the truelearn python library, which contains a dataset and a series of online learner state models that are essential to facilitate research on learner engagement modelling.truelearn family of models was designed following the ""open learner"" concept, using humanly-intuitive user representations. this family of scalable, online models also help end-users visualise the learner models, which may in the future facilitate user interaction with their models/recommenders. the extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytics practitioners. the experiments show the utility of both the dataset and the library with predictive performance significantly exceeding comparative baseline models. the dataset contains a large amount of ai-related educational videos, which are of interest for building and validating ai-specific educational recommenders.",,2023-12-30,,"['yuxiang qiu', 'karim djemili', 'denis elezi', 'aaneel shalman', 'maría pérez-ortiz', 'emine yilmaz', 'john shawe-taylor', 'sahan bulathwela']"
2401.05470,modelling species distributions with deep learning to predict plant   extinction risk and assess climate change impacts,q-bio.pe cs.lg stat.ap,"the post-2020 global biodiversity framework needs ambitious, research-based targets. estimating the accelerated extinction risk due to climate change is critical. the international union for conservation of nature (iucn) measures the extinction risk of species. automatic methods have been developed to provide information on the iucn status of under-assessed taxa. however, these compensatory methods are based on current species characteristics, mainly geographical, which precludes their use in future projections. here, we evaluate a novel method for classifying the iucn status of species benefiting from the generalisation power of species distribution models based on deep learning. our method matches state-of-the-art classification performance while relying on flexible sdm-based features that capture species' environmental preferences. cross-validation yields average accuracies of 0.61 for status classification and 0.78 for binary classification. climate change will reshape future species distributions. under the species-environment equilibrium hypothesis, sdm projections approximate plausible future outcomes. two extremes of species dispersal capacity are considered: unlimited or null. the projected species distributions are translated into features feeding our iucn classification method. finally, trends in threatened species are analysed over time and i) by continent and as a function of average ii) latitude or iii) altitude. the proportion of threatened species is increasing globally, with critical rates in africa, asia and south america. furthermore, the proportion of threatened species is predicted to peak around the two tropics, at the equator, in the lowlands and at altitudes of 800-1,500 m.",,2024-01-10,,"['joaquim estopinan', 'pierre bonnet', 'maximilien servajean', 'françois munoz', 'alexis joly']"
2401.05535,improving the accuracy and interpretability of random forests via forest   pruning,stat.ml cs.ai cs.lg math.oc,"decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. however, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. in the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. to this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the lasso methodology. extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods proposed is more accurate than the original random forest, while just using a small fraction of the trees, aiding result interpretability. compared to current state-of-the-art forest pruning methods, namely sequential forward selection and (a variation of) sequential backward selection, our methods tend to outperform both of them, whether in terms of accuracy, number of trees employed, or both.",,2024-01-10,2024-01-23,['albert dorador']
2401.05574,a general theory for robust clustering via trimmed mean,math.st stat.ml stat.th,"clustering is a fundamental tool in statistical machine learning in the presence of heterogeneous data. many recent results focus primarily on optimal mislabeling guarantees, when data are distributed around centroids with sub-gaussian errors. yet, the restrictive sub-gaussian model is often invalid in practice, since various real-world applications exhibit heavy tail distributions around the centroids or suffer from possible adversarial attacks that call for robust clustering with a robust data-driven initialization. in this paper, we introduce a hybrid clustering technique with a novel multivariate trimmed mean type centroid estimate to produce mislabeling guarantees under a weak initialization condition for general error distributions around the centroids. a matching lower bound is derived, up to factors depending on the number of clusters. in addition, our approach also produces the optimal mislabeling even in the presence of adversarial outliers. our results reduce to the sub-gaussian case when errors follow sub-gaussian distributions. to solve the problem thoroughly, we also present novel data-driven robust initialization techniques and show that, with probabilities approaching one, these initial centroid estimates are sufficiently good for the subsequent clustering algorithm to achieve the optimal mislabeling rates. furthermore, we demonstrate that the lloyd algorithm is suboptimal for more than two clusters even when errors are gaussian, and for two clusters when errors distributions have heavy tails. both simulated data and real data examples lend further support to both of our robust initialization procedure and clustering algorithm.",,2024-01-10,2024-02-02,"['soham jana', 'jianqing fan', 'sanjeev kulkarni']"
2401.05579,an augmented surprise-guided sequential learning framework for   predicting the melt pool geometry,cs.lg stat.ml,"metal additive manufacturing (mam) has reshaped the manufacturing industry, offering benefits like intricate design, minimal waste, rapid prototyping, material versatility, and customized solutions. however, its full industry adoption faces hurdles, particularly in achieving consistent product quality. a crucial aspect for mam's success is understanding the relationship between process parameters and melt pool characteristics. integrating artificial intelligence (ai) into mam is essential. traditional machine learning (ml) methods, while effective, depend on large datasets to capture complex relationships, a significant challenge in mam due to the extensive time and resources required for dataset creation. our study introduces a novel surprise-guided sequential learning framework, surpriseaf-bo, signaling a significant shift in mam. this framework uses an iterative, adaptive learning process, modeling the dynamics between process parameters and melt pool characteristics with limited data, a key benefit in mam's cyber manufacturing context. compared to traditional ml models, our sequential learning method shows enhanced predictive accuracy for melt pool dimensions. further improving our approach, we integrated a conditional tabular generative adversarial network (ctgan) into our framework, forming the ct-surpriseaf-bo. this produces synthetic data resembling real experimental data, improving learning effectiveness. this enhancement boosts predictive precision without requiring additional physical experiments. our study demonstrates the power of advanced data-driven techniques in cyber manufacturing and the substantial impact of sequential ai and ml, particularly in overcoming mam's traditional challenges.",,2024-01-10,,"['ahmed shoyeb raihan', 'hamed khosravi', 'tanveer hossain bhuiyan', 'imtiaz ahmed']"
2401.05716,kernelized normalizing constant estimation: bridging bayesian quadrature   and bayesian optimization,cs.lg stat.ml,"in this paper, we study the problem of estimating the normalizing constant $\int e^{-\lambda f(x)}dx$ through queries to the black-box function $f$, where $f$ belongs to a reproducing kernel hilbert space (rkhs), and $\lambda$ is a problem parameter. we show that to estimate the normalizing constant within a small relative error, the level of difficulty depends on the value of $\lambda$: when $\lambda$ approaches zero, the problem is similar to bayesian quadrature (bq), while when $\lambda$ approaches infinity, the problem is similar to bayesian optimization (bo). more generally, the problem varies between bq and bo. we find that this pattern holds true even when the function evaluations are noisy, bringing new aspects to this topic. our findings are supported by both algorithm-independent lower bounds and algorithmic upper bounds, as well as simulation studies conducted on a variety of benchmark functions.",,2024-01-11,,"['xu cai', 'jonathan scarlett']"
2401.05934,combining normalizing flows and quasi-monte carlo,stat.co stat.ml,"recent advances in machine learning have led to the development of new methods for enhancing monte carlo methods such as markov chain monte carlo (mcmc) and importance sampling (is). one such method is normalizing flows, which use a neural network to approximate a distribution by evaluating it pointwise. normalizing flows have been shown to improve the performance of mcmc and is. on the other side, (randomized) quasi-monte carlo methods are used to perform numerical integration. they replace the random sampling of monte carlo by a sequence which cover the hypercube more uniformly, resulting in better convergence rates for the error that plain monte carlo. in this work, we combine these two methods by using quasi-monte carlo to sample the initial distribution that is transported by the flow. we demonstrate through numerical experiments that this combination can lead to an estimator with significantly lower variance than if the flow was sampled with a classic monte carlo.",,2024-01-11,,['charly andral']
2401.05982,a tree-based varying coefficient model,stat.ml cs.lg,"the paper introduces a tree-based varying coefficient model (vcm) where the varying coefficients are modelled using the cyclic gradient boosting machine (cgbm) from delong et al. (2023). modelling the coefficient functions using a cgbm allows for dimension-wise early stopping and feature importance scores. the dimension-wise early stopping not only reduces the risk of dimension-specific overfitting, but also reveals differences in model complexity across dimensions. the use of feature importance scores allows for simple feature selection and easy model interpretation. the model is evaluated on the same simulated and real data examples as those used in richman and w\""uthrich (2023), and the results show that it produces results in terms of out of sample loss that are comparable to those of their neural network-based vcm called localglmnet.",,2024-01-11,2024-01-15,"['henning zakrisson', 'mathias lindholm']"
2401.06091,a closer look at auroc and auprc under class imbalance,cs.lg stat.me,"in machine learning (ml), a widespread adage is that the area under the precision-recall curve (auprc) is a superior metric for model comparison to the area under the receiver operating characteristic (auroc) for binary classification tasks with class imbalance. this paper challenges this notion through novel mathematical analysis, illustrating that auroc and auprc can be concisely related in probabilistic terms. we demonstrate that auprc, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels. this bias can inadvertently heighten algorithmic disparities. prompted by these insights, a thorough review of existing ml literature was conducted, utilizing large language models to analyze over 1.5 million papers from arxiv. our investigation focused on the prevalence and substantiation of the purported auprc superiority. the results expose a significant deficit in empirical backing and a trend of misattributions that have fuelled the widespread acceptance of auprc's supposed advantages. our findings represent a dual contribution: a significant technical advancement in understanding metric behaviors and a stark warning about unchecked assumptions in the ml community. all experiments are accessible at https://github.com/mmcdermott/auc_is_all_you_need.",,2024-01-11,2024-02-25,"['matthew b. a. mcdermott', 'lasse hyldig hansen', 'haoran zhang', 'giovanni angelotti', 'jack gallifant']"
2401.06325,faster sampling without isoperimetry via diffusion-based monte carlo,stat.ml cs.lg math.oc stat.co,"to sample from a general target distribution $p_*\propto e^{-f_*}$ beyond the isoperimetric condition, huang et al. (2023) proposed to perform sampling through reverse diffusion, giving rise to diffusion-based monte carlo (dmc). specifically, dmc follows the reverse sde of a diffusion process that transforms the target distribution to the standard gaussian, utilizing a non-parametric score estimation. however, the original dmc algorithm encountered high gradient complexity, resulting in an exponential dependency on the error tolerance $\epsilon$ of the obtained samples. in this paper, we demonstrate that the high complexity of dmc originates from its redundant design of score estimation, and proposed a more efficient algorithm, called rs-dmc, based on a novel recursive score estimation method. in particular, we first divide the entire diffusion process into multiple segments and then formulate the score estimation step (at any time step) as a series of interconnected mean estimation and sampling subproblems accordingly, which are correlated in a recursive manner. importantly, we show that with a proper design of the segment decomposition, all sampling subproblems will only need to tackle a strongly log-concave distribution, which can be very efficient to solve using the langevin-based samplers with a provably rapid convergence rate. as a result, we prove that the gradient complexity of rs-dmc only has a quasi-polynomial dependency on $\epsilon$, which significantly improves exponential gradient complexity in huang et al. (2023). furthermore, under commonly used dissipative conditions, our algorithm is provably much faster than the popular langevin-based algorithms. our algorithm design and theoretical framework illuminate a novel direction for addressing sampling problems, which could be of broader applicability in the community.",,2024-01-11,,"['xunpeng huang', 'difan zou', 'hanze dong', 'yian ma', 'tong zhang']"
2401.06447,a comprehensive framework for multi-fidelity surrogate modeling with   noisy data: a gray-box perspective,stat.me stat.co stat.ml,"computer simulations (a.k.a. white-box models) are more indispensable than ever to model intricate engineering systems. however, computational models alone often fail to fully capture the complexities of reality. when physical experiments are accessible though, it is of interest to enhance the incomplete information offered by computational models. gray-box modeling is concerned with the problem of merging information from data-driven (a.k.a. black-box) models and white-box (i.e., physics-based) models. in this paper, we propose to perform this task by using multi-fidelity surrogate models (mfsms). a mfsm integrates information from models with varying computational fidelity into a new surrogate model. the multi-fidelity surrogate modeling framework we propose handles noise-contaminated data and is able to estimate the underlying noise-free high-fidelity function. our methodology emphasizes on delivering precise estimates of the uncertainty in its predictions in the form of confidence and prediction intervals, by quantitatively incorporating the different types of uncertainty that affect the problem, arising from measurement noise and from lack of knowledge due to the limited experimental design budget on both the high- and low-fidelity models. applied to gray-box modeling, our mfsm framework treats noisy experimental data as the high-fidelity and the white-box computational models as their low-fidelity counterparts. the effectiveness of our methodology is showcased through synthetic examples and a wind turbine application.",,2024-01-12,,"['katerina giannoukou', 'stefano marelli', 'bruno sudret']"
2401.06481,machine learning a fixed point action for su(3) gauge theory with a   gauge equivariant convolutional neural network,hep-lat cs.lg hep-ph stat.ml,"fixed point lattice actions are designed to have continuum classical properties unaffected by discretization effects and reduced lattice artifacts at the quantum level. they provide a possible way to extract continuum physics with coarser lattices, thereby allowing to circumvent problems with critical slowing down and topological freezing toward the continuum limit. a crucial ingredient for practical applications is to find an accurate and compact parametrization of a fixed point action, since many of its properties are only implicitly defined. here we use machine learning methods to revisit the question of how to parametrize fixed point actions. in particular, we obtain a fixed point action for four-dimensional su(3) gauge theory using convolutional neural networks with exact gauge invariance. the large operator space allows us to find superior parametrizations compared to previous studies, a necessary first step for future monte carlo simulations.",,2024-01-12,,"['kieran holland', 'andreas ipp', 'david i. müller', 'urs wenger']"
2401.06523,boosting causal additive models,stat.ml cs.lg math.pr math.st stat.th,"we present a boosting-based method to learn additive structural equation models (sems) from observational data, with a focus on the theoretical aspects of determining the causal order among variables. we introduce a family of score functions based on arbitrary regression techniques, for which we establish necessary conditions to consistently favor the true causal ordering. our analysis reveals that boosting with early stopping meets these criteria and thus offers a consistent score function for causal orderings. to address the challenges posed by high-dimensional data sets, we adapt our approach through a component-wise gradient descent in the space of additive sems. our simulation study underlines our theoretical results for lower dimensions and demonstrates that our high-dimensional adaptation is competitive with state-of-the-art methods. in addition, it exhibits robustness with respect to the choice of the hyperparameters making the procedure easy to tune.",,2024-01-12,,"['maximilian kertel', 'nadja klein']"
2401.06557,treatment-aware hyperbolic representation learning for causal effect   estimation with social networks,cs.lg cs.ai cs.si stat.me,"estimating the individual treatment effect (ite) from observational data is a crucial research topic that holds significant value across multiple domains. how to identify hidden confounders poses a key challenge in ite estimation. recent studies have incorporated the structural information of social networks to tackle this challenge, achieving notable advancements. however, these methods utilize graph neural networks to learn the representation of hidden confounders in euclidean space, disregarding two critical issues: (1) the social networks often exhibit a scalefree structure, while euclidean embeddings suffer from high distortion when used to embed such graphs, and (2) each ego-centric network within a social network manifests a treatment-related characteristic, implying significant patterns of hidden confounders. to address these issues, we propose a novel method called treatment-aware hyperbolic representation learning (tahyper). firstly, tahyper employs the hyperbolic space to encode the social networks, thereby effectively reducing the distortion of confounder representation caused by euclidean embeddings. secondly, we design a treatment-aware relationship identification module that enhances the representation of hidden confounders by identifying whether an individual and her neighbors receive the same treatment. extensive experiments on two benchmark datasets are conducted to demonstrate the superiority of our method.",,2024-01-12,,"['ziqiang cui', 'xing tang', 'yang qiao', 'bowei he', 'liang chen', 'xiuqiang he', 'chen ma']"
2401.06564,valid causal inference with unobserved confounding in high-dimensional   settings,stat.me stat.ml,"various methods have recently been proposed to estimate causal effects with confidence intervals that are uniformly valid over a set of data generating processes when high-dimensional nuisance models are estimated by post-model-selection or machine learning estimators. these methods typically require that all the confounders are observed to ensure identification of the effects. we contribute by showing how valid semiparametric inference can be obtained in the presence of unobserved confounders and high-dimensional nuisance models. we propose uncertainty intervals which allow for unobserved confounding, and show that the resulting inference is valid when the amount of unobserved confounding is small relative to the sample size; the latter is formalized in terms of convergence rates. simulation experiments illustrate the finite sample properties of the proposed intervals and investigate an alternative procedure that improves the empirical coverage of the intervals when the amount of unobserved confounding is large. finally, a case study on the effect of smoking during pregnancy on birth weight is used to illustrate the use of the methods introduced to perform a sensitivity analysis to unobserved confounding.",,2024-01-12,,"['niloofar moosavi', 'tetiana gorbach', 'xavier de luna']"
2401.06738,noise-adaptive (accelerated) stochastic heavy-ball momentum,math.oc cs.lg stat.ml,"we analyze the convergence of stochastic heavy ball (shb) momentum in the smooth, strongly-convex setting. kidambi et al. (2018) show that shb (with small mini-batches) cannot attain an accelerated rate of convergence even for quadratics, and conjecture that the practical gain of shb is a by-product of mini-batching. we substantiate this claim by showing that shb can obtain an accelerated rate when the mini-batch size is larger than some threshold. in particular, for strongly-convex quadratics with condition number $\kappa$, we prove that shb with the standard step-size and momentum parameters results in an $o\left(\exp(-\frac{t}{\sqrt{\kappa}}) + \sigma \right)$ convergence rate, where $t$ is the number of iterations and $\sigma^2$ is the variance in the stochastic gradients. to ensure convergence to the minimizer, we propose a multi-stage approach that results in a noise-adaptive $o\left(\exp\left(-\frac{t}{\sqrt{\kappa}} \right) + \frac{\sigma}{t}\right)$ rate. for general strongly-convex functions, we use the averaging interpretation of shb along with exponential step-sizes to prove an $o\left(\exp\left(-\frac{t}{\kappa} \right) + \frac{\sigma^2}{t} \right)$ convergence to the minimizer in a noise-adaptive manner. finally, we empirically demonstrate the effectiveness of the proposed algorithms.",,2024-01-12,,"['anh dang', 'reza babanezhad', 'sharan vaswani']"
2401.06740,a deep implicit-explicit minimizing movement method for option pricing   in jump-diffusion models,q-fin.cp cs.lg cs.na math.na math.pr stat.ml,"we develop a novel deep learning approach for pricing european basket options written on assets that follow jump-diffusion dynamics. the option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type artificial neural networks (anns) for each time step. the integral operator is discretized via two different approaches: a) a sparse-grid gauss--hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ann-based high-dimensional special-purpose quadrature rule. crucially, the proposed ann is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. the performance and robustness with respect to the dimension of the methods are assessed in a series of numerical experiments involving the merton jump-diffusion model.",,2024-01-12,,"['emmanuil h. georgoulis', 'antonis papapantoleon', 'costas smaragdakis']"
2401.06864,deep learning with dags,stat.ml cs.lg econ.em stat.me,"social science theories often postulate causal relationships among a set of variables or events. although directed acyclic graphs (dags) are increasingly used to represent these theories, their full potential has not yet been realized in practice. as non-parametric causal models, dags require no assumptions about the functional form of the hypothesized relationships. nevertheless, to simplify the task of empirical evaluation, researchers tend to invoke such assumptions anyway, even though they are typically arbitrary and do not reflect any theoretical content or prior knowledge. moreover, functional form assumptions can engender bias, whenever they fail to accurately capture the complexity of the causal system under investigation. in this article, we introduce causal-graphical normalizing flows (cgnfs), a novel approach to causal inference that leverages deep neural networks to empirically evaluate theories represented as dags. unlike conventional approaches, cgnfs model the full joint distribution of the data according to a dag supplied by the analyst, without relying on stringent assumptions about functional form. in this way, the method allows for flexible, semi-parametric estimation of any causal estimand that can be identified from the dag, including total effects, conditional effects, direct and indirect effects, and path-specific effects. we illustrate the method with a reanalysis of blau and duncan's (1967) model of status attainment and zhou's (2019) model of conditional versus controlled mobility. to facilitate adoption, we provide open-source software together with a series of online tutorials for implementing cgnfs. the article concludes with a discussion of current limitations and directions for future development.",,2024-01-12,,"['sourabh balgi', 'adel daoud', 'jose m. peña', 'geoffrey t. wodtke', 'jesse zhou']"
2401.06899,analyses and concerns in precision medicine: a statistical perspective,cs.lg stat.ap,"this article explores the critical role of statistical analysis in precision medicine. it discusses how personalized healthcare is enhanced by statistical methods that interpret complex, multidimensional datasets, focusing on predictive modeling, machine learning algorithms, and data visualization techniques. the paper addresses challenges in data integration and interpretation, particularly with diverse data sources like electronic health records (ehrs) and genomic data. it also delves into ethical considerations such as patient privacy and data security. in addition, the paper highlights the evolution of statistical analysis in medicine, core statistical methodologies in precision medicine, and future directions in the field, emphasizing the integration of artificial intelligence (ai) and machine learning (ml).",,2024-01-12,,['xiaofei chen']
2401.06922,open ran lstm traffic prediction and slice management using deep   reinforcement learning,cs.lg cs.ai cs.ni cs.sy eess.sy stat.ml,"with emerging applications such as autonomous driving, smart cities, and smart factories, network slicing has become an essential component of 5g and beyond networks as a means of catering to a service-aware network. however, managing different network slices while maintaining quality of services (qos) is a challenge in a dynamic environment. to address this issue, this paper leverages the heterogeneous experiences of distributed units (dus) in oran systems and introduces a novel approach to oran slicing xapp using distributed deep reinforcement learning (ddrl). additionally, to enhance the decision-making performance of the rl agent, a prediction rapp based on long short-term memory (lstm) is incorporated to provide additional information from the dynamic environment to the xapp. simulation results demonstrate significant improvements in network performance, particularly in reducing qos violations. this emphasizes the importance of using the prediction rapp and distributed actors' information jointly as part of a dynamic xapp.",,2024-01-12,,"['fatemeh lotfi', 'fatemeh afghah']"
2401.06925,modeling latent selection with structural causal models,cs.ai cs.lg math.st stat.me stat.ml stat.th,"selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. we introduce a conditioning operation on structural causal models (scms) to model latent selection from a causal perspective. we show that the conditioning operation transforms an scm with the presence of an explicit latent selection mechanism into an scm without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original scm. furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of scms, and commutes with marginalization. thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. we demonstrate by example how classical results of causal inference can be generalized to include selection bias and how the conditioning operation helps with modeling of real-world problems.",,2024-01-12,,"['leihao chen', 'onno zoeter', 'joris m. mooij']"
2401.06967,nhanes-gcp: leveraging the google cloud platform and bigquery ml for   reproducible machine learning with data from the national health and   nutrition examination survey,q-bio.qm cs.lg stat.ap,"summary: nhanes, the national health and nutrition examination survey, is a program of studies led by the centers for disease control and prevention (cdc) designed to assess the health and nutritional status of adults and children in the united states (u.s.). nhanes data is frequently used by biostatisticians and clinical scientists to study health trends across the u.s., but every analysis requires extensive data management and cleaning before use and this repetitive data engineering collectively costs valuable research time and decreases the reproducibility of analyses. here, we introduce nhanes-gcp, a cloud development kit for terraform (cdktf) infrastructure-as-code (iac) and data build tool (dbt) resources built on the google cloud platform (gcp) that automates the data engineering and management aspects of working with nhanes data. with current gcp pricing, nhanes-gcp costs less than $2 to run and less than $15/yr of ongoing costs for hosting the nhanes data, all while providing researchers with clean data tables that can readily be integrated for large-scale analyses. we provide examples of leveraging bigquery ml to carry out the process of selecting data, integrating data, training machine learning and statistical models, and generating results all from a single sql-like query. nhanes-gcp is designed to enhance the reproducibility of analyses and create a well-engineered nhanes data resource for statistics, machine learning, and fine-tuning large language models (llms).   availability and implementation"" nhanes-gcp is available at https://github.com/in-vivo-group/nhanes-gcp",,2024-01-12,,"['b. ross katz', 'abdul khan', 'james york-winegar', 'alexander j. titus']"
2401.06980,joint unsupervised and supervised training for automatic speech   recognition via bilevel optimization,cs.cl cs.lg stat.ml,"in this paper, we present a novel bilevel optimization-based training approach to training acoustic models for automatic speech recognition (asr) tasks that we term {bi-level joint unsupervised and supervised training (bl-just)}. {bl-just employs a lower and upper level optimization with an unsupervised loss and a supervised loss respectively, leveraging recent advances in penalty-based bilevel optimization to solve this challenging asr problem with affordable complexity and rigorous convergence guarantees.} to evaluate bl-just, extensive experiments on the librispeech and ted-lium v2 datasets have been conducted. bl-just achieves superior performance over the commonly used pre-training followed by fine-tuning strategy.",,2024-01-13,,"['a f m saif', 'xiaodong cui', 'han shen', 'songtao lu', 'brian kingsbury', 'tianyi chen']"
2401.07012,an adrc-incorporated stochastic gradient descent algorithm for latent   factor analysis,cs.lg cs.sy eess.sy stat.ml,"high-dimensional and incomplete (hdi) matrix contains many complex interactions between numerous nodes. a stochastic gradient descent (sgd)-based latent factor analysis (lfa) model is remarkably effective in extracting valuable information from an hdi matrix. however, such a model commonly encounters the problem of slow convergence because a standard sgd algorithm only considers the current learning error to compute the stochastic gradient without considering the historical and future state of the learning error. to address this critical issue, this paper innovatively proposes an adrc-incorporated sgd (ads) algorithm by refining the instance learning error by considering the historical and future state by following the principle of an adrc controller. with it, an ads-based lfa model is further achieved for fast and accurate latent factor analysis on an hdi matrix. empirical studies on two hdi datasets demonstrate that the proposed model outperforms the state-of-the-art lfa models in terms of computational efficiency and accuracy for predicting the missing data of an hdi matrix.",,2024-01-13,,"['jinli li', 'ye yuan']"
2401.07110,hebbian learning from first principles,cond-mat.dis-nn stat.ml,"recently, the original storage prescription for the hopfield model of neural networks -- as well as for its dense generalizations -- has been turned into a genuine hebbian learning rule by postulating the expression of its hamiltonian for both the supervised and unsupervised protocols. in these notes, first, we obtain these explicit expressions by relying upon maximum entropy extremization \`a la jaynes. beyond providing a formal derivation of these recipes for hebbian learning, this construction also highlights how lagrangian constraints within entropy extremization force network's outcomes on neural correlations: these try to mimic the empirical counterparts hidden in the datasets provided to the network for its training and, the denser the network, the longer the correlations that it is able to capture. next, we prove that, in the big data limit, whatever the presence of a teacher (or its lacking), not only these hebbian learning rules converge to the original storage prescription of the hopfield model but also their related free energies (and, thus, the statistical mechanical picture provided by amit, gutfreund and sompolinsky is fully recovered). as a sideline, we show mathematical equivalence among standard cost functions (hamiltonian), preferred in statistical mechanical jargon, and quadratic loss functions, preferred in machine learning terminology. remarks on the exponential hopfield model (as the limit of dense networks with diverging density) and semi-supervised protocols are also provided.",,2024-01-13,,"['linda albanese', 'adriano barra', 'pierluigi bianco', 'fabrizio durante', 'diego pallara']"
2401.07111,bayesian signal matching for transfer learning in erp-based brain   computer interface,stat.ap stat.co,"an event-related potential (erp)-based brain-computer interface (bci) speller system assists people with disabilities communicate by decoding electroencephalogram (eeg) signals. a p300-erp embedded in eeg signals arises in response to a rare, but relevant event (target) among a series of irrelevant events (non-target). different machine learning methods have constructed binary classifiers to detect target events, known as calibration. existing calibration strategy only uses data from participants themselves with lengthy training time, causing biased p300 estimation and decreasing prediction accuracy. to resolve this issue, we propose a bayesian signal matching (bsm) framework for calibrating the eeg signals from a new participant using data from source participants. bsm specifies the joint distribution of stimulus-specific eeg signals among source participants via a bayesian hierarchical mixture model. we apply the inference strategy: if source and new participants are similar, they share the same set of model parameters, otherwise, they keep their own sets of model parameters; we predict on the testing data using parameters of the baseline cluster directly. our hierarchical framework can be generalized to other base classifiers with clear likelihood specifications. we demonstrate the advantages of bsm using simulations and focus on the real data analysis among participants with neuro-degenerative diseases.",,2024-01-13,,"['tianwen ma', 'jane e. huggins', 'jian kang']"
2401.07174,on the (in)compatibility between group fairness and individual fairness,math.st cs.cy cs.lg stat.ml stat.th,"we study the compatibility between the optimal statistical parity solutions and individual fairness. while individual fairness seeks to treat similar individuals similarly, optimal statistical parity aims to provide similar treatment to individuals who share relative similarity within their respective sensitive groups. the two fairness perspectives, while both desirable from a fairness perspective, often come into conflict in applications. our goal in this work is to analyze the existence of this conflict and its potential solution. in particular, we establish sufficient (sharp) conditions for the compatibility between the optimal (post-processing) statistical parity $l^2$ learning and the ($k$-lipschitz or $(\epsilon,\delta)$) individual fairness requirements. furthermore, when there exists a conflict between the two, we first relax the former to the pareto frontier (or equivalently the optimal trade-off) between $l^2$ error and statistical disparity, and then analyze the compatibility between the frontier and the individual fairness requirements. our analysis identifies regions along the pareto frontier that satisfy individual fairness requirements. (lastly, we provide individual fairness guarantees for the composition of a trained model and the optimal post-processing step so that one can determine the compatibility of the post-processed model.) this provides practitioners with a valuable approach to attain pareto optimality for statistical parity while adhering to the constraints of individual fairness.",,2024-01-13,,"['shizhou xu', 'thomas strohmer']"
2401.07187,"a survey on statistical theory of deep learning: approximation, training   dynamics, and generative models",stat.ml cs.lg math.st stat.th,"in this article, we review the literature on statistical theories of neural networks from three perspectives. in the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression or classification. these results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks, in that tools from the approximation theory are adopted. through these constructions, the width and depth of the networks can be expressed in terms of sample size, data dimension, and function smoothness. nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. this motivates us to review the training dynamics of neural networks in the second part. specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' in particular, two well-known paradigms are reviewed: the neural tangent kernel (ntk) paradigm, and mean-field (mf) paradigm. in the last part, we review the most recent theoretical advancements in generative models including generative adversarial networks (gans), diffusion models, and in-context learning (icl) in the large language models (llms). the former two models are known to be the main pillars of the modern generative ai era, while icl is a strong capability of llms in learning from a few examples in the context. finally, we conclude the paper by suggesting several promising directions for deep learning theory.",,2024-01-13,,"['namjoon suh', 'guang cheng']"
2401.07206,probabilistic reduced-dimensional vector autoregressive modeling with   oblique projections,stat.ml cs.lg cs.sy eess.sy,"in this paper, we propose a probabilistic reduced-dimensional vector autoregressive (predvar) model to extract low-dimensional dynamics from high-dimensional noisy data. the model utilizes an oblique projection to partition the measurement space into a subspace that accommodates the reduced-dimensional dynamics and a complementary static subspace. an optimal oblique decomposition is derived for the best predictability regarding prediction error covariance. building on this, we develop an iterative predvar algorithm using maximum likelihood and the expectation-maximization (em) framework. this algorithm alternately updates the estimates of the latent dynamics and optimal oblique projection, yielding dynamic latent variables with rank-ordered predictability and an explicit latent var model that is consistent with the outer projection model. the superior performance and efficiency of the proposed approach are demonstrated using data sets from a synthesized lorenz system and an industrial process from eastman chemical.",,2024-01-14,,"['yanfang mo', 's. joe qin']"
2401.07231,use of prior knowledge to discover causal additive models with   unobserved variables and its application to time series data,cs.lg stat.me stat.ml,"this paper proposes two methods for causal additive models with unobserved variables (cam-uv). cam-uv assumes that the causal functions take the form of generalized additive models and that latent confounders are present. first, we propose a method that leverages prior knowledge for efficient causal discovery. then, we propose an extension of this method for inferring causality in time series data. the original cam-uv algorithm differs from other existing causal function models in that it does not seek the causal order between observed variables, but rather aims to identify the causes for each observed variable. therefore, the first proposed method in this paper utilizes prior knowledge, such as understanding that certain variables cannot be causes of specific others. moreover, by incorporating the prior knowledge that causes precedes their effects in time, we extend the first algorithm to the second method for causal discovery in time series data. we validate the first proposed method by using simulated data to demonstrate that the accuracy of causal discovery increases as more prior knowledge is accumulated. additionally, we test the second proposed method by comparing it with existing time series causal discovery methods, using both simulated data and real-world data.",,2024-01-14,2024-01-17,"['takashi nicholas maeda', 'shohei shimizu']"
2401.07298,efficient frameworks for generalized low-rank matrix bandit problems,stat.ml cs.lg,"in the stochastic contextual low-rank matrix bandit problem, the expected reward of an action is given by the inner product between the action's feature matrix and some fixed, but initially unknown $d_1$ by $d_2$ matrix $\theta^*$ with rank $r \ll \{d_1, d_2\}$, and an agent sequentially takes actions based on past experience to maximize the cumulative reward. in this paper, we study the generalized low-rank matrix bandit problem, which has been recently proposed in \cite{lu2021low} under the generalized linear model (glm) framework. to overcome the computational infeasibility and theoretical restrain of existing algorithms on this problem, we first propose the g-estt framework that modifies the idea from \cite{jun2019bilinear} by using stein's method on the subspace estimation and then leverage the estimated subspaces via a regularization idea. furthermore, we remarkably improve the efficiency of g-estt by using a novel exclusion idea on the estimated subspace instead, and propose the g-ests framework. we also show that g-estt can achieve the $\tilde{o}(\sqrt{(d_1+d_2)mrt})$ bound of regret while g-ests can achineve the $\tilde{o}(\sqrt{(d_1+d_2)^{3/2}mr^{3/2}t})$ bound of regret under mild assumption up to logarithm terms, where $m$ is some problem dependent value. under a reasonable assumption that $m = o((d_1+d_2)^2)$ in our problem setting, the regret of g-estt is consistent with the current best regret of $\tilde{o}((d_1+d_2)^{3/2} \sqrt{rt}/d_{rr})$~\citep{lu2021low} ($d_{rr}$ will be defined later). for completeness, we conduct experiments to illustrate that our proposed algorithms, especially g-ests, are also computationally tractable and consistently outperform other state-of-the-art (generalized) linear matrix bandit methods based on a suite of simulations.",,2024-01-14,,"['yue kang', 'cho-jui hsieh', 'thomas c. m. lee']"
2401.07371,a data-driven resilience framework of directionality configuration based   on topological credentials in road networks,cs.si cs.lg stat.ap,"roadway reconfiguration is a crucial aspect of transportation planning, aiming to enhance traffic flow, reduce congestion, and improve overall road network performance with existing infrastructure and resources. this paper presents a novel roadway reconfiguration technique by integrating optimization based brute force search approach and decision support framework to rank various roadway configurations for better performance. the proposed framework incorporates a multi-criteria decision analysis (mcda) approach, combining input from generated scenarios during the optimization process. by utilizing data from optimization, the model identifies total betweenness centrality (tbc), system travel time (stt), and total link traffic flow (tltf) as the most influential decision variables. the developed framework leverages graph theory to model the transportation network topology and apply network science metrics as well as stochastic user equilibrium traffic assignment to assess the impact of each roadway configuration on the overall network performance. to rank the roadway configurations, the framework employs machine learning algorithms, such as ridge regression, to determine the optimal weights for each criterion (i.e., tbc, stt, tltf). moreover, the network-based analysis ensures that the selected configurations not only optimize individual roadway segments but also enhance system-level efficiency, which is particularly helpful as the increasing frequency and intensity of natural disasters and other disruptive events underscore the critical need for resilient transportation networks. by integrating multi-criteria decision analysis, machine learning, and network science metrics, the proposed framework would enable transportation planners to make informed and data-driven decisions, leading to more sustainable, efficient, and resilient roadway configurations.",,2024-01-14,,"['h m imran kays', 'khondhaker al momin', 'k. k. ""muralee"" muraleetharan', 'arif mohaimin sadri']"
2401.07463,"consistency of semi-supervised learning, stochastic tug-of-war games,   and the p-laplacian",math.st cs.lg cs.na math.ap math.na math.pr stat.th,"in this paper we give a broad overview of the intersection of partial differential equations (pdes) and graph-based semi-supervised learning. the overview is focused on a large body of recent work on pde continuum limits of graph-based learning, which have been used to prove well-posedness of semi-supervised learning algorithms in the large data limit. we highlight some interesting research directions revolving around consistency of graph-based semi-supervised learning, and present some new results on the consistency of p-laplacian semi-supervised learning using the stochastic tug-of-war game interpretation of the p-laplacian. we also present the results of some numerical experiments that illustrate our results and suggest directions for future work.",,2024-01-14,,"['jeff calder', 'nadejda drenska']"
2401.07606,redex: beyond fixed representation methods via convex optimization,cs.lg math.oc stat.ml,"optimizing neural networks is a difficult task which is still not well understood. on the other hand, fixed representation methods such as kernels and random features have provable optimization guarantees but inferior performance due to their inherent inability to learn the representations. in this paper, we aim at bridging this gap by presenting a novel architecture called redex (reduced expander extractor) that is as expressive as neural networks and can also be trained in a layer-wise fashion via a convex program with semi-definite constraints and optimization guarantees. we also show that redex provably surpasses fixed representation methods, in the sense that it can efficiently learn a family of target functions which fixed representation methods cannot.",,2024-01-15,,"['amit daniely', 'mariano schain', 'gilad yehudai']"
2401.07627,cost-sensitive feature selection for support vector machines,stat.ml cs.lg,"feature selection is a crucial procedure in data science tasks such as classification, since it identifies the relevant variables, making thus the classification procedures more interpretable, cheaper in terms of measurement and more effective by reducing noise and data overfit. the relevance of features in a classification procedure is linked to the fact that misclassifications costs are frequently asymmetric, since false positive and false negative cases may have very different consequences. however, off-the-shelf feature selection procedures seldom take into account such cost-sensitivity of errors.   in this paper we propose a mathematical-optimization-based feature selection procedure embedded in one of the most popular classification procedures, namely, support vector machines, accommodating asymmetric misclassification costs. the key idea is to replace the traditional margin maximization by minimizing the number of features selected, but imposing upper bounds on the false positive and negative rates. the problem is written as an integer linear problem plus a quadratic convex problem for support vector machines with both linear and radial kernels.   the reported numerical experience demonstrates the usefulness of the proposed feature selection procedure. indeed, our results on benchmark data sets show that a substantial decrease of the number of features is obtained, whilst the desired trade-off between false positive and false negative rates is achieved.",10.1016/j.cor.2018.03.005,2024-01-15,,"['sandra benítez-peña', 'rafael blanquero', 'emilio carrizosa', 'pepa ramírez-cobo']"
2401.07694,stochastic optimization with arbitrary recurrent data sampling,math.oc cs.lg stat.ml,"for obtaining optimal first-order convergence guarantee for stochastic optimization, it is necessary to use a recurrent data sampling algorithm that samples every data point with sufficient frequency. most commonly used data sampling algorithms (e.g., i.i.d., mcmc, random reshuffling) are indeed recurrent under mild assumptions. in this work, we show that for a particular class of stochastic optimization algorithms, we do not need any other property (e.g., independence, exponential mixing, and reshuffling) than recurrence in data sampling algorithms to guarantee the optimal rate of first-order convergence. namely, using regularized versions of minimization by incremental surrogate optimization (miso), we show that for non-convex and possibly non-smooth objective functions, the expected optimality gap converges at an optimal rate $o(n^{-1/2})$ under general recurrent sampling schemes. furthermore, the implied constant depends explicitly on the `speed of recurrence', measured by the expected amount of time to visit a given data point either averaged (`target time') or supremized (`hitting time') over the current location. we demonstrate theoretically and empirically that convergence can be accelerated by selecting sampling algorithms that cover the data set most effectively. we discuss applications of our general framework to decentralized optimization and distributed non-negative matrix factorization.",,2024-01-15,,"['william g. powell', 'hanbaek lyu']"
2401.07711,efficient nonparametric tensor decomposition for binary and count data,cs.lg stat.ml,"in numerous applications, binary reactions or event counts are observed and stored within high-order tensors. tensor decompositions (tds) serve as a powerful tool to handle such high-dimensional and sparse data. however, many traditional tds are explicitly or implicitly designed based on the gaussian distribution, which is unsuitable for discrete data. moreover, most tds rely on predefined multi-linear structures, such as cp and tucker formats. therefore, they may not be effective enough to handle complex real-world datasets. to address these issues, we propose ented, an \underline{e}fficient \underline{n}onparametric \underline{te}nsor \underline{d}ecomposition for binary and count tensors. specifically, we first employ a nonparametric gaussian process (gp) to replace traditional multi-linear structures. next, we utilize the \pg augmentation which provides a unified framework to establish conjugate models for binary and count distributions. finally, to address the computational issue of gps, we enhance the model by incorporating sparse orthogonal variational inference of inducing points, which offers a more effective covariance approximation within gps and stochastic natural gradient updates for nonparametric models. we evaluate our model on several real-world tensor completion tasks, considering binary and count datasets. the results manifest both better performance and computational advantages of the proposed model.",,2024-01-15,,"['zerui tao', 'toshihisa tanaka', 'qibin zhao']"
2401.07733,conformal approach to gaussian process surrogate evaluation with   coverage guarantees,stat.ml cs.lg,"gaussian processes (gps) are a bayesian machine learning approach widely used to construct surrogate models for the uncertainty quantification of computer simulation codes in industrial applications. it provides both a mean predictor and an estimate of the posterior prediction variance, the latter being used to produce bayesian credibility intervals. interpreting these intervals relies on the gaussianity of the simulation model as well as the well-specification of the priors which are not always appropriate. we propose to address this issue with the help of conformal prediction. in the present work, a method for building adaptive cross-conformal prediction intervals is proposed by weighting the non-conformity score with the posterior standard deviation of the gp. the resulting conformal prediction intervals exhibit a level of adaptivity akin to bayesian credibility sets and display a significant correlation with the surrogate model local approximation error, while being free from the underlying model assumptions and having frequentist coverage guarantees. these estimators can thus be used for evaluating the quality of a gp surrogate model and can assist a decision-maker in the choice of the best prior for the specific application of the gp. the performance of the method is illustrated through a panel of numerical examples based on various reference databases. moreover, the potential applicability of the method is demonstrated in the context of surrogate modeling of an expensive-to-evaluate simulator of the clogging phenomenon in steam generators of nuclear reactors.",,2024-01-15,,"['edgar jaber', 'vincent blot', 'nicolas brunel', 'vincent chabridon', 'emmanuel remy', 'bertrand iooss', 'didier lucor', 'mathilde mougeot', 'alessandro leite']"
2401.08002,discovery of generalizable tbi phenotypes using multivariate time-series   clustering,cs.lg q-bio.qm stat.ap,"traumatic brain injury (tbi) presents a broad spectrum of clinical presentations and outcomes due to its inherent heterogeneity, leading to diverse recovery trajectories and varied therapeutic responses. while many studies have delved into tbi phenotyping for distinct patient populations, identifying tbi phenotypes that consistently generalize across various settings and populations remains a critical research gap. our research addresses this by employing multivariate time-series clustering to unveil tbi's dynamic intricates. utilizing a self-supervised learning-based approach to clustering multivariate time-series data with missing values (slac-time), we analyzed both the research-centric track-tbi and the real-world mimic-iv datasets. remarkably, the optimal hyperparameters of slac-time and the ideal number of clusters remained consistent across these datasets, underscoring slac-time's stability across heterogeneous datasets. our analysis revealed three generalizable tbi phenotypes ({\alpha}, \b{eta}, and {\gamma}), each exhibiting distinct non-temporal features during emergency department visits, and temporal feature profiles throughout icu stays. specifically, phenotype {\alpha} represents mild tbi with a remarkably consistent clinical presentation. in contrast, phenotype \b{eta} signifies severe tbi with diverse clinical manifestations, and phenotype {\gamma} represents a moderate tbi profile in terms of severity and clinical diversity. age is a significant determinant of tbi outcomes, with older cohorts recording higher mortality rates. importantly, while certain features varied by age, the core characteristics of tbi manifestations tied to each phenotype remain consistent across diverse populations.",,2024-01-15,,"['hamid ghaderi', 'brandon foreman', 'chandan k. reddy', 'vignesh subbian']"
2401.08016,contextual bandits with stage-wise constraints,cs.lg stat.ml,"we study contextual bandits in the presence of a stage-wise constraint (a constraint at each round), when the constraint must be satisfied both with high probability and in expectation. obviously the setting where the constraint is in expectation is a relaxation of the one with high probability. we start with the linear case where both the contextual bandit problem (reward function) and the stage-wise constraint (cost function) are linear. in each of the high probability and in expectation settings, we propose an upper-confidence bound algorithm for the problem and prove a $t$-round regret bound for it. our algorithms balance exploration and constraint satisfaction using a novel idea that scales the radii of the reward and cost confidence sets with different scaling factors. we also prove a lower-bound for this constrained problem, show how our algorithms and analyses can be extended to multiple constraints, and provide simulations to validate our theoretical results. in the high probability setting, we describe the minimum requirements for the action set in order for our algorithm to be tractable. in the setting that the constraint is in expectation, we further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting with regret analysis. finally, we extend our results to the case where the reward and cost functions are both non-linear. we propose an algorithm for this case and prove a regret bound for it that characterize the function class complexity by the eluder dimension.",,2024-01-15,,"['aldo pacchiano', 'mohammad ghavamzadeh', 'peter bartlett']"
2401.08070,automated lag-selection for multi-step univariate time series forecast   using bayesian optimization: forecast station-wise monthly rainfall of nine   divisional cities of bangladesh,stat.ap,"rainfall is an essential hydrological component, and most of the economic activities of an agrarian country like bangladesh depend on rainfall. an accurate rainfall forecast can help make necessary decisions and reduce the damages caused by heavy or low to no rainfall. the monthly average rainfall is a time series data, and recently, long short-term memory (lstm) neural networks are being used heavily for time series forecasting problems. one major challenge of forecasting using lstms is to select the appropriate number of lag values. in this research, we considered the number of lag values selected as a hyperparameter of lstm; it, with the other hyperparameters determining lstms structure, has been optimized using bayesian optimization. we used our proposed method to forecast rainfall for nine different weather stations of bangladesh. finally, the performance of the proposed model has been compared with some other lstm with different lag-selection methods and some several popular machine learning and statistical forecasting models.",,2024-01-15,,"['rezoanoor rahman', 'fariha taskin']"
2401.08150,differentially private sliced inverse regression: minimax optimality and   algorithm,stat.ml cs.cr cs.lg math.st stat.th,"privacy preservation has become a critical concern in high-dimensional data analysis due to the growing prevalence of data-driven applications. proposed by li (1991), sliced inverse regression has emerged as a widely utilized statistical technique for reducing covariate dimensionality while maintaining sufficient statistical information. in this paper, we propose optimally differentially private algorithms specifically designed to address privacy concerns in the context of sufficient dimension reduction. we proceed to establish lower bounds for differentially private sliced inverse regression in both the low and high-dimensional settings. moreover, we develop differentially private algorithms that achieve the minimax lower bounds up to logarithmic factors. through a combination of simulations and real data analysis, we illustrate the efficacy of these differentially private algorithms in safeguarding privacy while preserving vital information within the reduced dimension space. as a natural extension, we can readily offer analogous lower and upper bounds for differentially private sparse principal component analysis, a topic that may also be of potential interest to the statistical and machine learning community.",,2024-01-16,,"['xintao xia', 'linjun zhang', 'zhanrui cai']"
2401.08167,"fundamental limits of community detection from multi-view data:   multi-layer, dynamic and partially labeled block models",math.st cs.it cs.si math.it math.pr stat.ml stat.th,"multi-view data arises frequently in modern network analysis e.g. relations of multiple types among individuals in social network analysis, longitudinal measurements of interactions among observational units, annotated networks with noisy partial labeling of vertices etc. we study community detection in these disparate settings via a unified theoretical framework, and investigate the fundamental thresholds for community recovery. we characterize the mutual information between the data and the latent parameters, provided the degrees are sufficiently large. based on this general result, (i) we derive a sharp threshold for community detection in an inhomogeneous multilayer block model \citep{chen2022global}, (ii) characterize a sharp threshold for weak recovery in a dynamic stochastic block model \citep{matias2017statistical}, and (iii) identify the limiting mutual information in an unbalanced partially labeled block model. our first two results are derived modulo coordinate-wise convexity assumptions on specific functions -- we provide extensive numerical evidence for their correctness. finally, we introduce iterative algorithms based on approximate message passing for community detection in these problems.",,2024-01-16,,"['xiaodong yang', 'buyu lin', 'subhabrata sen']"
2401.08169,statistical test for attention map in vision transformer,stat.ml cs.lg,"the vision transformer (vit) demonstrates exceptional performance in various computer vision tasks. attention is crucial for vit to capture complex wide-ranging relationships among image patches, allowing the model to weigh the importance of image patches and aiding our understanding of the decision-making process. however, when utilizing the attention of vit as evidence in high-stakes decision-making tasks such as medical diagnostics, a challenge arises due to the potential of attention mechanisms erroneously focusing on irrelevant regions. in this study, we propose a statistical test for vit's attentions, enabling us to use the attentions as reliable quantitative evidence indicators for vit's decision-making with a rigorously controlled error rate. using the framework called selective inference, we quantify the statistical significance of attentions in the form of p-values, which enables the theoretically grounded quantification of the false positive detection probability of attentions. we demonstrate the validity and the effectiveness of the proposed method through numerical experiments and applications to brain image diagnoses.",,2024-01-16,2024-01-19,"['tomohiro shiraishi', 'daiki miwa', 'teruyuki katsuoka', 'vo nguyen le duy', 'kouichi taji', 'ichiro takeuchi']"
2401.08290,causal machine learning for moderation effects,econ.em stat.ml,"it is valuable for any decision maker to know the impact of decisions (treatments) on average and for subgroups. the causal machine learning literature has recently provided tools for estimating group average treatment effects (gate) to understand treatment heterogeneity better. this paper addresses the challenge of interpreting such differences in treatment effects between groups while accounting for variations in other covariates. we propose a new parameter, the balanced group average treatment effect (bgate), which measures a gate with a specific distribution of a priori-determined covariates. by taking the difference of two bgates, we can analyse heterogeneity more meaningfully than by comparing two gates. the estimation strategy for this parameter is based on double/debiased machine learning for discrete treatments in an unconfoundedness setting, and the estimator is shown to be $\sqrt{n}$-consistent and asymptotically normal under standard conditions. adding additional identifying assumptions allows specific balanced differences in treatment effects between groups to be interpreted causally, leading to the causal balanced group average treatment effect. we explore the finite sample properties in a small-scale simulation study and demonstrate the usefulness of these parameters in an empirical example.",,2024-01-16,,"['nora bearth', 'michael lechner']"
2401.08375,sparse pca with false discovery rate controlled variable selection,stat.ml cs.lg,"sparse principal component analysis (pca) aims at mapping large dimensional data to a linear subspace of lower dimension. by imposing loading vectors to be sparse, it performs the double duty of dimension reduction and variable selection. sparse pca algorithms are usually expressed as a trade-off between explained variance and sparsity of the loading vectors (i.e., number of selected variables). as a high explained variance is not necessarily synonymous with relevant information, these methods are prone to select irrelevant variables. to overcome this issue, we propose an alternative formulation of sparse pca driven by the false discovery rate (fdr). we then leverage the terminating-random experiments (t-rex) selector to automatically determine an fdr-controlled support of the loading vectors. a major advantage of the resulting t-rex pca is that no sparsity parameter tuning is required. numerical experiments and a stock market data example demonstrate a significant performance improvement.",,2024-01-16,,"['jasin machkour', 'arnaud breloy', 'michael muma', 'daniel p. palomar', 'frédéric pascal']"
2401.08488,a novel approach in solving stochastic generalized linear regression via   nonconvex programming,stat.ml math.oc,"generalized linear regressions, such as logistic regressions or poisson regressions, are long-studied regression analysis approaches, and their applications are widely employed in various classification problems. our study considers a stochastic generalized linear regression model as a stochastic problem with chance constraints and tackles it using nonconvex programming techniques. clustering techniques and quantile estimation are also used to estimate random data's mean and variance-covariance matrix. metrics for measuring the performance of logistic regression are used to assess the model's efficacy, including the f1 score, precision score, and recall score. the results of the proposed algorithm were over 1 to 2 percent better than the ordinary logistic regression model on the same dataset with the above assessment criteria.",,2024-01-16,,"['vu duc anh', 'tran anh tuan', 'tran ngoc thang', 'nguyen thi ngoc anh']"
2401.08626,validation and comparison of non-stationary cognitive models: a   diffusion model application,q-bio.nc stat.me,"cognitive processes undergo various fluctuations and transient states across different temporal scales. superstatistics are emerging as a flexible framework for incorporating such non-stationary dynamics into existing cognitive model classes. in this work, we provide the first experimental validation of superstatistics and formal comparison of four non-stationary diffusion decision models in a specifically designed perceptual decision-making task. task difficulty and speed-accuracy trade-off were systematically manipulated to induce expected changes in model parameters. to validate our models, we assess whether the inferred parameter trajectories align with the patterns and sequences of the experimental manipulations. to address computational challenges, we present novel deep learning techniques for amortized bayesian estimation and comparison of models with time-varying parameters. our findings indicate that transition models incorporating both gradual and abrupt parameter shifts provide the best fit to the empirical data. moreover, we find that the inferred parameter trajectories closely mirror the sequence of experimental manipulations. posterior re-simulations further underscore the ability of the models to faithfully reproduce critical data patterns. accordingly, our results suggest that the inferred non-stationary dynamics may reflect actual changes in the targeted psychological constructs. we argue that our initial experimental validation paves the way for the widespread application of superstatistics in cognitive modeling and beyond.",,2023-12-07,2024-01-26,"['lukas schumacher', 'martin schnuerch', 'andreas voss', 'stefan t. radev']"
2401.08691,towards responsible ai in banking: addressing bias for fair   decision-making,stat.ml cs.cy cs.lg stat.ap,"in an era characterized by the pervasive integration of artificial intelligence into decision-making processes across diverse industries, the demand for trust has never been more pronounced. this thesis embarks on a comprehensive exploration of bias and fairness, with a particular emphasis on their ramifications within the banking sector, where ai-driven decisions bear substantial societal consequences. in this context, the seamless integration of fairness, explainability, and human oversight is of utmost importance, culminating in the establishment of what is commonly referred to as ""responsible ai"". this emphasizes the critical nature of addressing biases within the development of a corporate culture that aligns seamlessly with both ai regulations and universal human rights standards, particularly in the realm of automated decision-making systems. nowadays, embedding ethical principles into the development, training, and deployment of ai models is crucial for compliance with forthcoming european regulations and for promoting societal good. this thesis is structured around three fundamental pillars: understanding bias, mitigating bias, and accounting for bias. these contributions are validated through their practical application in real-world scenarios, in collaboration with intesa sanpaolo. this collaborative effort not only contributes to our understanding of fairness but also provides practical tools for the responsible implementation of ai-based decision-making systems. in line with open-source principles, we have released bias on demand and fairview as accessible python packages, further promoting progress in the field of ai fairness.",,2024-01-13,,['alessandro castelnovo']
2401.08702,do we really even need data?,stat.me cs.lg,"as artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g. rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as outcome variables. though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. in this paper, we characterize the statistical challenges inherent to this so-called ``inference with predicted data'' problem and elucidate three potential sources of error: (i) the relationship between predicted outcomes and their true, unobserved counterparts, (ii) robustness of the machine learning model to resampling or uncertainty about the training data, and (iii) appropriately propagating not just bias but also uncertainty from predictions into the ultimate inference procedure.",,2024-01-14,2024-02-02,"['kentaro hoffman', 'stephen salerno', 'awan afiaz', 'jeffrey t. leek', 'tyler h. mccormick']"
2401.08735,a framework for scalable ambient air pollution concentration estimation,stat.ap cs.lg,"ambient air pollution remains a critical issue in the united kingdom, where data on air pollution concentrations form the foundation for interventions aimed at improving air quality. however, the current air pollution monitoring station network in the uk is characterized by spatial sparsity, heterogeneous placement, and frequent temporal data gaps, often due to issues such as power outages. we introduce a scalable data-driven supervised machine learning model framework designed to address temporal and spatial data gaps by filling missing measurements. this approach provides a comprehensive dataset for england throughout 2018 at a 1kmx1km hourly resolution. leveraging machine learning techniques and real-world data from the sparsely distributed monitoring stations, we generate 355,827 synthetic monitoring stations across the study area, yielding data valued at approximately \pounds70 billion. validation was conducted to assess the model's performance in forecasting, estimating missing locations, and capturing peak concentrations. the resulting dataset is of particular interest to a diverse range of stakeholders engaged in downstream assessments supported by outdoor air pollution concentration data for no2, o3, pm10, pm2.5, and so2. this resource empowers stakeholders to conduct studies at a higher resolution than was previously possible.",,2024-01-16,,"['liam j berrisford', 'lucy s neal', 'helen j buttery', 'benjamin r evans', 'ronaldo menezes']"
2401.08788,the impact of differential feature under-reporting on algorithmic   fairness,cs.lg cs.cy stat.ml,"predictive risk models in the public sector are commonly developed using administrative data that is more complete for subpopulations that more greatly rely on public services. in the united states, for instance, information on health care utilization is routinely available to government agencies for individuals supported by medicaid and medicare, but not for the privately insured. critiques of public sector algorithms have identified such differential feature under-reporting as a driver of disparities in algorithmic decision-making. yet this form of data bias remains understudied from a technical viewpoint. while prior work has examined the fairness impacts of additive feature noise and features that are clearly marked as missing, the setting of data missingness absent indicators (i.e. differential feature under-reporting) has been lacking in research attention. in this work, we present an analytically tractable model of differential feature under-reporting which we then use to characterize the impact of this kind of data bias on algorithmic fairness. we demonstrate how standard missing data methods typically fail to mitigate bias in this setting, and propose a new set of methods specifically tailored to differential feature under-reporting. our results show that, in real world data settings, under-reporting typically leads to increasing disparities. the proposed solution methods show success in mitigating increases in unfairness.",,2024-01-16,,"['nil-jana akpinar', 'zachary c. lipton', 'alexandra chouldechova']"
2401.08865,the effect of intrinsic dataset properties on generalization: unraveling   learning differences between natural and medical images,cs.cv cs.lg eess.iv stat.ml,"this paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. we address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic ``label sharpness'' ($k_\mathcal{f}$) of medical imaging datasets, a metric which we propose. next, we demonstrate an additional benefit of measuring the label sharpness of a training set: it is negatively correlated with the trained model's adversarial robustness, which notably leads to models for medical images having a substantially higher vulnerability to adversarial attack. finally, we extend our $d_{data}$ formalism to the related metric of learned representation intrinsic dimension ($d_{repr}$), derive a generalization scaling law with respect to $d_{repr}$, and show that $d_{data}$ serves as an upper bound for $d_{repr}$. our theoretical results are supported by thorough experiments with six models and eleven natural and medical imaging datasets over a range of training set sizes. our findings offer insights into the influence of intrinsic dataset properties on generalization, representation learning, and robustness in deep neural networks. code link: https://github.com/mazurowski-lab/intrinsic-properties",,2024-01-16,2024-02-21,"['nicholas konz', 'maciej a. mazurowski']"
2401.08978,trade-off between dependence and complexity for nonparametric learning   -- an empirical process approach,math.st stat.ml stat.th,"empirical process theory for i.i.d. observations has emerged as a ubiquitous tool for understanding the generalization properties of various statistical problems. however, in many applications where the data exhibit temporal dependencies (e.g., in finance, medical imaging, weather forecasting etc.), the corresponding empirical processes are much less understood. motivated by this observation, we present a general bound on the expected supremum of empirical processes under standard $\beta/\rho$-mixing assumptions. unlike most prior work, our results cover both the long and the short-range regimes of dependence. our main result shows that a non-trivial trade-off between the complexity of the underlying function class and the dependence among the observations characterizes the learning rate in a large class of nonparametric problems. this trade-off reveals a new phenomenon, namely that even under long-range dependence, it is possible to attain the same rates as in the i.i.d. setting, provided the underlying function class is complex enough. we demonstrate the practical implications of our findings by analyzing various statistical estimators in both fixed and growing dimensions. our main examples include a comprehensive case study of generalization error bounds in nonparametric regression over smoothness classes in fixed as well as growing dimension using neural nets, shape-restricted multivariate convex regression, estimating the optimal transport (wasserstein) distance between two probability distributions, and classification under the mammen-tsybakov margin condition -- all under appropriate mixing assumptions. in the process, we also develop bounds on $l_r$ ($1\le r\le 2$)-localized empirical processes with dependent observations, which we then leverage to get faster rates for (a) tuning-free adaptation, and (b) set-structured learning problems.",,2024-01-17,,"['nabarun deb', 'debarghya mukherjee']"
2401.09016,fast parallel sampling under isoperimetry,cs.ds math.st stat.ml stat.th,"we show how to sample in parallel from a distribution $\pi$ over $\mathbb r^d$ that satisfies a log-sobolev inequality and has a smooth log-density, by parallelizing the langevin (resp. underdamped langevin) algorithms. we show that our algorithm outputs samples from a distribution $\hat\pi$ that is close to $\pi$ in kullback--leibler (kl) divergence (resp. total variation (tv) distance), while using only $\log(d)^{o(1)}$ parallel rounds and $\widetilde{o}(d)$ (resp. $\widetilde o(\sqrt d)$) gradient evaluations in total. this constitutes the first parallel sampling algorithms with tv distance guarantees.   for our main application, we show how to combine the tv distance guarantees of our algorithms with prior works and obtain rnc sampling-to-counting reductions for families of discrete distribution on the hypercube $\{\pm 1\}^n$ that are closed under exponential tilts and have bounded covariance. consequently, we obtain an rnc sampler for directed eulerian tours and asymmetric determinantal point processes, resolving open questions raised in prior works.",,2024-01-17,,"['nima anari', 'sinho chewi', 'thuy-duong vuong']"
2401.09028,a novel interpretable fusion analytic framework for investigating   functional brain connectivity differences in cognitive impairments,stat.ap,"functional magnetic resonance imaging (fmri) data is characterized by its complexity and high--dimensionality, encompassing signals from various regions of interests (rois) that exhibit intricate correlations. analyzing fmri data directly proves challenging due to its intricate structure. nevertheless, rois convey crucial information about brain activities through their connections, offering insights into distinctive brain activity characteristics between different groups. to address this, we propose a cutting-edge interpretable fusion analytic framework that facilitates the identification and understanding of roi connectivity disparities between two groups, thereby revealing their unique features. our novel approach encompasses three key steps. firstly, we construct roi functional connectivity networks (fcns) to effectively manage fmri data. secondly, employing the fcns, we utilize a self--attention deep learning model for binary classification, generating an attention distribution that encodes group differences. lastly, we employ a latent space item-response model to extract group representative roi features, visualizing these features on the group summary fcns. we validate the effectiveness of our framework by analyzing four types of cognitive impairments, showcasing its capability to identify significant rois contributing to the differences between the two disease groups. this novel interpretable fusion analytic framework holds immense potential for advancing our understanding of cognitive impairments and could pave the way for more targeted therapeutic interventions.",,2024-01-17,,"['yeseul jeon', 'jeong-jae kim', 'sumin yu', 'junggu choi', 'sanghoon han']"
2401.09073,fixed-budget differentially private best arm identification,cs.lg cs.ai cs.it math.it math.st stat.ml stat.th,"we study best arm identification (bai) in linear bandits in the fixed-budget regime under differential privacy constraints, when the arm rewards are supported on the unit interval. given a finite budget $t$ and a privacy parameter $\varepsilon>0$, the goal is to minimise the error probability in finding the arm with the largest mean after $t$ sampling rounds, subject to the constraint that the policy of the decision maker satisfies a certain {\em $\varepsilon$-differential privacy} ($\varepsilon$-dp) constraint. we construct a policy satisfying the $\varepsilon$-dp constraint (called {\sc dp-bai}) by proposing the principle of {\em maximum absolute determinants}, and derive an upper bound on its error probability. furthermore, we derive a minimax lower bound on the error probability, and demonstrate that the lower and the upper bounds decay exponentially in $t$, with exponents in the two bounds matching order-wise in (a) the sub-optimality gaps of the arms, (b) $\varepsilon$, and (c) the problem complexity that is expressible as the sum of two terms, one characterising the complexity of standard fixed-budget bai (without privacy constraints), and the other accounting for the $\varepsilon$-dp constraint. additionally, we present some auxiliary results that contribute to the derivation of the lower bound on the error probability. these results, we posit, may be of independent interest and could prove instrumental in proving lower bounds on error probabilities in several other bandit problems. whereas prior works provide results for bai in the fixed-budget regime without privacy constraints or in the fixed-confidence regime with privacy constraints, our work fills the gap in the literature by providing the results for bai in the fixed-budget regime under the $\varepsilon$-dp constraint.",,2024-01-17,,"['zhirui chen', 'p. n. karthik', 'yeow meng chee', 'vincent y. f. tan']"
2401.09125,understanding heterophily for graph neural networks,cs.lg stat.ml,"graphs with heterophily have been regarded as challenging scenarios for graph neural networks (gnns), where nodes are connected with dissimilar neighbors through various patterns. in this paper, we present theoretical understandings of the impacts of different heterophily patterns for gnns by incorporating the graph convolution (gc) operations into fully connected networks via the proposed heterophilous stochastic block models (hsbm), a general random graph model that can accommodate diverse heterophily patterns. firstly, we show that by applying a gc operation, the separability gains are determined by two factors, i.e., the euclidean distance of the neighborhood distributions and $\sqrt{\mathbb{e}\left[\operatorname{deg}\right]}$, where $\mathbb{e}\left[\operatorname{deg}\right]$ is the averaged node degree. it reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. secondly, we show that the topological noise has a detrimental impact on separability, which is equivalent to degrading $\mathbb{e}\left[\operatorname{deg}\right]$. finally, when applying multiple gc operations, we show that the separability gains are determined by the normalized distance of the $l$-powered neighborhood distributions. it indicates that the nodes still possess separability as $l$ goes to infinity in a wide range of regimes. extensive experiments on both synthetic and real-world data verify the effectiveness of our theory.",,2024-01-17,,"['junfu wang', 'yuanfang guo', 'liang yang', 'yunhong wang']"
2401.09144,monitoring machine learning forecasts for platform data streams,stat.ap stat.ml,"data stream forecasts are essential inputs for decision making at digital platforms. machine learning algorithms are appealing candidates to produce such forecasts. yet, digital platforms require a large-scale forecast framework that can flexibly respond to sudden performance drops. re-training ml algorithms at the same speed as new data batches enter is usually computationally too costly. on the other hand, infrequent re-training requires specifying the re-training frequency and typically comes with a severe cost of forecast deterioration. to ensure accurate and stable forecasts, we propose a simple data-driven monitoring procedure to answer the question when the ml algorithm should be re-trained. instead of investigating instability of the data streams, we test if the incoming streaming forecast loss batch differs from a well-defined reference batch. using a novel dataset constituting 15-min frequency data streams from an on-demand logistics platform operating in london, we apply the monitoring procedure to popular ml algorithms including random forest, xgboost and lasso. we show that monitor-based re-training produces accurate forecasts compared to viable benchmarks while preserving computational feasibility. moreover, the choice of monitoring procedure is more important than the choice of ml algorithm, thereby permitting practitioners to combine the proposed monitoring procedure with one's favorite forecasting algorithm.",,2024-01-17,,"['jeroen rombouts', 'ines wilms']"
2401.09184,a two-scale complexity measure for deep learning models,stat.ml cs.lg,"we introduce a novel capacity measure 2sed for statistical models based on the effective dimension. the new quantity provably bounds the generalization error under mild assumptions on the model. furthermore, simulations on standard data sets and popular model architectures show that 2sed correlates well with the training error. for markovian models, we show how to efficiently approximate 2sed from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. simulation results suggest that the approximation is good for different prominent models and data sets.",,2024-01-17,,"['massimiliano datres', 'gian paolo leonardi', 'alessio figalli', 'david sutter']"
2401.09191,an optimal transport approach for computing adversarial training lower   bounds in multiclass classification,cs.lg math.oc stat.ml,"despite the success of deep learning-based algorithms, it is widely known that neural networks may fail to be robust. a popular paradigm to enforce robustness is adversarial training (at), however, this introduces many computational and theoretical difficulties. recent works have developed a connection between at in the multiclass classification setting and multimarginal optimal transport (mot), unlocking a new set of tools to study this problem. in this paper, we leverage the mot connection to propose computationally tractable numerical algorithms for computing universal lower bounds on the optimal adversarial risk and identifying optimal classifiers. we propose two main algorithms based on linear programming (lp) and entropic regularization (sinkhorn). our key insight is that one can harmlessly truncate the higher order interactions between classes, preventing the combinatorial run times typically encountered in mot problems. we validate these results with experiments on mnist and cifar-$10$, which demonstrate the tractability of our approach.",,2024-01-17,,"['nicolas garcia trillos', 'matt jacobs', 'jakwang kim', 'matthew werenski']"
2401.09259,mitigating distribution shift in machine learning-augmented hybrid   simulation,math.na cs.na math.ds stat.ml,"we study the problem of distribution shift generally arising in machine-learning augmented hybrid simulation, where parts of simulation algorithms are replaced by data-driven surrogates. we first establish a mathematical framework to understand the structure of machine-learning augmented hybrid simulation problems, and the cause and effect of the associated distribution shift. we show correlations between distribution shift and simulation error both numerically and theoretically. then, we propose a simple methodology based on tangent-space regularized estimator to control the distribution shift, thereby improving the long-term accuracy of the simulation results. in the linear dynamics case, we provide a thorough theoretical analysis to quantify the effectiveness of the proposed method. moreover, we conduct several numerical experiments, including simulating a partially known reaction-diffusion equation and solving navier-stokes equations using the projection method with a data-driven pressure solver. in all cases, we observe marked improvements in simulation accuracy under the proposed method, especially for systems with high degrees of distribution shift, such as those with relatively strong non-linear reaction mechanisms, or flows at large reynolds numbers.",,2024-01-17,,"['jiaxi zhao', 'qianxiao li']"
2401.09339,central limit theorem for two-timescale stochastic approximation with   markovian noise: theory and applications,stat.ml cs.lg math.oc,"two-timescale stochastic approximation (ttsa) is among the most general frameworks for iterative stochastic algorithms. this includes well-known stochastic optimization methods such as sgd variants and those designed for bilevel or minimax problems, as well as reinforcement learning like the family of gradient-based temporal difference (gtd) algorithms. in this paper, we conduct an in-depth asymptotic analysis of ttsa under controlled markovian noise via central limit theorem (clt), uncovering the coupled dynamics of ttsa influenced by the underlying markov chain, which has not been addressed by previous clt results of ttsa only with martingale difference noise. building upon our clt, we expand its application horizon of efficient sampling strategies from vanilla sgd to a wider ttsa context in distributed learning, thus broadening the scope of hu et al. (2022). in addition, we leverage our clt result to deduce the statistical properties of gtd algorithms with nonlinear function approximation using markovian samples and show their identical asymptotic performance, a perspective not evident from current finite-time bounds.",,2024-01-17,2024-02-13,"['jie hu', 'vishwaraj doshi', 'do young eun']"
2401.09346,high confidence level inference is almost free using parallel stochastic   optimization,stat.ml cs.lg,"uncertainty quantification for estimation through stochastic optimization solutions in an online setting has gained popularity recently. this paper introduces a novel inference method focused on constructing confidence intervals with efficient computation and fast convergence to the nominal level. specifically, we propose to use a small number of independent multi-runs to acquire distribution information and construct a t-based confidence interval. our method requires minimal additional computation and memory beyond the standard updating of estimates, making the inference process almost cost-free. we provide a rigorous theoretical guarantee for the confidence interval, demonstrating that the coverage is approximately exact with an explicit convergence rate and allowing for high confidence level inference. in particular, a new gaussian approximation result is developed for the online estimators to characterize the coverage properties of our confidence intervals in terms of relative errors. additionally, our method also allows for leveraging parallel computing to further accelerate calculations using multiple cores. it is easy to implement and can be integrated with existing stochastic algorithms without the need for complicated modifications.",,2024-01-17,,"['wanrong zhu', 'zhipeng lou', 'ziyang wei', 'wei biao wu']"
2401.09376,unlocking unlabeled data: ensemble learning with the hui- walter   paradigm for performance estimation in online and static settings,cs.lg math.st stat.ml stat.th,"in the realm of machine learning and statistical modeling, practitioners often work under the assumption of accessible, static, labeled data for evaluation and training. however, this assumption often deviates from reality where data may be private, encrypted, difficult- to-measure, or unlabeled. in this paper, we bridge this gap by adapting the hui-walter paradigm, a method traditionally applied in epidemiology and medicine, to the field of machine learning. this approach enables us to estimate key performance metrics such as false positive rate, false negative rate, and priors in scenarios where no ground truth is available. we further extend this paradigm for handling online data, opening up new possibilities for dynamic data environments. our methodology involves partitioning data into latent classes to simulate multiple data populations (if natural populations are unavailable) and independently training models to replicate multiple tests. by cross-tabulating binary outcomes across ensemble categorizers and multiple populations, we are able to estimate unknown parameters through gibbs sampling, eliminating the need for ground-truth or labeled data. this paper showcases the potential of our methodology to transform machine learning practices by allowing for accurate model assessment under dynamic and uncertain data conditions.",,2024-01-17,,"['kevin slote', 'elaine lee']"
2401.09415,randomized kaczmarz with geometrically smoothed momentum,math.na cs.na math.pr stat.ml,"this paper studies the effect of adding geometrically smoothed momentum to the randomized kaczmarz algorithm, which is an instance of stochastic gradient descent on a linear least squares loss function. we prove a result about the expected error in the direction of singular vectors of the matrix defining the least squares loss. we present several numerical examples illustrating the utility of our result and pose several questions.",,2024-01-17,,"['seth j. alderman', 'roan w. luikart', 'nicholas f. marshall']"
2401.09435,reasoning with random sets: an agenda for the future,math.st cs.ai stat.ml stat.th,"in this paper, we discuss a potential agenda for future work in the theory of random sets and belief functions, touching upon a number of focal issues: the development of a fully-fledged theory of statistical reasoning with random sets, including the generalisation of logistic regression and of the classical laws of probability; the further development of the geometric approach to uncertainty, to include general random sets, a wider range of uncertainty measures and alternative geometric representations; the application of this new theory to high-impact areas such as climate change, machine learning and statistical learning theory.",,2023-12-19,,['fabio cuzzolin']
2401.09456,parametric constraints for bayesian knowledge tracing from first   principles,cs.cy cs.lg stat.ml,"bayesian knowledge tracing (bkt) is a probabilistic model of a learner's state of mastery corresponding to a knowledge component. it considers the learner's state of mastery as a ""hidden"" or latent binary variable and updates this state based on the observed correctness of the learner's response using parameters that represent transition probabilities between states. bkt is often represented as a hidden markov model and the expectation-maximization (em) algorithm is used to infer these parameters. however, this algorithm can suffer from several issues including producing multiple viable sets of parameters, settling into a local minima, producing degenerate parameter values, and a high computational cost during fitting. this paper takes a ""from first principles"" approach to deriving constraints that can be imposed on the bkt parameter space. starting from the basic mathematical truths of probability and building up to the behaviors expected of the bkt parameters in real systems, this paper presents a mathematical derivation that results in succinct constraints that can be imposed on the bkt parameter space. since these constraints are necessary conditions, they can be applied prior to fitting in order to reduce computational cost and the likelihood of issues that can emerge from the em procedure. in order to see that promise through, the paper further introduces a novel algorithm for estimating bkt parameters subject to the newly defined constraints. while the issue of degenerate parameter values has been reported previously, this paper is the first, to our best knowledge, to derive the constrains from first principles while also presenting an algorithm that respects those constraints.",,2023-12-22,,"['denis shchepakin', 'sreecharan sankaranarayanan', 'dawn zimmaro']"
2401.09492,uncertainty-aware calibration of a hot-wire anemometer with gaussian   process regression,cs.lg stat.ap stat.ml,"expensive ultrasonic anemometers are usually required to measure wind speed accurately. the aim of this work is to overcome the loss of accuracy of a low cost hot-wire anemometer caused by the changes of air temperature, by means of a probabilistic calibration using gaussian process regression. gaussian process regression is a non-parametric, bayesian, and supervised learning method designed to make predictions of an unknown target variable as a function of one or more known input variables. our approach is validated against real datasets, obtaining a good performance in inferring the actual wind speed values. by performing, before its real use in the field, a calibration of the hot-wire anemometer taking into account air temperature, permits that the wind speed can be estimated for the typical range of ambient temperatures, including a grounded uncertainty estimation for each speed measure.",10.1109/jsen.2019.2915093,2024-01-16,,"['rubén antonio garcía-ruiz', 'josé luis blanco-claraco', 'javier lópez-martínez', 'ángel jesús callejón-ferre']"
2401.09499,functional autoencoder for smoothing and representation learning,cs.lg stat.ml,"a common pipeline in functional data analysis is to first convert the discretely observed data to smooth functions, and then represent the functions by a finite-dimensional vector of coefficients summarizing the information. existing methods for data smoothing and dimensional reduction mainly focus on learning the linear mappings from the data space to the representation space, however, learning only the linear representations may not be sufficient. in this study, we propose to learn the nonlinear representations of functional data using neural network autoencoders designed to process data in the form it is usually collected without the need of preprocessing. we design the encoder to employ a projection layer computing the weighted inner product of the functional data and functional weights over the observed timestamp, and the decoder to apply a recovery layer that maps the finite-dimensional vector extracted from the functional data back to functional space using a set of predetermined basis functions. the developed architecture can accommodate both regularly and irregularly spaced data. our experiments demonstrate that the proposed method outperforms functional principal component analysis in terms of prediction and classification, and maintains superior smoothing ability and better computational efficiency in comparison to the conventional autoencoders under both linear and nonlinear settings.",,2024-01-17,,"['sidi wu', 'cédric beaulac', 'jiguo cao']"
2401.09602,evaluating tree-based imputation methods as an alternative to mice pmm   for drawing inference in empirical studies,stat.ap stat.ml,"dealing with missing data is an important problem in statistical analysis that is often addressed with imputation procedures. the performance and validity of such methods are of great importance for their application in empirical studies. while the prevailing method of multiple imputation by chained equations (mice) with predictive mean matching (pmm) is considered standard in the social science literature, the increase in complex datasets may require more advanced approaches based on machine learning. in particular, tree-based imputation methods have emerged as very competitive approaches. however, the performance and validity are not completely understood, particularly compared to the standard mice pmm. this is especially true for inference in linear models. in this study, we investigate the impact of various imputation methods on coefficient estimation, type i error, and power, to gain insights that can help empirical researchers deal with missingness more effectively. we explore mice pmm alongside different tree-based methods, such as mice with random forest (rf), chained random forests with and without pmm (missranger), and extreme gradient boosting (mixgboost), conducting a realistic simulation study using the german national educational panel study (neps) as the original data source. our results reveal that random forest-based imputations, especially mice rf and missranger with pmm, consistently perform better in most scenarios. standard mice pmm shows partially increased bias and overly conservative test decisions, particularly with non-true zero coefficients. our results thus underscore the potential advantages of tree-based imputation methods, albeit with a caveat that all methods perform worse with an increased missingness, particularly missranger.",,2024-01-17,,"['jakob schwerter', 'ketevan gurtskaia', 'andrés romero', 'birgit zeyer-gliozzo', 'markus pauly']"
2401.09629,multiple locally linear kernel machines,cs.lg stat.ml,"in this paper we propose a new non-linear classifier based on a combination of locally linear classifiers. a well known optimization formulation is given as we cast the problem in a $\ell_1$ multiple kernel learning (mkl) problem using many locally linear kernels. since the number of such kernels is huge, we provide a scalable generic mkl training algorithm handling streaming kernels. with respect to the inference time, the resulting classifier fits the gap between high accuracy but slow non-linear classifiers (such as classical mkl) and fast but low accuracy linear classifiers.",,2024-01-17,,['david picard']
2401.09660,data-driven assessment of the county-level breast cancer incidence in   the united states: impacts of modifiable and non-modifiable factors,stat.ap,"female breast cancer (fbc) incidence rate (ir) varies greatly by counties across the united states (us). factors responsible for such high spatial disparities are not well understood, making it challenging to design effective intervention strategies. we predicted fbc irs using prevailing machine learning techniques for 1,754 us counties with a female population over 10,000. outlier counties with the unexpectedly high or low fbc irs were identified by controlling the non-modifiable factors (demographics and socioeconomics). impacts of the modifiable factors (lifestyle, healthcare accessibility, and environment) were mapped. our study also shed light on hidden fbc risk factors at the regional scale. methods developed in our study may be used to discover the place-specific, population-level, modifiable factors for the intervention of other types of cancer or chronic diseases.",,2024-01-17,,"['tingting zhao', 'qing han', 'jinfeng zhang']"
2401.09681,harnessing density ratios for online reinforcement learning,cs.lg stat.ml,"the theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. however, the notion of density ratio modeling, an emerging paradigm in offline rl, has been largely absent from online rl, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online rl is to collect such a dataset without having one to start. in this work we show -- perhaps surprisingly -- that density ratio-based algorithms have online counterparts. assuming only the existence of an exploratory distribution with good coverage, a structural condition known as coverability (xie et al., 2023), we give a new algorithm (glow) that uses density ratio realizability and value function realizability to perform sample-efficient online exploration. glow addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration. glow is computationally inefficient; we complement it with a more efficient counterpart, hyglow, for the hybrid rl setting (song et al., 2022) wherein online rl is augmented with additional offline data. hyglow is derived as a special case of a more general meta-algorithm that provides a provable black-box reduction from hybrid rl to offline rl, which may be of independent interest.",,2024-01-17,,"['philip amortila', 'dylan j. foster', 'nan jiang', 'ayush sekhari', 'tengyang xie']"
2401.09787,querying easily flip-flopped samples for deep active learning,cs.lg cs.ai stat.ml,"active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. one effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. the sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. to address this issue, this paper proposes the {\it least disagree metric} (ldm), defined as the smallest probability of disagreement of the predicted label, and an estimator for ldm proven to be asymptotically consistent under mild assumptions. the estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. the ldm-based active learning is performed by querying unlabeled data with the smallest ldm. experimental results show that our ldm-based active learning algorithm obtains state-of-the-art overall performance on all considered datasets and deep architectures.",,2024-01-18,,"['seong jin cho', 'gwangsu kim', 'junghyun lee', 'jinwoo shin', 'chang d. yoo']"
2401.09840,freed++: improving rl agents for fragment-based molecule generation by   thorough reproduction,q-bio.bm cs.lg stat.ml,"a rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. molecular docking is a common technique for evaluating protein-molecule interactions. recently, reinforcement learning (rl) has emerged as a promising approach to generating molecules with the docking score (ds) as a reward. in this work, we reproduce, scrutinize and improve the recent rl model for molecule generation called freed (arxiv:2110.01219). extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. we show that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches.",,2024-01-18,,"['alexander telepov', 'artem tsypin', 'kuzma khrabrov', 'sergey yakukhnov', 'pavel strashnov', 'petr zhilyaev', 'egor rumiantsev', 'daniel ezhov', 'manvel avetisian', 'olga popova', 'artur kadurin']"
2401.09979,false discovery rate control for gaussian graphical models via   neighborhood screening,stat.ml cs.lg,"gaussian graphical models emerge in a wide range of fields. they model the statistical relationships between variables as a graph, where an edge between two variables indicates conditional dependence. unfortunately, well-established estimators, such as the graphical lasso or neighborhood selection, are known to be susceptible to a high prevalence of false edge detections. false detections may encourage inaccurate or even incorrect scientific interpretations, with major implications in applications, such as biomedicine or healthcare. in this paper, we introduce a nodewise variable selection approach to graph learning and provably control the false discovery rate of the selected edge set at a self-estimated level. a novel fusion method of the individual neighborhoods outputs an undirected graph estimate. the proposed method is parameter-free and does not require tuning by the user. benchmarks against competing false discovery rate controlling methods in numerical experiments considering different graph topologies show a significant gain in performance.",,2024-01-18,,"['taulant koka', 'jasin machkour', 'michael muma']"
2401.10204,maximal-capacity discrete memoryless channel identification,cs.it math.it stat.ml,"the problem of identifying the channel with the highest capacity among several discrete memoryless channels (dmcs) is considered. the problem is cast as a pure-exploration multi-armed bandit problem, which follows the practical use of training sequences to sense the communication channel statistics. a capacity estimator is proposed and tight confidence bounds on the estimator error are derived. based on this capacity estimator, a gap-elimination algorithm termed bestchanid is proposed, which is oblivious to the capacity-achieving input distribution and is guaranteed to output the dmc with the largest capacity, with a desired confidence. furthermore, two additional algorithms naivechansel and medianchanel, that output with certain confidence a dmc with capacity close to the maximal, are introduced. each of those algorithms is beneficial in a different regime and can be used as a subroutine in bestchanid. the sample complexity of all algorithms is analyzed as a function of the desired confidence parameter, the number of channels, and the channels' input and output alphabet sizes. the cost of best channel identification is shown to scale quadratically with the alphabet size, and a fundamental lower bound for the required number of channel senses to identify the best channel with a certain confidence is derived.",,2024-01-18,,"['maximilian egger', 'rawad bitar', 'antonia wachter-zeh', 'deniz gündüz', 'nir weinberger']"
2401.10566,robust multi-modal density estimation,cs.lg stat.ml,"development of multi-modal, probabilistic prediction models has lead to a need for comprehensive evaluation metrics. while several metrics can characterize the accuracy of machine-learned models (e.g., negative log-likelihood, jensen-shannon divergence), these metrics typically operate on probability densities. applying them to purely sample-based prediction models thus requires that the underlying density function is estimated. however, common methods such as kernel density estimation (kde) have been demonstrated to lack robustness, while more complex methods have not been evaluated in multi-modal estimation problems. in this paper, we present rome (robust multi-modal density estimator), a non-parametric approach for density estimation which addresses the challenge of estimating multi-modal, non-normal, and highly correlated distributions. rome utilizes clustering to segment a multi-modal set of samples into multiple uni-modal ones and then combines simple kde estimates obtained for individual clusters in a single multi-modal estimate. we compared our approach to state-of-the-art methods for density estimation as well as ablations of rome, showing that it not only outperforms established methods but is also more robust to a variety of distributions. our results demonstrate that rome can overcome the issues of over-fitting and over-smoothing exhibited by other estimators, promising a more robust evaluation of probabilistic machine learning models.",,2024-01-19,,"['anna mészáros', 'julian f. schumann', 'javier alonso-mora', 'arkady zgonnikov', 'jens kober']"
2401.10791,early alignment in two-layer networks training is a two-edged sword,cs.lg stat.ml,"training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. the scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. this work provides a general and quantitative description of the early alignment phase, originally introduced by maennel et al. (2018) . for small initialisation and one hidden relu layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. this alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. this sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and only converge to a spurious stationary point instead.",,2024-01-19,,"['etienne boursier', 'nicolas flammarion']"
2401.10796,reliability analysis for data-driven noisy models using active learning,stat.co stat.ap stat.me,"reliability analysis aims at estimating the failure probability of an engineering system. it often requires multiple runs of a limit-state function, which usually relies on computationally intensive simulations. traditionally, these simulations have been considered deterministic, i.e., running them multiple times for a given set of input parameters always produces the same output. however, this assumption does not always hold, as many studies in the literature report non-deterministic computational simulations (also known as noisy models). in such cases, running the simulations multiple times with the same input will result in different outputs. similarly, data-driven models that rely on real-world data may also be affected by noise. this characteristic poses a challenge when performing reliability analysis, as many classical methods, such as form and sorm, are tailored to deterministic models. to bridge this gap, this paper provides a novel methodology to perform reliability analysis on models contaminated by noise. in such cases, noise introduces latent uncertainty into the reliability estimator, leading to an incorrect estimation of the real underlying reliability index, even when using monte carlo simulation. to overcome this challenge, we propose the use of denoising regression-based surrogate models within an active learning reliability analysis framework. specifically, we combine gaussian process regression with a noise-aware learning function to efficiently estimate the probability of failure of the underlying noise-free model. we showcase the effectiveness of this methodology on standard benchmark functions and a finite element model of a realistic structural frame.",,2024-01-19,,"['anderson v. pires', 'maliki moustapha', 'stefano marelli', 'bruno sudret']"
2401.10811,simulation based bayesian optimization,stat.ml cs.lg,"bayesian optimization (bo) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations. bo constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function. for smooth continuous search spaces, gaussian processes (gps) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions. however, in complex scenarios involving optimizations over categorical or mixed covariate spaces, gps may not be ideal.   this paper introduces simulation based bayesian optimization (sbbo) as a novel approach to optimizing acquisition functions that only requires \emph{sampling-based} access to posterior predictive distributions. sbbo allows the use of surrogate probabilistic models tailored for combinatorial spaces with discrete variables. any bayesian model in which posterior inference is carried out through markov chain monte carlo can be selected as the surrogate model in sbbo. in applications involving combinatorial optimization, we demonstrate empirically the effectiveness of sbbo method using various choices of surrogate models.",,2024-01-19,,"['roi naveiro', 'becky tang']"
2401.10867,learning optimal dynamic treatment regimes from longitudinal data,stat.me,"studies often report estimates of the average treatment effect. while the ate summarizes the effect of a treatment on average, it does not provide any information about the effect of treatment within any individual. a treatment strategy that uses an individual's information to tailor treatment to maximize benefit is known as an optimal dynamic treatment rule. treatment, however, is typically not limited to a single point in time; consequently, learning an optimal rule for a time-varying treatment may involve not just learning the extent to which the comparative treatments' benefits vary across the characteristics of individuals, but also learning the extent to which the comparative treatments' benefits vary as relevant circumstances evolve within an individual. the goal of this paper is to provide a tutorial for estimating odtr from longitudinal observational and clinical trial data for applied researchers. we describe an approach that uses a doubly-robust unbiased transformation of the conditional average treatment effect. we then learn a time-varying odtr for when to increase buprenorphine-naloxone dose to minimize return-to-regular-opioid-use among patients with opioid use disorder. our analysis highlights the utility of odtrs in the context of sequential decision making: the learned odtr outperforms a clinically defined strategy.",,2024-01-19,,"['nicholas t. williams', 'katherine l. hoffman iván díaz', 'kara e. rudolph']"
2401.10923,online estimation of the inverse of the hessian for stochastic   optimization with application to universal stochastic newton algorithms,math.oc stat.ml,"this paper addresses second-order stochastic optimization for estimating the minimizer of a convex function written as an expectation. a direct recursive estimation technique for the inverse hessian matrix using a robbins-monro procedure is introduced. this approach enables to drastically reduces computational complexity. above all, it allows to develop universal stochastic newton methods and investigate the asymptotic efficiency of the proposed approach. this work so expands the application scope of secondorder algorithms in stochastic optimization.",,2024-01-15,,"['antoine godichon-baggioni', 'wei lu', 'bruno portier']"
2401.10927,debiasing and a local analysis for population clustering using   semidefinite programming,stat.ml cs.lg,"in this paper, we consider the problem of partitioning a small data sample of size $n$ drawn from a mixture of $2$ sub-gaussian distributions. in particular, we analyze computational efficient algorithms proposed by the same author, to partition data into two groups approximately according to their population of origin given a small sample. this work is motivated by the application of clustering individuals according to their population of origin using $p$ markers, when the divergence between any two of the populations is small. we build upon the semidefinite relaxation of an integer quadratic program that is formulated essentially as finding the maximum cut on a graph, where edge weights in the cut represent dissimilarity scores between two nodes based on their $p$ features. here we use $\delta^2 :=p \gamma$ to denote the $\ell_2^2$ distance between two centers (mean vectors), namely, $\mu^{(1)}$, $\mu^{(2)}$ $\in$ $\mathbb{r}^p$. the goal is to allow a full range of tradeoffs between $n, p, \gamma$ in the sense that partial recovery (success rate $< 100\%$) is feasible once the signal to noise ratio $s^2 := \min\{np \gamma^2, \delta^2\}$ is lower bounded by a constant. importantly, we prove that the misclassification error decays exponentially with respect to the snr $s^2$. this result was introduced earlier without a full proof. we therefore present the full proof in the present work. finally, for balanced partitions, we consider a variant of the sdp1, and show that the new estimator has a superb debiasing property. this is novel to the best of our knowledge.",,2024-01-15,,['shuheng zhou']
2401.10989,provably scalable black-box variational inference with structured   variational families,stat.ml cs.lg stat.co,"variational families with full-rank covariance approximations are known not to work well in black-box variational inference (bbvi), both empirically and theoretically. in fact, recent computational complexity results for bbvi have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean field families. this is particularly critical to hierarchical bayesian models with local variables; their dimensionality increases with the size of the datasets. consequently, one gets an iteration complexity with an explicit $\mathcal{o}(n^2)$ dependence on the dataset size $n$. in this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. we rigorously prove that certain scale matrix structures can achieve a better iteration complexity of $\mathcal{o}(n)$, implying better scaling with respect to $n$. we empirically verify our theoretical results on large-scale hierarchical models.",,2024-01-19,,"['joohwan ko', 'kyurae kim', 'woo chang kim', 'jacob r. gardner']"
2401.11081,learning from aggregate responses: instance level versus bag level loss   functions,cs.lg cs.ai math.st stat.ml stat.th,"due to the rise of privacy concerns, in many practical applications the training data is aggregated before being shared with the learner, in order to protect privacy of users' sensitive responses. in an aggregate learning framework, the dataset is grouped into bags of samples, where each bag is available only with an aggregate response, providing a summary of individuals' responses in that bag. in this paper, we study two natural loss functions for learning from aggregate responses: bag-level loss and the instance-level loss. in the former, the model is learnt by minimizing a loss between aggregate responses and aggregate model predictions, while in the latter the model aims to fit individual predictions to the aggregate responses. in this work, we show that the instance-level loss can be perceived as a regularized form of the bag-level loss. this observation lets us compare the two approaches with respect to bias and variance of the resulting estimators, and introduce a novel interpolating estimator which combines the two approaches. for linear regression tasks, we provide a precise characterization of the risk of the interpolating estimator in an asymptotic regime where the size of the training set grows in proportion to the features dimension. our analysis allows us to theoretically understand the effect of different factors, such as bag size on the model prediction risk. in addition, we propose a mechanism for differentially private learning from aggregate responses and derive the optimal bag size in terms of prediction risk-privacy trade-off. we also carry out thorough experiments to corroborate our theory and show the efficacy of the interpolating estimator.",,2024-01-19,,"['adel javanmard', 'lin chen', 'vahab mirrokni', 'ashwinkumar badanidiyuru', 'gang fu']"
2401.11103,efficient data shapley for weighted nearest neighbor algorithms,cs.ds cs.lg stat.ml,"this work aims to address an open problem in data valuation literature concerning the efficient computation of data shapley for weighted $k$ nearest neighbor algorithm (wknn-shapley). by considering the accuracy of hard-label knn with discretized weights as the utility function, we reframe the computation of wknn-shapley into a counting problem and introduce a quadratic-time algorithm, presenting a notable improvement from $o(n^k)$, the best result from existing literature. we develop a deterministic approximation algorithm that further improves computational efficiency while maintaining the key fairness properties of the shapley value. through extensive experiments, we demonstrate wknn-shapley's computational efficiency and its superior performance in discerning data quality compared to its unweighted counterpart.",,2024-01-19,,"['jiachen t. wang', 'prateek mittal', 'ruoxi jia']"
2401.11130,identification and estimation of conditional average partial causal   effects via instrumental variable,cs.lg stat.ml,"there has been considerable recent interest in estimating heterogeneous causal effects. in this paper, we introduce conditional average partial causal effects (capce) to reveal the heterogeneity of causal effects with continuous treatment. we provide conditions for identifying capce in an instrumental variable setting. we develop three families of capce estimators: sieve, parametric, and reproducing kernel hilbert space (rkhs)-based, and analyze their statistical properties. we illustrate the proposed capce estimators on synthetic and real-world data.",,2024-01-20,,"['yuta kawakami', 'manabu kuroki', 'jin tian']"
2401.11250,afs-bm: enhancing model performance through adaptive feature selection   with binary masking,cs.lg eess.sp stat.ml,"we study the problem of feature selection in general machine learning (ml) context, which is one of the most critical subjects in the field. although, there exist many feature selection methods, however, these methods face challenges such as scalability, managing high-dimensional data, dealing with correlated features, adapting to variable feature importance, and integrating domain knowledge. to this end, we introduce the ``adaptive feature selection with binary masking"" (afs-bm) which remedies these problems. afs-bm achieves this by joint optimization for simultaneous feature selection and model training. in particular, we do the joint optimization and binary masking to continuously adapt the set of features and model parameters during the training process. this approach leads to significant improvements in model accuracy and a reduction in computational requirements. we provide an extensive set of experiments where we compare afs-bm with the established feature selection methods using well-known datasets from real-life competitions. our results show that afs-bm makes significant improvement in terms of accuracy and requires significantly less computational complexity. this is due to afs-bm's ability to dynamically adjust to the changing importance of features during the training process, which an important contribution to the field. we openly share our code for the replicability of our results and to facilitate further research.",10.21203/rs.3.rs-3881366/v1,2024-01-20,,"['mehmet y. turali', 'mehmet e. lorasdagi', 'ali t. koc', 'suleyman s. kozat']"
2401.11263,estimating heterogeneous treatment effect from survival outcomes via   (orthogonal) censoring unbiased learning,stat.me stat.ml,"methods for estimating heterogeneous treatment effects (hte) from observational data have largely focused on continuous or binary outcomes, with less attention paid to survival outcomes and almost none to settings with competing risks. in this work, we develop censoring unbiased transformations (cuts) for survival outcomes both with and without competing risks.after converting time-to-event outcomes using these cuts, direct application of hte learners for continuous outcomes yields consistent estimates of heterogeneous cumulative incidence effects, total effects, and separable direct effects. our cuts enable application of a much larger set of state of the art hte learners for censored outcomes than had previously been available, especially in competing risks settings. we provide generic model-free learner-specific oracle inequalities bounding the finite-sample excess risk. the oracle efficiency results depend on the oracle selector and estimated nuisance functions from all steps involved in the transformation. we demonstrate the empirical performance of the proposed methods in simulation studies.",,2024-01-20,,"['shenbo xu', 'raluca cobzaru', 'bang zheng', 'stan n. finkelstein', 'roy e. welsch', 'kenney ng', 'ioanna tzoulaki', 'zach shahn']"
2401.11380,moma: model-based mirror ascent for offline reinforcement learning,cs.lg math.st stat.me stat.ml stat.th,"model-based offline reinforcement learning methods (rl) have achieved state-of-the-art performance in many decision-making problems thanks to their sample efficiency and generalizability. despite these advancements, existing model-based offline rl approaches either focus on theoretical studies without developing practical algorithms or rely on a restricted parametric policy space, thus not fully leveraging the advantages of an unrestricted policy space inherent to model-based methods. to address this limitation, we develop moma, a model-based mirror ascent algorithm with general function approximations under partial coverage of offline data. moma distinguishes itself from existing literature by employing an unrestricted policy class. in each iteration, moma conservatively estimates the value function by a minimization procedure within a confidence set of transition models in the policy evaluation step, then updates the policy with general function approximations instead of commonly-used parametric policy classes in the policy improvement step. under some mild assumptions, we establish theoretical guarantees of moma by proving an upper bound on the suboptimality of the returned policy. we also provide a practically implementable, approximate version of the algorithm. the effectiveness of moma is demonstrated via numerical studies.",,2024-01-20,,"['mao hong', 'zhiyue zhang', 'yue wu', 'yanxun xu']"
2401.11554,transfer learning under covariate shift: local $k$-nearest neighbours   regression with heavy-tailed design,math.st stat.th,"covariate shift is a common transfer learning scenario where the marginal distributions of input variables vary between source and target data while the conditional distribution of the output variable remains consistent. the existing notions describing differences between marginal distributions face limitations in handling scenarios with unbounded support, particularly when the target distribution has a heavier tail. to overcome these challenges, we introduce a new concept called density ratio exponent to quantify the relative decay rates of marginal distributions' tails under covariate shift. furthermore, we propose the local k-nearest neighbour regressor for transfer learning, which adapts the number of nearest neighbours based on the marginal likelihood of each test sample. from a theoretical perspective, convergence rates with and without supervision information on the target domain are established. those rates indicate that our estimator achieves faster convergence rates when the density ratio exponent satisfies certain conditions, highlighting the benefits of using density estimation for determining different numbers of nearest neighbours for each test sample. our contributions enhance the understanding and applicability of transfer learning under covariate shift, especially in scenarios with unbounded support and heavy-tailed distributions.",,2024-01-21,,"['petr zamolodtchikov', 'hanyuan hang']"
2401.11562,enhancing selectivity using wasserstein distance based reweighing,stat.ml cs.lg q-bio.qm,"given two labeled data-sets $\mathcal{s}$ and $\mathcal{t}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\mathcal{s}$ approaches the limiting distribution that would have resulted by training on $\mathcal{t}$.   on the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.   our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization. as a motivating application, we train a neural net to recognize small molecule binders to mnk2 (a map kinase, responsible for cell signaling) which are non-binders to mnk1 (a highly similar protein). we tune the algorithm's parameter so that overall change in holdout loss is negligible, but the selectivity, i.e., the fraction of top 100 mnk2 binders that are mnk1 non-binders, increases from 54\% to 95\%, as a result of our reweighing. of the 43 distinct small molecules predicted to be most selective from the enamine catalog, 2 small molecules were experimentally verified to be selective, i.e., they reduced the enzyme activity of mnk2 below 50\% but not mnk1, at 10$\mu$m -- a 5\% success rate.",,2024-01-21,,['pratik worah']
2401.11600,understanding the generalization benefits of late learning rate decay,cs.lg stat.ml,"why do neural networks trained with large learning rates for a longer time often lead to better generalization? in this paper, we delve into this question by examining the relation between training and testing loss in neural networks. through visualization of these losses, we note that the training trajectory with a large learning rate navigates through the minima manifold of the training loss, finally nearing the neighborhood of the testing loss minimum. motivated by these findings, we introduce a nonlinear model whose loss landscapes mirror those observed for real neural networks. upon investigating the training process using sgd on our model, we demonstrate that an extended phase with a large learning rate steers our model towards the minimum norm solution of the training loss, which may achieve near-optimal generalization, thereby affirming the empirically observed benefits of late learning rate decay.",,2024-01-21,,"['yinuo ren', 'chao ma', 'lexing ying']"
2401.11618,efficient local linearity regularization to overcome catastrophic   overfitting,cs.lg cs.ai cs.cr stat.ml,"catastrophic overfitting (co) in single-step adversarial training (at) results in abrupt drops in the adversarial test accuracy (even down to 0%). for models trained with multi-step at, it has been observed that the loss function behaves locally linearly with respect to the input, this is however lost in single-step at. to address co in single-step at, several methods have been proposed to enforce local linearity of the loss via regularization. however, these regularization terms considerably slow down training due to double backpropagation. instead, in this work, we introduce a regularization term, called elle, to mitigate co effectively and efficiently in classical at evaluations, as well as some more difficult regimes, e.g., large adversarial perturbations and long training schedules. our regularization term can be theoretically linked to curvature of the loss function and is computationally cheaper than previous methods by avoiding double backpropagation. our thorough experimental validation demonstrates that our work does not suffer from co, even in challenging settings where previous works suffer from it. we also notice that adapting our regularization parameter during training (elle-a) greatly improves the performance, specially in large $\epsilon$ setups. our implementation is available in https://github.com/lions-epfl/elle .",,2024-01-21,2024-02-28,"['elias abad rocamora', 'fanghui liu', 'grigorios g. chrysos', 'pablo m. olmos', 'volkan cevher']"
2401.11646,nonparametric estimation via variance-reduced sketching,stat.ml cs.lg cs.na math.na stat.me,"nonparametric models are of great interest in various scientific and engineering disciplines. classical kernel methods, while numerically robust and statistically sound in low-dimensional settings, become inadequate in higher-dimensional settings due to the curse of dimensionality. in this paper, we introduce a new framework called variance-reduced sketching (vrs), specifically designed to estimate density functions and nonparametric regression functions in higher dimensions with a reduced curse of dimensionality. our framework conceptualizes multivariable functions as infinite-size matrices, and facilitates a new sketching technique motivated by numerical linear algebra literature to reduce the variance in estimation problems. we demonstrate the robust numerical performance of vrs through a series of simulated experiments and real-world data applications. notably, vrs shows remarkable improvement over existing neural network estimators and classical kernel methods in numerous density estimation and nonparametric regression models. additionally, we offer theoretical justifications for vrs to support its ability to deliver nonparametric estimation with a reduced curse of dimensionality.",,2024-01-21,,"['yuehaw khoo', 'yifan peng', 'daren wang']"
2401.11665,accelerating approximate thompson sampling with underdamped langevin   monte carlo,stat.ml cs.ai cs.lg,"approximate thompson sampling with langevin monte carlo broadens its reach from gaussian posterior sampling to encompass more general smooth posteriors. however, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. to address this, we propose an approximate thompson sampling strategy, utilizing underdamped langevin monte carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. this design improves the sample complexity for realizing logarithmic regrets from $\mathcal{\tilde o}(d)$ to $\mathcal{\tilde o}(\sqrt{d})$. the scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.",,2024-01-21,2024-02-19,"['haoyang zheng', 'wei deng', 'christian moya', 'guang lin']"
2401.11742,knowledge navigation: inferring the interlocking map of knowledge from   research trajectories,cs.ir cs.dl stat.ap,"""if i have seen further, it is by standing on the shoulders of giants,"" isaac newton's renowned statement hints that new knowledge builds upon existing foundations, which means there exists an interdependent relationship between knowledge, which, yet uncovered, is implied in the historical development of scientific systems for hundreds of years. by leveraging natural language processing techniques, this study introduces an innovative embedding scheme designed to infer the ""knowledge interlocking map."" this map, derived from the research trajectories of millions of scholars, reveals the intricate connections among knowledge. we validate that the inferred map effectively delineates disciplinary boundaries and captures the intricate relationships between diverse concepts. the utility of the interlocking map is showcased through multiple applications. firstly, we demonstrated the multi-step analogy inferences within the knowledge space and the functional connectivity between concepts in different disciplines. secondly, we trace the evolution of knowledge across domains, observing trends such as shifts from ""theoretical"" to ""applied"" or ""chemistry"" to ""biomedical"" along predefined functional directions. lastly, by analyzing the high-dimensional knowledge network structure, we found that knowledge connects each other with shorter global pathways, and the interdisciplinary knowledge plays a critical role in accessibility of the global knowledge network. our framework offers a novel approach to mining knowledge inheritance pathways in extensive scientific literature, which is of great significance for understanding scientific development patterns, tailoring scientific learning trajectories, and accelerating scientific progress.",,2024-01-22,2024-01-27,"['shibing xiang', 'xin jiang', 'bing liu', 'yurui huang', 'chaolin tian', 'yifang ma']"
2401.11842,subgroup analysis methods for time-to-event outcomes in heterogeneous   randomized controlled trials,stat.me stat.ap stat.ml,"non-significant randomized control trials can hide subgroups of good responders to experimental drugs, thus hindering subsequent development. identifying such heterogeneous treatment effects is key for precision medicine and many post-hoc analysis methods have been developed for that purpose. while several benchmarks have been carried out to identify the strengths and weaknesses of these methods, notably for binary and continuous endpoints, similar systematic empirical evaluation of subgroup analysis for time-to-event endpoints are lacking. this work aims to fill this gap by evaluating several subgroup analysis algorithms in the context of time-to-event outcomes, by means of three different research questions: is there heterogeneity? what are the biomarkers responsible for such heterogeneity? who are the good responders to treatment? in this context, we propose a new synthetic and semi-synthetic data generation process that allows one to explore a wide range of heterogeneity scenarios with precise control on the level of heterogeneity. we provide an open source python package, available on github, containing our generation process and our comprehensive benchmark framework. we hope this package will be useful to the research community for future investigations of heterogeneity of treatment effects and subgroup analysis methods benchmarking.",,2024-01-22,2024-01-23,"['valentine perrin', 'nathan noiry', 'nicolas loiseau', 'alex nowak']"
2401.11940,low-tubal-rank tensor recovery via factorized gradient descent,cs.lg math.oc stat.ml,"this paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. traditional approaches tackling such a problem require the computation of tensor singular value decomposition (t-svd), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the burer-monteiro (bm) method. precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (fgd). this strategy eliminates the need for t-svd computation, thereby reducing computational costs and storage requirements. we provide rigorous theoretical analysis to ensure the convergence of fgd under both noise-free and noisy situations. additionally, it is worth noting that our method does not require the precise estimation of the tensor tubal-rank. even in cases where the tubal-rank is slightly overestimated, our approach continues to demonstrate robust performance. a series of experiments have been carried out to demonstrate that, as compared to other popular ones, our approach exhibits superior performance in multiple scenarios, in terms of the faster computational speed and the smaller convergence error.",,2024-01-22,2024-02-02,"['zhiyu liu', 'zhi han', 'yandong tang', 'xi-le zhao', 'yao wang']"
2401.11954,rumboost: gradient boosted random utility models,cs.lg stat.ml,"this paper introduces the rumboost model, a novel discrete choice modelling approach that combines the interpretability and behavioural robustness of random utility models (rums) with the generalisation and predictive ability of deep learning methods. we obtain the full functional form of non-linear utility specifications by replacing each linear parameter in the utility functions of a rum with an ensemble of gradient boosted regression trees. this enables piece-wise constant utility values to be imputed for all alternatives directly from the data for any possible combination of input variables. we introduce additional constraints on the ensembles to ensure three crucial features of the utility specifications: (i) dependency of the utilities of each alternative on only the attributes of that alternative, (ii) monotonicity of marginal utilities, and (iii) an intrinsically interpretable functional form, where the exact response of the model is known throughout the entire input space. furthermore, we introduce an optimisation-based smoothing technique that replaces the piece-wise constant utility values of alternative attributes with monotonic piece-wise cubic splines to identify non-linear parameters with defined gradient. we demonstrate the potential of the rumboost model compared to various ml and random utility benchmark models for revealed preference mode choice data from london. the results highlight the great predictive performance and the direct interpretability of our proposed approach. furthermore, the smoothed attribute utility functions allow for the calculation of various behavioural indicators and marginal utilities. finally, we demonstrate the flexibility of our methodology by showing how the rumboost model can be extended to complex model specifications, including attribute interactions, correlation within alternative error terms and heterogeneity within the population.",,2024-01-22,,"['nicolas salvadé', 'tim hillel']"
2401.11974,cross-validation conformal risk control,cs.lg stat.ml,"conformal risk control (crc) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees. generalizing conformal prediction (cp), with crc, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate. the original crc requires the available data set to be split between training and validation data sets. this can be problematic when data availability is limited, resulting in inefficient set predictors. in this paper, a novel crc method is introduced that is based on cross-validation, rather than on validation as the original crc. the proposed cross-validation crc (cv-crc) extends a version of the jackknife-minmax from cp to crc, allowing for the control of a broader range of risk functions. cv-crc is proved to offer theoretical guarantees on the average risk of the set predictor. furthermore, numerical experiments show that cv-crc can reduce the average set size with respect to crc when the available data are limited.",,2024-01-22,,"['kfir m. cohen', 'sangwoo park', 'osvaldo simeone', 'shlomo shamai']"
2401.12000,integrating statistical significance and discriminative power in pattern   discovery,cs.lg stat.ml,"pattern discovery plays a central role in both descriptive and predictive tasks across multiple domains. actionable patterns must meet rigorous statistical significance criteria and, in the presence of target variables, further uphold discriminative power. our work addresses the underexplored area of guiding pattern discovery by integrating statistical significance and discriminative power criteria into state-of-the-art algorithms while preserving pattern quality. we also address how pattern quality thresholds, imposed by some algorithms, can be rectified to accommodate these additional criteria. to test the proposed methodology, we select the triclustering task as the guiding pattern discovery case and extend well-known greedy and multi-objective optimization triclustering algorithms, $\delta$-trimax and trigen, that use various pattern quality criteria, such as mean squared residual (msr), least squared lines (lsl), and multi slope measure (msl). results from three case studies show the role of the proposed methodology in discovering patterns with pronounced improvements of discriminative power and statistical significance without quality deterioration, highlighting its importance in supervisedly guiding the search. although the proposed methodology is motivated over multivariate time series data, it can be straightforwardly extended to pattern discovery tasks involving multivariate, n-way (n>3), transactional, and sequential data structures.   availability: the code is freely available at https://github.com/jupitersmight/mof_triclustering under the mit license.",,2024-01-22,,"['leonardo alexandre', 'rafael s. costa', 'rui henriques']"
2401.12058,the dimension strikes back with gradients: generalization of gradient   methods in stochastic convex optimization,cs.lg stat.ml,"we study the generalization performance of gradient methods in the fundamental stochastic convex optimization setting, focusing on its dimension dependence. first, for full-batch gradient descent (gd) we give a construction of a learning problem in dimension $d=o(n^2)$, where the canonical version of gd (tuned for optimal performance of the empirical risk) trained with $n$ training examples converges, with constant probability, to an approximate empirical risk minimizer with $\omega(1)$ population excess risk. our bound translates to a lower bound of $\omega (\sqrt{d})$ on the number of training examples required for standard gd to reach a non-trivial test error, answering an open question raised by feldman (2016) and amir, koren, and livni (2021b) and showing that a non-trivial dimension dependence is unavoidable. furthermore, for standard one-pass stochastic gradient descent (sgd), we show that an application of the same construction technique provides a similar $\omega(\sqrt{d})$ lower bound for the sample complexity of sgd to reach a non-trivial empirical error, despite achieving optimal test performance. this again provides an exponential improvement in the dimension dependence compared to previous work (koren, livni, mansour, and sherman, 2022), resolving an open question left therein.",,2024-01-22,,"['matan schliserman', 'uri sherman', 'tomer koren']"
2401.12195,using spatial extreme-value theory with machine learning to model and   understand spatially compounding extremes,stat.ap,"when extreme weather events affect large areas, their regional to sub-continental spatial scale is important for their impacts. we propose a novel methodology that combines spatial extreme-value theory with a machine learning (ml) algorithm to model weather extremes and quantify probabilities associated with the occurrence, intensity and spatial extent of these events. the model is here applied to western european summertime heat extremes. using new loss functions adapted to extreme values, we fit a theoretically-motivated spatial model to extreme positive temperature anomaly fields from 1959-2022, using the daily 500-hpa geopotential height fields across the euro-atlantic region and the local soil moisture as predictors. our generative model reveals the importance of individual circulation features in determining different facets of heat extremes, thereby enriching our process understanding of them from a data-driven perspective. the occurrence, intensity, and spatial extent of heat extremes are sensitive to the relative position of individual ridges and troughs that are part of a large-scale wave pattern. heat extremes in europe are thus the result of a complex interplay between local and remote physical processes. our approach is able to extrapolate beyond the range of the data to make risk-related probabilistic statements, and applies more generally to other weather extremes. it also offers an attractive alternative to physical model-based techniques, or to ml approaches that optimise scores focusing on predicting well the bulk instead of the tail of the data distribution.",,2024-01-22,,"['jonathan koh', 'daniel steinfeld', 'olivia martius']"
2401.12216,mitigating covariate shift in misspecified regression with applications   to reinforcement learning,stat.ml cs.lg math.oc,"a pervasive phenomenon in machine learning applications is distribution shift, where training and deployment conditions for a machine learning model differ. as distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects. in this paper, we study the effect of distribution shift in the presence of model misspecification, specifically focusing on $l_{\infty}$-misspecified regression and adversarial covariate shift, where the regression target remains fixed while the covariate distribution changes arbitrarily. we show that empirical risk minimization, or standard least squares regression, can result in undesirable misspecification amplification where the error due to misspecification is amplified by the density ratio between the training and testing distributions. as our main result, we develop a new algorithm -- inspired by robust optimization techniques -- that avoids this undesirable behavior, resulting in no misspecification amplification while still obtaining optimal statistical rates. as applications, we use this regression procedure to obtain new guarantees in offline and online reinforcement learning with misspecification and establish new separations between previously studied structural conditions and notions of coverage.",,2024-01-22,,"['philip amortila', 'tongyi cao', 'akshay krishnamurthy']"
2401.12236,the surprising harmfulness of benign overfitting for adversarial   robustness,cs.lg cs.cr stat.ml,"recent empirical and theoretical studies have established the generalization capabilities of large machine learning models that are trained to (approximately or exactly) fit noisy data. in this work, we prove a surprising result that even if the ground truth itself is robust to adversarial examples, and the benignly overfitted model is benign in terms of the ``standard'' out-of-sample risk objective, this benign overfitting process can be harmful when out-of-sample data are subject to adversarial manipulation. more specifically, our main results contain two parts: (i) the min-norm estimator in overparameterized linear model always leads to adversarial vulnerability in the ``benign overfitting'' setting; (ii) we verify an asymptotic trade-off result between the standard risk and the ``adversarial'' risk of every ridge regression estimator, implying that under suitable conditions these two items cannot both be small at the same time by any single choice of the ridge regularization parameter. furthermore, under the lazy training regime, we demonstrate parallel results on two-layer neural tangent kernel (ntk) model, which align with empirical observations in deep neural networks. our finding provides theoretical insights into the puzzling phenomenon observed in practice, where the true target function (e.g., human) is robust against adverasrial attack, while beginly overfitted neural networks lead to models that are not robust.",,2024-01-19,2024-01-25,"['yifan hao', 'tong zhang']"
2401.12253,accelerating sinkhorn algorithm with sparse newton iterations,math.oc cs.lg stat.ml,"computing the optimal transport distance between statistical distributions is a fundamental task in machine learning. one remarkable recent advancement is entropic regularization and the sinkhorn algorithm, which utilizes only matrix scaling and guarantees an approximated solution with near-linear runtime. despite the success of the sinkhorn algorithm, its runtime may still be slow due to the potentially large number of iterations needed for convergence. to achieve possibly super-exponential convergence, we present sinkhorn-newton-sparse (sns), an extension to the sinkhorn algorithm, by introducing early stopping for the matrix scaling steps and a second stage featuring a newton-type subroutine. adopting the variational viewpoint that the sinkhorn algorithm maximizes a concave lyapunov potential, we offer the insight that the hessian matrix of the potential function is approximately sparse. sparsification of the hessian results in a fast $o(n^2)$ per-iteration complexity, the same as the sinkhorn algorithm. in terms of total iteration count, we observe that the sns algorithm converges orders of magnitude faster across a wide range of practical cases, including optimal transportation between empirical distributions and calculating the wasserstein $w_1, w_2$ distance of discretized densities. the empirical performance is corroborated by a rigorous bound on the approximate sparsity of the hessian matrix.",,2024-01-20,,"['xun tang', 'michael shavlovsky', 'holakou rahmanian', 'elisa tardini', 'kiran koshy thekumparampil', 'tesi xiao', 'lexing ying']"
2401.12272,transfer learning for nonparametric regression: non-asymptotic minimax   analysis and adaptive procedure,stat.ml cs.lg,"transfer learning for nonparametric regression is considered. we first study the non-asymptotic minimax risk for this problem and develop a novel estimator called the confidence thresholding estimator, which is shown to achieve the minimax optimal risk up to a logarithmic factor. our results demonstrate two unique phenomena in transfer learning: auto-smoothing and super-acceleration, which differentiate it from nonparametric regression in a traditional setting. we then propose a data-driven algorithm that adaptively achieves the minimax risk up to a logarithmic factor across a wide range of parameter spaces. simulation studies are conducted to evaluate the numerical performance of the adaptive transfer learning algorithm, and a real-world example is provided to demonstrate the benefits of the proposed method.",,2024-01-22,,"['t. tony cai', 'hongming pu']"
2401.12340,contrastive learning and cycle consistency-based transductive transfer   learning for target annotation,cs.cv cs.ai cs.lg eess.iv stat.ml,"annotating automatic target recognition (atr) is a highly challenging task, primarily due to the unavailability of labeled data in the target domain. hence, it is essential to construct an optimal target domain classifier by utilizing the labeled information of the source domain images. the transductive transfer learning (ttl) method that incorporates a cyclegan-based unpaired domain translation network has been previously proposed in the literature for effective atr annotation. although this method demonstrates great potential for atr, it severely suffers from lower annotation performance, higher fr\'echet inception distance (fid) score, and the presence of visual artifacts in the synthetic images. to address these issues, we propose a hybrid contrastive learning base unpaired domain translation (h-cut) network that achieves a significantly lower fid score. it incorporates both attention and entropy to emphasize the domain-specific region, a noisy feature mixup module to generate high variational synthetic negative patches, and a modulated noise contrastive estimation (monce) loss to reweight all negative patches using optimal transport for better performance. our proposed contrastive learning and cycle-consistency-based ttl (c3ttl) framework consists of two h-cut networks and two classifiers. it simultaneously optimizes cycle-consistency, monce, and identity losses. in c3ttl, two h-cut networks have been employed through a bijection mapping to feed the reconstructed source domain images into a pretrained classifier to guide the optimal target domain classifier. extensive experimental analysis conducted on three atr datasets demonstrates that the proposed c3ttl method is effective in annotating civilian and military vehicles, as well as ship targets.",10.1109/taes.2023.3337768,2024-01-22,,"['shoaib meraj sami', 'md mahedi hasan', 'nasser m. nasrabadi', 'raghuveer rao']"
2401.12369,subgroupte: advancing treatment effect estimation with subgroup   identification,cs.lg stat.me,"precise estimation of treatment effects is crucial for evaluating intervention effectiveness. while deep learning models have exhibited promising performance in learning counterfactual representations for treatment effect estimation (tee), a major limitation in most of these models is that they treat the entire population as a homogeneous group, overlooking the diversity of treatment effects across potential subgroups that have varying treatment effects. this limitation restricts the ability to precisely estimate treatment effects and provide subgroup-specific treatment recommendations. in this paper, we propose a novel treatment effect estimation model, named subgroupte, which incorporates subgroup identification in tee. subgroupte identifies heterogeneous subgroups with different treatment responses and more precisely estimates treatment effects by considering subgroup-specific causal effects. in addition, subgroupte iteratively optimizes subgrouping and treatment effect estimation networks to enhance both estimation and subgroup identification. comprehensive experiments on the synthetic and semi-synthetic datasets exhibit the outstanding performance of subgroupte compared with the state-of-the-art models on treatment effect estimation. additionally, a real-world study demonstrates the capabilities of subgroupte in enhancing personalized treatment recommendations for patients with opioid use disorder (oud) by advancing treatment effect estimation with subgroup identification.",,2024-01-22,,"['seungyeon lee', 'ruoqi liu', 'wenyu song', 'lang li', 'ping zhang']"
2401.12418,towards improved variational inference for deep bayesian models,cs.lg stat.ml,"deep learning has revolutionized the last decade, being at the forefront of extraordinary advances in a wide range of tasks including computer vision, natural language processing, and reinforcement learning, to name but a few. however, it is well-known that deep models trained via maximum likelihood estimation tend to be overconfident and give poorly-calibrated predictions. bayesian deep learning attempts to address this by placing priors on the model parameters, which are then combined with a likelihood to perform posterior inference. unfortunately, for deep models, the true posterior is intractable, forcing the user to resort to approximations. in this thesis, we explore the use of variational inference (vi) as an approximation, as it is unique in simultaneously approximating the posterior and providing a lower bound to the marginal likelihood. if tight enough, this lower bound can be used to optimize hyperparameters and to facilitate model selection. however, this capacity has rarely been used to its full extent for bayesian neural networks, likely because the approximate posteriors typically used in practice can lack the flexibility to effectively bound the marginal likelihood. we therefore explore three aspects of bayesian learning for deep models: 1) we ask whether it is necessary to perform inference over as many parameters as possible, or whether it is reasonable to treat many of them as optimizable hyperparameters; 2) we propose a variational posterior that provides a unified view of inference in bayesian neural networks and deep gaussian processes; 3) we demonstrate how vi can be improved in certain deep gaussian process models by analytically removing symmetries from the posterior, and performing inference on gram matrices instead of features. we hope that our contributions will provide a stepping stone to fully realize the promises of vi in the future.",10.17863/cam.102025,2024-01-22,,['sebastian w. ober']
2401.12476,bayesian identification of nonseparable hamiltonians with multiplicative   noise using deep learning and reduced-order modeling,stat.ml cs.lg math.ds physics.data-an stat.co,"this paper presents a structure-preserving bayesian approach for learning nonseparable hamiltonian systems using stochastic dynamic models allowing for statistically-dependent, vector-valued additive and multiplicative measurement noise. the approach is comprised of three main facets. first, we derive a gaussian filter for a statistically-dependent, vector-valued, additive and multiplicative noise model that is needed to evaluate the likelihood within the bayesian posterior. second, we develop a novel algorithm for cost-effective application of bayesian system identification to high-dimensional systems. third, we demonstrate how structure-preserving methods can be incorporated into the proposed framework, using nonseparable hamiltonians as an illustrative system class. we compare the bayesian method to a state-of-the-art machine learning method on a canonical nonseparable hamiltonian model and a chaotic double pendulum model with small, noisy training datasets. the results show that using the bayesian posterior as a training objective can yield upwards of 724 times improvement in hamiltonian mean squared error using training data with up to 10% multiplicative noise compared to a standard training objective. lastly, we demonstrate the utility of the novel algorithm for parameter estimation of a 64-dimensional model of the spatially-discretized nonlinear schr\""odinger equation with data corrupted by up to 20% multiplicative noise.",,2024-01-22,,"['nicholas galioto', 'harsh sharma', 'boris kramer', 'alex arkady gorodetsky']"
2401.12482,nonparametric logistic regression with deep learning,math.st stat.ml stat.th,"consider the nonparametric logistic regression problem. in the logistic regression, we usually consider the maximum likelihood estimator, and the excess risk is the expectation of the kullback-leibler (kl) divergence between the true and estimated conditional class probabilities. however, in the nonparametric logistic regression, the kl divergence could diverge easily, and thus, the convergence of the excess risk is difficult to prove or does not hold. several existing studies show the convergence of the kl divergence under strong assumptions. in most cases, our goal is to estimate the true conditional class probabilities. thus, instead of analyzing the excess risk itself, it suffices to show the consistency of the maximum likelihood estimator in some suitable metric. in this paper, using a simple unified approach for analyzing the nonparametric maximum likelihood estimator (npmle), we directly derive the convergence rates of the npmle in the hellinger distance under mild assumptions. although our results are similar to the results in some existing studies, we provide simple and more direct proofs for these results. as an important application, we derive the convergence rates of the npmle with deep neural networks and show that the derived rate nearly achieves the minimax optimal rate.",,2024-01-22,,"['atsutomo yara', 'yoshikazu terada']"
2401.12485,adiabatic quantum support vector machines,cs.lg cs.ai quant-ph stat.ml,"adiabatic quantum computers can solve difficult optimization problems (e.g., the quadratic unconstrained binary optimization problem), and they seem well suited to train machine learning models. in this paper, we describe an adiabatic quantum approach for training support vector machines. we show that the time complexity of our quantum approach is an order of magnitude better than the classical approach. next, we compare the test accuracy of our quantum approach against a classical approach that uses the scikit-learn library in python across five benchmark datasets (iris, wisconsin breast cancer (wbc), wine, digits, and lambeq). we show that our quantum approach obtains accuracies on par with the classical approach. finally, we perform a scalability study in which we compute the total training times of the quantum approach and the classical approach with increasing number of features and number of data points in the training dataset. our scalability results show that the quantum approach obtains a 3.5--4.5 times speedup over the classical approach on datasets with many (millions of) features.",,2024-01-22,,"['prasanna date', 'dong jun woun', 'kathleen hamilton', 'eduardo a. coello perez', 'mayanka chandra shekhar', 'francisco rios', 'john gounley', 'in-saeng suh', 'travis humble', 'georgia tourassi']"
2401.12588,interpreting equivariant representations,cs.lg stat.ml,"latent representations are used extensively for downstream tasks, such as visualization, interpolation or feature extraction of deep learning models. invariant and equivariant neural networks are powerful and well-established models for enforcing inductive biases. in this paper, we demonstrate that the inductive bias imposed on the by an equivariant model must also be taken into account when using latent representations. we show how not accounting for the inductive biases leads to decreased performance on downstream tasks, and vice versa, how accounting for inductive biases can be done effectively by using an invariant projection of the latent representations. we propose principles for how to choose such a projection, and show the impact of using these principles in two common examples: first, we study a permutation equivariant variational auto-encoder trained for molecule graph generation; here we show that invariant projections can be designed that incur no loss of information in the resulting invariant representation. next, we study a rotation-equivariant representation used for image classification. here, we illustrate how random invariant projections can be used to obtain an invariant representation with a high degree of retained information. in both cases, the analysis of invariant latent representations proves superior to their equivariant counterparts. finally, we illustrate that the phenomena documented here for equivariant neural networks have counterparts in standard neural networks where invariance is encouraged via augmentation. thus, while these ambiguities may be known by experienced developers of equivariant models, we make both the knowledge as well as effective tools to handle the ambiguities available to the broader community.",,2024-01-23,,"['andreas abildtrup hansen', 'anna calissano', 'aasa feragen']"
2401.12667,feature selection via robust weighted score for high dimensional binary   class-imbalanced gene expression data,stat.ml cs.lg,"in this paper, a robust weighted score for unbalanced data (rowsu) is proposed for selecting the most discriminative feature for high dimensional gene expression binary classification with class-imbalance problem. the method addresses one of the most challenging problems of highly skewed class distributions in gene expression datasets that adversely affect the performance of classification algorithms. first, the training dataset is balanced by synthetically generating data points from minority class observations. second, a minimum subset of genes is selected using a greedy search approach. third, a novel weighted robust score, where the weights are computed by support vectors, is introduced to obtain a refined set of genes. the highest-scoring genes based on this approach are combined with the minimum subset of genes selected by the greedy search approach to form the final set of genes. the novel method ensures the selection of the most discriminative genes, even in the presence of skewed class distribution, thus improving the performance of the classifiers. the performance of the proposed rowsu method is evaluated on $6$ gene expression datasets. classification accuracy and sensitivity are used as performance metrics to compare the proposed rowsu algorithm with several other state-of-the-art methods. boxplots and stability plots are also constructed for a better understanding of the results. the results show that the proposed method outperforms the existing feature selection procedures based on classification performance from k nearest neighbours (knn) and random forest (rf) classifiers.",,2024-01-23,,"['zardad khan', 'amjad ali', 'saeed aldahmani']"
2401.12697,a computationally efficient approach to false discovery rate control and   power maximisation via randomisation and mirror statistic,stat.me stat.ap,"simultaneously performing variable selection and inference in high-dimensional regression models is an open challenge in statistics and machine learning. the increasing availability of vast amounts of variables requires the adoption of specific statistical procedures to accurately select the most important predictors in a high-dimensional space, while controlling the false discovery rate (fdr) arising from the underlying multiple hypothesis testing. in this paper we propose the joint adoption of the mirror statistic approach to fdr control, coupled with outcome randomisation to maximise the statistical power of the variable selection procedure. through extensive simulations we show how our proposed strategy allows to combine the benefits of the two techniques. the mirror statistic is a flexible method to control fdr, which only requires mild model assumptions, but requires two sets of independent regression coefficient estimates, usually obtained after splitting the original dataset. outcome randomisation is an alternative to data splitting, that allows to generate two independent outcomes, which can then be used to estimate the coefficients that go into the construction of the mirror statistic. the combination of these two approaches provides increased testing power in a number of scenarios, such as highly correlated covariates and high percentages of active variables. moreover, it is scalable to very high-dimensional problems, since the algorithm has a low memory footprint and only requires a single run on the full dataset, as opposed to iterative alternatives such as multiple data splitting.",,2024-01-23,,"['marco molinari', 'magne thoresen']"
2401.12708,deep neural network benchmarks for selective classification,cs.lg cs.ai stat.ml,"with the increasing deployment of machine learning models in many socially-sensitive tasks, there is a growing demand for reliable and trustworthy predictions. one way to accomplish these requirements is to allow a model to abstain from making a prediction when there is a high risk of making an error. this requires adding a selection mechanism to the model, which selects those examples for which the model will provide a prediction. the selective classification framework aims to design a mechanism that balances the fraction of rejected predictions (i.e., the proportion of examples for which the model does not make a prediction) versus the improvement in predictive performance on the selected predictions. multiple selective classification frameworks exist, most of which rely on deep neural network architectures. however, the empirical evaluation of the existing approaches is still limited to partial comparisons among methods and settings, providing practitioners with little insight into their relative merits. we fill this gap by benchmarking 18 baselines on a diverse set of 44 datasets that includes both image and tabular data. moreover, there is a mix of binary and multiclass tasks. we evaluate these approaches using several criteria, including selective error rate, empirical coverage, distribution of rejected instance's classes, and performance on out-of-distribution instances. the results indicate that there is not a single clear winner among the surveyed baselines, and the best method depends on the users' objectives.",,2024-01-23,,"['andrea pugnana', 'lorenzo perini', 'jesse davis', 'salvatore ruggieri']"
2401.12824,mapping: debiasing graph neural networks for fair node classification   with limited sensitive information leakage,cs.lg stat.ml,"despite remarkable success in diverse web-based applications, graph neural networks(gnns) inherit and further exacerbate historical discrimination and social stereotypes, which critically hinder their deployments in high-stake domains such as online clinical diagnosis, financial crediting, etc. however, current fairness research that primarily craft on i.i.d data, cannot be trivially replicated to non-i.i.d. graph structures with topological dependence among samples. existing fair graph learning typically favors pairwise constraints to achieve fairness but fails to cast off dimensional limitations and generalize them into multiple sensitive attributes; besides, most studies focus on in-processing techniques to enforce and calibrate fairness, constructing a model-agnostic debiasing gnn framework at the pre-processing stage to prevent downstream misuses and improve training reliability is still largely under-explored. furthermore, previous work on gnns tend to enhance either fairness or privacy individually but few probe into their interplays. in this paper, we propose a novel model-agnostic debiasing framework named mapping (\underline{m}asking \underline{a}nd \underline{p}runing and message-\underline{p}assing train\underline{ing}) for fair node classification, in which we adopt the distance covariance($dcov$)-based fairness constraints to simultaneously reduce feature and topology biases in arbitrary dimensions, and combine them with adversarial debiasing to confine the risks of attribute inference attacks. experiments on real-world datasets with different gnn variants demonstrate the effectiveness and flexibility of mapping. our results show that mapping can achieve better trade-offs between utility and fairness, and mitigate privacy risks of sensitive information leakage.",,2024-01-23,,"['ying song', 'balaji palanisamy']"
2401.12911,pretraining and the lasso,stat.me,"pretraining is a popular and powerful paradigm in machine learning. as an example, suppose one has a modest-sized dataset of images of cats and dogs, and plans to fit a deep neural network to classify them from the pixel features. with pretraining, we start with a neural network trained on a large corpus of images, consisting of not just cats and dogs but hundreds of other image types. then we fix all of the network weights except for the top layer (which makes the final classification) and train (or ""fine tune"") those weights on our dataset. this often results in dramatically better performance than the network trained solely on our smaller dataset.   in this paper, we ask the question ""can pretraining help the lasso?"". we develop a framework for the lasso in which an overall model is fit to a large set of data, and then fine-tuned to a specific task on a smaller dataset. this latter dataset can be a subset of the original dataset, but does not need to be. we find that this framework has a wide variety of applications, including stratified models, multinomial targets, multi-response models, conditional average treatment estimation and even gradient boosting.   in the stratified model setting, the pretrained lasso pipeline estimates the coefficients common to all groups at the first stage, and then group specific coefficients at the second ""fine-tuning"" stage. we show that under appropriate assumptions, the support recovery rate of the common coefficients is superior to that of the usual lasso trained only on individual groups. this separate identification of common and individual coefficients can also be useful for scientific understanding.",,2024-01-23,2024-02-22,"['erin craig', 'mert pilanci', 'thomas le menestrel', 'balasubramanian narasimhan', 'manuel rivas', 'roozbeh dehghannasiri', 'julia salzman', 'jonathan taylor', 'robert tibshirani']"
2401.12923,deep multitask neural networks for solving some stochastic optimal   control problems,stat.ml cs.lg cs.sy eess.sy,"most existing neural network-based approaches for solving stochastic optimal control problems using the associated backward dynamic programming principle rely on the ability to simulate the underlying state variables. however, in some problems, this simulation is infeasible, leading to the discretization of state variable space and the need to train one neural network for each data point. this approach becomes computationally inefficient when dealing with large state variable spaces. in this paper, we consider a class of this type of stochastic optimal control problems and introduce an effective solution employing multitask neural networks. to train our multitask neural network, we introduce a novel scheme that dynamically balances the learning across tasks. through numerical experiments on real-world derivatives pricing problems, we prove that our method outperforms state-of-the-art approaches.",,2024-01-23,2024-01-26,['christian yeo']
2401.12926,dsdm: model-aware dataset selection with datamodels,cs.lg stat.ml,"when selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. however, in practice the opposite can often happen: we find that selecting according to similarity with ""high quality"" data sources may not increase (and can even hurt) performance compared to randomly selecting data.   to develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. this framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. our resulting method greatly improves language model (lm) performance on both pre-specified tasks and previously unseen tasks. specifically, choosing target tasks representative of standard lm problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods.",,2024-01-23,,"['logan engstrom', 'axel feldmann', 'aleksander madry']"
2401.12934,reward-relevance-filtered linear offline reinforcement learning,stat.ml cs.lg math.oc,"this paper studies offline reinforcement learning with linear function approximation in a setting with decision-theoretic, but not estimation sparsity. the structural restrictions of the data-generating process presume that the transitions factor into a sparse component that affects the reward and could affect additional exogenous dynamics that do not affect the reward. although the minimally sufficient adjustment set for estimation of full-state transition properties depends on the whole state, the optimal policy and therefore state-action value function depends only on the sparse component: we call this causal/decision-theoretic sparsity. we develop a method for reward-filtering the estimation of the state-action value function to the sparse component by a modification of thresholded lasso in least-squares policy evaluation. we provide theoretical guarantees for our reward-filtered linear fitted-q-iteration, with sample complexity depending only on the size of the sparse component.",,2024-01-23,,['angela zhou']
2401.12950,bayesian semi-structured subspace inference,cs.lg stat.ml,"semi-structured regression models enable the joint modeling of interpretable structured and complex unstructured feature effects. the structured model part is inspired by statistical models and can be used to infer the input-output relationship for features of particular importance. the complex unstructured part defines an arbitrary deep neural network and thereby provides enough flexibility to achieve competitive prediction performance. while these models can also account for aleatoric uncertainty, there is still a lack of work on accounting for epistemic uncertainty. in this paper, we address this problem by presenting a bayesian approximation for semi-structured regression models using subspace inference. to this end, we extend subspace inference for joint posterior sampling from a full parameter space for structured effects and a subspace for unstructured effects. apart from this hybrid sampling scheme, our method allows for tunable complexity of the subspace and can capture multiple minima in the loss landscape. numerical experiments validate our approach's efficacy in recovering structured effect parameter posteriors in semi-structured models and approaching the full-space posterior distribution of mcmc for increasing subspace dimension. further, our approach exhibits competitive predictive performance across simulated and real-world datasets.",,2024-01-23,,"['daniel dold', 'david rügamer', 'beate sick', 'oliver dürr']"
2401.13009,comparative study of causal discovery methods for cyclic models with   hidden confounders,cs.lg stat.me stat.ml,"nowadays, the need for causal discovery is ubiquitous. a better understanding of not just the stochastic dependencies between parts of a system, but also the actual cause-effect relations, is essential for all parts of science. thus, the need for reliable methods to detect causal directions is growing constantly. in the last 50 years, many causal discovery algorithms have emerged, but most of them are applicable only under the assumption that the systems have no feedback loops and that they are causally sufficient, i.e. that there are no unmeasured subsystems that can affect multiple measured variables. this is unfortunate since those restrictions can often not be presumed in practice. feedback is an integral feature of many processes, and real-world systems are rarely completely isolated and fully measured. fortunately, in recent years, several techniques, that can cope with cyclic, causally insufficient systems, have been developed. and with multiple methods available, a practical application of those algorithms now requires knowledge of the respective strengths and weaknesses. here, we focus on the problem of causal discovery for sparse linear models which are allowed to have cycles and hidden confounders. we have prepared a comprehensive and thorough comparative study of four causal discovery techniques: two versions of the llc method [10] and two variants of the asp-based algorithm [11]. the evaluation investigates the performance of those techniques for various experiments with multiple interventional setups and different dataset sizes.",,2024-01-23,,"['boris lorbeer', 'mustafa mohsen']"
2401.13096,probabilistic demand forecasting with graph neural networks,cs.lg stat.ml,"demand forecasting is a prominent business use case that allows retailers to optimize inventory planning, logistics, and core business decisions. one of the key challenges in demand forecasting is accounting for relationships and interactions between articles. most modern forecasting approaches provide independent article-level predictions that do not consider the impact of related articles. recent research has attempted addressing this challenge using graph neural networks (gnns) and showed promising results. this paper builds on previous research on gnns and makes two contributions. first, we integrate a gnn encoder into a state-of-the-art deepar model. the combined model produces probabilistic forecasts, which are crucial for decision-making under uncertainty. second, we propose to build graphs using article attribute similarity, which avoids reliance on a pre-defined graph structure. experiments on three real-world datasets show that the proposed approach consistently outperforms non-graph benchmarks. we also show that our approach produces article embeddings that encode article similarity and demand dynamics and are useful for other downstream business tasks beyond forecasting.",,2024-01-23,,"['nikita kozodoi', 'elizaveta zinovyeva', 'simon valentin', 'joão pereira', 'rodrigo agundez']"
2401.13112,discount: distributional counterfactual explanation with optimal   transport,cs.ai stat.ml,"counterfactual explanations (ce) is the de facto method for providing insight and interpretability in black-box decision-making models by identifying alternative input instances that lead to different outcomes. this paper extends the concept of ces to a distributional context, broadening the scope from individual data points to entire input and output distributions, named distributional counterfactual explanation (dce). in dce, our focus shifts to analyzing the distributional properties of the factual and counterfactual, drawing parallels to the classical approach of assessing individual instances and their resulting decisions. we leverage optimal transport (ot) to frame a chance-constrained optimization problem, aiming to derive a counterfactual distribution that closely aligns with its factual counterpart, substantiated by statistical confidence. our proposed optimization method, discount, strategically balances this confidence across both input and output distributions. this algorithm is accompanied by an analysis of its convergence rate. the efficacy of our proposed method is substantiated through a series of illustrative case studies, highlighting its potential in providing deep insights into decision-making models.",,2024-01-23,2024-01-27,"['lei you', 'lele cao', 'mattias nilsson']"
2401.13216,on principled local optimization methods for federated learning,cs.lg cs.dc math.oc stat.ml,"federated learning (fl), a distributed learning paradigm that scales on-device learning collaboratively, has emerged as a promising approach for decentralized ai applications. local optimization methods such as federated averaging (fedavg) are the most prominent methods for fl applications. despite their simplicity and popularity, the theoretical understanding of local optimization methods is far from clear. this dissertation aims to advance the theoretical foundation of local methods in the following three directions.   first, we establish sharp bounds for fedavg, the most popular algorithm in federated learning. we demonstrate how fedavg may suffer from a notion we call iterate bias, and how an additional third-order smoothness assumption may mitigate this effect and lead to better convergence rates. we explain this phenomenon from a stochastic differential equation (sde) perspective.   second, we propose federated accelerated stochastic gradient descent (fedac), the first principled acceleration of fedavg, which provably improves the convergence rate and communication efficiency. our technique uses on a potential-based perturbed iterate analysis, a novel stability analysis of generalized accelerated sgd, and a strategic tradeoff between acceleration and stability.   third, we study the federated composite optimization problem, which extends the classic smooth setting by incorporating a shared non-smooth regularizer. we show that direct extensions of fedavg may suffer from the ""curse of primal averaging,"" resulting in slow convergence. as a solution, we propose a new primal-dual algorithm, federated dual averaging, which overcomes the curse of primal averaging by employing a novel inter-client dual averaging procedure.",,2024-01-23,,['honglin yuan']
2401.13237,quantum natural gradient without monotonicity,quant-ph cond-mat.stat-mech cs.it math.it physics.comp-ph stat.ml,"natural gradient (ng) is an information-geometric optimization method that plays a crucial role, especially in the estimation of parameters for machine learning models like neural networks. to apply ng to quantum systems, the quantum natural gradient (qng) was introduced and utilized for noisy intermediate-scale devices. additionally, a mathematically equivalent approach to qng, known as the stochastic reconfiguration method, has been implemented to enhance the performance of quantum monte carlo methods. it is worth noting that these methods are based on the symmetric logarithmic derivative (sld) metric, which is one of the monotone metrics. so far, monotonicity has been believed to be a guiding principle to construct a geometry in physics. in this paper, we propose generalized qng by removing the condition of monotonicity. initially, we demonstrate that monotonicity is a crucial condition for conventional qng to be optimal. subsequently, we provide analytical and numerical evidence showing that non-monotone qng outperforms conventional qng based on the sld metric in terms of convergence speed.",,2024-01-24,,"['toi sasaki', 'hideyuki miyahara']"
2401.13314,an explicit scheme for pathwise xva computations,q-fin.rm cs.na math.na q-fin.cp stat.ml,"motivated by the equations of cross valuation adjustments (xvas) in the realistic case where capital is deemed fungible as a source of funding for variation margin, we introduce a simulation/regression scheme for a class of anticipated bsdes, where the coefficient entails a conditional expected shortfall of the martingale part of the solution. the scheme is explicit in time and uses neural network least-squares and quantile regressions for the embedded conditional expectations and expected shortfall computations. an a posteriori monte carlo validation procedure allows assessing the regression error of the scheme at each time step. the superiority of this scheme with respect to picard iterations is illustrated in a high-dimensional and hybrid market/default risks xva use-case.",,2024-01-24,,"['lokman abbas-turki', 'stéphane crépey', 'botao li', 'bouazza saadeddine']"
2401.13335,full bayesian significance testing for neural networks,stat.ml cs.ai cs.lg,"significance testing aims to determine whether a proposition about the population distribution is the truth or not given observations. however, traditional significance testing often needs to derive the distribution of the testing statistic, failing to deal with complex nonlinear relationships. in this paper, we propose to conduct full bayesian significance testing for neural networks, called \textit{n}fbst, to overcome the limitation in relationship characterization of traditional approaches. a bayesian neural network is utilized to fit the nonlinear and multi-dimensional relationships with small errors and avoid hard theoretical derivation by computing the evidence value. besides, \textit{n}fbst can test not only global significance but also local and instance-wise significance, which previous testing methods don't focus on. moreover, \textit{n}fbst is a general framework that can be extended based on the measures selected, such as grad-\textit{n}fbst, lrp-\textit{n}fbst, deeplift-\textit{n}fbst, lime-\textit{n}fbst. a range of experiments on both simulated and real data are conducted to show the advantages of our method.",,2024-01-24,,"['zehua liu', 'zimeng li', 'jingyuan wang', 'yue he']"
2401.13544,beyond concept bottleneck models: how to make black boxes intervenable?,cs.lg stat.ml,"recently, interpretable machine learning has re-explored concept bottleneck models (cbm), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts. a compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output. in this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set. furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models. empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. we demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. to showcase the practical utility of the proposed techniques, we apply them to deep chest x-ray classifiers and show that fine-tuned black boxes can be as intervenable and more performant than cbms.",,2024-01-24,,"['ričards marcinkevičs', 'sonia laguna', 'moritz vandenhirtz', 'julia e. vogt']"
2401.13624,can overfitted deep neural networks in adversarial training generalize?   -- an approximation viewpoint,stat.ml cs.lg,"adversarial training is a widely used method to improve the robustness of deep neural networks (dnns) over adversarial perturbations. however, it is empirically observed that adversarial training on over-parameterized networks often suffers from the \textit{robust overfitting}: it can achieve almost zero adversarial training error while the robust generalization performance is not promising. in this paper, we provide a theoretical understanding of the question of whether overfitted dnns in adversarial training can generalize from an approximation viewpoint. specifically, our main results are summarized into three folds: i) for classification, we prove by construction the existence of infinitely many adversarial training classifiers on over-parameterized dnns that obtain arbitrarily small adversarial training error (overfitting), whereas achieving good robust generalization error under certain conditions concerning the data quality, well separated, and perturbation level. ii) linear over-parameterization (meaning that the number of parameters is only slightly larger than the sample size) is enough to ensure such existence if the target function is smooth enough. iii) for regression, our results demonstrate that there also exist infinitely many overfitted dnns with linear over-parameterization in adversarial training that can achieve almost optimal rates of convergence for the standard generalization error. overall, our analysis points out that robust overfitting can be avoided but the required model capacity will depend on the smoothness of the target function, while a robust generalization gap is inevitable. we hope our analysis will give a better understanding of the mathematical foundations of robustness in dnns from an approximation view.",,2024-01-24,,"['zhongjie shi', 'fanghui liu', 'yuan cao', 'johan a. k. suykens']"
2401.13665,entrywise inference for causal panel data: a simple and instance-optimal   approach,math.st econ.em stat.me stat.ml stat.th,"in causal inference with panel data under staggered adoption, the goal is to estimate and derive confidence intervals for potential outcomes and treatment effects. we propose a computationally efficient procedure, involving only simple matrix algebra and singular value decomposition. we derive non-asymptotic bounds on the entrywise error, establishing its proximity to a suitably scaled gaussian variable. despite its simplicity, our procedure turns out to be instance-optimal, in that our theoretical scaling matches a local instance-wise lower bound derived via a bayesian cram\'{e}r-rao argument. using our insights, we develop a data-driven procedure for constructing entrywise confidence intervals with pre-specified coverage guarantees. our analysis is based on a general inferential toolbox for the svd algorithm applied to the matrix denoising model, which might be of independent interest.",,2024-01-24,,"['yuling yan', 'martin j. wainwright']"
2401.13708,accelerating hyperbolic t-sne,cs.hc cs.ai cs.lg q-bio.qm stat.ml,"the need to understand the structure of hierarchical or high-dimensional data is present in a variety of fields. hyperbolic spaces have proven to be an important tool for embedding computations and analysis tasks as their non-linear nature lends itself well to tree or graph data. subsequently, they have also been used in the visualization of high-dimensional data, where they exhibit increased embedding performance. however, none of the existing dimensionality reduction methods for embedding into hyperbolic spaces scale well with the size of the input data. that is because the embeddings are computed via iterative optimization schemes and the computation cost of every iteration is quadratic in the size of the input. furthermore, due to the non-linear nature of hyperbolic spaces, euclidean acceleration structures cannot directly be translated to the hyperbolic setting. this paper introduces the first acceleration structure for hyperbolic embeddings, building upon a polar quadtree. we compare our approach with existing methods and demonstrate that it computes embeddings of similar quality in significantly less time. implementation and scripts for the experiments can be found at https://graphics.tudelft.nl/accelerating-hyperbolic-tsne.",,2024-01-23,,"['martin skrodzki', 'hunter van geffen', 'nicolas f. chaves-de-plaza', 'thomas höllt', 'elmar eisemann', 'klaus hildebrandt']"
2401.13744,conformal prediction sets improve human decision making,cs.lg cs.hc stat.ml,"in response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. in this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. with statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. the results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-ai teams.",,2024-01-24,2024-02-01,"['jesse c. cresswell', 'yi sui', 'bhargava kumar', 'noël vouitsis']"
2401.13751,a systematic approach to robustness modelling for deep convolutional   neural networks,cs.lg cs.ai cs.cv stat.ml,"convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. the recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. in particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. as such, we examine the role of the number of hidden layers in the resnet model, demonstrated on the mnist, cifar10, cifar100 datasets. we test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. to encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a function of time and relate that to a novel metric that allows us to quickly determine whether or not the cost of training a model outweighs the cost of attacking it. using this approach, we are able to approximate the expected failure rate using a small number of specially crafted samples rather than increasingly larger benchmark datasets. we demonstrate the efficacy of this technique on both the mnist and cifar10 datasets using 8-, 16-, 32-, and 64-bit floating-point numbers, various data pre-processing techniques, and several attacks on five configurations of the resnet model. then, using empirical measurements, we examine the various trade-offs between cost, robustness, latency, and reliability to find that larger models do not significantly aid in adversarial robustness despite costing significantly more to train.",,2024-01-24,,"['charles meyers', 'mohammad reza saleh sedghpour', 'tommy löfstedt', 'erik elmroth']"
2401.13787,bayesian analysis of the beta regression model subject to linear   inequality restrictions with application,stat.me stat.co,"rerecent studies in machine learning are based on models in which parameters or state variables are bounded restricted. these restrictions are from prior information to ensure the validity of scientific theories or structural consistency based on physical phenomena. the valuable information contained in the restrictions must be considered during the estimation process to improve estimation accuracy. many researchers have focused on linear regression models subject to linear inequality restrictions, but generalized linear models have received little attention. in this paper, the parameters of beta bayesian regression models subjected to linear inequality restrictions are estimated. the proposed bayesian restricted estimator, which is demonstrated by simulated studies, outperforms ordinary estimators. even in the presence of multicollinearity, it outperforms the ridge estimator in terms of the standard deviation and the mean squared error. the results confirm that the proposed bayesian restricted estimator makes sparsity in parameter estimating without using the regularization penalty. finally, a real data set is analyzed by the new proposed bayesian estimation method.",,2024-01-24,,"['solmaz seifollahi', 'hossein bevrani', 'kristofer mansson']"
2401.13848,a v2x-based privacy preserving federated measuring and learning system,cs.lg cs.ai cs.cr stat.ml,"future autonomous vehicles (avs) will use a variety of sensors that generate a vast amount of data. naturally, this data not only serves self-driving algorithms; but can also assist other vehicles or the infrastructure in real-time decision-making. consequently, vehicles shall exchange their measurement data over vehicle-to-everything (v2x) technologies. moreover, predicting the state of the road network might be beneficial too. with such a prediction, we might mitigate road congestion, balance parking lot usage, or optimize the traffic flow. that would decrease transportation costs as well as reduce its environmental impact.   in this paper, we propose a federated measurement and learning system that provides real-time data to fellow vehicles over vehicle-to-vehicle (v2v) communication while also operating a federated learning (fl) scheme over the vehicle-to-network (v2n) link to create a predictive model of the transportation network. as we are yet to have real-world av data, we model it with a non-iid (independent and identically distributed) dataset to evaluate the capabilities of the proposed system in terms of performance and privacy. results indicate that the proposed fl scheme improves learning performance and prevents eavesdropping at the aggregator server side.",,2024-01-24,,"['levente alekszejenkó', 'tadeusz dobrowiecki']"
2401.13875,is temperature sample efficient for softmax gaussian mixture of experts?,stat.ml cs.lg,"dense-to-sparse gating mixture of experts (moe) has recently become an effective alternative to a well-known sparse moe. rather than fixing the number of activated experts as in the latter model, which could limit the investigation of potential experts, the former model utilizes the temperature to control the softmax weight distribution and the sparsity of the moe during training in order to stabilize the expert specialization. nevertheless, while there are previous attempts to theoretically comprehend the sparse moe, a comprehensive analysis of the dense-to-sparse gating moe has remained elusive. therefore, we aim to explore the impacts of the dense-to-sparse gate on the maximum likelihood estimation under the gaussian moe in this paper. we demonstrate that due to interactions between the temperature and other model parameters via some partial differential equations, the convergence rates of parameter estimations are slower than any polynomial rates, and could be as slow as $\mathcal{o}(1/\log(n))$, where $n$ denotes the sample size. to address this issue, we propose using a novel activation dense-to-sparse gate, which routes the output of a linear layer to an activation function before delivering them to the softmax function. by imposing linearly independence conditions on the activation function and its derivatives, we show that the parameter estimation rates are significantly improved to polynomial rates.",,2024-01-24,,"['huy nguyen', 'pedram akbarian', 'nhat ho']"
2401.13880,principal component regression to study the impact of economic factors   on disadvantaged communities,stat.ap,"the council on environmental quality's climate and economic justice screening tool defines ""disadvantaged communities"" (dac) in the usa, highlighting census tracts where benefits of climate and energy investments are not accruing. we use a principal component generalized linear model, which addresses the intertwined nature of economic factors, income and employment and model their relationship to dac status. our study 1) identifies the most significant income groups and employment industries that impact dac status, 2) provides the probability of dac status across census tracts and compares the predictive accuracy with widely used machine learning approaches, 3) obtains historical predictions of the probability of dac status, 4) obtains spatial downscaling of dac status across block groups. our study provides valuable insights for policymakers and stakeholders to develop strategies that promote sustainable development and address inequities in climate and energy investments in the usa.",,2024-01-24,,"['narmadha m. mohankumar', 'milan jain', 'heng wan', 'sumitrra ganguli', 'kyle d. wilson', 'david m. anderson']"
2401.13884,"constant stepsize q-learning: distributional convergence, bias and   extrapolation",stat.ml cs.lg math.oc,"stochastic approximation (sa) is a widely used algorithmic approach in various fields, including optimization and reinforcement learning (rl). among rl algorithms, q-learning is particularly popular due to its empirical success. in this paper, we study asynchronous q-learning with constant stepsize, which is commonly used in practice for its fast convergence. by connecting the constant stepsize q-learning to a time-homogeneous markov chain, we show the distributional convergence of the iterates in wasserstein distance and establish its exponential convergence rate. we also establish a central limit theory for q-learning iterates, demonstrating the asymptotic normality of the averaged iterates. moreover, we provide an explicit expansion of the asymptotic bias of the averaged iterate in stepsize. specifically, the bias is proportional to the stepsize up to higher-order terms and we provide an explicit expression for the linear coefficient. this precise characterization of the bias allows the application of richardson-romberg (rr) extrapolation technique to construct a new estimate that is provably closer to the optimal q function. numerical results corroborate our theoretical finding on the improvement of the rr extrapolation method.",,2024-01-24,,"['yixuan zhang', 'qiaomin xie']"
2401.13913,spectral clustering for discrete distributions,cs.lg cs.ai stat.ml,"discrete distribution clustering (d2c) was often solved by wasserstein barycenter methods. these methods are under a common assumption that clusters can be well represented by barycenters, which may not hold in many real applications. in this work, we propose a simple yet effective framework based on spectral clustering and distribution affinity measures (e.g., maximum mean discrepancy and wasserstein distance) for d2c. to improve the scalability, we propose to use linear optimal transport to construct affinity matrices efficiently on large datasets. we provide theoretical guarantees for the success of the proposed methods in clustering distributions. experiments on synthetic and real data show that our methods outperform the baselines largely in terms of both clustering accuracy and computational efficiency.",,2024-01-24,,"['zixiao wang', 'dong qiao', 'jicong fan']"
2401.13929,reinforcement learning with hidden markov models for discovering   decision-making dynamics,cs.lg stat.ap stat.me stat.ml,"major depressive disorder (mdd) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for mdd. to measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. reinforcement learning (rl) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. recent findings suggest the inadequacy of characterizing reward learning solely based on a single rl model; instead, there may be a switching of decision-making processes between multiple strategies. an important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with mdd. motivated by the probabilistic reward task (prt) within the embarc study, we propose a novel rl-hmm framework for analyzing reward-based decision-making. our model accommodates learning strategy switching between two distinct approaches under a hidden markov model (hmm): subjects making decisions based on the rl model or opting for random choices. we account for continuous rl state space and allow time-varying transition probabilities in the hmm. we introduce a computationally efficient em algorithm for parameter estimation and employ a nonparametric bootstrap for inference. we apply our approach to the embarc study to show that mdd patients are less engaged in rl compared to the healthy controls, and engagement is associated with brain activities in the negative affect circuitry during an emotional conflict task.",,2024-01-24,,"['xingche guo', 'donglin zeng', 'yuanjia wang']"
2401.13935,a new paradigm for counterfactual reasoning in fairness and recourse,cs.ai cs.cy stat.ml,"counterfactuals and counterfactual reasoning underpin numerous techniques for auditing and understanding artificial intelligence (ai) systems. the traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. for this reason, the starting point for causal reasoning about legal protections and demographic data in ai is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. we ask, for example, what would have happened had your race been different? an inherent limitation of this paradigm is that some demographic interventions -- like interventions on race -- may not translate into the formalisms of interventional counterfactuals. in this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. we ask instead, what would explain a counterfactual outcome for you as you actually are or could be? this alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions.",,2024-01-24,,"['lucius e. j. bynum', 'joshua r. loftus', 'julia stoyanovich']"
2401.13975,sparse signal recovery and source localization via covariance learning,stat.me,"in the multiple measurements vector (mmv) model, measurement vectors are connected to unknown, jointly sparse signal vectors through a linear regression model employing a single known measurement matrix (or dictionary). typically, the number of atoms (columns of the dictionary) is greater than the number measurements and the sparse signal recovery problem is generally ill-posed. in this paper, we treat the signals and measurement noise as independent gaussian random vectors with unknown signal covariance matrix and noise variance, respectively, and derive fixed point (fp) equation for solving the likelihood equation for signal powers, thereby enabling the recovery of the sparse signal support (sources with non-zero variances). two practical algorithms, a block coordinate descent (bcd) and a cyclic coordinate descent (ccd) algorithms, that leverage on the fp characterization of the likelihood equation are then proposed. additionally, a greedy pursuit method, analogous to popular simultaneous orthogonal matching pursuit (omp), is introduced. our numerical examples demonstrate effectiveness of the proposed covariance learning (cl) algorithms both in classic sparse signal recovery as well as in direction-of-arrival (doa) estimation problems where they perform favourably compared to the state-of-the-art algorithms under a broad variety of settings.",,2024-01-25,,['esa ollila']
2401.14142,"energy-based concept bottleneck models: unifying prediction, concept   intervention, and probabilistic interpretations",cs.cv cs.ai cs.lg stat.ml,"existing methods, such as concept bottleneck models (cbms), have been successful in providing concept-based interpretations for black-box deep learning models. they typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. however, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., ""yellow breast"") does not help correct highly correlated concepts (e.g., ""yellow belly""), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label ""kentucky warbler"" and a concept ""black bill"", what is the probability that the model correctly predicts another concept ""black crown""), therefore failing to provide deeper insight into how a black-box model works. in response to these limitations, we propose energy-based concept bottleneck models (ecbms). our ecbms use a set of neural networks to define the joint energy of candidate (input, concept, class) tuples. with such a unified interface, prediction, concept correction, and conditional dependency quantification are then represented as conditional probabilities, which are generated by composing different energy functions. our ecbms address both limitations of existing cbms, providing higher accuracy and richer concept interpretations. empirical results show that our approach outperforms the state-of-the-art on real-world datasets.",,2024-01-25,2024-02-26,"['xinyue xu', 'yi qin', 'lu mi', 'hao wang', 'xiaomeng li']"
2401.14161,adapting tree-based multiple imputation methods for multi-level data? a   simulation study,stat.ap stat.ml,"this simulation study evaluates the effectiveness of multiple imputation (mi) techniques for multilevel data. it compares the performance of traditional multiple imputation by chained equations (mice) with tree-based methods such as chained random forests with predictive mean matching and extreme gradient boosting. adapted versions that include dummy variables for cluster membership are also included for the tree-based methods. methods are evaluated for coefficient estimation bias, statistical power, and type i error rates on simulated hierarchical data with different cluster sizes (25 and 50) and levels of missingness (10\% and 50\%). coefficients are estimated using random intercept and random slope models. the results show that while mice is preferred for accurate rejection rates, extreme gradient boosting is advantageous for reducing bias. furthermore, the study finds that bias levels are similar across different cluster sizes, but rejection rates tend to be less favorable with fewer clusters (lower power, higher type i error). in addition, the inclusion of cluster dummies in tree-based methods improves estimation for level 1 variables, but is less effective for level 2 variables. when data become too complex and mice is too slow, extreme gradient boosting is a good alternative for hierarchical data.   keywords: multiple imputation; multi-level data; mice; missranger; mixgb",,2024-01-25,,"['ketevan gurtskaia', 'jakob schwerter', 'philipp doebler']"
2401.14210,at the junction between deep learning and statistics of extremes:   formalizing the landslide hazard definition,cs.lg physics.geo-ph stat.ap stat.ml,"the most adopted definition of landslide hazard combines spatial information about landslide location (susceptibility), threat (intensity), and frequency (return period). only the first two elements are usually considered and estimated when working over vast areas. even then, separate models constitute the standard, with frequency being rarely investigated. frequency and intensity are intertwined and depend on each other because larger events occur less frequently and vice versa. however, due to the lack of multi-temporal inventories and joint statistical models, modelling such properties via a unified hazard model has always been challenging and has yet to be attempted. here, we develop a unified model to estimate landslide hazard at the slope unit level to address such gaps. we employed deep learning, combined with a model motivated by extreme-value theory to analyse an inventory of 30 years of observed rainfall-triggered landslides in nepal and assess landslide hazard for multiple return periods. we also use our model to further explore landslide hazard for the same return periods under different climate change scenarios up to the end of the century. our results show that the proposed model performs excellently and can be used to model landslide hazard in a unified manner. geomorphologically, we find that under both climate change scenarios (ssp245 and ssp885), landslide hazard is likely to increase up to two times on average in the lower himalayan regions while remaining the same in the middle himalayan region whilst decreasing slightly in the upper himalayan region areas.",,2024-01-25,,"['ashok dahal', 'raphaël huser', 'luigi lombardo']"
2401.14283,information leakage detection through approximate bayes-optimal   prediction,stat.ml cs.lg,"in today's data-driven world, the proliferation of publicly available information intensifies the challenge of information leakage (il), raising security concerns. il involves unintentionally exposing secret (sensitive) information to unauthorized parties via systems' observable information. conventional statistical approaches, which estimate mutual information (mi) between observable and secret information for detecting il, face challenges such as the curse of dimensionality, convergence, computational complexity, and mi misestimation. furthermore, emerging supervised machine learning (ml) methods, though effective, are limited to binary system-sensitive information and lack a comprehensive theoretical framework. to address these limitations, we establish a theoretical framework using statistical learning theory and information theory to accurately quantify and detect il. we demonstrate that mi can be accurately estimated by approximating the log-loss and accuracy of the bayes predictor. as the bayes predictor is typically unknown in practice, we propose to approximate it with the help of automated machine learning (automl). first, we compare our mi estimation approaches against current baselines, using synthetic data sets generated using the multivariate normal (mvn) distribution with known mi. second, we introduce a cut-off technique using one-sided statistical tests to detect il, employing the holm-bonferroni correction to increase confidence in detection decisions. our study evaluates il detection performance on real-world data sets, highlighting the effectiveness of the bayes predictor's log-loss estimation, and finds our proposed method to effectively estimate mi on synthetic data sets and thus detect ils accurately.",,2024-01-25,,"['pritha gupta', 'marcel wever', 'eyke hüllermeier']"
2401.14340,estimation of partially known gaussian graphical models with score-based   structural priors,stat.ml cs.lg,"we propose a novel algorithm for the support estimation of partially known gaussian graphical models that incorporates prior information about the underlying graph. in contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed langevin diffusion to generate samples from the posterior distribution. since the langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). numerical experiments demonstrate the benefits of our approach.",,2024-01-25,2024-02-23,"['martín sevilla', 'antonio garcía marques', 'santiago segarra']"
2401.14343,class-attribute priors: adapting optimization to heterogeneity and   fairness objective,cs.lg cs.cy stat.ml,"modern classification problems exhibit heterogeneities across individual classes: each class may have unique attributes, such as sample size, label quality, or predictability (easy vs difficult), and variable importance at test-time. without care, these heterogeneities impede the learning process, most notably, when optimizing fairness objectives. confirming this, under a gaussian mixture setting, we show that the optimal svm classifier for balanced accuracy needs to be adaptive to the class attributes. this motivates us to propose cap: an effective and general method that generates a class-specific learning strategy (e.g. hyperparameter) based on the attributes of that class. this way, optimization process better adapts to heterogeneities. cap leads to substantial improvements over the naive approach of assigning separate hyperparameters to each class. we instantiate cap for loss function design and post-hoc logit adjustment, with emphasis on label-imbalanced problems. we show that cap is competitive with prior art and its flexibility unlocks clear benefits for fairness objectives beyond balanced accuracy. finally, we evaluate cap on problems with label noise as well as weighted test objectives to showcase how cap can jointly adapt to different heterogeneities.",,2024-01-25,,"['xuechen zhang', 'mingchen li', 'jiasi chen', 'christos thrampoulidis', 'samet oymak']"
2401.14345,uncovering heterogeneity of solar flare mechanism with mixture models,astro-ph.sr stat.ap stat.me,"the physics of solar flares occurring on the sun is highly complex and far from fully understood. however, observations show that solar eruptions are associated with the intense kilogauss fields of active regions, where free energies are stored with field-aligned electric currents. with the advent of high-quality data sources such as the geostationary operational environmental satellites (goes) and solar dynamics observatory (sdo)/helioseismic and magnetic imager (hmi), recent works on solar flare forecasting have been focusing on data-driven methods. in particular, black box machine learning and deep learning models are increasingly adopted in which underlying data structures are not modeled explicitly. if the active regions indeed follow the same laws of physics, there should be similar patterns shared among them, reflected by the observations. yet, these black box models currently used in the literature do not explicitly characterize the heterogeneous nature of the solar flare data, within and between active regions. in this paper, we propose two finite mixture models designed to capture the heterogeneous patterns of active regions and their associated solar flare events. with extensive numerical studies, we demonstrate the usefulness of our proposed method for both resolving the sample imbalance issue and modeling the heterogeneity for rare energetic solar flare events.",,2024-01-25,,"['bach viet do', 'yang chen', 'xuanlong nguyen', 'ward manchester']"
2401.14411,precision mars entry navigation with atmospheric density adaptation via   neural networks,cs.lg cs.sy eess.sy stat.ap,"discrepancies between the true martian atmospheric density and the onboard density model can significantly impair the performance of spacecraft entry navigation filters. this work introduces a new approach to online filtering for martian entry by using a neural network to estimate atmospheric density and employing a consider analysis to account for the uncertainty in the estimate. the network is trained on an exponential atmospheric density model, and its parameters are dynamically adapted in real time to account for any mismatches between the true and estimated densities. the adaptation of the network is formulated as a maximum likelihood problem, leveraging the measurement innovations of the filter to identify optimal network parameters. the incorporation of a neural network enables the use of stochastic optimizers known for their efficiency in the machine learning domain within the context of the maximum likelihood approach. performance comparisons against previous approaches are conducted in various realistic mars entry navigation scenarios, resulting in superior estimation accuracy and precise alignment of the estimated density with a broad selection of realistic martian atmospheres sampled from perturbed mars-gram data.",,2024-01-17,,"['felipe giraldo-grueso', 'andrey a. popov', 'renato zanetti']"
2401.14421,multi-agent based transfer learning for data-driven air traffic   applications,cs.lg cs.ma cs.sy eess.sy stat.ml,"research in developing data-driven models for air traffic management (atm) has gained a tremendous interest in recent years. however, data-driven models are known to have long training time and require large datasets to achieve good performance. to address the two issues, this paper proposes a multi-agent bidirectional encoder representations from transformers (ma-bert) model that fully considers the multi-agent characteristic of the atm system and learns air traffic controllers' decisions, and a pre-training and fine-tuning transfer learning framework. by pre-training the ma-bert on a large dataset from a major airport and then fine-tuning it to other airports and specific air traffic applications, a large amount of the total training time can be saved. in addition, for newly adopted procedures and constructed airports where no historical data is available, this paper shows that the pre-trained ma-bert can achieve high performance by updating regularly with little data. the proposed transfer learning framework and ma-bert are tested with the automatic dependent surveillance-broadcast data recorded in 3 airports in south korea in 2019.",,2024-01-23,,"['chuhao deng', 'hong-cheol choi', 'hyunsang park', 'inseok hwang']"
2401.14429,[re] the discriminative kalman filter for bayesian filtering with   nonlinear and non-gaussian observation models,cs.lg cs.ro eess.sp stat.ml,"kalman filters provide a straightforward and interpretable means to estimate hidden or latent variables, and have found numerous applications in control, robotics, signal processing, and machine learning. one such application is neural decoding for neuroprostheses. in 2020, burkhart et al. thoroughly evaluated their new version of the kalman filter that leverages bayes' theorem to improve filter performance for highly non-linear or non-gaussian observation models. this work provides an open-source python alternative to the authors' matlab algorithm. specifically, we reproduce their most salient results for neuroscientific contexts and further examine the efficacy of their filter using multiple random seeds and previously unused trials from the authors' dataset. all experiments were performed offline on a single computer.",,2024-01-24,,"['josue casco-rodriguez', 'caleb kemere', 'richard g. baraniuk']"
2401.14442,improving antibody humanness prediction using patent data,q-bio.qm cs.lg stat.ml,"we investigate the potential of patent data for improving the antibody humanness prediction using a multi-stage, multi-loss training process. humanness serves as a proxy for the immunogenic response to antibody therapeutics, one of the major causes of attrition in drug discovery and a challenging obstacle for their use in clinical settings. we pose the initial learning stage as a weakly-supervised contrastive-learning problem, where each antibody sequence is associated with possibly multiple identifiers of function and the objective is to learn an encoder that groups them according to their patented properties. we then freeze a part of the contrastive encoder and continue training it on the patent data using the cross-entropy loss to predict the humanness score of a given antibody sequence. we illustrate the utility of the patent data and our approach by performing inference on three different immunogenicity datasets, unseen during training. our empirical results demonstrate that the learned model consistently outperforms the alternative baselines and establishes new state-of-the-art on five out of six inference tasks, irrespective of the used metric.",,2024-01-25,2024-01-31,"['talip ucar', 'aubin ramon', 'dino oglic', 'rebecca croasdale-wood', 'tom diethe', 'pietro sormanni']"
2401.14483,"four facets of forecast felicity: calibration, predictiveness,   randomness and regret",cs.lg stat.ml,"machine learning is about forecasting. forecasts, however, obtain their usefulness only through their evaluation. machine learning has traditionally focused on types of losses and their corresponding regret. currently, the machine learning community regained interest in calibration. in this work, we show the conceptual equivalence of calibration and regret in evaluating forecasts. we frame the evaluation problem as a game between a forecaster, a gambler and nature. putting intuitive restrictions on gambler and forecaster, calibration and regret naturally fall out of the framework. in addition, this game links evaluation of forecasts to randomness of outcomes. random outcomes with respect to forecasts are equivalent to good forecasts with respect to outcomes. we call those dual aspects, calibration and regret, predictiveness and randomness, the four facets of forecast felicity.",,2024-01-25,,"['rabanus derr', 'robert c. williamson']"
2401.14498,predictive analysis for optimizing port operations,cs.lg cs.sy eess.sy stat.ap stat.ml,"maritime transport is a pivotal logistics mode for the long-distance and bulk transportation of goods. however, the intricate planning involved in this mode is often hindered by uncertainties, including weather conditions, cargo diversity, and port dynamics, leading to increased costs. consequently, accurately estimating vessel total (stay) time at port and potential delays becomes imperative for effective planning and scheduling in port operations. this study aims to develop a port operation solution with competitive prediction and classification capabilities for estimating vessel total and delay times. this research addresses a significant gap in port analysis models for vessel stay and delay times, offering a valuable contribution to the field of maritime logistics. the proposed solution is designed to assist decision-making in port environments and predict service delays. this is demonstrated through a case study on brazil ports. additionally, feature analysis is used to understand the key factors impacting maritime logistics, enhancing the overall understanding of the complexities involved in port operations.",,2024-01-25,,"['aniruddha rajendra rao', 'haiyan wang', 'chetan gupta']"
2401.14539,understanding disparities in post hoc machine learning explanation,cs.lg cs.cy stat.ml,"previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across 'race' and 'gender' as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network models that are better able to capture the underlying functional form in comparison to linear models. we also observe consistent findings regarding the effect of concept shift and omitted variable bias on explanation disparities in the adult income dataset. overall, results indicate that disparities in model explanations can also depend on data and model properties. based on this systematic investigation, we provide recommendations for the design of explanation methods that mitigate undesirable disparities.",,2024-01-25,,"['vishwali mhasawade', 'salman rahman', 'zoe haskell-craig', 'rumi chunara']"
2401.14593,robust estimation of the tail index of a single parameter pareto   distribution from grouped data,stat.me math.st q-fin.rm stat.co stat.ml stat.th,"numerous robust estimators exist as alternatives to the maximum likelihood estimator (mle) when a completely observed ground-up loss severity sample dataset is available. however, the options for robust alternatives to mle become significantly limited when dealing with grouped loss severity data, with only a handful of methods like least squares, minimum hellinger distance, and optimal bounded influence function available. this paper introduces a novel robust estimation technique, the method of truncated moments (mtum), specifically designed to estimate the tail index of a pareto distribution from grouped data. inferential justification of mtum is established by employing the central limit theorem and validating them through a comprehensive simulation study.",,2024-01-25,2024-02-20,['chudamani poudyal']
2401.14655,distributionally robust optimization and robust statistics,stat.me,"we review distributionally robust optimization (dro), a principled approach for constructing statistical estimators that hedge against the impact of deviations in the expected loss between the training and deployment environments. many well-known estimators in statistics and machine learning (e.g. adaboost, lasso, ridge regression, dropout training, etc.) are distributionally robust in a precise sense. we hope that by discussing the dro interpretation of well-known estimators, statisticians who may not be too familiar with dro may find a way to access the dro literature through the bridge between classical results and their dro equivalent formulation. on the other hand, the topic of robustness in statistics has a rich tradition associated with removing the impact of contamination. thus, another objective of this paper is to clarify the difference between dro and classical statistical robustness. as we will see, these are two fundamentally different philosophies leading to completely different types of estimators. in dro, the statistician hedges against an environment shift that occurs after the decision is made; thus dro estimators tend to be pessimistic in an adversarial setting, leading to a min-max type formulation. in classical robust statistics, the statistician seeks to correct contamination that occurred before a decision is made; thus robust statistical estimators tend to be optimistic leading to a min-min type formulation.",,2024-01-26,,"['jose blanchet', 'jiajin li', 'sirui lin', 'xuhui zhang']"
2401.14657,validating climate models with spherical convolutional wasserstein   distance,stat.ap cs.lg physics.ao-ph stat.ml,"the validation of global climate models is crucial to ensure the accuracy and efficacy of model output. we introduce the spherical convolutional wasserstein distance to more comprehensively measure differences between climate models and reanalysis data. this new similarity measure accounts for spatial variability using convolutional projections and quantifies local differences in the distribution of climate variables. we apply this method to evaluate the historical model outputs of the coupled model intercomparison project (cmip) members by comparing them to observational and reanalysis data products. additionally, we investigate the progression from cmip phase 5 to phase 6 and find modest improvements in the phase 6 models regarding their ability to produce realistic climatologies.",,2024-01-26,,"['robert c. garrett', 'trevor harris', 'bo li', 'zhuo wang']"
2401.14722,a nonparametric bayes approach to online activity prediction,stat.me cs.lg stat.ap stat.ml,"accurately predicting the onset of specific activities within defined timeframes holds significant importance in several applied contexts. in particular, accurate prediction of the number of future users that will be exposed to an intervention is an important piece of information for experimenters running online experiments (a/b tests). in this work, we propose a novel approach to predict the number of users that will be active in a given time period, as well as the temporal trajectory needed to attain a desired user participation threshold. we model user activity using a bayesian nonparametric approach which allows us to capture the underlying heterogeneity in user engagement. we derive closed-form expressions for the number of new users expected in a given period, and a simple monte carlo algorithm targeting the posterior distribution of the number of days needed to attain a desired number of users; the latter is important for experimental planning. we illustrate the performance of our approach via several experiments on synthetic and real world data, in which we show that our novel method outperforms existing competitors.",,2024-01-26,,"['mario beraha', 'lorenzo masoero', 'stefano favaro', 'thomas s. richardson']"
2401.14868,particle-mala and particle-mgrad: gradient-based mcmc methods for   high-dimensional state-space models,stat.co stat.ml,"state-of-the-art methods for bayesian inference in state-space models are (a) conditional sequential monte carlo (csmc) algorithms; (b) sophisticated 'classical' mcmc algorithms like mala, or mgrad from titsias and papaspiliopoulos (2018, arxiv:1610.09641v3 [stat.ml]). the former propose $n$ particles at each time step to exploit the model's 'decorrelation-over-time' property and thus scale favourably with the time horizon, $t$ , but break down if the dimension of the latent states, $d$, is large. the latter leverage gradient-/prior-informed local proposals to scale favourably with $d$ but exhibit sub-optimal scalability with $t$ due to a lack of model-structure exploitation. we introduce methods which combine the strengths of both approaches. the first, particle-mala, spreads $n$ particles locally around the current state using gradient information, thus extending mala to $t > 1$ time steps and $n > 1$ proposals. the second, particle-mgrad, additionally incorporates (conditionally) gaussian prior dynamics into the proposal, thus extending the mgrad algorithm to $t > 1$ time steps and $n > 1$ proposals. we prove that particle-mgrad interpolates between csmc and particle-mala, resolving the 'tuning problem' of choosing between csmc (superior for highly informative prior dynamics) and particle-mala (superior for weakly informative prior dynamics). we similarly extend other 'classical' mcmc approaches like auxiliary mala, agrad, and preconditioned crank-nicolson-langevin (pcnl) to $t > 1$ time steps and $n > 1$ proposals. in experiments, for both highly and weakly informative prior dynamics, our methods substantially improve upon both csmc and sophisticated 'classical' mcmc approaches.",,2024-01-26,,"['adrien corenflos', 'axel finke']"
2401.14884,p3ls: partial least squares under privacy preservation,stat.ml cs.cr cs.lg,"modern manufacturing value chains require intelligent orchestration of processes across company borders in order to maximize profits while fostering social and environmental sustainability. however, the implementation of integrated, systems-level approaches for data-informed decision-making along value chains is currently hampered by privacy concerns associated with cross-organizational data exchange and integration. we here propose privacy-preserving partial least squares (p3ls) regression, a novel federated learning technique that enables cross-organizational data integration and process modeling with privacy guarantees. p3ls involves a singular value decomposition (svd) based pls algorithm and employs removable, random masks generated by a trusted authority in order to protect the privacy of the data contributed by each data holder. we demonstrate the capability of p3ls to vertically integrate process data along a hypothetical value chain consisting of three parties and to improve the prediction performance on several process-related key performance indicators. furthermore, we show the numerical equivalence of p3ls and pls model components on simulated data and provide a thorough privacy analysis of the former. moreover, we propose a mechanism for determining the relevance of the contributed data to the problem being addressed, thus creating a basis for quantifying the contribution of participants.",,2024-01-26,,"['du nguyen duy', 'ramin nikzad-langerodi']"
2401.14893,a structured regression approach for evaluating model performance across   intersectional subgroups,cs.lg cs.cy stat.ap stat.ml,"disaggregated evaluation is a central task in ai fairness assessment, with the goal to measure an ai system's performance across different subgroups defined by combinations of demographic or other sensitive attributes. the standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. however, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are considered in many disaggregated evaluations. in this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. we also provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectional groups. we evaluate our approach on two publicly available datasets, and several variants of semi-synthetic data. the results show that our method is considerably more accurate than the standard approach, especially for small subgroups, and goodness-of-fit testing helps identify the key factors that drive differences in performance.",,2024-01-26,,"['christine herlihy', 'kimberly truong', 'alexandra chouldechova', 'miroslav dudik']"
2401.14943,analysing the influence of macroeconomic factors on credit risk in the   uk banking sector,cs.ir stat.ap,"macroeconomic factors have a critical impact on banking credit risk, which cannot be directly controlled by banks, and therefore, there is a need for an early credit risk warning system based on the macroeconomy. by comparing different predictive models (traditional statistical and machine learning algorithms), this study aims to examine the macroeconomic determinants impact on the uk banking credit risk and assess the most accurate credit risk estimate using predictive analytics. this study found that the variance-based multi-split decision tree algorithm is the most precise predictive model with interpretable, reliable, and robust results. our model performance achieved 95% accuracy and evidenced that unemployment and inflation rate are significant credit risk predictors in the uk banking context. our findings provided valuable insights such as a positive association between credit risk and inflation, the unemployment rate, and national savings, as well as a negative relationship between credit risk and national debt, total trade deficit, and national income. in addition, we empirically showed the relationship between national savings and non-performing loans, thus proving the paradox of thrift. these findings benefit the credit risk management team in monitoring the macroeconomic factors thresholds and implementing critical reforms to mitigate credit risk.",10.3390/analytics3010005,2024-01-26,,"['hemlata sharma', 'aparna andhalkar', 'oluwaseun ajao', 'bayode ogunleye']"
2401.14973,discovering group dynamics in synchronous time series via hierarchical   recurrent switching-state models,stat.ml cs.lg,"we seek to model a collection of time series arising from multiple entities interacting over the same time period. recent work focused on modeling individual time series is inadequate for our intended applications, where collective system-level behavior influences the trajectories of individual entities. to address such problems, we present a new hierarchical switching-state model that can be trained in an unsupervised fashion to simultaneously explain both system-level and individual-level dynamics. we employ a latent system-level discrete state markov chain that drives latent entity-level chains which in turn govern the dynamics of each observed time series. feedback from the observations to the chains at both the entity and system levels improves flexibility via context-dependent state transitions. our hierarchical switching recurrent dynamical models can be learned via closed-form variational coordinate ascent updates to all latent chains that scale linearly in the number of individual time series. this is asymptotically no more costly than fitting separate models for each entity. experiments on synthetic and real datasets show that our model can produce better forecasts of future entity behavior than existing methods. moreover, the availability of latent state chains at both the entity and system level enables interpretation of group dynamics.",,2024-01-26,,"['michael wojnowicz', 'preetish rath', 'eric miller', 'jeffrey miller', 'clifford hancock', ""meghan o'donovan"", 'seth elkin-frankston', 'thaddeus brunye', 'michael c. hughes']"
2401.14989,mapping-to-parameter nonlinear functional regression with novel b-spline   free knot placement algorithm,cs.lg stat.ml,"we propose a novel approach to nonlinear functional regression, called the mapping-to-parameter function model, which addresses complex and nonlinear functional regression problems in parameter space by employing any supervised learning technique. central to this model is the mapping of function data from an infinite-dimensional function space to a finite-dimensional parameter space. this is accomplished by concurrently approximating multiple functions with a common set of b-spline basis functions by any chosen order, with their knot distribution determined by the iterative local placement algorithm, a newly proposed free knot placement algorithm. in contrast to the conventional equidistant knot placement strategy that uniformly distributes knot locations based on a predefined number of knots, our proposed algorithms determine knot location according to the local complexity of the input or output functions. the performance of our knot placement algorithms is shown to be robust in both single-function approximation and multiple-function approximation contexts. furthermore, the effectiveness and advantage of the proposed prediction model in handling both function-on-scalar regression and function-on-function regression problems are demonstrated through several real data applications, in comparison with four groups of state-of-the-art methods.",,2024-01-26,,"['chengdong shi', 'ching-hsun tseng', 'wei zhao', 'xiao-jun zeng']"
2401.15092,a note on the capacity of the binary perceptron,math.pr cs.dm cs.lg stat.ml,"determining the capacity $\alpha_c$ of the binary perceptron is a long-standing problem. krauth and mezard (1989) conjectured an explicit value of $\alpha_c$, approximately equal to .833, and a rigorous lower bound matching this prediction was recently established by ding and sun (2019). regarding the upper bound, kim and roche (1998) and talagrand (1999) independently showed that $\alpha_c$ < .996, while krauth and mezard outlined an argument which can be used to show that $\alpha_c$ < .847. the purpose of this expository note is to record a complete proof of the bound $\alpha_c$ < .847. the proof is a conditional first moment method combined with known results on the spherical perceptron",,2024-01-22,,"['dylan j. altschuler', 'konstantin tikhomirov']"
2401.15122,a multi-grained symmetric differential equation model for learning   protein-ligand binding dynamics,cs.lg cs.ai q-bio.bm q-bio.qm stat.ml,"in drug discovery, molecular dynamics (md) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. there has been a long history of improving the efficiency of md simulations through better numerical methods and, more recently, by utilizing machine learning (ml) methods. yet, challenges remain, such as accurate modeling of extended-timescale simulations. to address this issue, we propose neuralmd, the first ml surrogate that can facilitate numerical md and provide accurate simulations in protein-ligand binding. we propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. specifically, we propose (1) a bindingnet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory under newtonian mechanics. for the experiment, we design ten single-trajectory and three multi-trajectory binding simulation tasks. we show the efficiency and effectiveness of neuralmd, with a 2000$\times$ speedup over standard numerical md simulation and outperforming all other ml approaches by up to 80% under the stability metric. we further qualitatively show that neuralmd reaches more stable binding predictions compared to other machine learning methods.",,2024-01-26,2024-02-01,"['shengchao liu', 'weitao du', 'yanjing li', 'zhuoxinran li', 'vignesh bhethanabotla', 'nakul rampal', 'omar yaghi', 'christian borgs', 'anima anandkumar', 'hongyu guo', 'jennifer chayes']"
2401.15139,fdr-controlled portfolio optimization for sparse financial index   tracking,q-fin.pm cs.lg stat.me stat.ml,"in high-dimensional data analysis, such as financial index tracking or biomedical applications, it is crucial to select the few relevant variables while maintaining control over the false discovery rate (fdr). in these applications, strong dependencies often exist among the variables (e.g., stock returns), which can undermine the fdr control property of existing methods like the model-x knockoff method or the t-rex selector. to address this issue, we have expanded the t-rex framework to accommodate overlapping groups of highly correlated variables. this is achieved by integrating a nearest neighbors penalization mechanism into the framework, which provably controls the fdr at the user-defined target level. a real-world example of sparse index tracking demonstrates the proposed method's ability to accurately track the s&p 500 index over the past 20 years based on a small number of stocks. an open-source implementation is provided within the r package trexselector on cran.",,2024-01-26,2024-01-30,"['jasin machkour', 'daniel p. palomar', 'michael muma']"
2401.15248,better representations via adversarial training in pre-training: a   theoretical perspective,cs.lg stat.ml,"pre-training is known to generate universal representations for downstream tasks in large-scale deep learning such as large language models. existing literature, e.g., \cite{kim2020adversarial}, empirically observe that the downstream tasks can inherit the adversarial robustness of the pre-trained model. we provide theoretical justifications for this robustness inheritance phenomenon. our theoretical results reveal that feature purification plays an important role in connecting the adversarial robustness of the pre-trained model and the downstream tasks in two-layer neural networks. specifically, we show that (i) with adversarial training, each hidden node tends to pick only one (or a few) feature; (ii) without adversarial training, the hidden nodes can be vulnerable to attacks. this observation is valid for both supervised pre-training and contrastive learning. with purified nodes, it turns out that clean training is enough to achieve adversarial robustness in downstream tasks.",,2024-01-26,,"['yue xing', 'xiaofeng lin', 'qifan song', 'yi xu', 'belinda zeng', 'guang cheng']"
2401.15254,finite sample confidence regions for linear regression parameters using   arbitrary predictors,stat.ml cs.lg math.st stat.th,"we explore a novel methodology for constructing confidence regions for parameters of linear models, using predictions from any arbitrary predictor. our framework requires minimal assumptions on the noise and can be extended to functions deviating from strict linearity up to some adjustable threshold, thereby accommodating a comprehensive and pragmatically relevant set of functions. the derived confidence regions can be cast as constraints within a mixed integer linear programming framework, enabling optimisation of linear objectives. this representation enables robust optimization and the extraction of confidence intervals for specific parameter coordinates. unlike previous methods, the confidence region can be empty, which can be used for hypothesis testing. finally, we validate the empirical applicability of our method on synthetic data.",,2024-01-26,,"['charles guille-escuret', 'eugene ndiaye']"
2401.15257,an overview of modern machine learning methods for effect measure   modification analyses in high-dimensional settings,stat.ap,"a primary concern of public health researchers involves identifying and quantifying heterogeneous exposure effects across population subgroups. understanding the magnitude and direction of these effects on a given scale provides researchers the ability to recommend policy prescriptions and assess the external validity of findings. furthermore, increasing popularity in fields such as precision medicine that rely on accurate estimation of high-dimensional interaction effects has highlighted the importance of understanding effect modification. traditional methods for effect measure modification analyses include parametric regression modeling with either stratified analyses and corresponding heterogeneity tests or including an interaction term in a multivariable model. however, these methods require manual model specification and are often impractical or not feasible to conduct by hand in high-dimensional settings. recent developments in machine learning aim to solve this issue by automating heterogeneous subgroup identification and effect estimation. in this paper, we summarize and provide the intuition behind modern machine learning methods for effect measure modification analyses to serve as a reference for public health researchers. we discuss their implementation in r, provide annotated syntax and review available supplemental analysis tools by assessing the heterogeneous effects of drought on stunting among children in the demographic and health survey data set as a case study.",,2024-01-26,,"['michael cheung', 'anna dimitrova', 'tarik benmarhnia']"
2401.15262,asymptotic behavior of adversarial training estimator under   $\ell_\infty$-perturbation,math.st cs.lg stat.me stat.ml stat.th,"adversarial training has been proposed to hedge against adversarial attacks in machine learning and statistical models. this paper focuses on adversarial training under $\ell_\infty$-perturbation, which has recently attracted much research attention. the asymptotic behavior of the adversarial training estimator is investigated in the generalized linear model. the results imply that the limiting distribution of the adversarial training estimator under $\ell_\infty$-perturbation could put a positive probability mass at $0$ when the true parameter is $0$, providing a theoretical guarantee of the associated sparsity-recovery ability. alternatively, a two-step procedure is proposed -- adaptive adversarial training, which could further improve the performance of adversarial training under $\ell_\infty$-perturbation. specifically, the proposed procedure could achieve asymptotic unbiasedness and variable-selection consistency. numerical experiments are conducted to show the sparsity-recovery ability of adversarial training under $\ell_\infty$-perturbation and to compare the empirical performance between classic adversarial training and adaptive adversarial training.",,2024-01-26,,"['yiling xie', 'xiaoming huo']"
2401.15447,continuous treatment effect estimation using gradient interpolation and   kernel smoothing,cs.lg stat.ml,"we address the individualized continuous treatment effect (icte) estimation problem where we predict the effect of any continuous-valued treatment on an individual using observational data. the main challenge in this estimation task is the potential confounding of treatment assignment with an individual's covariates in the training data, whereas during inference icte requires prediction on independently sampled treatments. in contrast to prior work that relied on regularizers or unstable gan training, we advocate the direct approach of augmenting training individuals with independently sampled treatments and inferred counterfactual outcomes. we infer counterfactual outcomes using a two-pronged strategy: a gradient interpolation for close-to-observed treatments, and a gaussian process based kernel smoothing which allows us to downweigh high variance inferences. we evaluate our method on five benchmarks and show that our method outperforms six state-of-the-art methods on the counterfactual estimation error. we analyze the superior performance of our method by showing that (1) our inferred counterfactual responses are more accurate, and (2) adding them to the training data reduces the distributional distance between the confounded training distribution and test distribution where treatment is independent of covariates. our proposed method is model-agnostic and we show that it improves icte accuracy of several existing models.",,2024-01-27,,"['lokesh nagalapatti', 'akshay iyer', 'abir de', 'sunita sarawagi']"
2401.15500,data-driven estimation of the false positive rate of the bayes binary   classifier via soft labels,cs.lg cs.it math.it stat.ml,"classification is a fundamental task in many applications on which data-driven methods have shown outstanding performances. however, it is challenging to determine whether such methods have achieved the optimal performance. this is mainly because the best achievable performance is typically unknown and hence, effectively estimating it is of prime importance. in this paper, we consider binary classification problems and we propose an estimator for the false positive rate (fpr) of the bayes classifier, that is, the optimal classifier with respect to accuracy, from a given dataset. our method utilizes soft labels, or real-valued labels, which are gaining significant traction thanks to their properties. we thoroughly examine various theoretical properties of our estimator, including its consistency, unbiasedness, rate of convergence, and variance. to enhance the versatility of our estimator beyond soft labels, we also consider noisy labels, which encompass binary labels. for noisy labels, we develop effective fpr estimators by leveraging a denoising technique and the nadaraya-watson estimator. due to the symmetry of the problem, our results can be readily applied to estimate the false negative rate of the bayes classifier.",,2024-01-27,,"['minoh jeong', 'martina cardone', 'alex dytso']"
2401.15502,differentially private bayesian tests,stat.ml cs.cr cs.lg,"differential privacy has emerged as an significant cornerstone in the realm of scientific hypothesis testing utilizing confidential data. in reporting scientific discoveries, bayesian tests are widely adopted since they effectively circumnavigate the key criticisms of p-values, namely, lack of interpretability and inability to quantify evidence in support of the competing hypotheses. we present a novel differentially private bayesian hypotheses testing framework that arise naturally under a principled data generative mechanism, inherently maintaining the interpretability of the resulting inferences. furthermore, by focusing on differentially private bayes factors based on widely used test statistics, we circumvent the need to model the complete data generative mechanism and ensure substantial computational benefits. we also provide a set of sufficient conditions to establish results on bayes factor consistency under the proposed framework. the utility of the devised technology is showcased via several numerical experiments.",,2024-01-27,,"['abhisek chakraborty', 'saptati datta']"
2401.15519,large deviation analysis of score-based hypothesis testing,eess.sp stat.me,"score-based statistical models play an important role in modern machine learning, statistics, and signal processing. for hypothesis testing, a score-based hypothesis test is proposed in \cite{wu2022score}. we analyze the performance of this score-based hypothesis testing procedure and derive upper bounds on the probabilities of its type i and ii errors. we prove that the exponents of our error bounds are asymptotically (in the number of samples) tight for the case of simple null and alternative hypotheses. we calculate these error exponents explicitly in specific cases and provide numerical studies for various other scenarios of interest.",,2024-01-27,2024-02-03,"['enmao diao', 'taposh banerjee', 'vahid tarokh']"
2401.15520,oracle-efficient hybrid online learning with unknown distribution,cs.lg stat.ml,"we study the problem of oracle-efficient hybrid online learning when the features are generated by an unknown i.i.d. process and the labels are generated adversarially. assuming access to an (offline) erm oracle, we show that there exists a computationally efficient online predictor that achieves a regret upper bounded by $\tilde{o}(t^{\frac{3}{4}})$ for a finite-vc class, and upper bounded by $\tilde{o}(t^{\frac{p+1}{p+2}})$ for a class with $\alpha$ fat-shattering dimension $\alpha^{-p}$. this provides the first known oracle-efficient sublinear regret bounds for hybrid online learning with an unknown feature generation process. in particular, it confirms a conjecture of lazaric and munos (jcss 2012). we then extend our result to the scenario of shifting distributions with $k$ changes, yielding a regret of order $\tilde{o}(t^{\frac{4}{5}}k^{\frac{1}{5}})$. finally, we establish a regret of $\tilde{o}((k^{\frac{2}{3}}(\log|\mathcal{h}|)^{\frac{1}{3}}+k)\cdot t^{\frac{4}{5}})$ for the contextual $k$-armed bandits with a finite policy set $\mathcal{h}$, i.i.d. generated contexts from an unknown distribution, and adversarially generated costs.",,2024-01-27,,"['changlong wu', 'jin sima', 'wojciech szpankowski']"
2401.15566,on the robustness of cross-concentrated sampling for matrix completion,stat.ml cs.it cs.lg math.it math.oc,"matrix completion is one of the crucial tools in modern data science research. recently, a novel sampling model for matrix completion coined cross-concentrated sampling (ccs) has caught much attention. however, the robustness of the ccs model against sparse outliers remains unclear in the existing studies. in this paper, we aim to answer this question by exploring a novel robust ccs completion problem. a highly efficient non-convex iterative algorithm, dubbed robust cur completion (rcurc), is proposed. the empirical performance of the proposed algorithm, in terms of both efficiency and robustness, is verified in synthetic and real datasets.",,2024-01-27,,"['hanqin cai', 'longxiu huang', 'chandra kundu', 'bowen su']"
2401.15567,positive semidefinite supermartingales and randomized matrix   concentration inequalities,math.pr math.fa math.st stat.me stat.ml stat.th,"we present new concentration inequalities for either martingale dependent or exchangeable random symmetric matrices under a variety of tail conditions, encompassing now-standard chernoff bounds to self-normalized heavy-tailed settings. these inequalities are often randomized in a way that renders them strictly tighter than existing deterministic results in the literature, are typically expressed in the loewner order, and are sometimes valid at arbitrary data-dependent stopping times. along the way, we explore the theory of positive semidefinite supermartingales and maximal inequalities, a natural matrix analog of scalar nonnegative supermartingales that is potentially of independent interest.",,2024-01-27,2024-02-26,"['hongjian wang', 'aaditya ramdas']"
2401.15610,prevalidated ridge regression is a highly-efficient drop-in replacement   for logistic regression for high-dimensional data,cs.lg stat.ml,"logistic regression is a ubiquitous method for probabilistic classification. however, the effectiveness of logistic regression depends upon careful and relatively computationally expensive tuning, especially for the regularisation hyperparameter, and especially in the context of high-dimensional data. we present a prevalidated ridge regression model that closely matches logistic regression in terms of classification error and log-loss, particularly for high-dimensional data, while being significantly more computationally efficient and having effectively no hyperparameters beyond regularisation. we scale the coefficients of the model so as to minimise log-loss for a set of prevalidated predictions derived from the estimated leave-one-out cross-validation error. this exploits quantities already computed in the course of fitting the ridge regression model in order to find the scaling parameter with nominal additional computational expense.",,2024-01-28,,"['angus dempster', 'geoffrey i. webb', 'daniel f. schmidt']"
2401.15623,gt-pca: effective and interpretable dimensionality reduction with   general transform-invariant principal component analysis,stat.ml cs.lg stat.me,"data analysis often requires methods that are invariant with respect to specific transformations, such as rotations in case of images or shifts in case of images and time series. while principal component analysis (pca) is a widely-used dimension reduction technique, it lacks robustness with respect to these transformations. modern alternatives, such as autoencoders, can be invariant with respect to specific transformations but are generally not interpretable. we introduce general transform-invariant principal component analysis (gt-pca) as an effective and interpretable alternative to pca and autoencoders. we propose a neural network that efficiently estimates the components and show that gt-pca significantly outperforms alternative methods in experiments based on synthetic and real data.",,2024-01-28,,['florian heinrichs']
2401.15645,ensemble-based annealed importance sampling,stat.co cs.lg cs.na math.na physics.comp-ph stat.ml,"sampling from a multimodal distribution is a fundamental and challenging problem in computational science and statistics. among various approaches proposed for this task, one popular method is annealed importance sampling (ais). in this paper, we propose an ensemble-based version of ais by combining it with population-based monte carlo methods to improve its efficiency. by keeping track of an ensemble instead of a single particle along some continuation path between the starting distribution and the target distribution, we take advantage of the interaction within the ensemble to encourage the exploration of undiscovered modes. specifically, our main idea is to utilize either the snooker algorithm or the genetic algorithm used in evolutionary monte carlo. we discuss how the proposed algorithm can be implemented and derive a partial differential equation governing the evolution of the ensemble under the continuous time and mean-field limit. we also test the efficiency of the proposed algorithm on various continuous and discrete distributions.",,2024-01-28,,"['haoxuan chen', 'lexing ying']"
2401.15791,improving kernel-based nonasymptotic simultaneous confidence bands,stat.ml cs.lg math.st stat.th,"the paper studies the problem of constructing nonparametric simultaneous confidence bands with nonasymptotic and distribition-free guarantees. the target function is assumed to be band-limited and the approach is based on the theory of paley-wiener reproducing kernel hilbert spaces. the starting point of the paper is a recently developed algorithm to which we propose three types of improvements. first, we relax the assumptions on the noises by replacing the symmetricity assumption with a weaker distributional invariance principle. then, we propose a more efficient way to estimate the norm of the target function, and finally we enhance the construction of the confidence bands by tightening the constraints of the underlying convex optimization problems. the refinements are also illustrated through numerical experiments.",10.1016/j.ifacol.2023.10.1047,2024-01-28,,"['balázs csanád csáji', 'bálint horváth']"
2401.15792,sample complexity of the sign-perturbed sums identification method:   scalar case,stat.ml cs.lg cs.sy eess.sy math.st stat.th,"sign-perturbed sum (sps) is a powerful finite-sample system identification algorithm which can construct confidence regions for the true data generating system with exact coverage probabilities, for any finite sample size. sps was developed in a series of papers and it has a wide range of applications, from general linear systems, even in a closed-loop setup, to nonlinear and nonparametric approaches. although several theoretical properties of sps were proven in the literature, the sample complexity of the method was not analysed so far. this paper aims to fill this gap and provides the first results on the sample complexity of sps. here, we focus on scalar linear regression problems, that is we study the behaviour of sps confidence intervals. we provide high probability upper bounds, under three different sets of assumptions, showing that the sizes of sps confidence intervals shrink at a geometric rate around the true parameter, if the observation noises are subgaussian. we also show that similar bounds hold for the previously proposed outer approximation of the confidence region. finally, we present simulation experiments comparing the theoretical and the empirical convergence rates.",10.1016/j.ifacol.2023.10.1048,2024-01-28,,"['szabolcs szentpéteri', 'balázs csanád csáji']"
2401.15796,high-dimensional false discovery rate control for dependent variables,stat.me stat.ml,"algorithms that ensure reproducible findings from large-scale, high-dimensional data are pivotal in numerous signal processing applications. in recent years, multivariate false discovery rate (fdr) controlling methods have emerged, providing guarantees even in high-dimensional settings where the number of variables surpasses the number of samples. however, these methods often fail to reliably control the fdr in the presence of highly dependent variable groups, a common characteristic in fields such as genomics and finance. to tackle this critical issue, we introduce a novel framework that accounts for general dependency structures. our proposed dependency-aware t-rex selector integrates hierarchical graphical models within the t-rex framework to effectively harness the dependency structure among variables. leveraging martingale theory, we prove that our variable penalization mechanism ensures fdr control. we further generalize the fdr-controlling framework by stating and proving a clear condition necessary for designing both graphical and non-graphical models that capture dependencies. additionally, we formulate a fully integrated optimal calibration algorithm that concurrently determines the parameters of the graphical model and the t-rex framework, such that the fdr is controlled while maximizing the number of selected variables. numerical experiments and a breast cancer survival analysis use-case demonstrate that the proposed method is the only one among the state-of-the-art benchmark methods that controls the fdr and reliably detects genes that have been previously identified to be related to breast cancer. an open-source implementation is available within the r package trexselector on cran.",,2024-01-28,2024-01-30,"['jasin machkour', 'michael muma', 'daniel p. palomar']"
2401.15800,provably stable feature rankings with shap and lime,stat.ml cs.lg,"feature attributions are ubiquitous tools for understanding the predictions of machine learning models. however, popular methods for scoring input variables such as shap and lime suffer from high instability due to random sampling. leveraging ideas from multiple hypothesis testing, we devise attribution methods that correctly rank the most important features with high probability. our algorithm rankshap guarantees that the $k$ highest shapley values have the proper ordering with probability exceeding $1-\alpha$. empirical results demonstrate its validity and impressive computational efficiency. we also build on previous work to yield similar results for lime, ensuring the most important features are selected in the right order.",,2024-01-28,,"['jeremy goldwasser', 'giles hooker']"
2401.15801,on the statistical properties of generative adversarial models for low   intrinsic data dimension,stat.ml cs.ai cs.lg math.st stat.th,"despite the remarkable empirical successes of generative adversarial networks (gans), the theoretical guarantees for their statistical accuracy remain rather pessimistic. in particular, the data distributions on which gans are applied, such as natural images, are often hypothesized to have an intrinsic low-dimensional structure in a typically high-dimensional feature space, but this is often not reflected in the derived rates in the state-of-the-art analyses. in this paper, we attempt to bridge the gap between the theory and practice of gans and their bidirectional variant, bi-directional gans (bigans), by deriving statistical guarantees on the estimated densities in terms of the intrinsic dimension of the data and the latent space. we analytically show that if one has access to $n$ samples from the unknown target distribution and the network architectures are properly chosen, the expected wasserstein-1 distance of the estimates from the target scales as $o\left( n^{-1/d_\mu } \right)$ for gans and $o\left( n^{-1/(d_\mu+\ell)} \right)$ for bigans, where $d_\mu$ and $\ell$ are the upper wasserstein-1 dimension of the data-distribution and latent-space dimension, respectively. the theoretical analyses not only suggest that these methods successfully avoid the curse of dimensionality, in the sense that the exponent of $n$ in the error rates does not depend on the data dimension but also serve to bridge the gap between the theoretical analyses of gans and the known sharp rates from optimal transport literature. additionally, we demonstrate that gans can effectively achieve the minimax optimal rate even for non-smooth underlying distributions, with the use of larger generator networks.",,2024-01-28,,"['saptarshi chakraborty', 'peter l. bartlett']"
2401.15838,distributed markov chain monte carlo sampling based on the alternating   direction method of multipliers,stat.ml cs.lg cs.ma math.oc stat.co,"many machine learning applications require operating on a spatially distributed dataset. despite technological advances, privacy considerations and communication constraints may prevent gathering the entire dataset in a central unit. in this paper, we propose a distributed sampling scheme based on the alternating direction method of multipliers, which is commonly used in the optimization literature due to its fast convergence. in contrast to distributed optimization, distributed sampling allows for uncertainty quantification in bayesian inference tasks. we provide both theoretical guarantees of our algorithm's convergence and experimental evidence of its superiority to the state-of-the-art. for our theoretical results, we use convex optimization tools to establish a fundamental inequality on the generated local sample iterates. this inequality enables us to show convergence of the distribution associated with these iterates to the underlying target distribution in wasserstein distance. in simulation, we deploy our algorithm on linear and logistic regression tasks and illustrate its fast convergence compared to existing gradient-based methods.",,2024-01-28,,"['alexandros e. tzikas', 'licio romao', 'mert pilanci', 'alessandro abate', 'mykel j. kochenderfer']"
2401.15846,meta-learning for neural network-based temporal point processes,cs.lg stat.ml,"human activities generate various event sequences such as taxi trip records, bike-sharing pick-ups, crime occurrence, and infectious disease transmission. the point process is widely used in many applications to predict such events related to human activities. however, point processes present two problems in predicting events related to human activities. first, recent high-performance point process models require the input of sufficient numbers of events collected over a long period (i.e., long sequences) for training, which are often unavailable in realistic situations. second, the long-term predictions required in real-world applications are difficult. to tackle these problems, we propose a novel meta-learning approach for periodicity-aware prediction of future events given short sequences. the proposed method first embeds short sequences into hidden representations (i.e., task representations) via recurrent neural networks for creating predictions from short sequences. it then models the intensity of the point process by monotonic neural networks (mnns), with the input being the task representations. we transfer the prior knowledge learned from related tasks and can improve event prediction given short sequences of target tasks. we design the mnns to explicitly take temporal periodic patterns into account, contributing to improved long-term prediction performance. experiments on multiple real-world datasets demonstrate that the proposed method has higher prediction performance than existing alternatives.",,2024-01-28,,"['yoshiaki takimoto', 'yusuke tanaka', 'tomoharu iwata', 'maya okawa', 'hideaki kim', 'hiroyuki toda', 'takeshi kurashima']"
2401.15889,sliced wasserstein with random-path projecting directions,stat.ml cs.ai cs.cv cs.lg,"slicing distribution selection has been used as an effective technique to improve the performance of parameter estimators based on minimizing sliced wasserstein distance in applications. previous works either utilize expensive optimization to select the slicing distribution or use slicing distributions that require expensive sampling methods. in this work, we propose an optimization-free slicing distribution that provides a fast sampling for the monte carlo estimation of expectation. in particular, we introduce the random-path projecting direction (rpd) which is constructed by leveraging the normalized difference between two random vectors following the two input measures. from the rpd, we derive the random-path slicing distribution (rpsd) and two variants of sliced wasserstein, i.e., the random-path projection sliced wasserstein (rpsw) and the importance weighted random-path projection sliced wasserstein (iwrpsw). we then discuss the topological, statistical, and computational properties of rpsw and iwrpsw. finally, we showcase the favorable performance of rpsw and iwrpsw in gradient flow and the training of denoising diffusion generative models on images.",,2024-01-28,,"['khai nguyen', 'shujian zhang', 'tam le', 'nhat ho']"
2401.15890,probabilistic guarantees of stochastic recursive gradient in non-convex   finite sum problems,stat.ml cs.lg math.oc math.st stat.th,"this paper develops a new dimension-free azuma-hoeffding type bound on summation norm of a martingale difference sequence with random individual bounds. with this novel result, we provide high-probability bounds for the gradient norm estimator in the proposed algorithm prob-sarah, which is a modified version of the stochastic recursive gradient algorithm (sarah), a state-of-art variance reduced algorithm that achieves optimal computational complexity in expectation for the finite sum problem. the in-probability complexity by prob-sarah matches the best in-expectation result up to logarithmic factors. empirical experiments demonstrate the superior probabilistic performance of prob-sarah on real datasets compared to other popular algorithms.",,2024-01-29,,"['yanjie zhong', 'jiaqi li', 'soumendra lahiri']"
2401.15903,toward the identifiability of comparative deep generative models,cs.lg q-bio.gn stat.me,"deep generative models (dgms) are versatile tools for learning data representations while adequately incorporating domain knowledge such as the specification of conditional probability distributions. recently proposed dgms tackle the important task of comparing data sets from different sources. one such example is the setting of contrastive analysis that focuses on describing patterns that are enriched in a target data set compared to a background data set. the practical deployment of those models often assumes that dgms naturally infer interpretable and modular latent representations, which is known to be an issue in practice. consequently, existing methods often rely on ad-hoc regularization schemes, although without any theoretical grounding. here, we propose a theory of identifiability for comparative dgms by extending recent advances in the field of non-linear independent component analysis. we show that, while these models lack identifiability across a general class of mixing functions, they surprisingly become identifiable when the mixing function is piece-wise affine (e.g., parameterized by a relu neural network). we also investigate the impact of model misspecification, and empirically show that previously proposed regularization techniques for fitting comparative dgms help with identifiability when the number of latent variables is not known in advance. finally, we introduce a novel methodology for fitting comparative dgms that improves the treatment of multiple data sources via multi-objective optimization and that helps adjust the hyperparameter for the regularization in an interpretable manner, using constrained optimization. we empirically validate our theory and new methodology using simulated data as well as a recent data set of genetic perturbations in cells profiled via single-cell rna sequencing.",,2024-01-29,,"['romain lopez', 'jan-christian huetter', 'ehsan hajiramezanali', 'jonathan pritchard', 'aviv regev']"
2401.16294,dual feature-based and example-based explanation methods,cs.lg cs.ai stat.ml,"a new approach to the local and global explanation is proposed. it is based on selecting a convex hull constructed for the finite number of points around an explained instance. the convex hull allows us to consider a dual representation of instances in the form of convex combinations of extreme points of a produced polytope. instead of perturbing new instances in the euclidean feature space, vectors of convex combination coefficients are uniformly generated from the unit simplex, and they form a new dual dataset. a dual linear surrogate model is trained on the dual dataset. the explanation feature importance values are computed by means of simple matrix calculations. the approach can be regarded as a modification of the well-known model lime. the dual representation inherently allows us to get the example-based explanation. the neural additive model is also considered as a tool for implementing the example-based explanation approach. many numerical experiments with real datasets are performed for studying the approach. the code of proposed algorithms is available.",,2024-01-29,,"['andrei v. konstantinov', 'boris v. kozlov', 'stanislav r. kirpichenko', 'lev v. utkin']"
2401.16320,prepare non-classical collective spin state by reinforcement learning,quant-ph stat.ml,"we propose a scheme leveraging reinforcement learning to engineer control fields for generating non-classical states. it is exemplified by the application to prepare spin squeezed state for an open collective spin model where a linear control term is designed to govern the dynamics. the reinforcement learning agent determines the temporal sequence of control pulses, commencing from coherent spin state in an environment characterized by dissipation and dephasing. when compared to constant control scenarios, this approach provides various control sequences maintaining collective spin squeezing and entanglement. it is observed that denser application of the control pulses enhances the performance of the outcomes. furthermore, there is a minor enhancement in the performance by adding control actions. the proposed strategy demonstrates increased effectiveness for larger systems. and thermal excitations of the reservoir are detrimental to the control outcomes. it should be confirmed that this is an open-loop strategy by closed-loop simulation, circumventing collapse of quantum state induced by measurements. thanks to the flexible replaceability of the optimization modules and the controlled system, this research paves the way for its application in manipulating other quantum systems.",,2024-01-29,,"['x. l. zhao', 'y. m. zhao', 'm. li', 't. t. li', 'q. liu', 's. guo', 'x. x. yi']"
2401.16335,iterative data smoothing: mitigating reward overfitting and   overoptimization in rlhf,cs.lg cs.ai cs.cl stat.ml,"reinforcement learning from human feedback (rlhf) is a pivotal technique that aligns language models closely with human-centric values. the initial phase of rlhf involves learning human values using a reward model from ranking data. it is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. this paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'iterative data smoothing' (ids). the core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. our empirical findings highlight the superior performance of this approach over the traditional methods.",,2024-01-29,,"['banghua zhu', 'michael i. jordan', 'jiantao jiao']"
2401.16407,is k-fold cross validation the best model selection method for machine   learning?,stat.ml cs.lg eess.iv eess.sp,"as a technique that can compactly represent complex patterns, machine learning has significant potential for predictive inference. k-fold cross-validation (cv) is the most common approach to ascertaining the likelihood that a machine learning outcome is generated by chance and frequently outperforms conventional hypothesis testing. this improvement uses measures directly obtained from machine learning classifications, such as accuracy, that do not have a parametric description. to approach a frequentist analysis within machine learning pipelines, a permutation test or simple statistics from data partitions (i.e. folds) can be added to estimate confidence intervals. unfortunately, neither parametric nor non-parametric tests solve the inherent problems around partitioning small sample-size datasets and learning from heterogeneous data sources. the fact that machine learning strongly depends on the learning parameters and the distribution of data across folds recapitulates familiar difficulties around excess false positives and replication. the origins of this problem are demonstrated by simulating common experimental circumstances, including small sample sizes, low numbers of predictors, and heterogeneous data sources. a novel statistical test based on k-fold cv and the upper bound of the actual error (k-fold cubv) is composed, where uncertain predictions of machine learning with cv are bounded by the \emph{worst case} through the evaluation of concentration inequalities. probably approximately correct-bayesian upper bounds for linear classifiers in combination with k-fold cv is used to estimate the empirical error. the performance with neuroimaging datasets suggests this is a robust criterion for detecting effects, validating accuracy values obtained from machine learning whilst avoiding excess false positives.",,2024-01-29,,"['juan m gorriz', 'f segovia', 'j ramirez', 'a ortiz', 'j. suckling']"
2401.16410,retasa: a nonparametric functional estimation approach for addressing   continuous target shift,stat.ml cs.lg,"the presence of distribution shifts poses a significant challenge for deploying modern machine learning models in real-world applications. this work focuses on the target shift problem in a regression setting (zhang et al., 2013; nguyen et al., 2016). more specifically, the target variable y (also known as the response variable), which is continuous, has different marginal distributions in the training source and testing domain, while the conditional distribution of features x given y remains the same. while most literature focuses on classification tasks with finite target space, the regression problem has an infinite dimensional target space, which makes many of the existing methods inapplicable. in this work, we show that the continuous target shift problem can be addressed by estimating the importance weight function from an ill-posed integral equation. we propose a nonparametric regularized approach named retasa to solve the ill-posed integral equation and provide theoretical justification for the estimated importance weight function. the effectiveness of the proposed method has been demonstrated with extensive numerical studies on synthetic and real-world datasets.",,2024-01-29,,"['hwanwoo kim', 'xin zhang', 'jiwei zhao', 'qinglong tian']"
2401.16418,boolean logic as an error feedback mechanism,stat.ml cs.lg,"the notion of boolean logic backpropagation was introduced to build neural networks with weights and activations being boolean numbers. most of computations can be done with boolean logic instead of real arithmetic, both during training and inference phases. but the underlying discrete optimization problem is np-hard, and the boolean logic has no guarantee. in this work we propose the first convergence analysis, under standard non-convex assumptions.",,2024-01-29,,['louis leconte']
2401.16419,semi-parametric expert bayesian network learning with gaussian processes   and horseshoe priors,cs.lg stat.ml,"this paper proposes a model learning semi-parametric relationships in an expert bayesian network (sebn) with linear parameter and structure constraints. we use gaussian processes and a horseshoe prior to introduce minimal nonlinear components. to prioritize modifying the expert graph over adding new edges, we optimize differential horseshoe scales. in real-world datasets with unknown truth, we generate diverse graphs to accommodate user input, addressing identifiability issues and enhancing interpretability. evaluation on synthetic and uci liver disorders datasets, using metrics like structural hamming distance and test likelihood, demonstrates our models outperform state-of-the-art semi-parametric bayesian network model.",,2024-01-29,,"['yidou weng', 'finale doshi-velez']"
2401.16421,two stones hit one bird: bilevel positional encoding for better length   extrapolation,cs.lg cs.ai cs.cl stat.ml,"in this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called bilevel positional encoding (bipe). for each position, our bipe blends an intra-segment encoding and an inter-segment encoding. the intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding. the inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding. theoretical analysis shows this disentanglement of positional information makes learning more effective. the empirical results also show that our bipe has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities.",,2024-01-29,,"['zhenyu he', 'guhao feng', 'shengjie luo', 'kai yang', 'di he', 'jingjing xu', 'zhi zhang', 'hongxia yang', 'liwei wang']"
2401.16432,improving conversion rate prediction via self-supervised pre-training in   online advertising,cs.ir cs.lg stat.ml,"the task of predicting conversion rates (cvr) lies at the heart of online advertising systems aiming to optimize bids to meet advertiser performance requirements. even with the recent rise of deep neural networks, these predictions are often made by factorization machines (fm), especially in commercial settings where inference latency is key. these models are trained using the logistic regression framework on labeled tabular data formed from past user activity that is relevant to the task at hand.   many advertisers only care about click-attributed conversions. a major challenge in training models that predict conversions-given-clicks comes from data sparsity - clicks are rare, conversions attributed to clicks are even rarer. however, mitigating sparsity by adding conversions that are not click-attributed to the training set impairs model calibration. since calibration is critical to achieving advertiser goals, this is infeasible.   in this work we use the well-known idea of self-supervised pre-training, and use an auxiliary auto-encoder model trained on all conversion events, both click-attributed and not, as a feature extractor to enrich the main cvr prediction model. since the main model does not train on non click-attributed conversions, this does not impair calibration. we adapt the basic self-supervised pre-training idea to our online advertising setup by using a loss function designed for tabular data, facilitating continual learning by ensuring auto-encoder stability, and incorporating a neural network into a large-scale real-time ad auction that ranks tens of thousands of ads, under strict latency constraints, and without incurring a major engineering cost. we show improvements both offline, during training, and in an online a/b test. following its success in a/b tests, our solution is now fully deployed to the yahoo native advertising system.",10.1109/bigdata59044.2023.10386162,2024-01-25,,"['alex shtoff', 'yohay kaplan', 'ariel raviv']"
2401.16487,active learning of boltzmann samplers and potential energies with   quantum mechanical accuracy,physics.chem-ph physics.comp-ph stat.ml,"extracting consistent statistics between relevant free-energy minima of a molecular system is essential for physics, chemistry and biology. molecular dynamics (md) simulations can aid in this task but are computationally expensive, especially for systems that require quantum accuracy. to overcome this challenge, we develop an approach combining enhanced sampling with deep generative models and active learning of a machine learning potential (mlp). we introduce an adaptive markov chain monte carlo framework that enables the training of one normalizing flow (nf) and one mlp per state. we simulate several markov chains in parallel until they reach convergence, sampling the boltzmann distribution with an efficient use of energy evaluations. at each iteration, we compute the energy of a subset of the nf-generated configurations using density functional theory (dft), we predict the remaining configuration's energy with the mlp and actively train the mlp using the dft-computed energies. leveraging the trained nf and mlp models, we can compute thermodynamic observables such as free-energy differences or optical spectra. we apply this method to study the isomerization of an ultrasmall silver nanocluster, belonging to a set of systems with diverse applications in the fields of medicine and catalysis.",,2024-01-29,,"['ana molina-taborda', 'pilar cossio', 'olga lopez-acevedo', 'marylou gabrié']"
2401.16563,topological detection of phenomenological bifurcations with unreliable   kernel densities,math.at math.ds stat.me stat.ml,"phenomenological (p-type) bifurcations are qualitative changes in stochastic dynamical systems whereby the stationary probability density function (pdf) changes its topology. the current state of the art for detecting these bifurcations requires reliable kernel density estimates computed from an ensemble of system realizations. however, in several real world signals such as big data, only a single system realization is available -- making it impossible to estimate a reliable kernel density. this study presents an approach for detecting p-type bifurcations using unreliable density estimates. the approach creates an ensemble of objects from topological data analysis (tda) called persistence diagrams from the system's sole realization and statistically analyzes the resulting set. we compare several methods for replicating the original persistence diagram including gibbs point process modelling, pairwise interaction point modelling, and subsampling. we show that for the purpose of predicting a bifurcation, the simple method of subsampling exceeds the other two methods of point process modelling in performance.",,2024-01-29,,"['sunia tanweer', 'firas a. khasawneh']"
2401.16567,parallel affine transformation tuning of markov chain monte carlo,stat.me stat.ml,"the performance of markov chain monte carlo samplers strongly depends on the properties of the target distribution such as its covariance structure, the location of its probability mass and its tail behavior. we explore the use of bijective affine transformations of the sample space to improve the properties of the target distribution and thereby the performance of samplers running in the transformed space. in particular, we propose a flexible and user-friendly scheme for adaptively learning the affine transformation during sampling. moreover, the combination of our scheme with gibbsian polar slice sampling is shown to produce samples of high quality at comparatively low computational cost in several settings based on real-world data.",,2024-01-29,,"['philip schär', 'michael habeck', 'daniel rudolf']"
2401.16571,individualized multi-treatment response curves estimation using rbf-net   with shared neurons,stat.me stat.ap stat.ml,"heterogeneous treatment effect estimation is an important problem in precision medicine. specific interests lie in identifying the differential effect of different treatments based on some external covariates. we propose a novel non-parametric treatment effect estimation method in a multi-treatment setting. our non-parametric modeling of the response curves relies on radial basis function (rbf)-nets with shared hidden neurons. our model thus facilitates modeling commonality among the treatment outcomes. the estimation and inference schemes are developed under a bayesian framework and implemented via an efficient markov chain monte carlo algorithm, appropriately accommodating uncertainty in all aspects of the analysis. the numerical performance of the method is demonstrated through simulation experiments. applying our proposed method to mimic data, we obtain several interesting findings related to the impact of different treatment strategies on the length of icu stay and 12-hour sofa score for sepsis patients who are home-discharged.",,2024-01-29,2024-02-08,"['peter chang', 'arkaprava roy']"
2401.16596,prising: privacy-preserving peer effect estimation via ising model,stat.me cs.cr cs.si math.st stat.ml stat.th,"the ising model, originally developed as a spin-glass model for ferromagnetic elements, has gained popularity as a network-based model for capturing dependencies in agents' outputs. its increasing adoption in healthcare and the social sciences has raised privacy concerns regarding the confidentiality of agents' responses. in this paper, we present a novel $(\varepsilon,\delta)$-differentially private algorithm specifically designed to protect the privacy of individual agents' outcomes. our algorithm allows for precise estimation of the natural parameter using a single network through an objective perturbation technique. furthermore, we establish regret bounds for this algorithm and assess its performance on synthetic datasets and two real-world networks: one involving hiv status in a social network and the other concerning the political leaning of online blogs.",,2024-01-29,,"['abhinav chakraborty', 'anirban chatterjee', 'abhinandan dalal']"
2401.16612,learning a gaussian mixture for sparsity regularization in inverse   problems,stat.ml cs.lg,"in inverse problems, it is widely recognized that the incorporation of a sparsity prior yields a regularization effect on the solution. this approach is grounded on the a priori assumption that the unknown can be appropriately represented in a basis with a limited number of significant components, while most coefficients are close to zero. this occurrence is frequently observed in real-world scenarios, such as with piecewise smooth signals. in this study, we propose a probabilistic sparsity prior formulated as a mixture of degenerate gaussians, capable of modeling sparsity with respect to a generic basis. under this premise, we design a neural network that can be interpreted as the bayes estimator for linear inverse problems. additionally, we put forth both a supervised and an unsupervised training strategy to estimate the parameters of this network. to evaluate the effectiveness of our approach, we conduct a numerical comparison with commonly employed sparsity-promoting regularization techniques, namely lasso, group lasso, iterative hard thresholding, and sparse coding/dictionary learning. notably, our reconstructions consistently exhibit lower mean square error values across all $1$d datasets utilized for the comparisons, even in cases where the datasets significantly deviate from a gaussian mixture model.",,2024-01-29,,"['giovanni s. alberti', 'luca ratti', 'matteo santacesaria', 'silvia sciutto']"
2401.16655,rademacher complexity of neural odes via chen-fliess series,stat.ml cs.lg cs.sy eess.sy math.oc,"we show how continuous-depth neural ode models can be framed as single-layer, infinite-width nets using the chen--fliess series expansion for nonlinear odes. in this net, the output ""weights"" are taken from the signature of the control input -- a tool used to represent infinite-dimensional paths as a sequence of tensors -- which comprises iterated integrals of the control input over a simplex. the ""features"" are taken to be iterated lie derivatives of the output function with respect to the vector fields in the controlled ode model. the main result of this work applies this framework to derive compact expressions for the rademacher complexity of ode models that map an initial condition to a scalar output at some terminal time. the result leverages the straightforward analysis afforded by single-layer architectures. we conclude with some examples instantiating the bound for some specific systems and discuss potential follow-up work.",,2024-01-29,2024-01-30,"['joshua hanson', 'maxim raginsky']"
2401.16683,polynomial chaos expansions on principal geodesic grassmannian   submanifolds for surrogate modeling and uncertainty quantification,stat.ml cs.lg math.ds,"in this work we introduce a manifold learning-based surrogate modeling framework for uncertainty quantification in high-dimensional stochastic systems. our first goal is to perform data mining on the available simulation data to identify a set of low-dimensional (latent) descriptors that efficiently parameterize the response of the high-dimensional computational model. to this end, we employ principal geodesic analysis on the grassmann manifold of the response to identify a set of disjoint principal geodesic submanifolds, of possibly different dimension, that captures the variation in the data. since operations on the grassmann require the data to be concentrated, we propose an adaptive algorithm based on riemanniann k-means and the minimization of the sample frechet variance on the grassmann manifold to identify ""local"" principal geodesic submanifolds that represent different system behavior across the parameter space. polynomial chaos expansion is then used to construct a mapping between the random input parameters and the projection of the response on these local principal geodesic submanifolds. the method is demonstrated on four test cases, a toy-example that involves points on a hypersphere, a lotka-volterra dynamical system, a continuous-flow stirred-tank chemical reactor system, and a two-dimensional rayleigh-benard convection problem",,2024-01-29,,"['dimitris g. giovanis', 'dimitrios loukrezis', 'ioannis g. kevrekidis', 'michael d. shields']"
2401.16776,leveraging nested mlmc for sequential neural posterior estimation with   intractable likelihoods,stat.co cs.lg stat.ml,"sequential neural posterior estimation (snpe) techniques have been recently proposed for dealing with simulation-based models with intractable likelihoods. they are devoted to learning the posterior from adaptively proposed simulations using neural network-based conditional density estimators. as a snpe technique, the automatic posterior transformation (apt) method proposed by greenberg et al. (2019) performs notably and scales to high dimensional data. however, the apt method bears the computation of an expectation of the logarithm of an intractable normalizing constant, i.e., a nested expectation. although atomic apt was proposed to solve this by discretizing the normalizing constant, it remains challenging to analyze the convergence of learning. in this paper, we propose a nested apt method to estimate the involved nested expectation instead. this facilitates establishing the convergence analysis. since the nested estimators for the loss function and its gradient are biased, we make use of unbiased multi-level monte carlo (mlmc) estimators for debiasing. to further reduce the excessive variance of the unbiased estimators, this paper also develops some truncated mlmc estimators by taking account of the trade-off between the bias and the average cost. numerical experiments for approximating complex posteriors with multimodal in moderate dimensions are provided.",,2024-01-30,,"['xiliang yang', 'yifei xiong', 'zhijian he']"
2401.16832,analysis of knowledge tracing performance on synthesised student data,cs.cy cs.lg stat.ml,"knowledge tracing (kt) aims to predict the future performance of students by tracking the development of their knowledge states. despite all the recent progress made in this field, the application of kt models in education systems is still restricted from the data perspectives: 1) limited access to real life data due to data protection concerns, 2) lack of diversity in public datasets, 3) noises in benchmark datasets such as duplicate records. to resolve these problems, we simulated student data with three statistical strategies based on public datasets and tested their performance on two kt baselines. while we observe only minor performance improvement with additional synthetic data, our work shows that using only synthetic data for training can lead to similar performance as real data.",,2024-01-30,,"['panagiotis pagonis', 'kai hartung', 'di wu', 'munir georges', 'sören gröttrup']"
2401.16922,learning properties of quantum states without the i.i.d. assumption,quant-ph cs.it math.it math.pr math.st stat.th,"we develop a framework for learning properties of quantum states beyond the assumption of independent and identically distributed (i.i.d.) input states. we prove that, given any learning problem (under reasonable assumptions), an algorithm designed for i.i.d. input states can be adapted to handle input states of any nature, albeit at the expense of a polynomial increase in copy complexity. furthermore, we establish that algorithms which perform non-adaptive incoherent measurements can be extended to encompass non-i.i.d. input states while maintaining comparable error probabilities. this allows us, among others applications, to generalize the classical shadows of huang, kueng, and preskill to the non-i.i.d. setting at the cost of a small loss in efficiency. additionally, we can efficiently verify any pure state using clifford measurements, in a way that is independent of the ideal state. our main techniques are based on de finetti-style theorems supported by tools from information theory. in particular, we prove a new randomized local de finetti theorem that can be of independent interest.",,2024-01-30,,"['omar fawzi', 'richard kueng', 'damian markham', 'aadil oufkir']"
2401.16943,"dynamical system identification, model selection and model uncertainty   quantification by bayesian inference",stat.me nlin.cd stat.ml,"this study presents a bayesian maximum \textit{a~posteriori} (map) framework for dynamical system identification from time-series data. this is shown to be equivalent to a generalized zeroth-order tikhonov regularization, providing a rational justification for the choice of the residual and regularization terms, respectively, from the negative logarithms of the likelihood and prior distributions. in addition to the estimation of model coefficients, the bayesian interpretation gives access to the full apparatus for bayesian inference, including the ranking of models, the quantification of model uncertainties and the estimation of unknown (nuisance) hyperparameters. two bayesian algorithms, joint maximum \textit{a~posteriori} (jmap) and variational bayesian approximation (vba), are compared to the popular sindy algorithm for thresholded least-squares regression, by application to several dynamical systems with added noise. for multivariate gaussian likelihood and prior distributions, the bayesian formulation gives gaussian posterior and evidence distributions, in which the numerator terms can be expressed in terms of the mahalanobis distance or ``gaussian norm'' $||\vy-\hat{\vy}||^2_{m^{-1}} = (\vy-\hat{\vy})^\top {m^{-1}} (\vy-\hat{\vy})$, where $\vy$ is a vector variable, $\hat{\vy}$ is its estimator and $m$ is the covariance matrix. the posterior gaussian norm is shown to provide a robust metric for quantitative model selection.",,2024-01-30,,"['robert k. niven', 'laurent cordier', 'ali mohammad-djafari', 'markus abel', 'markus quade']"
2401.16985,multiple yield curve modeling and forecasting using deep learning,stat.ml cs.lg,"this manuscript introduces deep learning models that simultaneously describe the dynamics of several yield curves. we aim to learn the dependence structure among the different yield curves induced by the globalization of financial markets and exploit it to produce more accurate forecasts. by combining the self-attention mechanism and nonparametric quantile regression, our model generates both point and interval forecasts of future yields. the architecture is designed to avoid quantile crossing issues affecting multiple quantile regression models. numerical experiments conducted on two different datasets confirm the effectiveness of our approach. finally, we explore potential extensions and enhancements by incorporating deep ensemble methods and transfer learning mechanisms.",,2024-01-30,,"['ronald richman', 'salvatore scognamiglio']"
2401.16986,causal machine learning for cost-effective allocation of development aid,stat.ml cs.lg,"the sustainable development goals (sdgs) of the united nations provide a blueprint of a better future by 'leaving no one behind', and, to achieve the sdgs by 2030, poor countries require immense volumes of development aid. in this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. we demonstrate the effectiveness of our framework using data with official development aid earmarked to end hiv/aids in 105 countries, amounting to more than usd 5.2 billion. for this, we first show that our framework successfully computes heterogeneous treatment-response curves using semi-synthetic data. then, we demonstrate our framework using real-world hiv data. our framework points to large opportunities for a more effective aid allocation, suggesting that the total number of new hiv infections could be reduced by up to 3.3% (~50,000 cases) compared to the current allocation practice.",,2024-01-30,2024-01-31,"['milan kuzmanovic', 'dennis frauen', 'tobias hatt', 'stefan feuerriegel']"
2401.17037,bayesian optimization with noise-free observations: improved regret   bounds via random exploration,cs.lg cs.na math.na stat.ml,"this paper studies bayesian optimization with noise-free observations. we introduce new algorithms rooted in scattered data approximation that rely on a random exploration step to ensure that the fill-distance of query points decays at a near-optimal rate. our algorithms retain the ease of implementation of the classical gp-ucb algorithm and satisfy cumulative regret bounds that nearly match those conjectured in arxiv:2002.05096, hence solving a colt open problem. furthermore, the new algorithms outperform gp-ucb and other popular bayesian optimization strategies in several examples.",,2024-01-30,,"['hwanwoo kim', 'daniel sanz-alonso']"
2401.17041,gower's similarity coefficients with automatic weight selection,stat.me stat.ap stat.ml,"nearest-neighbor methods have become popular in statistics and play a key role in statistical learning. important decisions in nearest-neighbor methods concern the variables to use (when many potential candidates exist) and how to measure the dissimilarity between units. the first decision depends on the scope of the application while second depends mainly on the type of variables. unfortunately, relatively few options permit to handle mixed-type variables, a situation frequently encountered in practical applications. the most popular dissimilarity for mixed-type variables is derived as the complement to one of the gower's similarity coefficient. it is appealing because ranges between 0 and 1, being an average of the scaled dissimilarities calculated variable by variable, handles missing values and allows for a user-defined weighting scheme when averaging dissimilarities. the discussion on the weighting schemes is sometimes misleading since it often ignores that the unweighted ""standard"" setting hides an unbalanced contribution of the single variables to the overall dissimilarity. we address this drawback following the recent idea of introducing a weighting scheme that minimizes the differences in the correlation between each contributing dissimilarity and the resulting weighted gower's dissimilarity. in particular, this note proposes different approaches for measuring the correlation depending on the type of variables. the performances of the proposed approaches are evaluated in simulation studies related to classification and imputation of missing values.",,2024-01-30,,"[""marcello d'orazio""]"
2401.17077,dynamical survival analysis with controlled latent states,stat.ml cs.lg,"we consider the task of learning individual-specific intensities of counting processes from a set of static variables and irregularly sampled time series. we introduce a novel modelization approach in which the intensity is the solution to a controlled differential equation. we first design a neural estimator by building on neural controlled differential equations. in a second time, we show that our model can be linearized in the signature space under sufficient regularity conditions, yielding a signature-based estimator which we call coxsig. we provide theoretical learning guarantees for both estimators, before showcasing the performance of our models on a vast array of simulated and real-world datasets from finance, predictive maintenance and food supply chain management.",,2024-01-30,,"['linus bleistein', 'van-tuan nguyen', 'adeline fermanian', 'agathe guilloux']"
2401.17205,adaptive experiment design with synthetic controls,stat.ml cs.lg,"clinical trials are typically run in order to understand the effects of a new treatment on a given population of patients. however, patients in large populations rarely respond the same way to the same treatment. this heterogeneity in patient responses necessitates trials that investigate effects on multiple subpopulations - especially when a treatment has marginal or no benefit for the overall population but might have significant benefit for a particular subpopulation. motivated by this need, we propose syntax, an exploratory trial design that identifies subpopulations with positive treatment effect among many subpopulations. syntax is sample efficient as it (i) recruits and allocates patients adaptively and (ii) estimates treatment effects by forming synthetic controls for each subpopulation that combines control samples from other subpopulations. we validate the performance of syntax and provide insights into when it might have an advantage over conventional trial designs through experiments.",,2024-01-30,2024-02-09,"['alihan hüyük', 'zhaozhi qian', 'mihaela van der schaar']"
2401.17269,effect of weight quantization on learning models by typical case   analysis,stat.ml cs.lg,"this paper examines the quantization methods used in large-scale data analysis models and their hyperparameter choices. the recent surge in data analysis scale has significantly increased computational resource requirements. to address this, quantizing model weights has become a prevalent practice in data analysis applications such as deep learning. quantization is particularly vital for deploying large models on devices with limited computational resources. however, the selection of quantization hyperparameters, like the number of bits and value range for weight quantization, remains an underexplored area. in this study, we employ the typical case analysis from statistical physics, specifically the replica method, to explore the impact of hyperparameters on the quantization of simple learning models. our analysis yields three key findings: (i) an unstable hyperparameter phase, known as replica symmetry breaking, occurs with a small number of bits and a large quantization width; (ii) there is an optimal quantization width that minimizes error; and (iii) quantization delays the onset of overparameterization, helping to mitigate overfitting as indicated by the double descent phenomenon. we also discover that non-uniform quantization can enhance stability. additionally, we develop an approximate message-passing algorithm to validate our theoretical results.",,2024-01-30,,"['shuhei kashiwamura', 'ayaka sakata', 'masaaki imaizumi']"
2401.17426,superiority of multi-head attention in in-context linear regression,cs.lg cs.ai stat.ml,"we present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. while the existing literature predominantly focuses on the convergence of transformers with single-/multi-head attention, our research centers on comparing their performance. we conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention. when the number of in-context examples d increases, the prediction loss using single-/multi-head attention is in o(1/d), and the one for multi-head attention has a smaller multiplicative constant. in addition to the simplest data distribution setting, we consider more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. we observe that, in general, multi-head attention is preferred over single-head attention. our results verify the effectiveness of the design of multi-head attention in the transformer architecture.",,2024-01-30,,"['yingqian cui', 'jie ren', 'pengfei he', 'jiliang tang', 'yue xing']"
2401.17430,modeling of spatial extremes in environmental data science: time to move   away from max-stable processes,stat.me,"environmental data science for spatial extremes has traditionally relied heavily on max-stable processes. even though the popularity of these models has perhaps peaked with statisticians, they are still perceived and considered as the `state-of-the-art' in many applied fields. however, while the asymptotic theory supporting the use of max-stable processes is mathematically rigorous and comprehensive, we think that it has also been overused, if not misused, in environmental applications, to the detriment of more purposeful and meticulously validated models. in this paper, we review the main limitations of max-stable process models, and strongly argue against their systematic use in environmental studies. alternative solutions based on more flexible frameworks using the exceedances of variables above appropriately chosen high thresholds are discussed, and an outlook on future research is given, highlighting recommendations moving forward and the opportunities offered by hybridizing machine learning with extreme-value statistics.",,2024-01-30,,"['raphaël huser', 'thomas opitz', 'jennifer wadsworth']"
2401.17441,explaining predictive uncertainty by exposing second-order effects,cs.lg cs.ai stat.ml,"explainable ai has brought transparency into complex ml blackboxes, enabling, in particular, to identify which features these models use for their predictions. so far, the question of explaining predictive uncertainty, i.e. why a model 'doubts', has been scarcely studied. our investigation reveals that predictive uncertainty is dominated by second-order effects, involving single features or product interactions between them. we contribute a new method for explaining predictive uncertainty based on these second-order effects. computationally, our method reduces to a simple covariance computation over a collection of first-order explanations. our method is generally applicable, allowing for turning common attribution techniques (lrp, gradient x input, etc.) into powerful second-order uncertainty explainers, which we call covlrp, covgi, etc. the accuracy of the explanations our method produces is demonstrated through systematic quantitative evaluations, and the overall usefulness of our method is demonstrated via two practical showcases.",,2024-01-30,,"['florian bley', 'sebastian lapuschkin', 'wojciech samek', 'grégoire montavon']"
2401.17504,camu: disentangling causal effects in deep model unlearning,cs.lg stat.me,"machine unlearning requires removing the information of forgetting data while keeping the necessary information of remaining data. despite recent advancements in this area, existing methodologies mainly focus on the effect of removing forgetting data without considering the negative impact this can have on the information of the remaining data, resulting in significant performance degradation after data removal. although some methods try to repair the performance of remaining data after removal, the forgotten information can also return after repair. such an issue is due to the intricate intertwining of the forgetting and remaining data. without adequately differentiating the influence of these two kinds of data on the model, existing algorithms take the risk of either inadequate removal of the forgetting data or unnecessary loss of valuable information from the remaining data. to address this shortcoming, the present study undertakes a causal analysis of the unlearning and introduces a novel framework termed causal machine unlearning (camu). this framework adds intervention on the information of remaining data to disentangle the causal effects between forgetting data and remaining data. then camu eliminates the causal impact associated with forgetting data while concurrently preserving the causal relevance of the remaining data. comprehensive empirical results on various datasets and models suggest that camu enhances performance on the remaining data and effectively minimizes the influences of forgetting data. notably, this work is the first to interpret deep model unlearning tasks from a new perspective of causality and provide a solution based on causal analysis, which opens up new possibilities for future research in deep model unlearning.",,2024-01-30,,"['shaofei shen', 'chenhao zhang', 'alina bialkowski', 'weitong chen', 'miao xu']"
2401.17523,game-theoretic unlearnable example generator,cs.lg cs.cr stat.ml,"unlearnable example attacks are data poisoning attacks aiming to degrade the clean test accuracy of deep learning by adding imperceptible perturbations to the training samples, which can be formulated as a bi-level optimization problem. however, directly solving this optimization problem is intractable for deep neural networks. in this paper, we investigate unlearnable example attacks from a game-theoretic perspective, by formulating the attack as a nonzero sum stackelberg game. first, the existence of game equilibria is proved under the normal setting and the adversarial training setting. it is shown that the game equilibrium gives the most powerful poison attack in that the victim has the lowest test accuracy among all networks within the same hypothesis space, when certain loss functions are used. second, we propose a novel attack method, called the game unlearnable example (gue), which has three main gradients. (1) the poisons are obtained by directly solving the equilibrium of the stackelberg game with a first-order algorithm. (2) we employ an autoencoder-like generative network model as the poison attacker. (3) a novel payoff function is introduced to evaluate the performance of the poison. comprehensive experiments demonstrate that gue can effectively poison the model in various scenarios. furthermore, the gue still works by using a relatively small percentage of the training data to train the generator, and the poison generator can generalize to unseen data well. our implementation code can be found at https://github.com/hong-xian/gue.",,2024-01-30,,"['shuang liu', 'yihan wang', 'xiao-shan gao']"
2401.17573,tensor-based process control and monitoring for semiconductor   manufacturing with unstable disturbances,stat.ml cs.lg cs.sy eess.iv eess.sy,"with the development and popularity of sensors installed in manufacturing systems, complex data are collected during manufacturing processes, which brings challenges for traditional process control methods. this paper proposes a novel process control and monitoring method for the complex structure of high-dimensional image-based overlay errors (modeled in tensor form), which are collected in semiconductor manufacturing processes. the proposed method aims to reduce overlay errors using limited control recipes. we first build a high-dimensional process model and propose different tensor-on-vector regression algorithms to estimate parameters in the model to alleviate the curse of dimensionality. then, based on the estimate of tensor parameters, the exponentially weighted moving average (ewma) controller for tensor data is designed whose stability is theoretically guaranteed. considering the fact that low-dimensional control recipes cannot compensate for all high-dimensional disturbances on the image, control residuals are monitored to prevent significant drifts of uncontrollable high-dimensional disturbances. through extensive simulations and real case studies, the performances of parameter estimation algorithms and the ewma controller in tensor space are evaluated. compared with existing image-based feedback controllers, the superiority of our method is verified especially when disturbances are not stable.",,2024-01-30,,"['yanrong li', 'juan du', 'fugee tsung', 'wei jiang']"
2401.17675,convergence analysis of t-sne as a gradient flow for point cloud on a   manifold,stat.ml cs.ds cs.lg,"we present a theoretical foundation regarding the boundedness of the t-sne algorithm. t-sne employs gradient descent iteration with kullback-leibler (kl) divergence as the objective function, aiming to identify a set of points that closely resemble the original data points in a high-dimensional space, minimizing kl divergence. investigating t-sne properties such as perplexity and affinity under a weak convergence assumption on the sampled dataset, we examine the behavior of points generated by t-sne under continuous gradient flow. demonstrating that points generated by t-sne remain bounded, we leverage this insight to establish the existence of a minimizer for kl divergence.",,2024-01-31,,"['seonghyeon jeong', 'hau-tieng wu']"
2401.17737,hierarchical bias-driven stratification for interpretable causal effect   estimation,stat.me cs.lg stat.ml,"interpretability and transparency are essential for incorporating causal effect models from observational data into policy decision-making. they can provide trust for the model in the absence of ground truth labels to evaluate the accuracy of such models. to date, attempts at transparent causal effect estimation consist of applying post hoc explanation methods to black-box models, which are not interpretable. here, we present bicausetree: an interpretable balancing method that identifies clusters where natural experiments occur locally. our approach builds on decision trees with a customized objective function to improve balancing and reduce treatment allocation bias. consequently, it can additionally detect subgroups presenting positivity violations, exclude them, and provide a covariate-based definition of the target population we can infer from and generalize to. we evaluate the method's performance using synthetic and realistic datasets, explore its bias-interpretability tradeoff, and show that it is comparable with existing approaches.",,2024-01-31,,"['lucile ter-minassian', 'liran szlak', 'ehud karavani', 'chris holmes', 'yishai shimoni']"
2401.17760,regularized linear discriminant analysis using a nonlinear covariance   matrix estimator,stat.ml cs.lg eess.sp,"linear discriminant analysis (lda) is a widely used technique for data classification. the method offers adequate performance in many classification problems, but it becomes inefficient when the data covariance matrix is ill-conditioned. this often occurs when the feature space's dimensionality is higher than or comparable to the training data size. regularized lda (rlda) methods based on regularized linear estimators of the data covariance matrix have been proposed to cope with such a situation. the performance of rlda methods is well studied, with optimal regularization schemes already proposed. in this paper, we investigate the capability of a positive semidefinite ridge-type estimator of the inverse covariance matrix that coincides with a nonlinear (nl) covariance matrix estimator. the estimator is derived by reformulating the score function of the optimal classifier utilizing linear estimation methods, which eventually results in the proposed nl-rlda classifier. we derive asymptotic and consistent estimators of the proposed technique's misclassification rate under the assumptions of a double-asymptotic regime and multivariate gaussian model for the classes. the consistent estimator, coupled with a one-dimensional grid search, is used to set the value of the regularization parameter required for the proposed nl-rlda classifier. performance evaluations based on both synthetic and real data demonstrate the effectiveness of the proposed classifier. the proposed technique outperforms state-of-art methods over multiple datasets. when compared to state-of-the-art methods across various datasets, the proposed technique exhibits superior performance.",,2024-01-31,2024-02-07,"['maaz mahadi', 'tarig ballal', 'muhammad moinuddin', 'tareq y. al-naffouri', 'ubaid m. al-saggaf']"
2401.17763,convergence of expectation-maximization algorithm with mixed-integer   optimization,eess.sp stat.ml,"the convergence of expectation-maximization (em)-based algorithms typically requires continuity of the likelihood function with respect to all the unknown parameters (optimization variables). the requirement is not met when parameters comprise both discrete and continuous variables, making the convergence analysis nontrivial. this paper introduces a set of conditions that ensure the convergence of a specific class of em algorithms that estimate a mixture of discrete and continuous parameters. our results offer a new analysis technique for iterative algorithms that solve mixed-integer non-linear optimization problems. as a concrete example, we prove the convergence of the em-based sparse bayesian learning algorithm in [1] that estimates the state of a linear dynamical system with jointly sparse inputs and bursty missing observations. our results establish that the algorithm in [1] converges to the set of stationary points of the maximum likelihood cost with respect to the continuous optimization variables.",,2024-01-31,,['geethu joseph']
2401.17776,double infogan for contrastive analysis,cs.cv cs.ai stat.ml,"contrastive analysis (ca) deals with the discovery of what is common and what is distinctive of a target domain compared to a background one. this is of great interest in many applications, such as medical imaging. current state-of-the-art (sota) methods are latent variable models based on vae (ca-vaes). however, they all either ignore important constraints or they don't enforce fundamental assumptions. this may lead to sub-optimal solutions where distinctive factors are mistaken for common ones (or viceversa). furthermore, the generated images have a rather poor quality, typical of vaes, decreasing their interpretability and usefulness. here, we propose double infogan, the first gan based method for ca that leverages the high-quality synthesis of gan and the separation power of infogan. experimental results on four visual datasets, from simple synthetic examples to complex medical images, show that the proposed method outperforms sota ca-vaes in terms of latent separation and image quality. datasets and code are available online.",,2024-01-31,,"['florence carton', 'robin louiset', 'pietro gori']"
2401.17789,robustly overfitting latents for flexible neural image compression,cs.cv cs.lg stat.ml,"neural image compression has made a great deal of progress. state-of-the-art models are based on variational autoencoders and are outperforming classical models. neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image. while these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity. recent work shows how to use stochastic gumbel annealing (sga) to refine the latents of pre-trained neural image compression models. we extend this idea by introducing sga+, which contains three different methods that build upon sga. further, we give a detailed analysis of our proposed methods, show how they improve performance, and show that they are less sensitive to hyperparameter choices. besides, we show how each method can be extended to three- instead of two-class rounding. finally, we show how refinement of the latents with our best-performing method improves the compression performance on the tecnick dataset and how it can be deployed to partly move along the rate-distortion curve.",,2024-01-31,,"['yura perugachi-diaz', 'arwin gansekoele', 'sandjai bhulai']"
2401.17888,probabilistic model updating of steel frame structures using strain and   acceleration measurements: a multitask learning framework,stat.ap,"this paper proposes a multitask learning framework for probabilistic model updating by jointly using strain and acceleration measurements. this framework can enhance the structural damage assessment and response prediction of existing steel frame structures with quantified uncertainty. multitask learning may be used to address multiple similar inference tasks simultaneously to achieve a more robust prediction performance by transferring useful knowledge from one task to another, even in situations of data scarcity. in the proposed model-updating procedure, a spatial frame is decomposed into multiple planar frames that are viewed as multiple tasks and jointly analyzed based on the hierarchical bayesian model, leading to robust estimation results. the procedure uses a displacement-stress relationship in the modal space because it directly reflects the elemental stiffness and requires no prior knowledge concerning the mass, unlike most existing model-updating techniques. validation of the proposed framework by using a full-scale vibration test on a one-story, one-bay by one-bay moment resisting steel frame, wherein structural damage to the column bases is simulated by loosening the anchor bolts, is presented. the experimental results suggest that the displacement-stress relationship has sufficient sensitivity toward localized damage, and the bayesian multitask learning approach may result in the efficient use of measurements such that the uncertainty involved in model parameter estimation is reduced. the proposed framework facilitates more robust and informative model updating.",10.1016/j.strusafe.2024.102442,2024-01-31,,"['taro yaoyama', 'tatsuya itoi', 'jun iyama']"
2401.17898,multitask methods for predicting molecular properties from heterogeneous   data,physics.chem-ph stat.ml,"data generation remains a bottleneck in training surrogate models to predict molecular properties. we demonstrate that multitask gaussian process regression overcomes this limitation by leveraging both expensive and cheap data sources. in particular, we consider training sets constructed from coupled-cluster (cc) and density function theory (dft) data. we report that multitask surrogates can predict at cc level accuracy with a reduction to data generation cost by over an order of magnitude. of note, our approach allows the training set to include dft data generated by a heterogeneous mix of exchange-correlation functionals without imposing any artificial hierarchy on functional accuracy. more generally, the multitask framework can accommodate a wider range of training set structures -- including full disparity between the different levels of fidelity -- than existing kernel approaches based on $\delta$-learning, though we show that the accuracy of the two approaches can be similar. consequently, multitask regression can be a tool for reducing data generation costs even further by opportunistically exploiting existing data sources.",,2024-01-31,,"['katharine fisher', 'michael herbst', 'youssef marzouk']"
2401.17958,convergence analysis for general probability flow odes of diffusion   models in wasserstein distances,stat.ml cs.lg math.pr,"score-based generative modeling with probability flow ordinary differential equations (odes) has achieved remarkable success in a variety of applications. while various fast ode-based samplers have been proposed in the literature and employed in practice, the theoretical understandings about convergence properties of the probability flow ode are still quite limited. in this paper, we provide the first non-asymptotic convergence analysis for a general class of probability flow ode samplers in 2-wasserstein distance, assuming accurate score estimates. we then consider various examples and establish results on the iteration complexity of the corresponding ode-based samplers.",,2024-01-31,,"['xuefeng gao', 'lingjiong zhu']"
2401.18012,causal coordinated concurrent reinforcement learning,stat.ml cs.lg,"in this work, we propose a novel algorithmic framework for data sharing and coordinated exploration for the purpose of learning more data-efficient and better performing policies under a concurrent reinforcement learning (crl) setting. in contrast to other work which make the assumption that all agents act under identical environments, we relax this restriction and instead consider the formulation where each agent acts within an environment which shares a global structure but also exhibits individual variations. our algorithm leverages a causal inference algorithm in the form of additive noise model - mixture model (anm-mm) in extracting model parameters governing individual differentials via independence enforcement. we propose a new data sharing scheme based on a similarity measure of the extracted model parameters and demonstrate superior learning speeds on a set of autoregressive, pendulum and cart-pole swing-up tasks and finally, we show the effectiveness of diverse action selection between common agents under a sparse reward setting. to the best of our knowledge, this is the first work in considering non-identical environments in crl and one of the few works which seek to integrate causal inference with reinforcement learning (rl).",,2024-01-31,,"['tim tse', 'isaac chan', 'zhitang chen']"
2401.18017,causal discovery by kernel deviance measures with heterogeneous   transforms,stat.ml cs.lg,"the discovery of causal relationships in a set of random variables is a fundamental objective of science and has also recently been argued as being an essential component towards real machine intelligence. one class of causal discovery techniques are founded based on the argument that there are inherent structural asymmetries between the causal and anti-causal direction which could be leveraged in determining the direction of causation. to go about capturing these discrepancies between cause and effect remains to be a challenge and many current state-of-the-art algorithms propose to compare the norms of the kernel mean embeddings of the conditional distributions. in this work, we argue that such approaches based on rkhs embeddings are insufficient in capturing principal markers of cause-effect asymmetry involving higher-order structural variabilities of the conditional distributions. we propose kernel intrinsic invariance measure with heterogeneous transform (kiim-ht) which introduces a novel score measure based on heterogeneous transformation of rkhs embeddings to extract relevant higher-order moments of the conditional densities for causal discovery. inference is made via comparing the score of each hypothetical cause-effect direction. tests and comparisons on a synthetic dataset, a two-dimensional synthetic dataset and the real-world benchmark dataset t\""ubingen cause-effect pairs verify our approach. in addition, we conduct a sensitivity analysis to the regularization parameter to faithfully compare previous work to our method and an experiment with trials on varied hyperparameter values to showcase the robustness of our algorithm.",,2024-01-31,,"['tim tse', 'zhitang chen', 'shengyu zhu', 'yue liu']"
2401.18023,a cost-sensitive constrained lasso,stat.me stat.ml,"the lasso has become a benchmark data analysis procedure, and numerous variants have been proposed in the literature. although the lasso formulations are stated so that overall prediction error is optimized, no full control over the accuracy prediction on certain individuals of interest is allowed. in this work we propose a novel version of the lasso in which quadratic performance constraints are added to lasso-based objective functions, in such a way that threshold values are set to bound the prediction errors in the different groups of interest (not necessarily disjoint). as a result, a constrained sparse regression model is defined by a nonlinear optimization problem. this cost-sensitive constrained lasso has a direct application in heterogeneous samples where data are collected from distinct sources, as it is standard in many biomedical contexts. both theoretical properties and empirical studies concerning the new method are explored in this paper. in addition, two illustrations of the method on biomedical and sociological contexts are considered.",10.1007/s11634-020-00389-5,2024-01-31,,"['rafael blanquero', 'emilio carrizosa', 'pepa ramírez-cobo', 'm. remedios sillero-denamiel']"
2401.18039,"variable selection for na\""ive bayes classification",stat.ml cs.lg,"the na\""ive bayes has proven to be a tractable and efficient method for classification in multivariate analysis. however, features are usually correlated, a fact that violates the na\""ive bayes' assumption of conditional independence, and may deteriorate the method's performance. moreover, datasets are often characterized by a large number of features, which may complicate the interpretation of the results as well as slow down the method's execution.   in this paper we propose a sparse version of the na\""ive bayes classifier that is characterized by three properties. first, the sparsity is achieved taking into account the correlation structure of the covariates. second, different performance measures can be used to guide the selection of features. third, performance constraints on groups of higher interest can be included. our proposal leads to a smart search, which yields competitive running times, whereas the flexibility in terms of performance measure for classification is integrated. our findings show that, when compared against well-referenced feature selection approaches, the proposed sparse na\""ive bayes obtains competitive results regarding accuracy, sparsity and running times for balanced datasets. in the case of datasets with unbalanced (or with different importance) classes, a better compromise between classification rates for the different classes is achieved.",10.1016/j.cor.2021.105456,2024-01-31,,"['rafael blanquero', 'emilio carrizosa', 'pepa ramírez-cobo', 'm. remedios sillero-denamiel']"
2402.00054,predicting loss-of-function impact of genetic mutations: a machine   learning approach,q-bio.gn cs.lg stat.ap,"the innovation of next-generation sequencing (ngs) techniques has significantly reduced the price of genome sequencing, lowering barriers to future medical research; it is now feasible to apply genome sequencing to studies where it would have previously been cost-inefficient. identifying damaging or pathogenic mutations in vast amounts of complex, high-dimensional genome sequencing data may be of particular interest to researchers. thus, this paper's aims were to train machine learning models on the attributes of a genetic mutation to predict loftool scores (which measure a gene's intolerance to loss-of-function mutations). these attributes included, but were not limited to, the position of a mutation on a chromosome, changes in amino acids, and changes in codons caused by the mutation. models were built using the univariate feature selection technique f-regression combined with k-nearest neighbors (knn), support vector machine (svm), random sample consensus (ransac), decision trees, random forest, and extreme gradient boosting (xgboost). these models were evaluated using five-fold cross-validated averages of r-squared, mean squared error, root mean squared error, mean absolute error, and explained variance. the findings of this study include the training of multiple models with testing set r-squared values of 0.97.",,2024-01-26,,"['arshmeet kaur', 'morteza sarmadi']"
2402.00072,explainable ai for survival analysis: a median-shap approach,cs.lg stat.me stat.ml,"with the adoption of machine learning into routine clinical practice comes the need for explainable ai methods tailored to medical applications. shapley values have sparked wide interest for locally explaining models. here, we demonstrate their interpretation strongly depends on both the summary statistic and the estimator for it, which in turn define what we identify as an 'anchor point'. we show that the convention of using a mean anchor point may generate misleading interpretations for survival analysis and introduce median-shap, a method for explaining black-box models predicting individual survival times.",,2024-01-30,,"['lucile ter-minassian', 'sahra ghalebikesabi', 'karla diaz-ordaz', 'chris holmes']"
2402.00077,unlocking the power of multi-institutional data: integrating and   harmonizing genomic data across institutions,q-bio.gn cs.lg stat.me,"cancer is a complex disease driven by genomic alterations, and tumor sequencing is becoming a mainstay of clinical care for cancer patients. the emergence of multi-institution sequencing data presents a powerful resource for learning real-world evidence to enhance precision oncology. genie bpc, led by the american association for cancer research, establishes a unique database linking genomic data with clinical information for patients treated at multiple cancer centers. however, leveraging such multi-institutional sequencing data presents significant challenges. variations in gene panels result in loss of information when the analysis is conducted on common gene sets. additionally, differences in sequencing techniques and patient heterogeneity across institutions add complexity. high data dimensionality, sparse gene mutation patterns, and weak signals at the individual gene level further complicate matters. motivated by these real-world challenges, we introduce the bridge model. it uses a quantile-matched latent variable approach to derive integrated features to preserve information beyond common genes and maximize the utilization of all available data while leveraging information sharing to enhance both learning efficiency and the model's capacity to generalize. by extracting harmonized and noise-reduced lower-dimensional latent variables, the true mutation pattern unique to each individual is captured. we assess the model's performance and parameter estimation through extensive simulation studies. the extracted latent features from the bridge model consistently excel in predicting patient survival across six cancer types in genie bpc data.",,2024-01-30,,"['yuan chen', 'ronglai shen', 'xiwen feng', 'katherine panageas']"
2402.00152,deeper or wider: a perspective from optimal generalization error with   sobolev loss,cs.lg cs.na math.na stat.ml,"constructing the architecture of a neural network is a challenging pursuit for the machine learning community, and the dilemma of whether to go deeper or wider remains a persistent question. this paper explores a comparison between deeper neural networks (denns) with a flexible number of layers and wider neural networks (wenns) with limited hidden layers, focusing on their optimal generalization error in sobolev losses. analytical investigations reveal that the architecture of a neural network can be significantly influenced by various factors, including the number of sample points, parameters within the neural networks, and the regularity of the loss function. specifically, a higher number of parameters tends to favor wenns, while an increased number of sample points and greater regularity in the loss function lean towards the adoption of denns. we ultimately apply this theory to address partial differential equations using deep ritz and physics-informed neural network (pinn) methods, guiding the design of neural networks.",,2024-01-31,,"['yahong yang', 'juncai he']"
2402.00162,behind the myth of exploration in policy gradients,cs.lg stat.ml,"policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. to compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. first, they make it possible to smooth the learning objective and to eliminate local optima while preserving the global maximum. second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. in light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.",,2024-01-31,,"['adrien bolland', 'gaspard lambrechts', 'damien ernst']"
2402.00168,continuous treatment effects with surrogate outcomes,stat.ml cs.lg stat.me,"in many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. if the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully-observed samples alone may be biased. incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. in this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. importantly, we establish asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. extensive simulations show our methods enjoy appealing empirical performance.",,2024-01-31,,"['zhenghao zeng', 'david arbour', 'avi feller', 'raghavendra addanki', 'ryan rossi', 'ritwik sinha', 'edward h. kennedy']"
2402.00202,anytime-valid generalized universal inference on risk minimizers,stat.me,"a common goal in statistics and machine learning is estimation of unknowns. point estimates alone are of little value without an accompanying measure of uncertainty, but traditional uncertainty quantification methods, such as confidence sets and p-values, often require strong distributional or structural assumptions that may not be justified in modern problems. the present paper considers a very common case in machine learning, where the quantity of interest is the minimizer of a given risk (expected loss) function. for such cases, we propose a generalized universal procedure for inference on risk minimizers that features a finite-sample, frequentist validity property under mild distributional assumptions. one version of the proposed procedure is shown to be anytime-valid in the sense that it maintains validity properties regardless of the stopping rule used for the data collection process. we show how this anytime-validity property offers protection against certain factors contributing to the replication crisis in science.",,2024-01-31,,"['neil dey', 'ryan martin', 'jonathan p. williams']"
2402.00267,not all learnable distribution classes are privately learnable,cs.ds cs.cr stat.ml,"we give an example of a class of distributions that is learnable in total variation distance with a finite number of samples, but not learnable under $(\varepsilon, \delta)$-differential privacy. this refutes a conjecture of ashtiani.",,2024-01-31,2024-02-05,"['mark bun', 'gautam kamath', 'argyris mouzakis', 'vikrant singhal']"
2402.00295,comparative evaluation of traditional and deep learning-based   segmentation methods for spoil pile delineation using uav images,cs.cv stat.ap,"the stability of mine dumps is contingent upon the precise arrangement of spoil piles, taking into account their geological and geotechnical attributes. yet, on-site characterisation of individual piles poses a formidable challenge. the utilisation of image-based techniques for spoil pile characterisation, employing remotely acquired data through unmanned aerial systems, is a promising complementary solution. image processing, such as object-based classification and feature extraction, are dependent upon effective segmentation. this study refines and juxtaposes various segmentation approaches, specifically colour-based and morphology-based techniques. the objective is to enhance and evaluate avenues for object-based analysis for spoil characterisation within the context of mining environments. furthermore, a comparative analysis is conducted between conventional segmentation approaches and those rooted in deep learning methodologies. among the diverse segmentation approaches evaluated, the morphology-based deep learning segmentation approach, segment anything model (sam), exhibited superior performance in comparison to other approaches. this outcome underscores the efficacy of incorporating advanced morphological and deep learning techniques for accurate and efficient spoil pile characterisation. the findings of this study contribute valuable insights to the optimisation of segmentation strategies, thereby advancing the application of image-based techniques for the characterisation of spoil piles in mining environments.",,2024-01-31,,"['sureka thiruchittampalam', 'bikram p. banerjee', 'nancy f. glenn', 'simit raval']"
2402.00305,information-theoretic thresholds for planted dense cycles,math.st cs.it cs.si math.it stat.ml stat.th,"we study a random graph model for small-world networks which are ubiquitous in social and biological sciences. in this model, a dense cycle of expected bandwidth $n \tau$, representing the hidden one-dimensional geometry of vertices, is planted in an ambient random graph on $n$ vertices. for both detection and recovery of the planted dense cycle, we characterize the information-theoretic thresholds in terms of $n$, $\tau$, and an edge-wise signal-to-noise ratio $\lambda$. in particular, the information-theoretic thresholds differ from the computational thresholds established in a recent work for low-degree polynomial algorithms, thereby justifying the existence of statistical-to-computational gaps for this problem.",,2024-01-31,,"['cheng mao', 'alexander s. wein', 'shenduo zhang']"
2402.00332,comparing spectral bias and robustness for two-layer neural networks:   sgd vs adaptive random fourier features,cs.lg stat.ml,"we present experimental results highlighting two key differences resulting from the choice of training algorithm for two-layer neural networks. the spectral bias of neural networks is well known, while the spectral bias dependence on the choice of training algorithm is less studied. our experiments demonstrate that an adaptive random fourier features algorithm (arff) can yield a spectral bias closer to zero compared to the stochastic gradient descent optimizer (sgd). additionally, we train two identically structured classifiers, employing sgd and arff, to the same accuracy levels and empirically assess their robustness against adversarial noise attacks.",,2024-01-31,,"['aku kammonen', 'lisi liang', 'anamika pandey', 'raúl tempone']"
2402.00382,on the design-dependent suboptimality of the lasso,math.st stat.ml stat.th,"this paper investigates the effect of the design matrix on the ability (or inability) to estimate a sparse parameter in linear regression. more specifically, we characterize the optimal rate of estimation when the smallest singular value of the design matrix is bounded away from zero. in addition to this information-theoretic result, we provide and analyze a procedure which is simultaneously statistically optimal and computationally efficient, based on soft thresholding the ordinary least squares estimator. most surprisingly, we show that the lasso estimator -- despite its widespread adoption for sparse linear regression -- is provably minimax rate-suboptimal when the minimum singular value is small. we present a family of design matrices and sparse parameters for which we can guarantee that the lasso with any choice of regularization parameter -- including those which are data-dependent and randomized -- would fail in the sense that its estimation rate is suboptimal by polynomial factors in the sample size. our lower bound is strong enough to preclude the statistical optimality of all forms of the lasso, including its highly popular penalized, norm-constrained, and cross-validated variants.",,2024-02-01,,"['reese pathak', 'cong ma']"
2402.00388,cumulative distribution function based general temporal point processes,cs.lg cs.ai stat.ml,"temporal point processes (tpps) hold a pivotal role in modeling event sequences across diverse domains, including social networking and e-commerce, and have significantly contributed to the advancement of recommendation systems and information retrieval strategies. through the analysis of events such as user interactions and transactions, tpps offer valuable insights into behavioral patterns, facilitating the prediction of future trends. however, accurately forecasting future events remains a formidable challenge due to the intricate nature of these patterns. the integration of neural networks with tpps has ushered in the development of advanced deep tpp models. while these models excel at processing complex and nonlinear temporal data, they encounter limitations in modeling intensity functions, grapple with computational complexities in integral computations, and struggle to capture long-range temporal dependencies effectively. in this study, we introduce the cufun model, representing a novel approach to tpps that revolves around the cumulative distribution function (cdf). cufun stands out by uniquely employing a monotonic neural network for cdf representation, utilizing past events as a scaling factor. this innovation significantly bolsters the model's adaptability and precision across a wide range of data scenarios. our approach addresses several critical issues inherent in traditional tpp modeling: it simplifies log-likelihood calculations, extends applicability beyond predefined density function forms, and adeptly captures long-range temporal patterns. our contributions encompass the introduction of a pioneering cdf-based tpp model, the development of a methodology for incorporating past event information into future event prediction, and empirical validation of cufun's effectiveness through extensive experimentation on synthetic and real-world datasets.",,2024-02-01,,"['maolin wang', 'yu pan', 'zenglin xu', 'ruocheng guo', 'xiangyu zhao', 'wanyu wang', 'yiqi wang', 'zitao liu', 'langming liu']"
2402.00396,efficient exploration for llms,cs.lg cs.ai cs.cl stat.me stat.ml,"we present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. in our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. our best-performing agent generates queries using double thompson sampling, with uncertainty represented by an epistemic neural network. our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. further, both uncertainty estimation and the choice of exploration scheme play critical roles.",,2024-02-01,,"['vikranth dwaracherla', 'seyed mohammad asghari', 'botao hao', 'benjamin van roy']"
2402.00423,hierarchical integral probability metrics: a distance on random   probability measures with low sample complexity,math.st math.pr stat.th,"random probabilities are a key component to many nonparametric methods in statistics and machine learning. to quantify comparisons between different laws of random probabilities several works are starting to use the elegant wasserstein over wasserstein distance. in this paper we prove that the infinite-dimensionality of the space of probabilities drastically deteriorates its sample complexity, which is slower than any polynomial rate in the sample size. we thus propose a new distance that preserves many desirable properties of the former while achieving a parametric rate of convergence. in particular, our distance 1) metrizes weak convergence; 2) can be estimated numerically through samples with low complexity; 3) can be bounded analytically from above and below. the main ingredient are integral probability metrics, which lead to the name hierarchical ipm.",,2024-02-01,,"['marta catalano', 'hugo lavenant']"
2402.00501,equivalence of the empirical risk minimization to regularization on the   family of f-divergences,stat.ml cs.it cs.lg math.it,"the solution to empirical risk minimization with $f$-divergence regularization (erm-$f$dr) is presented under mild conditions on $f$. under such conditions, the optimal measure is shown to be unique. examples of the solution for particular choices of the function $f$ are presented. previously known solutions to common regularization choices are obtained by leveraging the flexibility of the family of $f$-divergences. these include the unique solutions to empirical risk minimization with relative entropy regularization (type-i and type-ii). the analysis of the solution unveils the following properties of $f$-divergences when used in the erm-$f$dr problem: $i\bigl)$ $f$-divergence regularization forces the support of the solution to coincide with the support of the reference measure, which introduces a strong inductive bias that dominates the evidence provided by the training data; and $ii\bigl)$ any $f$-divergence regularization is equivalent to a different $f$-divergence regularization with an appropriate transformation of the empirical risk function.",,2024-02-01,,"['francisco daunas', 'iñaki esnaola', 'samir m. perlaza', 'h. vincent poor']"
2402.00522,understanding the expressive power and mechanisms of transformer for   sequence modeling,cs.lg stat.ml,"we conduct a systematic study of the approximation properties of transformer for sequence modeling with long, sparse and complicated memory. we investigate the mechanisms through which different components of transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. our study reveals the roles of critical parameters in the transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.",,2024-02-01,2024-02-14,"['mingze wang', 'weinan e']"
2402.00544,quantum-assisted hilbert-space gaussian process regression,stat.co cs.lg quant-ph,"gaussian processes are probabilistic models that are commonly used as functional priors in machine learning. due to their probabilistic nature, they can be used to capture the prior information on the statistics of noise, smoothness of the functions, and training data uncertainty. however, their computational complexity quickly becomes intractable as the size of the data set grows. we propose a hilbert space approximation-based quantum algorithm for gaussian process regression to overcome this limitation. our method consists of a combination of classical basis function expansion with quantum computing techniques of quantum principal component analysis, conditional rotations, and hadamard and swap tests. the quantum principal component analysis is used to estimate the eigenvalues while the conditional rotations and the hadamard and swap tests are employed to evaluate the posterior mean and variance of the gaussian process. our method provides polynomial computational complexity reduction over the classical method.",,2024-02-01,,"['ahmad farooq', 'cristian a. galvis-florez', 'simo särkkä']"
2402.00592,uncertainty-aware partial-label learning,cs.lg stat.ml,"in real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. partial-label learning allows training classifiers in this weakly supervised setting. while state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. however, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. in this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages dempster-shafer theory. extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. additionally, we prove that our algorithm is risk-consistent.",,2024-02-01,,"['tobias fuchs', 'florian kalinke', 'klemens böhm']"
2402.00623,bayesian causal inference with gaussian process networks,stat.ml cs.lg stat.me,"causal discovery and inference from observational data is an essential problem in statistics posing both modeling and computational challenges. these are typically addressed by imposing strict assumptions on the joint distribution such as linearity. we consider the problem of the bayesian estimation of the effects of hypothetical interventions in the gaussian process network (gpn) model, a flexible causal framework which allows describing the causal relationships nonparametrically. we detail how to perform causal inference on gpns by simulating the effect of an intervention across the whole network and propagating the effect of the intervention on downstream variables. we further derive a simpler computational approximation by estimating the intervention distribution as a function of local variables only, modeling the conditional distributions via additive gaussian processes. we extend both frameworks beyond the case of a known causal graph, incorporating uncertainty about the causal structure via markov chain monte carlo methods. simulation studies show that our approach is able to identify the effects of hypothetical interventions with non-gaussian, non-linear observational data and accurately reflect the posterior uncertainty of the causal estimates. finally we compare the results of our gpn-based causal inference approach to existing methods on a dataset of $a.~thaliana$ gene expressions.",,2024-02-01,,"['enrico giudice', 'jack kuipers', 'giusi moffa']"
2402.00645,spectrally transformed kernel regression,stat.ml cs.lg,"unlabeled data is a key component of modern machine learning. in general, the role of unlabeled data is to impose a form of smoothness, usually from the similarity information encoded in a base kernel, such as the $\epsilon$-neighbor kernel or the adjacency matrix of a graph. this work revisits the classical idea of spectrally transformed kernel regression (stkr), and provides a new class of general and scalable stkr estimators able to leverage unlabeled data. intuitively, via spectral transformation, stkr exploits the data distribution for which unlabeled data can provide additional information. first, we show that stkr is a principled and general approach, by characterizing a universal type of ""target smoothness"", and proving that any sufficiently smooth function can be learned by stkr. second, we provide scalable stkr implementations for the inductive setting and a general transformation function, while prior work is mostly limited to the transductive setting. third, we derive statistical guarantees for two scenarios: stkr with a known polynomial transformation, and stkr with kernel pca when the transformation is unknown. overall, we believe that this work helps deepen our understanding of how to work with unlabeled data, and its generality makes it easier to inspire new methods.",,2024-02-01,,"['runtian zhai', 'rattana pukdee', 'roger jin', 'maria-florina balcan', 'pradeep ravikumar']"
2402.00728,dropout-based rashomon set exploration for efficient predictive   multiplicity estimation,cs.lg stat.ml,"predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. this presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications. measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the rashomon set, in potentially huge hypothesis spaces. to address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the rashomon set. we provide rigorous theoretical derivations to connect the dropout parameters to properties of the rashomon set, and empirically evaluate our framework through extensive experimentation. numerical results show that our technique consistently outperforms baselines in terms of the effectiveness of predictive multiplicity metric estimation, with runtime speedup up to $20\times \sim 5000\times$. with efficient rashomon set exploration and metric estimation, mitigation of predictive multiplicity is then achieved through dropout ensemble and model selection.",,2024-02-01,,"['hsiang hsu', 'guihong li', 'shaohan hu', 'n/a chun-fu', 'n/a chen']"
2402.00743,benefits of transformer: in-context learning in linear regression tasks   with unstructured data,cs.lg cs.cl stat.ml,"in practice, it is observed that transformer-based models can learn concepts in context in the inference stage. while existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). however, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). in this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. we study the exact components in a transformer that facilitate the in-context learning. in particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token next to $x_i$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention.",,2024-02-01,,"['yue xing', 'xiaofeng lin', 'namjoon suh', 'qifan song', 'guang cheng']"
2402.00809,position paper: bayesian deep learning in the age of large-scale ai,cs.lg stat.ml,"in the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. however, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. bayesian deep learning (bdl) constitutes a promising avenue, offering advantages across these diverse settings. this paper posits that bdl can elevate the capabilities of deep learning. it revisits the strengths of bdl, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with bdl to unlock their full potential.",,2024-02-01,2024-02-06,"['theodore papamarkou', 'maria skoularidou', 'konstantina palla', 'laurence aitchison', 'julyan arbel', 'david dunson', 'maurizio filippone', 'vincent fortuin', 'philipp hennig', 'jose miguel hernandez lobato', 'aliaksandr hubin', 'alexander immer', 'theofanis karaletsos', 'mohammad emtiyaz khan', 'agustinus kristiadi', 'yingzhen li', 'stephan mandt', 'christopher nemeth', 'michael a. osborne', 'tim g. j. rudner', 'david rügamer', 'yee whye teh', 'max welling', 'andrew gordon wilson', 'ruqi zhang']"
2402.00847,bootstap: bootstrapped training for tracking-any-point,cs.cv stat.ml,"to endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. this can be formalized as tracking-any-point (tap), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. large-scale ground-truth training data for tap is only available in simulation, which currently has limited variety of objects and motion. in this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a tap model with minimal architectural changes, using a self-supervised student-teacher setup. we demonstrate state-of-the-art performance on the tap-vid benchmark surpassing previous results by a wide margin: for example, tap-vid-davis performance improves from 61.3% to 66.4%, and tap-vid-kinetics from 57.2% to 61.5%.",,2024-02-01,,"['carl doersch', 'yi yang', 'dilara gokay', 'pauline luc', 'skanda koppula', 'ankush gupta', 'joseph heyward', 'ross goroshin', 'joão carreira', 'andrew zisserman']"
2402.00849,score-based causal representation learning: linear and general   transformations,cs.lg stat.ml,"this paper addresses intervention-based causal representation learning (crl) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. linear and general transformations are investigated. the paper addresses both the identifiability and achievability aspects. identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. by drawing novel connections between score functions (i.e., the gradients of the logarithm of density functions) and crl, this paper designs a score-based class of algorithms that ensures both identifiability and achievability. first, the paper focuses on linear transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability. it also provides partial identifiability guarantees for soft interventions, including identifiability up to ancestors for general causal models and perfect latent graph recovery for sufficiently non-linear causal models. secondly, it focuses on general transformations and shows that two stochastic hard interventions per node suffice for identifiability. notably, one does not need to know which pair of interventional environments have the same node intervened.",,2024-02-01,2024-02-26,"['burak varıcı', 'emre acartürk', 'karthikeyan shanmugam', 'abhishek kumar', 'ali tajer']"
2402.00857,early time classification with accumulated accuracy gap control,cs.lg stat.ml,"early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input. in this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule. this data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification. we start by presenting a novel method that builds on the learn-then-test calibration framework to control this gap marginally, on average over i.i.d. instances. as this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times. numerical experiments demonstrate the effectiveness, applicability, and usefulness of our method. we show that our proposed early stopping mechanism reduces up to 94% of timesteps used for classification while achieving rigorous accuracy gap control.",,2024-02-01,,"['liran ringel', 'regev cohen', 'daniel freedman', 'michael elad', 'yaniv romano']"
2402.00899,weakly supervised learners for correction of ai errors with provable   performance guarantees,cs.lg cs.ai stat.ml,we present a new methodology for handling ai errors by introducing weakly supervised ai error correctors with a priori performance guarantees. these ai correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. the rejection of a decision can be used as a signal to suggest abstaining from making a decision. a key technical focus of the work is in providing performance guarantees for these new ai correctors through bounds on the probabilities of incorrect decisions. these bounds are distribution agnostic and do not rely on assumptions on the data dimension. our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.,,2024-01-31,2024-02-13,"['ivan y. tyukin', 'tatiana tyukina', 'daniel van helden', 'zedong zheng', 'evgeny m. mirkes', 'oliver j. sutton', 'qinghua zhou', 'alexander n. gorban', 'penelope allison']"
2402.00907,alpharank: an artificial intelligence approach for ranking and selection   problems,cs.lg stat.me,"we introduce alpharank, an artificial intelligence approach to address the fixed-budget ranking and selection (r&s) problems. we formulate the sequential sampling decision as a markov decision process and propose a monte carlo simulation-based rollout policy that utilizes classic r&s procedures as base policies for efficiently learning the value function of stochastic dynamic programming. we accelerate online sample-allocation by using deep reinforcement learning to pre-train a neural network model offline based on a given prior. we also propose a parallelizable computing framework for large-scale problems, effectively combining ""divide and conquer"" and ""recursion"" for enhanced scalability and efficiency. numerical experiments demonstrate that the performance of alpharank is significantly improved over the base policies, which could be attributed to alpharank's superior capability on the trade-off among mean, variance, and induced correlation overlooked by many existing policies.",,2024-01-31,,"['ruihan zhou', 'l. jeff hong', 'yijie peng']"
2402.00949,geometry of polynomial neural networks,math.ag cs.lg stat.ml,"we study the expressivity and learning process for polynomial neural networks (pnns) with monomial activation functions. the weights of the network parametrize the neuromanifold. in this paper, we study certain neuromanifolds using tools from algebraic geometry: we give explicit descriptions as semialgebraic sets and characterize their zariski closures, called neurovarieties. we study their dimension and associate an algebraic degree, the learning degree, to the neurovariety. the dimension serves as a geometric measure for the expressivity of the network, the learning degree is a measure for the complexity of training the network and provides upper bounds on the number of learnable functions. these theoretical results are accompanied with experiments.",,2024-02-01,,"['kaie kubjas', 'jiayi li', 'maximilian wiesmann']"
2402.01000,multivariate probabilistic time series forecasting with correlated   errors,stat.ml cs.lg,"modeling the correlations among errors is closely associated with how accurately the model can quantify predictive uncertainty in probabilistic time series forecasting. recent multivariate models have made significant progress in accounting for contemporaneous correlations among errors, while a common assumption on these errors is that they are temporally independent for the sake of statistical simplicity. however, real-world observations often deviate from this assumption, since errors usually exhibit substantial autocorrelation due to various factors such as the exclusion of temporally correlated covariates. in this work, we propose an efficient method, based on a low-rank-plus-diagonal parameterization of the covariance matrix, which can effectively characterize the autocorrelation of errors. the proposed method possesses several desirable properties: the complexity does not scale with the number of time series, the resulting covariance can be used for calibrating predictions, and it can seamlessly integrate with any model with gaussian-distributed errors. we empirically demonstrate these properties using two distinct neural forecasting models-gpvar and transformer. our experimental results confirm the effectiveness of our method in enhancing predictive accuracy and the quality of uncertainty quantification on multiple real-world datasets.",,2024-02-01,2024-02-07,"['vincent zhihao zheng', 'lijun sun']"
2402.01036,fisher information dissipation for time inhomogeneous stochastic   differential equations,math.pr cs.lg stat.ml,"we provide a lyapunov convergence analysis for time-inhomogeneous variable coefficient stochastic differential equations (sdes). three typical examples include overdamped, irreversible drift, and underdamped langevin dynamics. we first formula the probability transition equation of langevin dynamics as a modified gradient flow of the kullback-leibler divergence in the probability space with respect to time-dependent optimal transport metrics. this formulation contains both gradient and non-gradient directions depending on a class of time-dependent target distribution. we then select a time-dependent relative fisher information functional as a lyapunov functional. we develop a time-dependent hessian matrix condition, which guarantees the convergence of the probability density function of the sde. we verify the proposed conditions for several time-inhomogeneous langevin dynamics. for the overdamped langevin dynamics, we prove the $o(t^{-1/2})$ convergence in $l^1$ distance for the simulated annealing dynamics with a strongly convex potential function. for the irreversible drift langevin dynamics, we prove an improved convergence towards the target distribution in an asymptotic regime. we also verify the convergence condition for the underdamped langevin dynamics. numerical examples demonstrate the convergence results for the time-dependent langevin dynamics.",,2024-02-01,,"['qi feng', 'xinzhe zuo', 'wuchen li']"
2402.01050,distributed mcmc inference for bayesian non-parametric latent block   model,stat.ml cs.lg stat.co,"in this paper, we introduce a novel distributed markov chain monte carlo (mcmc) inference method for the bayesian non-parametric latent block model (disnplbm), employing the master/worker architecture. our non-parametric co-clustering algorithm divides observations and features into partitions using latent multivariate gaussian block distributions. the workload on rows is evenly distributed among workers, who exclusively communicate with the master and not among themselves. disnplbm demonstrates its impact on cluster labeling accuracy and execution times through experimental results. moreover, we present a real-use case applying our approach to co-cluster gene expression data. the code source is publicly available at https://github.com/redakhoufache/distributed-nplbm.",,2024-02-01,,"['reda khoufache', 'anisse belhadj', 'hanene azzag', 'mustapha lebbah']"
2402.01052,weakly convex regularisers for inverse problems: convergence of critical   points and primal-dual optimisation,math.oc cs.cv cs.lg stat.ml,"variational regularisation is the primary method for solving inverse problems, and recently there has been considerable work leveraging deeply learned regularisation for enhanced performance. however, few results exist addressing the convergence of such regularisation, particularly within the context of critical points as opposed to global minima. in this paper, we present a generalised formulation of convergent regularisation in terms of critical points, and show that this is achieved by a class of weakly convex regularisers. we prove convergence of the primal-dual hybrid gradient method for the associated variational problem, and, given a kurdyka-lojasiewicz condition, an $\mathcal{o}(\log{k}/k)$ ergodic convergence rate. finally, applying this theory to learned regularisation, we prove universal approximation for input weakly convex neural networks (iwcnn), and show empirically that iwcnns can lead to improved performance of learned adversarial regularisers for computed tomography (ct) reconstruction.",,2024-02-01,,"['zakhar shumaylov', 'jeremy budd', 'subhadip mukherjee', 'carola-bibiane schönlieb']"
2402.01055,multiclass learning from noisy labels for non-decomposable performance   measures,cs.lg stat.ml,"there has been much interest in recent years in learning good classifiers from data with noisy labels. most work on learning from noisy labels has focused on standard loss-based performance measures. however, many machine learning problems require using non-decomposable performance measures which cannot be expressed as the expectation or sum of a loss on individual examples; these include for example the h-mean, q-mean and g-mean in class imbalance settings, and the micro $f_1$ in information retrieval. in this paper, we design algorithms to learn from noisy labels for two broad classes of multiclass non-decomposable performance measures, namely, monotonic convex and ratio-of-linear, which encompass all the above examples. our work builds on the frank-wolfe and bisection based methods of narasimhan et al. (2015). in both cases, we develop noise-corrected versions of the algorithms under the widely studied family of class-conditional noise models. we provide regret (excess risk) bounds for our algorithms, establishing that even though they are trained on noisy data, they are bayes consistent in the sense that their performance converges to the optimal performance w.r.t. the clean (non-noisy) distribution. our experiments demonstrate the effectiveness of our algorithms in handling label noise.",,2024-02-01,2024-02-27,"['mingyuan zhang', 'shivani agarwal']"
2402.01089,no free prune: information-theoretic barriers to pruning at   initialization,stat.ml cs.lg,"the existence of ""lottery tickets"" arxiv:1803.03635 at or near initialization raises the tantalizing question of whether large models are necessary in deep learning, or whether sparse networks can be quickly identified and trained without ever training the dense models that contain them. however, efforts to find these sparse subnetworks without training the dense model (""pruning at initialization"") have been broadly unsuccessful arxiv:2009.08576. we put forward a theoretical explanation for this, based on the model's effective parameter count, $p_\text{eff}$, given by the sum of the number of non-zero weights in the final network and the mutual information between the sparsity mask and the data. we show the law of robustness of arxiv:2105.12806 extends to sparse networks with the usual parameter count replaced by $p_\text{eff}$, meaning a sparse neural network which robustly interpolates noisy data requires a heavily data-dependent mask. we posit that pruning during and after training outputs masks with higher mutual information than those produced by pruning at initialization. thus two networks may have the same sparsities, but differ in effective parameter count based on how they were trained. this suggests that pruning near initialization may be infeasible and explains why lottery tickets exist, but cannot be found fast (i.e. without training the full network). experiments on neural networks confirm that information gained during training may indeed affect model capacity.",,2024-02-01,,"['tanishq kumar', 'kevin luo', 'mark sellke']"
2402.01090,scalable higher-order tensor product spline models,stat.ml cs.lg stat.co,"in the current era of vast data and transparent machine learning, it is essential for techniques to operate at a large scale while providing a clear mathematical comprehension of the internal workings of the method. although there already exist interpretable semi-parametric regression methods for large-scale applications that take into account non-linearity in the data, the complexity of the models is still often limited. one of the main challenges is the absence of interactions in these models, which are left out for the sake of better interpretability but also due to impractical computational costs. to overcome this limitation, we propose a new approach using a factorization method to derive a highly scalable higher-order tensor product spline model. our method allows for the incorporation of all (higher-order) interactions of non-linear feature effects while having computational costs proportional to a model without interactions. we further develop a meaningful penalization scheme and examine the induced optimization problem. we conclude by evaluating the predictive and estimation performance of our method.",,2024-02-01,,['david rügamer']
2402.01092,a dynamical model of neural scaling laws,stat.ml cond-mat.dis-nn cs.lg,"on a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. this phenomenon is known as a neural scaling law. of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. we analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. this reproduces many observations about neural scaling laws. first, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. second, it has been observed that early in training, networks converge to their infinite-width dynamics at a rate $1/\textit{width}$ but at late time exhibit a rate $\textit{width}^{-c}$, where $c$ depends on the structure of the architecture and task. we show that our model exhibits this behavior. lastly, our theory shows how the gap between training and test loss can gradually build up over time due to repeated reuse of data.",,2024-02-01,,"['blake bordelon', 'alexander atanasov', 'cengiz pehlevan']"
2402.01095,how many views does your deep neural network use for prediction?,cs.lg cs.ai cs.cv stat.ml,"the generalization ability of deep neural networks (dnns) is still not fully understood, despite numerous theoretical and empirical analyses. recently, allen-zhu & li (2023) introduced the concept of multi-views to explain the generalization ability of dnns, but their main target is ensemble or distilled models, and no method for estimating multi-views used in a prediction of a specific input is discussed. in this paper, we propose minimal sufficient views (msvs), which is similar to multi-views but can be efficiently computed for real images. msvs is a set of minimal and distinct features in an input, each of which preserves a model's prediction for the input. we empirically show that there is a clear relationship between the number of msvs and prediction accuracy across models, including convolutional and transformer models, suggesting that a multi-view like perspective is also important for understanding the generalization ability of (non-ensemble or non-distilled) dnns.",,2024-02-01,,"['keisuke kawano', 'takuro kutsuna', 'keisuke sano']"
2402.01098,bayesian deep learning for remaining useful life estimation via stein   variational gradient descent,cs.lg stat.ml,"a crucial task in predictive maintenance is estimating the remaining useful life of physical systems. in the last decade, deep learning has improved considerably upon traditional model-based and statistical approaches in terms of predictive performance. however, in order to optimally plan maintenance operations, it is also important to quantify the uncertainty inherent to the predictions. this issue can be addressed by turning standard frequentist neural networks into bayesian neural networks, which are naturally capable of providing confidence intervals around the estimates. several methods exist for training those models. researchers have focused mostly on parametric variational inference and sampling-based techniques, which notoriously suffer from limited approximation power and large computational burden, respectively. in this work, we use stein variational gradient descent, a recently proposed algorithm for approximating intractable distributions that overcomes the drawbacks of the aforementioned techniques. in particular, we show through experimental studies on simulated run-to-failure turbofan engine degradation data that bayesian deep learning models trained via stein variational gradient descent consistently outperform with respect to convergence speed and predictive performance both the same models trained via parametric variational inference and their frequentist counterparts trained via backpropagation. furthermore, we propose a method to enhance performance based on the uncertainty information provided by the bayesian models. we release the source code at https://github.com/lucadellalib/bdl-rul-svgd.",,2024-02-01,,"['luca della libera', 'jacopo andreoli', 'davide dalle pezze', 'mirco ravanelli', 'gian antonio susto']"
2402.01111,near-optimal reinforcement learning with self-play under adaptivity   constraints,cs.lg cs.ai cs.ma stat.ml,"we study the problem of multi-agent reinforcement learning (marl) with adaptivity constraints -- a new problem motivated by real-world applications where deployments of new policies are costly and the number of policy updates must be minimized. for two-player zero-sum markov games, we design a (policy) elimination based algorithm that achieves a regret of $\widetilde{o}(\sqrt{h^3 s^2 abk})$, while the batch complexity is only $o(h+\log\log k)$. in the above, $s$ denotes the number of states, $a,b$ are the number of actions for the two players respectively, $h$ is the horizon and $k$ is the number of episodes. furthermore, we prove a batch complexity lower bound $\omega(\frac{h}{\log_{a}k}+\log\log k)$ for all algorithms with $\widetilde{o}(\sqrt{k})$ regret bound, which matches our upper bound up to logarithmic factors. as a byproduct, our techniques naturally extend to learning bandit games and reward-free marl within near optimal batch complexity. to the best of our knowledge, these are the first line of results towards understanding marl with low adaptivity.",,2024-02-01,,"['dan qiao', 'yu-xiang wang']"
2402.01112,gerontologic biostatistics 2.0: developments over 10+ years in the age   of data science,stat.me stat.ap stat.ot,"background: introduced in 2010, the sub-discipline of gerontologic biostatistics (gbs) was conceptualized to address the specific challenges in analyzing data from research studies involving older adults. however, the evolving technological landscape has catalyzed data science and statistical advancements since the original gbs publication, greatly expanding the scope of gerontologic research. there is a need to describe how these advancements enhance the analysis of multi-modal data and complex phenotypes that are hallmarks of gerontologic research. methods: this paper introduces gbs 2.0, an updated and expanded set of analytical methods reflective of the practice of gerontologic biostatistics in contemporary and future research. results: gbs 2.0 topics and relevant software resources include cutting-edge methods in experimental design; analytical techniques that include adaptations of machine learning, quantifying deep phenotypic measurements, high-dimensional -omics analysis; the integration of information from multiple studies, and strategies to foster reproducibility, replicability, and open science. discussion: the methodological topics presented here seek to update and expand gbs. by facilitating the synthesis of biostatistics and data science in gerontology, we aim to foster the next generation of gerontologic researchers.",,2024-02-01,,"['chixiang chen', 'michelle shardell', 'jaime lynn speiser', 'karen bandeen-roche', 'heather allore', 'thomas g travison', 'michael griswold', 'terrence e. murphy']"
2402.01139,online conformal prediction with decaying step sizes,stat.ml cs.lg stat.me,"we introduce a method for online conformal prediction with decaying step sizes. like previous methods, ours possesses a retrospective guarantee of coverage for arbitrary sequences. however, unlike previous methods, we can simultaneously estimate a population quantile when it exists. our theory and experiments indicate substantially improved practical properties: in particular, when the distribution is stable, the coverage is close to the desired level for every time point, not just on average over the observed sequence.",,2024-02-01,,"['anastasios n. angelopoulos', 'rina foygel barber', 'stephen bates']"
2402.01143,learning network representations with disentangled graph auto-encoder,cs.lg cs.ai stat.ml,"the (variational) graph auto-encoder is extensively employed for learning representations of graph-structured data. however, the formation of real-world graphs is a complex and heterogeneous process influenced by latent factors. existing encoders are fundamentally holistic, neglecting the entanglement of latent factors. this not only makes graph analysis tasks less effective but also makes it harder to understand and explain the representations. learning disentangled graph representations with (variational) graph auto-encoder poses significant challenges, and remains largely unexplored in the existing literature. in this article, we introduce the disentangled graph auto-encoder (dga) and disentangled variational graph auto-encoder (dvga), approaches that leverage generative models to learn disentangled representations. specifically, we first design a disentangled graph convolutional network with multi-channel message-passing layers, as the encoder aggregating information related to each disentangled latent factor. subsequently, a component-wise flow is applied to each channel to enhance the expressive capabilities of disentangled variational graph auto-encoder. additionally, we design a factor-wise decoder, considering the characteristics of disentangled representations. in order to further enhance the independence among representations, we introduce independence constraints on mapping channels for different latent factors. empirical experiments on both synthetic and real-world datasets show the superiority of our proposed method compared to several state-of-the-art baselines.",,2024-02-01,,"['di fan', 'chuanhou gao']"
2402.01148,the optimality of kernel classifiers in sobolev space,math.st cs.lg stat.ml stat.th,"kernel methods are widely used in machine learning, especially for classification problems. however, the theoretical analysis of kernel classification is still limited. this paper investigates the statistical performances of kernel classifiers. with some mild assumptions on the conditional probability $\eta(x)=\mathbb{p}(y=1\mid x=x)$, we derive an upper bound on the classification excess risk of a kernel classifier using recent advances in the theory of kernel regression. we also obtain a minimax lower bound for sobolev spaces, which shows the optimality of the proposed classifier. our theoretical results can be extended to the generalization error of overparameterized neural network classifiers. to make our theoretical results more applicable in realistic settings, we also propose a simple method to estimate the interpolation smoothness of $2\eta(x)-1$ and apply the method to real datasets.",,2024-02-02,,"['jianfa lai', 'zhifan li', 'dongming huang', 'qian lin']"
2402.01199,miqcqp reformulation of the relu neural networks lipschitz constant   estimation problem,math.oc stat.ml,"it is well established that to ensure or certify the robustness of a neural network, its lipschitz constant plays a prominent role. however, its calculation is np-hard. in this note, by taking into account activation regions at each layer as new constraints, we propose new quadratically constrained mip formulations for the neural network lipschitz estimation problem. the solutions of these problems give lower bounds and upper bounds of the lipschitz constant and we detail conditions when they coincide with the exact lipschitz constant.",,2024-02-02,,"['mohammed sbihi', 'sophie jan', 'nicolas couellan']"
2402.01258,transformers learn nonlinear features in context: nonconvex mean-field   dynamics on the attention landscape,stat.ml cs.lg,"large language models based on the transformer architecture have demonstrated impressive capabilities to learn in context. however, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. in this paper, we study the optimization of a transformer consisting of a fully connected layer followed by a linear attention layer. the mlp acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. we prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. we also analyze the second-order stability of mean-field dynamics and show that wasserstein gradient flow almost always avoids saddle points. furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. this represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest.",,2024-02-02,,"['juno kim', 'taiji suzuki']"
2402.01297,characterizing overfitting in kernel ridgeless regression through the   eigenspectrum,cs.lg stat.ml,"we derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension. for kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel.   our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting. this adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially. our analysis combines new random matrix theory (rmt) techniques with recent tools in the kernel ridge regression (krr) literature.",,2024-02-02,2024-02-05,"['tin sum cheng', 'aurelien lucchi', 'anastasis kratsios', 'david belius']"
2402.01341,fundamental properties of causal entropy and information gain,cs.lg cs.it math.it stat.ml,"recent developments enable the quantification of causal control given a structural causal model (scm). this has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. these measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. they have not yet been properly mathematically studied. our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. furthermore, we elucidate the relationship between causal entropy and stochastic interventions. we also propose definitions for causal conditional entropy and causal conditional information gain. overall, this exploration paves the way for enhancing causal machine learning tasks through the study of recently-proposed information theoretic quantities grounded in considerations about causality.",,2024-02-02,2024-02-19,"['francisco n. f. q. simoes', 'mehdi dastani', 'thijs van ommen']"
2402.01342,training-time neuron alignment through permutation subspace for   improving linear mode connectivity and model fusion,cs.lg stat.ml,"in deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the linear mode connectivity (lmc) landscape. overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms. previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation. however, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., vit, llm) due to numerous permutation matrices. thus, in this paper, we study training-time neuron alignment. our hypothesis suggests that training-time permutation subspace can reduce lmc barriers for free. we find that pruning at initialization supports this. beyond pruning, we introduce tna-pfn, a simple yet lossless algorithm using a partial gradient mask during training. tna-pfn is theoretically and empirically validated for reducing lmc barriers. it excels in wide model fusion applications, especially in federated learning, two algorithms based on tna-fpn that are proposed to show its prospects even under heterogeneous datasets. moreover, tna-pfn can enhance the generalization of model soup for vision transformers and cold fusion for pretrained language models.",,2024-02-02,,"['zexi li', 'zhiqi li', 'jie lin', 'tao shen', 'tao lin', 'chao wu']"
2402.01382,emergence of heavy tails in homogenized stochastic gradient descent,stat.ml cs.lg,"it has repeatedly been observed that loss minimization by stochastic gradient descent (sgd) leads to heavy-tailed distributions of neural network parameters. here, we analyze a continuous diffusion approximation of sgd, called homogenized stochastic gradient descent, show that it behaves asymptotically heavy-tailed, and give explicit upper and lower bounds on its tail-index. we validate these bounds in numerical experiments and show that they are typically close approximations to the empirical tail-index of sgd iterates. in addition, their explicit form enables us to quantify the interplay between optimization parameters and the tail-index. doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of sgd to avoid suboptimal local minima.",,2024-02-02,,"['zhe jiao', 'martin keller-ressel']"
2402.01399,a probabilistic model to explain self-supervised representation learning,cs.lg cs.ai stat.ml,"self-supervised learning (ssl) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. of the many approaches to ssl, contrastive methods, e.g. simclr, clip and vicreg, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. however, a theoretical understanding of the mechanism behind these methods eludes. we propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. we also justify links to mutual information and the use of a projection head. fitting our model generatively, as simve, improves performance over previous vae methods on common benchmarks (e.g. fashionmnist, cifar10, celeba), narrows the gap to discriminative methods on _content_ classification and, as our analysis predicts, outperforms them where _style_ information is required, taking a step toward task-agnostic representations.",,2024-02-02,,"['alice bizeul', 'bernhard schölkopf', 'carl allen']"
2402.01400,query-efficient correlation clustering with noisy oracle,stat.ml cs.ds cs.lg,"we study a general clustering setting in which we have $n$ elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the similarity between two elements. our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. we propose two novel formulations of online learning problems rooted in the paradigm of pure exploration in combinatorial multi-armed bandits (pe-cmab): fixed confidence and fixed budget settings. for both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. our results are the first examples of polynomial-time algorithms that work for the case of pe-cmab in which the underlying offline optimization problem is np-hard.",,2024-02-02,,"['yuko kuroki', 'atsushi miyauchi', 'francesco bonchi', 'wei chen']"
2402.01401,zero-shot machine unlearning at scale via lipschitz regularization,cs.lg cs.ai stat.ml,"to comply with ai and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. the key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. in this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. under such a definition, existing state-of-the-art methods are insufficient. building on the concepts of lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. we show this smoothing successfully results in forgetting while preserving general model performance. we perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of zero-shot unlearning.",,2024-02-02,2024-02-05,"['jack foster', 'kyle fogarty', 'stefan schoepf', 'cengiz öztireli', 'alexandra brintrup']"
2402.01434,conditioning non-linear and infinite-dimensional diffusion processes,stat.ml cs.lg stat.co,"generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation. to incorporate observed data for statistical and learning tasks, one needs to condition on observations. while recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored. this paper conditions function valued stochastic processes without prior discretisation. to do so, we use an infinite-dimensional version of girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (sde) for the conditioned process involving the score. we apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the fourier basis and then learn the coefficients of the score function with score matching methods.",,2024-02-02,,"['elizabeth louise baker', 'gefan yang', 'michael l. severinsen', 'christy anna hipsley', 'stefan sommer']"
2402.01450,improving importance estimation in covariate shift for providing   accurate prediction error,cs.lg stat.ml,"in traditional machine learning, the algorithms predictions are based on the assumption that the data follows the same distribution in both the training and the test datasets. however, in real world data this condition does not hold and, for instance, the distribution of the covariates changes whereas the conditional distribution of the targets remains unchanged. this situation is called covariate shift problem where standard error estimation may be no longer accurate. in this context, the importance is a measure commonly used to alleviate the influence of covariate shift on error estimations. the main drawback is that it is not easy to compute. the kullback-leibler importance estimation procedure (kliep) is capable of estimating importance in a promising way. despite its good performance, it fails to ignore target information, since it only includes the covariates information for computing the importance. in this direction, this paper explores the potential performance improvement if target information is considered in the computation of the importance. then, a redefinition of the importance arises in order to be generalized in this way. besides the potential improvement in performance, including target information make possible the application to a real application about plankton classification that motivates this research and characterized by its great dimensionality, since considering targets rather than covariates reduces the computation and the noise in the covariates. the impact of taking target information is also explored when logistic regression (lr), kernel mean matching (kmm), ensemble kernel mean matching (ekmm) and the naive predecessor of kliep called kernel density estimation (kde) methods estimate the importance. the experimental results lead to a more accurate error estimation using target information, especially in case of the more promising method kliep.",10.1016/j.eswa.2021.116376,2024-02-02,,"['laura fdez-díaz', 'sara gonzález tomillo', 'elena montañés', 'josé ramón quevedo']"
2402.01454,integrating large language models in causal discovery: a statistical   causal approach,cs.lg cs.ai stat.me stat.ml,"in practical statistical causal discovery (scd), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. to overcome these challenges, this paper proposes a novel methodology for causal inference, in which scd methods and knowledge based causal inference (kbci) with a large language model (llm) are synthesized through ""statistical causal prompting (scp)"" for llms and prior knowledge augmentation for scd. experiments have revealed that gpt-4 can cause the output of the llm-kbci and the scd result with prior knowledge from llm-kbci to approach the ground truth, and that the scd result can be further improved, if gpt-4 undergoes scp. furthermore, it has been clarified that an llm can improve scd with its background knowledge, even if the llm does not contain information on the dataset. the proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of llms to improve data-driven causal inference across diverse scientific domains.",,2024-02-02,,"['masayuki takayama', 'tadahisa okuda', 'thong pham', 'tatsuyoshi ikenoue', 'shingo fukuma', 'shohei shimizu', 'akiyoshi sannai']"
2402.01460,deep conditional generative learning: model and error analysis,stat.ml cs.lg,"we introduce an ordinary differential equation (ode) based deep generative method for learning a conditional distribution, named the conditional follmer flow. starting from a standard gaussian distribution, the proposed flow could efficiently transform it into the target conditional distribution at time 1. for effective implementation, we discretize the flow with euler's method where we estimate the velocity field nonparametrically using a deep neural network. furthermore, we derive a non-asymptotic convergence rate in the wasserstein distance between the distribution of the learned samples and the target distribution, providing the first comprehensive end-to-end error analysis for conditional distribution learning via ode flow. our numerical experiments showcase its effectiveness across a range of scenarios, from standard nonparametric conditional density estimation problems to more intricate challenges involving image data, illustrating its superiority over various existing conditional density estimation methods.",,2024-02-02,,"['jinyuan chang', 'zhao ding', 'yuling jiao', 'ruoxuan li', 'jerry zhijian yang']"
2402.01476,self-attention through kernel-eigen pair sparse variational gaussian   processes,cs.lg cs.ai cs.cv stat.ml,"while the great capability of transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by gaussian processes (gps). existing works apply gps with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. moreover, the complexity of deriving the gp posteriors remains high for large-scale data. in this work, we propose kernel-eigen pair sparse variational gaussian processes (kep-svgp) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by kernel svd (ksvd) and a reduced complexity is acquired. through kep-svgp, i) the svgp pair induced by the two sets of singular vectors from ksvd w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from ksvd, the derivation of svgp posteriors can be based on the inversion of a diagonal matrix containing singular values, contributing to a reduction in time complexity; iii) an evidence lower bound is derived so that variational parameters can be optimized towards this objective. experiments verify our excellent performances and efficiency on in-distribution, distribution-shift and out-of-distribution benchmarks.",,2024-02-02,,"['yingyi chen', 'qinghua tao', 'francesco tonin', 'johan a. k. suykens']"
2402.01484,connecting the dots: is mode-connectedness the key to feasible   sample-based inference in bayesian neural networks?,cs.lg stat.co stat.ml,"a major challenge in sample-based inference (sbi) for bayesian neural networks is the size and structure of the networks' parameter space. our work shows that successful sbi is possible by embracing the characteristic relationship between weight and function space, uncovering a systematic link between overparameterization and the difficulty of the sampling problem. through extensive experiments, we establish practical guidelines for sampling and convergence diagnosis. as a result, we present a bayesian deep ensemble approach as an effective solution with competitive performance and uncertainty quantification.",,2024-02-02,,"['emanuel sommer', 'lisa wimmer', 'theodore papamarkou', 'ludwig bothmann', 'bernd bischl', 'david rügamer']"
2402.01493,sliced-wasserstein estimation with spherical harmonics as control   variates,stat.ml cs.lg,"the sliced-wasserstein (sw) distance between probability measures is defined as the average of the wasserstein distances resulting for the associated one-dimensional projections. as a consequence, the sw distance can be written as an integral with respect to the uniform measure on the sphere and the monte carlo framework can be employed for calculating the sw distance. spherical harmonics are polynomials on the sphere that form an orthonormal basis of the set of square-integrable functions on the sphere. putting these two facts together, a new monte carlo method, hereby referred to as spherical harmonics control variates (shcv), is proposed for approximating the sw distance using spherical harmonics as control variates. the resulting approach is shown to have good theoretical properties, e.g., a no-error property for gaussian measures under a certain form of linear dependency between the variables. moreover, an improved rate of convergence, compared to monte carlo, is established for general measures. the convergence analysis relies on the lipschitz property associated to the sw integrand. several numerical experiments demonstrate the superior performance of shcv against state-of-the-art methods for sw distance computation.",,2024-02-02,,"['rémi leluc', 'aymeric dieuleveut', 'françois portier', 'johan segers', 'aigerim zhuman']"
2402.01502,why do random forests work? understanding tree ensembles as   self-regularizing adaptive smoothers,stat.ml cs.lg,"despite their remarkable effectiveness and broad application, the drivers of success underlying ensembles of trees are still not fully understood. in this paper, we highlight how interpreting tree ensembles as adaptive and self-regularizing smoothers can provide new intuition and deeper insight to this topic. we use this perspective to show that, when studied as smoothers, randomized tree ensembles not only make predictions that are quantifiably more smooth than the predictions of the individual trees they consist of, but also further regulate their smoothness at test-time based on the dissimilarity between testing and training inputs. first, we use this insight to revisit, refine and reconcile two recent explanations of forest success by providing a new way of quantifying the conjectured behaviors of tree ensembles objectively by measuring the effective degree of smoothing they imply. then, we move beyond existing explanations for the mechanisms by which tree ensembles improve upon individual trees and challenge the popular wisdom that the superior performance of forests should be understood as a consequence of variance reduction alone. we argue that the current high-level dichotomy into bias- and variance-reduction prevalent in statistics is insufficient to understand tree ensembles -- because the prevailing definition of bias does not capture differences in the expressivity of the hypothesis classes formed by trees and forests. instead, we show that forests can improve upon trees by three distinct mechanisms that are usually implicitly entangled. in particular, we demonstrate that the smoothing effect of ensembling can reduce variance in predictions due to noise in outcome generation, reduce variability in the quality of the learned function given fixed input data and reduce potential bias in learnable functions by enriching the available hypothesis space.",,2024-02-02,,"['alicia curth', 'alan jeffares', 'mihaela van der schaar']"
2402.01514,mapping the multiverse of latent representations,cs.lg math.at stat.ml,"echoing recent calls to counter reliability and robustness concerns in machine learning via multiverse analysis, we present presto, a principled framework for mapping the multiverse of machine-learning models that rely on latent representations. although such models enjoy widespread adoption, the variability in their embeddings remains poorly understood, resulting in unnecessary complexity and untrustworthy representations. our framework uses persistent homology to characterize the latent spaces arising from different combinations of diverse machine-learning methods, (hyper)parameter configurations, and datasets, allowing us to measure their pairwise (dis)similarity and statistically reason about their distributions. as we demonstrate both theoretically and empirically, our pipeline preserves desirable properties of collections of latent representations, and it can be leveraged to perform sensitivity analysis, detect anomalous embeddings, or efficiently and effectively navigate hyperparameter search spaces.",,2024-02-02,,"['jeremy wayland', 'corinna coupette', 'bastian rieck']"
2402.01543,adaptive optimization for prediction with missing data,cs.lg stat.ml,"when training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. in this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. we show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. we leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. in settings where data is strongly not missing at random, our methods achieve a 2-10% improvement in out-of-sample accuracy.",,2024-02-02,,"['dimitris bertsimas', 'arthur delarue', 'jean pauphilet']"
2402.01577,deep active learning for data mining from conflict text corpora,cs.cy cs.cl stat.ml,"high-resolution event data on armed conflict and related processes have revolutionized the study of political contention with datasets like ucdp ged, acled etc. however, most of these datasets limit themselves to collecting spatio-temporal (high-resolution) and intensity data. information on dynamics, such as targets, tactics, purposes etc. are rarely collected owing to the extreme workload of collecting data. however, most datasets rely on a rich corpus of textual data allowing further mining of further information connected to each event. this paper proposes one such approach that is inexpensive and high performance, leveraging active learning - an iterative process of improving a machine learning model based on sequential (guided) human input. active learning is employed to then step-wise train (fine-tuning) of a large, encoder-only language model adapted for extracting sub-classes of events relating to conflict dynamics. the approach shows performance similar to human (gold-standard) coding while reducing the amount of required human annotation by as much as 99%.",,2024-02-02,,['mihai croicu']
2402.01599,hyperparameter tuning via trajectory predictions: stochastic prox-linear   methods in matrix sensing,math.oc math.st stat.ml stat.th,"motivated by the desire to understand stochastic algorithms for nonconvex optimization that are robust to their hyperparameter choices, we analyze a mini-batched prox-linear iterative algorithm for the problem of recovering an unknown rank-1 matrix from rank-1 gaussian measurements corrupted by noise. we derive a deterministic recursion that predicts the error of this method and show, using a non-asymptotic framework, that this prediction is accurate for any batch-size and a large range of step-sizes. in particular, our analysis reveals that this method, though stochastic, converges linearly from a local initialization with a fixed step-size to a statistical error floor. our analysis also exposes how the batch-size, step-size, and noise level affect the (linear) convergence rate and the eventual statistical estimation error, and we demonstrate how to use our deterministic predictions to perform hyperparameter tuning (e.g. step-size and batch-size selection) without ever running the method. on a technical level, our analysis is enabled in part by showing that the fluctuations of the empirical iterates around our deterministic predictions scale with the error of the previous iterate.",,2024-02-02,,"['mengqi lou', 'kabir aladin verchand', 'ashwin pananjady']"
2402.01614,l2g2g: a scalable local-to-global network embedding with graph   autoencoders,cs.lg cs.ai cs.si stat.ml,"for analysing real-world networks, graph representation learning is a popular tool. these methods, such as a graph autoencoder (gae), typically rely on low-dimensional representations, also called embeddings, which are obtained through minimising a loss function; these embeddings are used with a decoder for downstream tasks such as node classification and edge prediction. while gaes tend to be fairly accurate, they suffer from scalability issues. for improved speed, a local2global approach, which combines graph patch embeddings based on eigenvector synchronisation, was shown to be fast and achieve good accuracy. here we propose l2g2g, a local2global method which improves gae accuracy without sacrificing scalability. this improvement is achieved by dynamically synchronising the latent node representations, while training the gaes. it also benefits from the decoder computing an only local patch loss. hence, aligning the local embeddings in each epoch utilises more information from the graph than a single post-training alignment does, while maintaining scalability. we illustrate on synthetic benchmarks, as well as real-world examples, that l2g2g achieves higher accuracy than the standard local2global approach and scales efficiently on the larger data sets. we find that for large and dense networks, it even outperforms the slow, but assumed more accurate, gaes.",,2024-02-02,,"['ruikang ouyang', 'andrew elliott', 'stratis limnios', 'mihai cucuringu', 'gesine reinert']"
2402.01629,position paper: generalized grammar rules and structure-based   generalization beyond classical equivariance for lexical tasks and   transduction,cs.cl cs.lg stat.ml,"compositional generalization is one of the main properties which differentiates lexical learning in humans from state-of-art neural networks. we propose a general framework for building models that can generalize compositionally using the concept of generalized grammar rules (ggrs), a class of symmetry-based compositional constraints for transduction tasks, which we view as a transduction analogue of equivariance constraints in physics-inspired tasks. besides formalizing generalized notions of symmetry for language transduction, our framework is general enough to contain many existing works as special cases. we present ideas on how ggrs might be implemented, and in the process draw connections to reinforcement learning and other areas of research.",,2024-02-02,,"['mircea petrache', 'shubhendu trivedi']"
2402.01632,beyond lengthscales: no-regret bayesian optimisation with unknown   hyperparameters of any type,cs.lg stat.ml,"bayesian optimisation requires fitting a gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. the commonly used maximum likelihood estimator for hyperparameters of the gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in bayesian optimisation. since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the gaussian process fit, theoretical analysis of bayesian optimisation with unknown hyperparameters is very challenging. previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel hilbert space norm and applied only to the frequentist case. we propose a novel algorithm, he-gp-ucb, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparameters of arbitrary form, and which supports both bayesian and frequentist settings. our proof idea is novel and can easily be extended to other variants of bayesian optimisation. we show this by extending our algorithm to the adversarially robust optimisation setting under unknown hyperparameters. finally, we empirically evaluate our algorithm on a set of toy problems and show that it can outperform the maximum likelihood estimator.",,2024-02-02,2024-02-13,"['juliusz ziomek', 'masaki adachi', 'michael a. osborne']"
2402.01635,knn algorithm for conditional mean and variance estimation with   automated uncertainty quantification and variable selection,stat.me cs.lg stat.co stat.ml,"in this paper, we introduce a knn-based regression method that synergizes the scalability and adaptability of traditional non-parametric knn models with a novel variable selection technique. this method focuses on accurately estimating the conditional mean and variance of random response variables, thereby effectively characterizing conditional distributions across diverse scenarios.our approach incorporates a robust uncertainty quantification mechanism, leveraging our prior estimation work on conditional mean and variance. the employment of knn ensures scalable computational efficiency in predicting intervals and statistical accuracy in line with optimal non-parametric rates. additionally, we introduce a new knn semi-parametric algorithm for estimating roc curves, accounting for covariates. for selecting the smoothing parameter k, we propose an algorithm with theoretical guarantees.incorporation of variable selection enhances the performance of the method significantly over conventional knn techniques in various modeling tasks. we validate the approach through simulations in low, moderate, and high-dimensional covariate spaces. the algorithm's effectiveness is particularly notable in biomedical applications as demonstrated in two case studies. concluding with a theoretical analysis, we highlight the consistency and convergence rate of our method over traditional knn models, particularly when the underlying regression model takes values in a low-dimensional space.",,2024-02-02,,"['marcos matabuena', 'juan c. vidal', 'oscar hernan madrid padilla', 'jukka-pekka onnela']"
2402.01710,exploring educational equity: a machine learning approach to unravel   achievement disparities in georgia,cs.cy cs.lg stat.ap,"the covid-19 pandemic has significantly exacerbated existing educational disparities in georgia's k-12 system, particularly in terms of racial and ethnic achievement gaps. utilizing machine learning methods, the study conducts a comprehensive analysis of student achievement rates across different demographics, regions, and subjects. the findings highlight a significant decline in proficiency in english and math during the pandemic, with a noticeable contraction in score distribution and a greater impact on economically disadvantaged and black students. socio-economic status, as represented by the directly certified percentage -- the percentage of students eligible for free lunch, emerges as the most crucial factor, with additional insights drawn from faculty resources such as teacher salaries and expenditure on instruction. the study also identifies disparities in achievement rates between urban and rural settings, as well as variations across counties, underscoring the influence of geographical and socio-economic factors. the data suggests that targeted interventions and resource allocation, particularly in schools with higher percentages of economically disadvantaged students, are essential for mitigating educational disparities.",,2024-01-25,,"['yichen ma', 'dima nazzal']"
2402.01779,plug-and-play image restoration with stochastic denoising regularization,eess.iv cs.cv cs.lg stat.ml,"plug-and-play (pnp) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on diffusion models (dm), where the denoiser is applied only on re-noised images. we propose a new pnp framework, called stochastic denoising regularization (snore), which applies the denoiser only on images with noise of the adequate level. it is based on an explicit stochastic regularization, which leads to a stochastic gradient descent algorithm to solve ill-posed inverse problems. a convergence analysis of this algorithm and its annealing extension is provided. experimentally, we prove that snore is competitive with respect to state-of-the-art methods on deblurring and inpainting tasks, both quantitatively and qualitatively.",,2024-02-01,,"['marien renaud', 'jean prost', 'arthur leclaire', 'nicolas papadakis']"
2402.01785,doublemldeep: estimation of causal effects with multimodal data,cs.lg cs.ai econ.em stat.me stat.ml,"this paper explores the use of unstructured, multimodal data, namely text and images, in causal inference and treatment effect estimation. we propose a neural network architecture that is adapted to the double machine learning (dml) framework, specifically the partially linear model. an additional contribution of our paper is a new method to generate a semi-synthetic dataset which can be used to evaluate the performance of causal effect estimation in the presence of text and images as confounders. the proposed methods and architectures are evaluated on the semi-synthetic dataset and compared to standard approaches, highlighting the potential benefit of using text and images directly in causal studies. our findings have implications for researchers and practitioners in economics, marketing, finance, medicine and data science in general who are interested in estimating causal quantities using non-traditional data.",,2024-02-01,,"['sven klaassen', 'jan teichert-kluge', 'philipp bach', 'victor chernozhukov', 'martin spindler', 'suhas vijaykumar']"
2402.01797,robust support vector machines via conic optimization,cs.lg math.oc stat.co,"we consider the problem of learning support vector machines robust to uncertainty. it has been established in the literature that typical loss functions, including the hinge loss, are sensible to data perturbations and outliers, thus performing poorly in the setting considered. in contrast, using the 0-1 loss or a suitable non-convex approximation results in robust estimators, at the expense of large computational costs. in this paper we use mixed-integer optimization techniques to derive a new loss function that better approximates the 0-1 loss compared with existing alternatives, while preserving the convexity of the learning problem. in our computational results, we show that the proposed estimator is competitive with the standard svms with the hinge loss in outlier-free regimes and better in the presence of outliers.",,2024-02-02,,"['valentina cepeda', 'andrés gómez', 'shaoning han']"
2402.01810,misspecification uncertainties in near-deterministic regression,stat.ml cs.lg physics.data-an,"the expected loss is an upper bound to the model generalization error which admits robust pac-bayes bounds for learning. however, loss minimization is known to ignore misspecification, where models cannot exactly reproduce observations. this leads to significant underestimates of parameter uncertainties in the large data, or underparameterized, limit. we analyze the generalization error of near-deterministic, misspecified and underparametrized surrogate models, a regime of broad relevance in science and engineering. we show posterior distributions must cover every training point to avoid a divergent generalization error and derive an ensemble {ansatz} that respects this constraint, which for linear models incurs minimal overhead. the efficient approach is demonstrated on model problems before application to high dimensional datasets in atomistic machine learning. parameter uncertainties from misspecification survive in the underparametrized limit, giving accurate prediction and bounding of test errors.",,2024-02-02,,"['thomas d swinburne', 'danny perez']"
2402.01845,multi-armed bandits with interference,cs.lg stat.ml,"experimentation with interference poses a significant challenge in contemporary online platforms. prior research on experimentation with interference has concentrated on the final output of a policy. the cumulative performance, while equally crucial, is less well understood. to address this gap, we introduce the problem of {\em multi-armed bandits with interference} (mabi), where the learner assigns an arm to each of $n$ experimental units over a time horizon of $t$ rounds. the reward of each unit in each round depends on the treatments of {\em all} units, where the influence of a unit decays in the spatial distance between units. furthermore, we employ a general setup wherein the reward functions are chosen by an adversary and may vary arbitrarily across rounds and units. we first show that switchback policies achieve an optimal {\em expected} regret $\tilde o(\sqrt t)$ against the best fixed-arm policy. nonetheless, the regret (as a random variable) for any switchback policy suffers a high variance, as it does not account for $n$. we propose a cluster randomization policy whose regret (i) is optimal in {\em expectation} and (ii) admits a high probability bound that vanishes in $n$.",,2024-02-02,,"['su jia', 'peter frazier', 'nathan kallus']"
2402.01855,spde priors for uncertainty quantification of end-to-end neural data   assimilation schemes,stat.ml cs.lg eess.iv,"the spatio-temporal interpolation of large geophysical datasets has historically been adressed by optimal interpolation (oi) and more sophisticated model-based or data-driven da techniques. in the last ten years, the link established between stochastic partial differential equations (spde) and gaussian markov random fields (gmrf) opened a new way of handling both large datasets and physically-induced covariance matrix in optimal interpolation. recent advances in the deep learning community also enables to adress this problem as neural architecture embedding data assimilation variational framework. the reconstruction task is seen as a joint learning problem of the prior involved in the variational inner cost and the gradient-based minimization of the latter: both prior models and solvers are stated as neural networks with automatic differentiation which can be trained by minimizing a loss function, typically stated as the mean squared error between some ground truth and the reconstruction. in this work, we draw from the spde-based gaussian processes to estimate complex prior models able to handle non-stationary covariances in both space and time and provide a stochastic framework for interpretability and uncertainty quantification. our neural variational scheme is modified to embed an augmented state formulation with both state and spde parametrization to estimate. instead of a neural prior, we use a stochastic pde as surrogate model along the data assimilation window. the training involves a loss function for both reconstruction task and spde prior model, where the likelihood of the spde parameters given the true states is involved in the training. because the prior is stochastic, we can easily draw samples in the prior distribution before conditioning to provide a flexible way to estimate the posterior distribution based on thousands of members.",,2024-02-02,,"['maxime beauchamp', 'nicolas desassis', 'j. emmanuel johnson', 'simon benaichouche', 'pierre tandeo', 'ronan fablet']"
2402.01865,what will my model forget? forecasting forgotten examples in language   model refinement,cs.lg cs.cl stat.ml,"language models deployed in the wild make errors. however, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. to this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. we train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. we propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on bart but fails on t5 models. we further show a black-box classifier based on inner products of example representations achieves better forecasting performance over a series of setups. finally, we show that we reduce forgetting of upstream pretraining examples by replaying examples that are forecasted to be forgotten, demonstrating the practical utility of forecasting example forgetting.",,2024-02-02,,"['xisen jin', 'xiang ren']"
2402.01868,challenges in training pinns: a loss landscape perspective,cs.lg math.oc stat.ml,"this paper explores challenges in training physics-informed neural networks (pinns), emphasizing the role of the loss landscape in the training process. we examine difficulties in minimizing the pinn loss function, particularly due to ill-conditioning caused by differential operators in the residual term. we compare gradient-based optimizers adam, l-bfgs, and their combination adam+l-bfgs, showing the superiority of adam+l-bfgs, and introduce a novel second-order optimizer, nysnewton-cg (nncg), which significantly improves pinn performance. theoretically, our work elucidates the connection between ill-conditioned differential operators and ill-conditioning in the pinn loss and shows the benefits of combining first- and second-order optimization methods. our work presents valuable insights and more powerful optimization strategies for training pinns, which could improve the utility of pinns for solving difficult partial differential equations.",,2024-02-02,,"['pratik rathore', 'weimu lei', 'zachary frangella', 'lu lu', 'madeleine udell']"
2402.01887,on f-divergence principled domain adaptation: an improved framework,stat.ml cs.cv cs.lg,"unsupervised domain adaptation (uda) plays a crucial role in addressing distribution shifts in machine learning. in this work, we improve the theoretical foundations of uda proposed by acuna et al. (2021) by refining their f-divergence-based discrepancy and additionally introducing a new measure, f-domain discrepancy (f-dd). by removing the absolute value function and incorporating a scaling parameter, f-dd yields novel target error and sample complexity bounds, allowing us to recover previous kl-based results and bridging the gap between algorithms and theory presented in acuna et al. (2021). leveraging a localization technique, we also develop a fast-rate generalization bound. empirical results demonstrate the superior performance of f-dd-based domain learning algorithms over previous works in popular uda benchmarks.",,2024-02-02,,"['ziqiao wang', 'yongyi mao']"
2402.01900,distributional off-policy evaluation with bellman residual minimization,stat.ml cs.lg,"we consider the problem of distributional off-policy evaluation which serves as the foundation of many distributional reinforcement learning (drl) algorithms. in contrast to most existing works (that rely on supremum-extended statistical distances such as supremum-wasserstein distance), we study the expectation-extended statistical distance for quantifying the distributional bellman residuals and show that it can upper bound the expected error of estimating the return distribution. based on this appealing property, by extending the framework of bellman residual minimization to drl, we propose a method called energy bellman residual minimizer (ebrm) to estimate the return distribution. we establish a finite-sample error bound for the ebrm estimator under the realizability assumption. furthermore, we introduce a variant of our method based on a multi-step bootstrapping procedure to enable multi-step extension. by selecting an appropriate step level, we obtain a better error bound for this variant of ebrm compared to a single-step ebrm, under some non-realizability settings. finally, we demonstrate the superior performance of our method through simulation studies, comparing with several existing methods.",,2024-02-02,,"['sungee hong', 'zhengling qi', 'raymond k. w. wong']"
2402.01929,"sample, estimate, aggregate: a recipe for causal discovery foundation   models",cs.lg stat.ml,"causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. however, the per-dataset nature of existing causal discovery algorithms renders them slow, data hungry, and brittle. inspired by foundation models, we propose a causal discovery framework where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. this method is enabled by the observations that the outputs from classical algorithms are fast to compute for small problems, informative of (marginal) data structure, and their structure outputs as objects remain comparable across datasets. our method achieves state-of-the-art performance on synthetic and realistic datasets, generalizes to data generating mechanisms not seen during training, and offers inference speeds that are orders of magnitude faster than existing models.",,2024-02-02,,"['menghua wu', 'yujia bao', 'regina barzilay', 'tommi jaakkola']"
2402.01972,combining t-learning and dr-learning: a framework for oracle-efficient   estimation of causal contrasts,stat.ml cs.lg stat.me,"we introduce efficient plug-in (ep) learning, a novel framework for the estimation of heterogeneous causal contrasts, such as the conditional average treatment effect and conditional relative risk. the ep-learning framework enjoys the same oracle-efficiency as neyman-orthogonal learning strategies, such as dr-learning and r-learning, while addressing some of their primary drawbacks, including that (i) their practical applicability can be hindered by loss function non-convexity; and (ii) they may suffer from poor performance and instability due to inverse probability weighting and pseudo-outcomes that violate bounds. to avoid these drawbacks, ep-learner constructs an efficient plug-in estimator of the population risk function for the causal contrast, thereby inheriting the stability and robustness properties of plug-in estimation strategies like t-learning. under reasonable conditions, ep-learners based on empirical risk minimization are oracle-efficient, exhibiting asymptotic equivalence to the minimizer of an oracle-efficient one-step debiased estimator of the population risk function. in simulation experiments, we illustrate that ep-learners of the conditional average treatment effect and conditional relative risk outperform state-of-the-art competitors, including t-learner, r-learner, and dr-learner. open-source implementations of the proposed methods are available in our r package hte3.",,2024-02-02,,"['lars van der laan', 'marco carone', 'alex luedtke']"
2402.02010,genformer: a deep-learning-based approach for generating multivariate   stochastic processes,cs.lg stat.ml,"stochastic generators are essential to produce synthetic realizations that preserve target statistical properties. we propose genformer, a stochastic generator for spatio-temporal multivariate stochastic processes. it is constructed using a transformer-based deep learning model that learns a mapping between a markov state sequence and time series values. the synthetic data generated by the genformer model preserves the target marginal distributions and approximately captures other desired statistical properties even in challenging applications involving a large number of spatial locations and a long simulation horizon. the genformer model is applied to simulate synthetic wind speed data at various stations in florida to calculate exceedance probabilities for risk management.",,2024-02-02,,"['haoran zhao', 'wayne isaac tan uy']"
2402.02041,$\alpha$-divergence loss function for neural density ratio estimation,stat.ml cs.lg,"recently, neural networks have produced state-of-the-art results for density-ratio estimation (dre), a fundamental technique in machine learning. however, existing methods bear optimization issues that arise from the loss functions of dre: a large sample requirement of kullback--leibler (kl)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. thus, an $\alpha$-divergence loss function ($\alpha$-div) that offers concise implementation and stable optimization is proposed in this paper. furthermore, technical justifications for the proposed loss function are presented. the stability of the proposed loss function is empirically demonstrated and the estimation accuracy of dre tasks is investigated. additionally, this study presents a sample requirement for dre using the proposed loss function in terms of the upper bound of $l_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional dre tasks.",,2024-02-03,2024-02-18,['yoshiaki kitazawa']
2402.02098,self-attention networks localize when qk-eigenspectrum concentrates,stat.ml cs.lg,"the self-attention mechanism prevails in modern machine learning. it has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. in recent years, mainly two arguments have connected attention localization to the model performances. one is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. the other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. these two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. to this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.",,2024-02-03,,"['han bao', 'ryuichiro hataya', 'ryo karakida']"
2402.02111,accelerating look-ahead in bayesian optimization: multilevel monte carlo   is all you need,stat.ml cs.lg math.oc math.pr stat.co stat.me,"we leverage multilevel monte carlo (mlmc) to improve the performance of multi-step look-ahead bayesian optimization (bo) methods that involve nested expectations and maximizations. the complexity rate of naive monte carlo degrades for nested operations, whereas mlmc is capable of achieving the canonical monte carlo convergence rate for this type of problem, independently of dimension and without any smoothness assumptions. our theoretical study focuses on the approximation improvements for one- and two-step look-ahead acquisition functions, but, as we discuss, the approach is generalizable in various ways, including beyond the context of bo. findings are verified numerically and the benefits of mlmc for bo are illustrated on several benchmark examples. code is available here https://github.com/shangda-yang/mlmcbo.",,2024-02-03,,"['shangda yang', 'vitaly zankin', 'maximilian balandat', 'stefan scherer', 'kevin carlberg', 'neil walton', 'kody j. h. law']"
2402.02152,position paper: why the shooting in the dark method dominates   recommender systems practice; a call to abandon anti-utopian thinking,cs.ir cs.lg stat.ml,"applied recommender systems research is in a curious position. while there is a very rigorous protocol for measuring performance by a/b testing, best practice for finding a `b' to test does not explicitly target performance but rather targets a proxy measure. the success or failure of a given a/b test then depends entirely on if the proposed proxy is better correlated to performance than the previous proxy. no principle exists to identify if one proxy is better than another offline, leaving the practitioners shooting in the dark. the purpose of this position paper is to question this anti-utopian thinking and argue that a non-standard use of the deep learning stacks actually has the potential to unlock reward optimizing recommendation.",,2024-02-03,2024-02-08,['david rohde']
2402.02162,a bayesian cluster validity index,stat.ml cs.lg math.st stat.th,"selecting the appropriate number of clusters is a critical step in applying clustering algorithms. to assist in this process, various cluster validity indices (cvis) have been developed. these indices are designed to identify the optimal number of clusters within a dataset. however, users may not always seek the absolute optimal number of clusters but rather a secondary option that better aligns with their specific applications. this realization has led us to introduce a bayesian cluster validity index (bcvi), which builds upon existing indices. the bcvi utilizes either dirichlet or generalized dirichlet priors, resulting in the same posterior distribution. we evaluate our bcvi using the wiroonsri index for hard clustering and the wiroonsri-preedasawakul index for soft clustering as underlying indices. we compare the performance of our proposed bcvi with that of the original underlying indices and several other existing cvis, including davies-bouldin, starczewski, xie-beni, and kwon2 indices. our bcvi offers clear advantages in situations where user expertise is valuable, allowing users to specify their desired range for the final number of clusters. to illustrate this, we conduct experiments classified into three different scenarios. additionally, we showcase the practical applicability of our approach through real-world datasets, such as mri brain tumor images. these tools will be published as a new r package 'bayescvi'.",,2024-02-03,2024-02-14,"['nathakhun wiroonsri', 'onthada preedasawakul']"
2402.02171,off-policy evaluation of slate bandit policies via optimizing   abstraction,stat.ml cs.lg,"we study off-policy evaluation (ope) in the problem of slate contextual bandits where a policy selects multi-dimensional actions known as slates. this problem is widespread in recommender systems, search engines, marketing, to medical applications, however, the typical inverse propensity scoring (ips) estimator suffers from substantial variance due to large action spaces, making effective ope a significant challenge. the pseudoinverse (pi) estimator has been introduced to mitigate the variance issue by assuming linearity in the reward function, but this can result in significant bias as this assumption is hard-to-verify from observed data and is often substantially violated. to address the limitations of previous estimators, we develop a novel estimator for ope of slate bandits, called latent ips (lips), which defines importance weights in a low-dimensional slate abstraction space where we optimize slate abstractions to minimize the bias and variance of lips in a data-driven way. by doing so, lips can substantially reduce the variance of ips without imposing restrictive assumptions on the reward function structure like linearity. through empirical evaluation, we demonstrate that lips substantially outperforms existing estimators, particularly in scenarios with non-linear rewards and large slate spaces.",10.1145/3589334.3645343,2024-02-03,2024-02-17,"['haruka kiyohara', 'masahiro nomura', 'yuta saito']"
2402.02187,graphical models for multivariate extremes,stat.me math.st stat.th,"graphical models in extremes have emerged as a diverse and quickly expanding research area in extremal dependence modeling. they allow for parsimonious statistical methodology and are particularly suited for enforcing sparsity in high-dimensional problems. in this work, we provide the fundamental concepts of extremal graphical models and discuss recent advances in the field. different existing perspectives on graphical extremes are presented in a unified way through graphical models for exponent measures. we discuss the important cases of nonparametric extremal graphical models on simple graph structures, and the parametric class of h\""usler--reiss models on arbitrary undirected graphs. in both cases, we describe model properties, methods for statistical inference on known graph structures, and structure learning algorithms when the graph is unknown. we illustrate different methods in an application to flight delay data at us airports.",,2024-02-03,,"['sebastian engelke', 'manuel hentschel', 'michaël lalancette', 'frank röttger']"
2402.02190,continuous tensor relaxation for finding diverse solutions in   combinatorial optimization problems,stat.ml cs.lg stat.co stat.me,"finding the best solution is the most common objective in combinatorial optimization (co) problems. however, a single solution may not be suitable in practical scenarios, as the objective functions and constraints are only approximations of original real-world situations. to tackle this, finding (i) ""heterogeneous solutions"", diverse solutions with distinct characteristics, and (ii) ""penalty-diversified solutions"", variations in constraint severity, are natural directions. this strategy provides the flexibility to select a suitable solution during post-processing. however, discovering these diverse solutions is more challenging than identifying a single solution. to overcome this challenge, this study introduces continual tensor relaxation annealing (ctra) for unsupervised-learning-based co solvers. ctra addresses various problems simultaneously by extending the continual relaxation approach, which transforms discrete decision variables into continual tensors. this method finds heterogeneous and penalty-diversified solutions through mutual interactions, where the choice of one solution affects the other choices. numerical experiments show that ctra enables ul-based solvers to find heterogeneous and penalty-diversified solutions much faster than existing ul-based solvers. moreover, these experiments reveal that ctra enhances the exploration ability.",,2024-02-03,,"['yuma ichikawa', 'hiroaki iwashita']"
2402.02229,vanilla bayesian optimization performs great in high dimensions,cs.lg stat.ml,"high-dimensional problems have long been considered the achilles' heel of bayesian optimization algorithms. spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. in this paper, we identify the degeneracies that make vanilla bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. moreover, we propose an enhancement to the prior assumptions that are typical to vanilla bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. our modification - a simple scaling of the gaussian process lengthscale prior with the dimensionality - reveals that standard bayesian optimization works drastically better than previously thought in high dimensions, clearly outperforming existing state-of-the-art algorithms on multiple commonly considered real-world high-dimensional tasks.",,2024-02-03,2024-02-25,"['carl hvarfner', 'erik orm hellsten', 'luigi nardi']"
2402.02239,distributional reduction: unifying dimensionality reduction and   clustering with gromov-wasserstein projection,cs.lg stat.ml,"unsupervised learning aims to capture the underlying structure of potentially large and high-dimensional datasets. traditionally, this involves using dimensionality reduction methods to project data onto interpretable spaces or organizing points into meaningful clusters. in practice, these methods are used sequentially, without guaranteeing that the clustering aligns well with the conducted dimensionality reduction. in this work, we offer a fresh perspective: that of distributions. leveraging tools from optimal transport, particularly the gromov-wasserstein distance, we unify clustering and dimensionality reduction into a single framework called distributional reduction. this allows us to jointly address clustering and dimensionality reduction with a single optimization problem. through comprehensive experiments, we highlight the versatility and interpretability of our method and show that it outperforms existing approaches across a variety of image and genomics datasets.",,2024-02-03,,"['hugues van assel', 'cédric vincent-cuaz', 'nicolas courty', 'rémi flamary', 'pascal frossard', 'titouan vayer']"
2402.02265,characterization of the distortion-perception tradeoff for finite   channels with arbitrary metrics,cs.it eess.sp math.it stat.ml,"whenever inspected by humans, reconstructed signals should not be distinguished from real ones. typically, such a high perceptual quality comes at the price of high reconstruction error, and vice versa. we study this distortion-perception (dp) tradeoff over finite-alphabet channels, for the wasserstein-$1$ distance induced by a general metric as the perception index, and an arbitrary distortion matrix. under this setting, we show that computing the dp function and the optimal reconstructions is equivalent to solving a set of linear programming problems. we provide a structural characterization of the dp tradeoff, where the dp function is piecewise linear in the perception index. we further derive a closed-form expression for the case of binary sources.",,2024-02-03,,"['dror freirich', 'nir weinberger', 'ron meir']"
2402.02287,future directions in foundations of graph machine learning,cs.lg cs.ai cs.dm cs.ne stat.ml,"machine learning on graphs, especially using graph neural networks (gnns), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. despite their practical success, our theoretical understanding of the properties of gnns remains highly incomplete. recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of gnns, predominantly employing combinatorial techniques. however, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of gnns when trained with stochastic first-order optimization techniques. in this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.",,2024-02-03,,"['christopher morris', 'nadav dym', 'haggai maron', 'i̇smail i̇lkan ceylan', 'fabrizio frasca', 'ron levie', 'derek lim', 'michael bronstein', 'martin grohe', 'stefanie jegelka']"
2402.02290,goodness-of-fit and clustering of spherical data: the quadratik package   in r and python,stat.co cs.lg cs.ms stat.ap stat.ml,"we introduce the quadratik package that incorporates innovative data analysis methodologies. the presented software, implemented in both r and python, offers a comprehensive set of goodness-of-fit tests and clustering techniques using kernel-based quadratic distances, thereby bridging the gap between the statistical and machine learning literatures. our software implements one, two and k-sample tests for goodness of fit, providing an efficient and mathematically sound way to assess the fit of probability distributions. expanded capabilities of our software include supporting tests for uniformity on the $d$-dimensional sphere based on poisson kernel densities, and algorithms for generating random samples from poisson kernel densities. particularly noteworthy is the incorporation of a unique clustering algorithm specifically tailored for spherical data that leverages a mixture of poisson-kernel-based densities on the sphere. alongside this, our software includes additional graphical functions, aiding the users in validating, as well as visualizing and representing clustering results. this enhances interpretability and usability of the analysis. in summary, our r and python packages serve as a powerful suite of tools, offering researchers and practitioners the means to delve deeper into their data, draw robust inference, and conduct potentially impactful analyses and inference across a wide array of disciplines.",,2024-02-03,,"['giovanni saraceno', 'marianthi markatou', 'raktim mukhopadhyay', 'mojgan golzy']"
2402.02306,a flexible bayesian g-formula for causal survival analyses with   time-dependent confounding,stat.me stat.co stat.ml,"in longitudinal observational studies with a time-to-event outcome, a common objective in causal analysis is to estimate the causal survival curve under hypothetical intervention scenarios within the study cohort. the g-formula is a particularly useful tool for this analysis. to enhance the traditional parametric g-formula approach, we developed a more adaptable bayesian g-formula estimator. this estimator facilitates both longitudinal predictive and causal inference. it incorporates bayesian additive regression trees in the modeling of the time-evolving generative components, aiming to mitigate bias due to model misspecification. specifically, we introduce a more general class of g-formulas for discrete survival data. these formulas can incorporate the longitudinal balancing scores, which serve as an effective method for dimension reduction and are vital when dealing with an expanding array of time-varying confounders. the minimum sufficient formulation of these longitudinal balancing scores is linked to the nature of treatment regimes, whether static or dynamic. for each type of treatment regime, we provide posterior sampling algorithms, which are grounded in the bayesian additive regression trees framework. we have conducted simulation studies to illustrate the empirical performance of our proposed bayesian g-formula estimators, and to compare them with existing parametric estimators. we further demonstrate the practical utility of our methods in real-world scenarios using data from the yale new haven health system's electronic health records.",,2024-02-03,,"['xinyuan chen', 'liangyuan hu', 'fan li']"
2402.02322,dynamic incremental optimization for best subset selection,cs.lg stat.ml,"best subset selection is considered the `gold standard' for many sparse learning problems. a variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. in this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. an efficient primal-dual algorithm is developed based on the primal and dual problem structures. by leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.",,2024-02-03,2024-02-05,"['shaogang ren', 'xiaoning qian']"
2402.02345,stereographic spherical sliced wasserstein distances,cs.lg cs.ai cs.cv stat.ml,"comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. the utility of optimal transport-based distances, such as the wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. this paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized radon transform, which we refer to as the stereographic spherical sliced wasserstein (s3w) distance. we carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both speed and accuracy through a wide range of numerical studies, including gradient flows and self-supervised learning.",,2024-02-04,,"['huy tran', 'yikun bai', 'abihith kothapalli', 'ashkan shahbazi', 'xinran liu', 'rocio diaz martin', 'soheil kolouri']"
2402.02357,multi-modal causal structure learning and root cause analysis,cs.lg stat.me,"effective root cause analysis (rca) is vital for swiftly restoring services, minimizing losses, and ensuring the smooth operation and management of complex systems. previous data-driven rca methods, particularly those employing causal discovery techniques, have primarily focused on constructing dependency or causal graphs for backtracking the root causes. however, these methods often fall short as they rely solely on data from a single modality, thereby resulting in suboptimal solutions. in this work, we propose mulan, a unified multi-modal causal structure learning method for root cause localization. we leverage a log-tailored language model to facilitate log representation learning, converting log sequences into time-series data. to explore intricate relationships across different modalities, we propose a contrastive learning-based approach to extract modality-invariant and modality-specific representations within a shared latent space. additionally, we introduce a novel key performance indicator-aware attention mechanism for assessing modality reliability and co-learning a final causal graph. finally, we employ random walk with restart to simulate system fault propagation and identify potential root causes. extensive experiments on three real-world datasets validate the effectiveness of our proposed framework.",,2024-02-04,,"['lecheng zheng', 'zhengzhang chen', 'jingrui he', 'haifeng chen']"
2402.02368,timer: transformers for time series analysis at scale,cs.lg stat.ml,"deep learning has contributed remarkably to the advancement of time series analysis. still, deep models can encounter performance bottlenecks in real-world small-sample scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. continuous progresses have been achieved as the emergence of large language models, exhibiting unprecedented ability in few-shot generalization, scalability, and task generality, which is however absent in time series models. to change the current practices of training small models on specific datasets from scratch, this paper aims at an early development of large time series models (ltsm). during pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (s3) format, and develop the gpt-style architecture toward ltsms. to meet diverse application needs, we convert forecasting, imputation, and anomaly detection of time series into a unified generative task. the outcome of this study is a time series transformer (timer), that is pre-trained by autoregressive next token prediction on large multi-domain datasets, and is fine-tuned to downstream scenarios with promising abilities as an ltsm.",,2024-02-04,,"['yong liu', 'haoran zhang', 'chenyu li', 'xiangdong huang', 'jianmin wang', 'mingsheng long']"
2402.02399,fredf: learning to forecast in frequency domain,cs.lg cs.ai stat.ap stat.ml,"time series modeling is uniquely challenged by the presence of autocorrelation in both historical and label sequences. current research predominantly focuses on handling autocorrelation within the historical sequence but often neglects its presence in the label sequence. specifically, emerging forecast models mainly conform to the direct forecast (df) paradigm, generating multi-step forecasts under the assumption of conditional independence within the label sequence. this assumption disregards the inherent autocorrelation in the label sequence, thereby limiting the performance of df-based models. in response to this gap, we introduce the frequency-enhanced direct forecast (fredf), which bypasses the complexity of label autocorrelation by learning to forecast in the frequency domain. our experiments demonstrate that fredf substantially outperforms existing state-of-the-art methods including itransformer and is compatible with a variety of forecast models.",,2024-02-04,,"['hao wang', 'licheng pan', 'zhichao chen', 'degui yang', 'sen zhang', 'yifei yang', 'xinggao liu', 'haoxuan li', 'dacheng tao']"
2402.02438,fast and interpretable support vector classification based on the   truncated anova decomposition,cs.lg cs.na math.na stat.ml,"support vector machines (svms) are an important tool for performing classification on scattered data, where one usually has to deal with many data points in high-dimensional spaces. we propose solving svms in primal form using feature maps based on trigonometric functions or wavelets. in small dimensional settings the fast fourier transform (fft) and related methods are a powerful tool in order to deal with the considered basis functions. for growing dimensions the classical fft-based methods become inefficient due to the curse of dimensionality. therefore, we restrict ourselves to multivariate basis functions, each one of them depends only on a small number of dimensions. this is motivated by the well-known sparsity of effects and recent results regarding the reconstruction of functions from scattered data in terms of truncated analysis of variance (anova) decomposition, which makes the resulting model even interpretable in terms of importance of the features as well as their couplings. the usage of small superposition dimensions has the consequence that the computational effort no longer grows exponentially but only polynomially with respect to the dimension. in order to enforce sparsity regarding the basis coefficients, we use the frequently applied $\ell_2$-norm and, in addition, $\ell_1$-norm regularization. the found classifying function, which is the linear combination of basis functions, and its variance can then be analyzed in terms of the classical anova decomposition of functions. based on numerical examples we show that we are able to recover the signum of a function that perfectly fits our model assumptions. we obtain better results with $\ell_1$-norm regularization, both in terms of accuracy and clarity of interpretability.",,2024-02-04,,"['kseniya akhalaya', 'franziska nestler', 'daniel potts']"
2402.02441,topox: a suite of python packages for machine learning on topological   domains,cs.lg cs.ai cs.ms stat.co,"we introduce topox, a python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of pytorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. the extensively documented and unit-tested source code of topox is available under mit license at https://pyt-team.github.io/.",,2024-02-04,2024-02-17,"['mustafa hajij', 'mathilde papillon', 'florian frantzen', 'jens agerberg', 'ibrahem aljabea', 'ruben ballester', 'claudio battiloro', 'guillermo bernárdez', 'tolga birdal', 'aiden brent', 'peter chin', 'sergio escalera', 'simone fiorellino', 'odin hoff gardaa', 'gurusankar gopalakrishnan', 'devendra govil', 'josef hoppe', 'maneel reddy karri', 'jude khouja', 'manuel lecha', 'neal livesay', 'jan meißner', 'soham mukherjee', 'alexander nikitin', 'theodore papamarkou', 'jaro prílepok', 'karthikeyan natesan ramamurthy', 'paul rosen', 'aldo guzmán-sáenz', 'alessandro salatiello', 'shreyas n. samaga', 'simone scardapane', 'michael t. schaub', 'luca scofano', 'indro spinelli', 'lev telyatnikov', 'quang truong', 'robin walters', 'maosheng yang', 'olga zaghen', 'ghada zamzmi', 'ali zia', 'nina miolane']"
2402.02459,on minimum trace factor analysis -- an old song sung to a new tune,stat.ml cs.lg stat.me,"dimensionality reduction methods, such as principal component analysis (pca) and factor analysis, are central to many problems in data science. there are, however, serious and well-understood challenges to finding robust low dimensional approximations for data with significant heteroskedastic noise. this paper introduces a relaxed version of minimum trace factor analysis (mtfa), a convex optimization method with roots dating back to the work of ledermann in 1940. this relaxation is particularly effective at not overfitting to heteroskedastic perturbations and addresses the commonly cited heywood cases in factor analysis and the recently identified ""curse of ill-conditioning"" for existing spectral methods. we provide theoretical guarantees on the accuracy of the resulting low rank subspace and the convergence rate of the proposed algorithm to compute that matrix. we develop a number of interesting connections to existing methods, including heteropca, lasso, and soft-impute, to fill an important gap in the already large literature on low rank matrix estimation. numerical experiments benchmark our results against several recent proposals for dealing with heteroskedastic noise.",,2024-02-04,,"['c. li', 'a. shkolnik']"
2402.02463,a fast method for lasso and logistic lasso,cs.lg stat.ml,"we propose a fast method for solving compressed sensing, lasso regression, and logistic lasso regression problems that iteratively runs an appropriate solver using an active set approach. we design a strategy to update the active set that achieves a large speedup over a single call of several solvers, including gradient projection for sparse reconstruction (gpsr), lassoglm of matlab, and glmnet. for compressed sensing, the hybrid of our method and gpsr is 31.41 times faster than gpsr on average for gaussian ensembles and 25.64 faster on average for binary ensembles. for lasso regression, the hybrid of our method and gpsr achieves a 30.67-fold average speedup in our experiments. in our experiments on logistic lasso regression, the hybrid of our method and lassoglm gives an 11.95-fold average speedup, and the hybrid of our method and glmnet gives a 1.40-fold average speedup.",,2024-02-04,,"['siu-wing cheng', 'man ting wong']"
2402.02556,a new approach for imprecise probabilities,math.st math.pr stat.ml stat.th,"this paper introduces a novel concept of interval probability measures that enables the representation of imprecise probabilities, or uncertainty, in a natural and coherent manner. within an algebra of sets, we introduce a notion of weak complementation denoted as $\psi$. the interval probability measure of an event $h$ is defined with respect to the set of indecisive eventualities $(\psi(h))^c$, which is included in the standard complement $h^c$.   we characterize a broad class of interval probability measures and define their properties. additionally, we establish an updating rule with respect to $h$, incorporating concepts of statistical independence and dependence. the interval distribution of a random variable is formulated, and a corresponding definition of stochastic dominance between two random variables is introduced. as a byproduct, a formal solution to the century-old keynes-ramsey controversy is presented.",,2024-02-04,,"['marcello basili', 'luca pratelli']"
2402.02637,$c^*$-algebraic machine learning: moving in a new direction,cs.lg math.oa stat.ml,"machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra. we propose a new direction for machine learning research: $c^*$-algebraic ml $-$ a cross-fertilization between $c^*$-algebra and machine learning. the mathematical concept of $c^*$-algebra is a natural generalization of the space of complex numbers. it enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models. we explain why and how to use $c^*$-algebras in machine learning, and provide technical considerations that go into the design of $c^*$-algebraic learning models in the contexts of kernel methods and neural networks. furthermore, we discuss open questions and challenges in $c^*$-algebraic ml and give our thoughts for future development and applications.",,2024-02-04,,"['yuka hashimoto', 'masahiro ikeda', 'hachem kadri']"
2402.02644,variational dag estimation via state augmentation with stochastic   permutations,cs.lg stat.ml,"estimating the structure of a bayesian network, in the form of a directed acyclic graph (dag), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. from a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the dag constraint and (ii) estimating a posterior over the underlying combinatorial space. we propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of dags and permutations. we carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. we show that our approach can outperform competitive bayesian and non-bayesian benchmarks on a range of synthetic and real datasets.",,2024-02-04,,"['edwin v. bonilla', 'pantelis elinas', 'he zhao', 'maurizio filippone', 'vassili kitsios', ""terry o'kane""]"
2402.02663,"counterfactual fairness is not demographic parity, and other   observations",cs.lg cs.cy stat.ml,"blanket statements of equivalence between causal concepts and purely probabilistic concepts should be approached with care. in this short note, i examine a recent claim that counterfactual fairness is equivalent to demographic parity. the claim fails to hold up upon closer examination. i will take the opportunity to address some broader misunderstandings about counterfactual fairness.",,2024-02-04,,['ricardo silva']
2402.02672,estimation of conditional average treatment effects on distributed data:   a privacy-preserving approach,stat.me cs.cr cs.lg,"estimation of conditional average treatment effects (cates) is an important topic in various fields such as medical and social sciences. cates can be estimated with high accuracy if distributed data across multiple parties can be centralized. however, it is difficult to aggregate such data if they contain privacy information. to address this issue, we proposed data collaboration double machine learning (dc-dml), a method that can estimate cate models with privacy preservation of distributed data, and evaluated the method through numerical experiments. our contributions are summarized in the following three points. first, our method enables estimation and testing of semi-parametric cate models without iterative communication on distributed data. semi-parametric or non-parametric cate models enable estimation and testing that is more robust to model mis-specification than parametric models. however, to our knowledge, no communication-efficient method has been proposed for estimating and testing semi-parametric or non-parametric cate models on distributed data. second, our method enables collaborative estimation between different parties as well as multiple time points because the dimensionality-reduced intermediate representations can be accumulated. third, our method performed as well or better than other methods in evaluation experiments using synthetic, semi-synthetic and real-world datasets.",,2024-02-04,,"['yuji kawamata', 'ryoki motai', 'yukihiko okada', 'akira imakura', 'tetsuya sakurai']"
2402.02684,efficient estimation of subgroup treatment effects using multi-source   data,stat.me,"investigators often use multi-source data (e.g., multi-center trials, meta-analyses of randomized trials, pooled analyses of observational cohorts) to learn about the effects of interventions in subgroups of some well-defined target population. such a target population can correspond to one of the data sources of the multi-source data or an external population in which the treatment and outcome information may not be available. we develop and evaluate methods for using multi-source data to estimate subgroup potential outcome means and treatment effects in a target population. we consider identifiability conditions and propose doubly robust estimators that, under mild conditions, are non-parametrically efficient and allow for nuisance functions to be estimated using flexible data-adaptive methods (e.g., machine learning techniques). we also show how to construct confidence intervals and simultaneous confidence bands for the estimated subgroup treatment effects. we examine the properties of the proposed estimators in simulation studies and compare performance against alternative estimators. we also conclude that our methods work well when the sample size of the target population is much larger than the sample size of the multi-source data. we illustrate the proposed methods in a meta-analysis of randomized trials for schizophrenia.",,2024-02-04,,"['guanbo wang', 'alexander levis', 'jon steingrimsson', 'issa dahabreh']"
2402.02687,poisson process for bayesian optimization,cs.lg cs.ai stat.ml,"bayesianoptimization(bo) is a sample-efficient black-box optimizer, and extensive methods have been proposed to build the absolute function response of the black-box function through a probabilistic surrogate model, including tree-structured parzen estimator (tpe), random forest (smac), and gaussian process (gp). however, few methods have been explored to estimate the relative rankings of candidates, which can be more robust to noise and have better practicality than absolute function responses, especially when the function responses are intractable but preferences can be acquired. to this end, we propose a novel ranking-based surrogate model based on the poisson process and introduce an efficient bo framework, namely poisson process bayesian optimization (popbo). two tailored acquisition functions are further derived from classic lcb and ei to accommodate it. compared to the classic gp-bo method, our popbo has lower computation costs and better robustness to noise, which is verified by abundant experiments. the results on both simulated and real-world benchmarks, including hyperparameter optimization (hpo) and neural architecture search (nas), show the effectiveness of popbo.",,2024-02-04,,"['xiaoxing wang', 'jiaxing li', 'chao xue', 'wei liu', 'weifeng liu', 'xiaokang yang', 'junchi yan', 'dacheng tao']"
2402.02692,statistical guarantees for link prediction using graph neural networks,cs.lg cs.si math.st stat.ml stat.th,"this paper derives statistical guarantees for the performance of graph neural networks (gnns) in link prediction tasks on graphs generated by a graphon. we propose a linear gnn architecture (lg-gnn) that produces consistent estimators for the underlying edge probabilities. we establish a bound on the mean squared error and give guarantees on the ability of lg-gnn to detect high-probability edges. our guarantees hold for both sparse and dense graphs. finally, we demonstrate some of the shortcomings of the classical gcn architecture, as well as verify our results on real and synthetic datasets.",,2024-02-04,2024-02-07,"['alan chung', 'amin saberi', 'morgane austern']"
2402.02697,deep equilibrium models are almost equivalent to not-so-deep explicit   models for high-dimensional gaussian mixtures,cs.lg stat.ml,"deep equilibrium models (deqs), as a typical implicit neural network, have demonstrated remarkable success on various tasks. there is, however, a lack of theoretical understanding of the connections and differences between implicit deqs and explicit neural network models. in this paper, leveraging recent advances in random matrix theory (rmt), we perform an in-depth analysis on the eigenspectra of the conjugate kernel (ck) and neural tangent kernel (ntk) matrices for implicit deqs, when the input data are drawn from a high-dimensional gaussian mixture. we prove, in this setting, that the spectral behavior of these implicit-cks and ntks depend on the deq activation function and initial weight variances, but only via a system of four nonlinear equations. as a direct consequence of this theoretical result, we demonstrate that a shallow explicit network can be carefully designed to produce the same ck or ntk as a given deq. despite derived here for gaussian mixture data, empirical results show the proposed theory and design principle also apply to popular real-world datasets.",,2024-02-04,,"['zenan ling', 'longbo li', 'zhanbo feng', 'yixuan zhang', 'feng zhou', 'robert c. qiu', 'zhenyu liao']"
2402.02700,sample complexity characterization for linear contextual mdps,cs.lg stat.ml,"contextual markov decision processes (cmdps) describe a class of reinforcement learning problems in which the transition kernels and reward functions can change over time with different mdps indexed by a context variable. while cmdps serve as an important framework to model many real-world applications with time-varying environments, they are largely unexplored from theoretical perspective. in this paper, we study cmdps under two linear function approximation models: model i with context-varying representations and common linear weights for all contexts; and model ii with common representations for all contexts and context-varying linear weights. for both models, we propose novel model-based algorithms and show that they enjoy guaranteed $\epsilon$-suboptimality gap with desired polynomial sample complexity. in particular, instantiating our result for the first model to the tabular cmdp improves the existing result by removing the reachability assumption. our result for the second model is the first-known result for such a type of function approximation models. comparison between our results for the two models further indicates that having context-varying features leads to much better sample efficiency than having common representations for all contexts under linear cmdps.",,2024-02-04,,"['junze deng', 'yuan cheng', 'shaofeng zou', 'yingbin liang']"
2402.02701,understanding what affects generalization gap in visual reinforcement   learning: theory and empirical evidence,cs.lg cs.ai stat.ml,"recently, there are many efforts attempting to learn useful policies for continuous control in visual reinforcement learning (rl). in this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment. many practical algorithms are proposed to handle this problem. however, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work. in this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap. our theoretical results are supported by the empirical evidence in the dmcontrol generalization benchmark (dmc-gb).",,2024-02-04,,"['jiafei lyu', 'le wan', 'xiu li', 'zongqing lu']"
2402.02702,causal inference under transportability assumptions for conditional   relative effect measures,stat.me math.st stat.th,"when extending inferences from a randomized trial to a new target population, an assumption of transportability of difference effect measures (e.g., conditional average treatment effects) -- or even stronger assumptions of transportability in expectation or distribution of potential outcomes -- is invoked to identify the marginal causal mean difference in the target population. however, many clinical investigators believe that relative effect measures conditional on covariates, such as conditional risk ratios and mean ratios, are more likely to be ``transportable'' across populations compared with difference effect measures. here, we examine the identification and estimation of the marginal counterfactual mean difference and ratio under a transportability assumption for conditional relative effect measures. we obtain identification results for two scenarios that often arise in practice when individuals in the target population (1) only have access to the control treatment, or (2) have access to the control and other treatments but not necessarily the experimental treatment evaluated in the trial. we then propose multiply robust and nonparametric efficient estimators that allow for the use of data-adaptive methods (e.g., machine learning techniques) to model the nuisance parameters. we examine the performance of the methods in simulation studies and illustrate their use with data from two trials of paliperidone for patients with schizophrenia. we conclude that the proposed methods are attractive when background knowledge suggests that the transportability assumption for conditional relative effect measures is more plausible than alternative assumptions.",,2024-02-04,,"['guanbo wang', 'alexander levis', 'jon steingrimsson', 'issa dahabreh']"
2402.02720,discounted adaptive online prediction,cs.lg stat.ml,"online learning is not always about memorizing everything. since the future can be statistically very different from the past, a critical challenge is to gracefully forget the history while new data comes in. to formalize this intuition, we revisit the classical notion of discounted regret using recently developed techniques in adaptive online learning. our main result is a new algorithm that adapts to the complexity of both the loss sequence and the comparator, improving the widespread non-adaptive algorithm - gradient descent with a constant learning rate. in particular, our theoretical guarantee does not require any structural assumption beyond convexity, and the algorithm is provably robust to suboptimal hyperparameter tuning. we further demonstrate such benefits through online conformal prediction, a downstream online learning task with set-membership decisions.",,2024-02-04,,"['zhiyu zhang', 'david bombara', 'heng yang']"
2402.02734,inva: integrative variational autoencoder for harmonization of   multi-modal neuroimaging data,eess.iv cs.cv cs.ne stat.ap stat.ml,"there is a significant interest in exploring non-linear associations among multiple images derived from diverse imaging modalities. while there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing approaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image. building on the literature of variational auto encoders (vaes), this article proposes a novel approach, referred to as integrative variational autoencoder (\texttt{inva}) method, which borrows information from multiple images obtained from different sources to draw predictive inference of an image. the proposed approach captures complex non-linear association between the outcome image and input images, while allowing rapid computation. numerical results demonstrate substantial advantages of \texttt{inva} over vaes, which typically do not allow borrowing information between input images. the proposed framework offers highly accurate predictive inferences for costly positron emission topography (pet) from multiple measures of cortical structure in human brain scans readily available from magnetic resonance imaging (mri).",,2024-02-05,,"['bowen lei', 'rajarshi guhaniyogi', 'krishnendu chandra', 'aaron scheffler', 'bani mallick']"
2402.02741,glocal hypergradient estimation with koopman operator,cs.lg stat.ml,"gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters. previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates. while global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal. in this paper, we propose glocal hypergradient estimation, blending ""global"" quality with ""local"" efficiency. to this end, we use the koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients. consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneously. through numerical experiments of hyperparameter optimization, including optimization of optimizers, we demonstrate the effectiveness of the glocal hypergradient estimation.",,2024-02-05,,"['ryuichiro hataya', 'yoshinobu kawahara']"
2402.02817,"bayes-optimal fair classification with linear disparity constraints via   pre-, in-, and post-processing",stat.ml cs.cy cs.lg,"machine learning algorithms may have disparate impacts on protected groups. to address this, we develop methods for bayes-optimal fair classification, aiming to minimize classification error subject to given group fairness constraints. we introduce the notion of \emph{linear disparity measures}, which are linear functions of a probabilistic classifier; and \emph{bilinear disparity measures}, which are also linear in the group-wise regression functions. we show that several popular disparity measures -- the deviations from demographic parity, equality of opportunity, and predictive equality -- are bilinear.   we find the form of bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the neyman-pearson lemma. for bilinear disparity measures, bayes-optimal fair classifiers become group-wise thresholding rules. our approach can also handle multiple fairness constraints (such as equalized odds), and the common scenario when the protected attribute cannot be used at the prediction phase.   leveraging our theoretical results, we design methods that learn fair bayes-optimal classifiers under bilinear disparity constraints. our methods cover three popular approaches to fairness-aware classification, via pre-processing (fair up- and down-sampling), in-processing (fair cost-sensitive classification) and post-processing (a fair plug-in rule). our methods control disparity directly while achieving near-optimal fairness-accuracy tradeoffs. we show empirically that our methods compare favorably to existing algorithms.",,2024-02-05,2024-02-06,"['xianli zeng', 'guang cheng', 'edgar dobriban']"
2402.02851,enhancing compositional generalization via compositional feature   alignment,cs.cv cs.lg stat.ml,"real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. in the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. this challenge naturally leads the quest for models with compositional generalization (cg) ability, where models can generalize to unseen domain-class combinations. to delve into the cg challenge, we develop cg-bench, a suite of cg benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as clip and dinov2, struggles with the challenge. to address this challenge, we propose compositional feature alignment (cfa), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen. we theoretically and empirically justify that cfa encourages compositional feature learning of pretrained models. we further conduct extensive experiments on cg-bench for clip and dinov2, two powerful pretrained vision foundation models. experiment results show that cfa outperforms common finetuning techniques in compositional generalization, corroborating cfa's efficacy in compositional feature learning.",,2024-02-05,,"['haoxiang wang', 'haozhe si', 'huajie shao', 'han zhao']"
2402.02857,non-asymptotic analysis of biased adaptive stochastic approximation,stat.ml cs.lg,"stochastic gradient descent (sgd) with adaptive steps is now widely used for training deep neural networks. most theoretical results assume access to unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use monte carlo methods. this paper provides a comprehensive non-asymptotic analysis of sgd with biased gradients and adaptive steps for convex and non-convex smooth functions. our study incorporates time-dependent bias and emphasizes the importance of controlling the bias and mean squared error (mse) of the gradient estimator. in particular, we establish that adagrad and rmsprop with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. finally, we provide experimental results using variational autoenconders (vae) that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning.",,2024-02-05,,"['sobihan surendran', 'antoine godichon-baggioni', 'adeline fermanian', 'sylvain le corff']"
2402.02858,deep autoregressive density nets vs neural ensembles for model-based   offline reinforcement learning,cs.lg stat.ml,"we consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization. following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. this approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. the standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. we challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the d4rl benchmark. we also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.",,2024-02-05,,"['abdelhakim benechehab', 'albert thomas', 'balázs kégl']"
2402.02859,importance sampling for online variational learning,stat.ap stat.me stat.ml,"this article addresses online variational estimation in state-space models. we focus on learning the smoothing distribution, i.e. the joint distribution of the latent states given the observations, using a variational approach together with monte carlo importance sampling. we propose an efficient algorithm for computing the gradient of the evidence lower bound (elbo) in the context of streaming data, where observations arrive sequentially. our contributions include a computationally efficient online elbo estimator, demonstrated performance in offline and true online settings, and adaptability for computing general expectations under joint smoothing distributions.",,2024-02-05,,"['mathis chagneux', 'pierre gloaguen', 'sylvain le corff', 'jimmy olsson']"
2402.02861,leveraging noisy observations in zero-sum games,cs.gt cs.it math.it stat.ml,"this paper studies an instance of zero-sum games in which one player (the leader) commits to its opponent (the follower) to choose its actions by sampling a given probability measure (strategy). the actions of the leader are observed by the follower as the output of an arbitrary channel. in response to that, the follower chooses its action based on its current information, that is, the leader's commitment and the corresponding noisy observation of its action. within this context, the equilibrium of the game with noisy action observability is shown to always exist and the necessary conditions for its uniqueness are identified. interestingly, the noisy observations have important impact on the cardinality of the follower's set of best responses. under particular conditions, such a set of best responses is proved to be a singleton almost surely. the proposed model captures any channel noise with a density with respect to the lebesgue measure. as an example, the case in which the channel is described by a gaussian probability measure is investigated.",,2024-02-05,,"['emmanouil m athanasakos', 'samir m perlaza']"
2402.02862,graph neural machine: a new model for learning with tabular data,stat.ml cs.lg,"in recent years, there has been a growing interest in mapping data from different domains to graph structures. among others, neural network models such as the multi-layer perceptron (mlp) can be modeled as graphs. in fact, mlps can be represented as directed acyclic graphs. graph neural networks (gnns) have recently become the standard tool for performing machine learning tasks on graphs. in this work, we show that an mlp is equivalent to an asynchronous message passing gnn model which operates on the mlp's graph representation. we then propose a new machine learning model for tabular data, the so-called graph neural machine (gnm), which replaces the mlp's directed acyclic graph with a nearly complete graph and which employs a synchronous message passing scheme. we show that a single gnm model can simulate multiple mlp models. we evaluate the proposed model in several classification and regression datasets. in most cases, the gnm model outperforms the mlp architecture.",,2024-02-05,,"['giannis nikolentzos', 'siyun wang', 'johannes lutzeyer', 'michalis vazirgiannis']"
2402.02898,bayesian federated inference for regression models with heterogeneous   multi-center populations,stat.ap stat.co stat.me stat.ml,"to estimate accurately the parameters of a regression model, the sample size must be large enough relative to the number of possible predictors for the model. in practice, sufficient data is often lacking, which can lead to overfitting of the model and, as a consequence, unreliable predictions of the outcome of new patients. pooling data from different data sets collected in different (medical) centers would alleviate this problem, but is often not feasible due to privacy regulation or logistic problems. an alternative route would be to analyze the local data in the centers separately and combine the statistical inference results with the bayesian federated inference (bfi) methodology. the aim of this approach is to compute from the inference results in separate centers what would have been found if the statistical analysis was performed on the combined data. we explain the methodology under homogeneity and heterogeneity across the populations in the separate centers, and give real life examples for better understanding. excellent performance of the proposed methodology is shown. an r-package to do all the calculations has been developed and is illustrated in this paper. the mathematical details are given in the appendix.",,2024-02-05,,"['marianne a jonker', 'hassan pazira', 'anthony cc coolen']"
2402.02909,digital twin for grey box modeling of multistory residential building   thermal dynamics,stat.ap cs.lg,"buildings energy efficiency is a widely researched topic, which is rapidly gaining popularity due to rising environmental concerns and the need for energy independence. in northern europe heating energy alone accounts for up to 70 percent of the total building energy consumption. industry 4.0 technologies such as iot, big data, cloud computing and machine learning, along with the creation of predictive and proactive digital twins, can help to reduce this number. however, buildings thermal dynamics is a very complex process that depends on many variables. as a result, commonly used physics-based white box models are time-consuming and require vast expertise. on the contrary, black box forecasting models, which rely primarily on building energy consumption data, lack fundamental insights and hinder re-use. in this study we propose an architecture to facilitate grey box modelling of building thermal dynamics while integrating real time iot data with 3d representation of buildings. the architecture is validated in a case study creating a digital twin platform that enables users to define the thermal dynamics of buildings based on physical laws and real data, thus facilitating informed decision making for the best heating energy optimization strategy. also, the created user interface enables stakeholders such as facility managers, energy providers or governing bodies to analyse, compare and evaluate buildings thermal dynamics without extensive expertise or time resources.",,2024-02-05,,"['lina morkunaite', 'justas kardoka', 'darius pupeikis', 'paris fokaides', 'vangelis angelakis']"
2402.02949,kernel pca for out-of-distribution detection,cs.lg stat.ml,"out-of-distribution (ood) detection is vital for the reliability of deep neural networks (dnns). existing works have shown the insufficiency of principal component analysis (pca) straightforwardly applied on the features of dnns in detecting ood data from in-distribution (ind) data. the failure of pca suggests that the network features residing in ood and ind are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings. in this work, we leverage the framework of kernel pca (kpca) for ood detection, seeking subspaces where ood and ind features are allocated with significantly different patterns. we devise two feature mappings that induce non-linear kernels in kpca to advocate the separability between ind and ood data in the subspace spanned by the principal components. given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\mathcal{o}(1)$ time complexity in inference. extensive empirical results on multiple ood data sets and network structures verify the superiority of our kpca-based detector in efficiency and efficacy with state-of-the-art ood detection performances.",,2024-02-05,,"['kun fang', 'qinghua tao', 'kexin lv', 'mingzhen he', 'xiaolin huang', 'jie yang']"
2402.02951,dynamic byzantine-robust learning: adapting to switching byzantine   workers,cs.lg cs.dc stat.ml,"byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. however, most techniques consider the static setting, wherein the identity of byzantine machines remains fixed during the learning process. this assumption does not capture real-world dynamic byzantine behaviors, which may include transient malfunctions or targeted temporal attacks. addressing this limitation, we propose $\textsf{dynabro}$ -- a new method capable of withstanding $\mathcal{o}(\sqrt{t})$ rounds of byzantine identity alterations (where $t$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting. our method combines a multi-level monte carlo (mlmc) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic byzantine strategies. additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentage of byzantine workers.",,2024-02-05,,"['ron dorfman', 'naseem yehya', 'kfir y. levy']"
2402.02952,on least squares estimation in softmax gating mixture of experts,stat.ml cs.lg,"mixture of experts (moe) model is a statistical machine learning design that aggregates multiple expert networks using a softmax gating function in order to form a more intricate and expressive model. despite being commonly used in several applications owing to their scalability, the mathematical and statistical properties of moe models are complex and difficult to analyze. as a result, previous theoretical works have primarily focused on probabilistic moe models by imposing the impractical assumption that the data are generated from a gaussian moe model. in this work, we investigate the performance of the least squares estimators (lse) under a deterministic moe model where the data are sampled according to a regression model, a setting that has remained largely unexplored. we establish a condition called strong identifiability to characterize the convergence behavior of various types of expert functions. we demonstrate that the rates for estimating strongly identifiable experts, namely the widely used feed forward networks with activation functions $\mathrm{sigmoid}(\cdot)$ and $\tanh(\cdot)$, are substantially faster than those of polynomial experts, which we show to exhibit a surprising slow estimation rate. our findings have important practical implications for expert selection.",,2024-02-05,,"['huy nguyen', 'nhat ho', 'alessandro rinaldo']"
2402.02969,towards understanding the word sensitivity of attention layers: a study   via random features,stat.ml cs.cl cs.lg,"unveiling the reasons behind the exceptional success of transformers requires a better understanding of why attention layers are suitable for nlp tasks. in particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long. our work studies this key property, dubbed word sensitivity (ws), in the prototypical setting of random features. we show that attention layers enjoy high ws, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. the argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., relu). in contrast, the ws of standard random features is of order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context. we then translate these results on the word sensitivity into generalization bounds: due to their low ws, random features provably cannot learn to distinguish between two sentences that differ only in a single word; in contrast, due to their high ws, random attention features have higher generalization capabilities. we validate our theoretical results with experimental evidence over the bert-base word embeddings of the imdb review dataset.",,2024-02-05,,"['simone bombari', 'marco mondelli']"
2402.02976,"boosting, voting classifiers and randomized sample compression schemes",cs.lg stat.ml,"in boosting, we aim to leverage multiple weak learners to produce a strong learner. at the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners. while many successful boosting algorithms, such as the iconic adaboost, produce voting classifiers, their theoretical performance has long remained sub-optimal: the best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners. in this work, we break this barrier by proposing a randomized boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the sample size. we obtain this result by building a general framework that extends sample compression methods to support randomized learning algorithms based on sub-sampling.",,2024-02-05,,"['arthur da cunha', 'kasper green larsen', 'martin ritzert']"
2402.02998,careful with that scalpel: improving gradient surgery with an ema,cs.lg stat.ml,"beyond minimizing a single training loss, many deep learning estimation pipelines rely on an auxiliary objective to quantify and encourage desirable properties of the model (e.g. performance on another dataset, robustness, agreement with a prior). although the simplest approach to incorporating an auxiliary loss is to sum it with the training loss as a regularizer, recent works have shown that one can improve performance by blending the gradients beyond a simple sum; this is known as gradient surgery. we cast the problem as a constrained minimization problem where the auxiliary objective is minimized among the set of minimizers of the training loss. to solve this bilevel problem, we follow a parameter update direction that combines the training loss gradient and the orthogonal projection of the auxiliary gradient to the training gradient. in a setting where gradients come from mini-batches, we explain how, using a moving average of the training loss gradients, we can carefully maintain this critical orthogonality property. we demonstrate that our method, bloop, can lead to much better performances on nlp and vision experiments than other gradient surgery methods without ema.",,2024-02-05,,"['yu-guan hsieh', 'james thornton', 'eugene ndiaye', 'michal klein', 'marco cuturi', 'pierre ablin']"
2402.03006,on the development of a practical bayesian optimisation algorithm for   expensive experiments and simulations with changing environmental conditions,cs.lg stat.ml,"experiments in engineering are typically conducted in controlled environments where parameters can be set to any desired value. this assumes that the same applies in a real-world setting -- an assumption that is often incorrect as many experiments are influenced by uncontrollable environmental conditions such as temperature, humidity and wind speed. when optimising such experiments, the focus should lie on finding optimal values conditionally on these uncontrollable variables. this article extends bayesian optimisation to the optimisation of systems in changing environments that include controllable and uncontrollable parameters. the extension fits a global surrogate model over all controllable and environmental variables but optimises only the controllable parameters conditional on measurements of the uncontrollable variables. the method is validated on two synthetic test functions and the effects of the noise level, the number of the environmental parameters, the parameter fluctuation, the variability of the uncontrollable parameters, and the effective domain size are investigated. envbo, the proposed algorithm resulting from this investigation, is applied to a wind farm simulator with eight controllable and one environmental parameter. envbo finds solutions for the full domain of the environmental variable that outperforms results from optimisation algorithms that only focus on a fixed environmental value in all but one case while using a fraction of their evaluation budget. this makes the proposed approach very sample-efficient and cost-effective. an off-the-shelf open-source version of envbo is available via the nubo python package.",,2024-02-05,,"['mike diessner', 'kevin j. wilson', 'richard d. whalley']"
2402.03104,high-dimensional bayesian optimization via covariance matrix adaptation   strategy,stat.ml cs.lg,"bayesian optimization (bo) is an effective method for finding the global optimum of expensive black-box functions. however, it is well known that applying bo to high-dimensional optimization problems is challenging. to address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use bo to optimize the objective function within these regions. in this paper, we propose a novel technique for defining the local regions using the covariance matrix adaptation (cma) strategy. specifically, we use cma to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function. based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum. our approach serves as a meta-algorithm as it can incorporate existing black-box bo optimizers, such as bo, turbo, and baxus, to find the global optimum of the objective function within our derived local regions. we evaluate our proposed method on various benchmark synthetic and real-world problems. the results demonstrate that our method outperforms existing state-of-the-art techniques.",,2024-02-05,,"['lam ngo', 'huong ha', 'jeffrey chan', 'vu nguyen', 'hongyu zhang']"
2402.03113,optimal sampling for stochastic and natural gradient descent,math.oc math.st stat.th,"we consider the problem of optimising the expected value of a loss functional over a nonlinear model class of functions, assuming that we have only access to realisations of the gradient of the loss. this is a classical task in statistics, machine learning and physics-informed machine learning. a straightforward solution is to replace the exact objective with a monte carlo estimate before employing standard first-order methods like gradient descent, which yields the classical stochastic gradient descent method. but replacing the true objective with an estimate ensues a ``generalisation error''. rigorous bounds for this error typically require strong compactness and lipschitz continuity assumptions while providing a very slow decay with sample size. we propose a different optimisation strategy relying on a natural gradient descent in which the true gradient is approximated in local linearisations of the model class via (quasi-)projections based on optimal sampling methods. under classical assumptions on the loss and the nonlinear model class, we prove that this scheme converges almost surely monotonically to a stationary point of the true objective and we provide convergence rates.",,2024-02-05,,"['robert gruhlke', 'anthony nouy', 'philipp trunschke']"
2402.03146,a multi-step loss function for robust learning of the dynamics in   model-based reinforcement learning,cs.lg stat.ml,"in model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data. a critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows. in this paper we tackle this issue by using a multi-step objective to train one-step models. our objective is a weighted sum of the mean squared error (mse) loss at various future horizons. we find that this new loss is particularly useful when the data is noisy (additive gaussian noise in the observations), which is often the case in real-life environments. to support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system. second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged r2-score on future prediction horizons. finally, in the pure batch reinforcement learning setting, we demonstrate that one-step models serve as strong baselines when dynamics are deterministic, while multi-step models would be more advantageous in the presence of noise, highlighting the potential of our approach in real-world applications.",,2024-02-05,,"['abdelhakim benechehab', 'albert thomas', 'giuseppe paolo', 'maurizio filippone', 'balázs kégl']"
2402.03167,decentralized bilevel optimization over graphs: loopless algorithmic   update and transient iteration complexity,math.oc cs.lg stat.ml,"stochastic bilevel optimization (sbo) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. to address large-scale sbo, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. however, current decentralized sbo algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. in this paper, we introduce a single-loop decentralized sbo (d-soba) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. d-soba achieves the state-of-the-art asymptotic rate, asymptotic gradient/hessian complexity, and transient iteration complexity under more relaxed assumptions compared to existing methods. numerical experiments validate our theoretical findings.",,2024-02-05,2024-02-26,"['boao kong', 'shuchen zhu', 'songtao lu', 'xinmeng huang', 'kun yuan']"
2402.03169,a random matrix approach to low-multilinear-rank tensor approximation,stat.ml cs.lg math.pr,"this work presents a comprehensive understanding of the estimation of a planted low-rank signal from a general spiked tensor model near the computational threshold. relying on standard tools from the theory of large random matrices, we characterize the large-dimensional spectral behavior of the unfoldings of the data tensor and exhibit relevant signal-to-noise ratios governing the detectability of the principal directions of the signal. these results allow to accurately predict the reconstruction performance of truncated multilinear svd (mlsvd) in the non-trivial regime. this is particularly important since it serves as an initialization of the higher-order orthogonal iteration (hooi) scheme, whose convergence to the best low-multilinear-rank approximation depends entirely on its initialization. we give a sufficient condition for the convergence of hooi and show that the number of iterations before convergence tends to $1$ in the large-dimensional limit.",,2024-02-05,,"['hugo lebeau', 'florent chatelain', 'romain couillet']"
2402.03254,minimum description length and generalization guarantees for   representation learning,stat.ml cs.it cs.lg math.it,"a major challenge in designing efficient statistical supervised learning algorithms is finding representations that perform well not only on available training samples but also on unseen data. while the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees.   in this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the ""minimum description length"" (mdl) of the labels or the latent variables (representations). rather than the mutual information between the encoder's input and the representation, which is often believed to reflect the algorithm's generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the ""multi-letter"" relative entropy between the distribution of the representations (or labels) of the training and test sets and a fixed prior. in particular, these new bounds reflect the structure of the encoder and are not vacuous for deterministic algorithms. our compressibility approach, which is information-theoretic in nature, builds upon that of blum-langford for pac-mdl bounds and introduces two essential ingredients: block-coding and lossy-compression. the latter allows our approach to subsume the so-called geometrical compressibility as a special case. to the best knowledge of the authors, the established generalization bounds are the first of their kind for information bottleneck (ib) type encoders and representation learning. finally, we partly exploit the theoretical results by introducing a new data-dependent prior. numerical simulations illustrate the advantages of well-chosen such priors over classical priors used in ib.",,2024-02-05,,"['milad sefidgaran', 'abdellatif zaidi', 'piotr krasnowski']"
2402.03256,learning best-in-class policies for the predict-then-optimize framework,cs.lg math.oc stat.ml,"we propose a novel family of decision-aware surrogate losses, called perturbation gradient (pg) losses, for the predict-then-optimize framework. these losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods. importantly, unlike existing surrogate losses, the approximation error of our pg losses vanishes as the number of samples grows. this implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings. this is the first such result in misspecified settings and we provide numerical evidence confirming our pg losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric. insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- pg losses offer a novel, theoretically justified, method for computationally tractable decision-aware learning.",,2024-02-05,2024-02-08,"['michael huang', 'vishal gupta']"
2402.03282,a framework for partially observed reward-states in rlhf,cs.lg cs.ai stat.ml,"the study of reinforcement learning from human feedback (rlhf) has gained prominence in recent years due to its role in the development of llms. neuroscience research shows that human responses to stimuli are known to depend on partially-observed ""internal states."" unfortunately current models of rlhf do not take take this into consideration. moreover most rlhf models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. to address these limitations, we model rlhf as reinforcement learning with partially observed reward-states (porrl). we show reductions from the the two dominant forms of human feedback in rlhf - cardinal and dueling feedback to porrl. for cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present por-ucrl and por-ucbvi. for dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regret. we then present the first explicit reduction that converts guarantees for cardinal regret to dueling regret. we show that our models and guarantees in both settings generalize and extend existing ones. finally, we identify a recursive structure on our model that could improve the statistical and computational tractability of porrl, giving examples from past work on rlhf as well as learning perfect reward machines, which porrl subsumes.",,2024-02-05,,"['chinmaya kausik', 'mirco mutti', 'aldo pacchiano', 'ambuj tewari']"
2402.03293,flora: low-rank adapters are secretly gradient compressors,cs.lg cs.ai stat.ml,"despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. to alleviate this, the low-rank adaptation (lora) is proposed to reduce the optimization states by training fewer parameters. however, lora restricts overall weight update matrices to be low-rank, limiting the model performance. in this work, we investigate the dynamics of lora and identify that it can be approximated by a random projection. based on this observation, we propose flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. we conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.",,2024-02-05,,"['yongchang hao', 'yanshuai cao', 'lili mou']"
2402.03295,ginger: an efficient curvature approximation with linear complexity for   general neural networks,cs.lg cs.ai math.oc stat.ml,"second-order optimization approaches like the generalized gauss-newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices. albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning. the major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix. these requirements are infeasible even with state-of-the-art hardware. in this work, we propose ginger, an eigendecomposition for the inverse of the generalized gauss-newton matrix. our method enjoys efficient linear memory and time complexity for each iteration. instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate. we provide the convergence result of ginger for non-convex objectives. our experiments on different tasks with different model architectures verify the effectiveness of our method. our code is publicly available.",,2024-02-05,,"['yongchang hao', 'yanshuai cao', 'lili mou']"
2402.03345,weakly supervised covariance matrices alignment through stiefel matrices   estimation for meg applications,eess.sp cs.lg stat.ml,"this paper introduces a novel domain adaptation technique for time series data, called mixing model stiefel adaptation (msa), specifically addressing the challenge of limited labeled signals in the target dataset. leveraging a domain-dependent mixing model and the optimal transport domain adaptation assumption, we exploit abundant unlabeled data in the target domain to ensure effective prediction by establishing pairwise correspondence with equivalent signal variances between domains. theoretical foundations are laid for identifying crucial stiefel matrices, essential for recovering underlying signal variances from a riemannian representation of observed signal covariances. we propose an integrated cost function that simultaneously learns these matrices, pairwise domain relationships, and a predictor, classifier, or regressor, depending on the task. applied to neuroscience problems, msa outperforms recent methods in brain-age regression with task variations using magnetoencephalography (meg) signals from the cam-can dataset.",,2024-01-24,,"['antoine collas', 'rémi flamary', 'alexandre gramfort']"
2402.03352,zeroth-order primal-dual alternating projection gradient algorithms for   nonconvex minimax problems with coupled linear constraints,math.oc cs.lg stat.ml,"in this paper, we study zeroth-order algorithms for nonconvex minimax problems with coupled linear constraints under the deterministic and stochastic settings, which have attracted wide attention in machine learning, signal processing and many other fields in recent years, e.g., adversarial attacks in resource allocation problems and network flow problems etc. we propose two single-loop algorithms, namely the zero-order primal-dual alternating projected gradient (zo-pdapg) algorithm and the zero-order regularized momentum primal-dual projected gradient algorithm (zo-rmpdpg), for solving deterministic and stochastic nonconvex-(strongly) concave minimax problems with coupled linear constraints. the iteration complexity of the two proposed algorithms to obtain an $\varepsilon$-stationary point are proved to be $\mathcal{o}(\varepsilon ^{-2})$ (resp. $\mathcal{o}(\varepsilon ^{-4})$) for solving nonconvex-strongly concave (resp. nonconvex-concave) minimax problems with coupled linear constraints under deterministic settings and $\tilde{\mathcal{o}}(\varepsilon ^{-3})$ (resp. $\tilde{\mathcal{o}}(\varepsilon ^{-6.5})$) under stochastic settings respectively. to the best of our knowledge, they are the first two zeroth-order algorithms with iterative complexity guarantees for solving nonconvex-(strongly) concave minimax problems with coupled linear constraints under the deterministic and stochastic settings.",,2024-01-26,,"['huiling zhang', 'zi xu', 'yuhong dai']"
2402.03447,challenges in variable importance ranking under correlation,stat.ml cs.lg stat.me,"variable importance plays a pivotal role in interpretable machine learning as it helps measure the impact of factors on the output of the prediction model. model agnostic methods based on the generation of ""null"" features via permutation (or related approaches) can be applied. such analysis is often utilized in pharmaceutical applications due to its ability to interpret black-box models, including tree-based ensembles. a major challenge and significant confounder in variable importance estimation however is the presence of between-feature correlation. recently, several adjustments to marginal permutation utilizing feature knockoffs were proposed to address this issue, such as the variable importance measure known as conditional predictive impact (cpi). assessment and evaluation of such approaches is the focus of our work. we first present a comprehensive simulation study investigating the impact of feature correlation on the assessment of variable importance. we then theoretically prove the limitation that highly correlated features pose for the cpi through the knockoff construction. while we expect that there is always no correlation between knockoff variables and its corresponding predictor variables, we prove that the correlation increases linearly beyond a certain correlation threshold between the predictor variables. our findings emphasize the absence of free lunch when dealing with high feature correlation, as well as the necessity of understanding the utility and limitations behind methods in variable importance estimation.",,2024-02-05,,"['annie liang', 'thomas jemielita', 'andy liaw', 'vladimir svetnik', 'lingkang huang', 'richard baumgartner', 'jason m. klusowski']"
2402.03460,breaking the curse of dimensionality with distributed neural computation,stat.ml cs.lg cs.na cs.ne math.co math.na,"we present a theoretical approach to overcome the curse of dimensionality using a neural computation algorithm which can be distributed across several machines. our modular distributed deep learning paradigm, termed \textit{neural pathways}, can achieve arbitrary accuracy while only loading a small number of parameters into gpu vram. formally, we prove that for every error level $\varepsilon>0$ and every lipschitz function $f:[0,1]^n\to \mathbb{r}$, one can construct a neural pathways model which uniformly approximates $f$ to $\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of $\mathcal{o}(\varepsilon^{-1})$ parameters to be loaded in memory and $\mathcal{o}(\varepsilon^{-1}\log(\varepsilon^{-1}))$ to be loaded during the forward pass. this improves the optimal bounds for traditional non-distributed deep learning models, namely relu mlps, which need $\mathcal{o}(\varepsilon^{-n/2})$ parameters to achieve the same accuracy. the only other available deep learning model that breaks the curse of dimensionality is mlps with super-expressive activation functions. however, we demonstrate that these models have an infinite vc dimension, even with bounded depth and width restrictions, unlike the neural pathways model. this implies that only the latter generalizes. our analysis is validated experimentally in both regression and classification tasks, demonstrating that our model exhibits superior performance compared to larger centralized benchmarks.",,2024-02-05,,"['haitz sáez de ocáriz borde', 'takashi furuya', 'anastasis kratsios', 'marc t. law']"
2402.03467,stochastic modified flows for riemannian stochastic gradient descent,cs.lg math.oc math.pr stat.ml,"we give quantitative estimates for the rate of convergence of riemannian stochastic gradient descent (rsgd) to riemannian gradient flow and to a diffusion process, the so-called riemannian stochastic modified flow (rsmf). using tools from stochastic differential geometry we show that, in the small learning rate regime, rsgd can be approximated by the solution to the rsmf driven by an infinite-dimensional wiener process. the rsmf accounts for the random fluctuations of rsgd and, thereby, increases the order of approximation compared to the deterministic riemannian gradient flow. the rsgd is build using the concept of a retraction map, that is, a cost efficient approximation of the exponential map, and we prove quantitative bounds for the weak error of the diffusion approximation under assumptions on the retraction map, the geometry of the manifold, and the random estimators of the gradient.",,2024-02-02,,"['benjamin gess', 'sebastian kassing', 'nimit rana']"
2402.03474,active region-based flare forecasting with sliding window multivariate   time series forest classifiers,astro-ph.sr cs.lg stat.ap,"over the past few decades, many applications of physics-based simulations and data-driven techniques (including machine learning and deep learning) have emerged to analyze and predict solar flares. these approaches are pivotal in understanding the dynamics of solar flares, primarily aiming to forecast these events and minimize potential risks they may pose to earth. although current methods have made significant progress, there are still limitations to these data-driven approaches. one prominent drawback is the lack of consideration for the temporal evolution characteristics in the active regions from which these flares originate. this oversight hinders the ability of these methods to grasp the relationships between high-dimensional active region features, thereby limiting their usability in operations. this study centers on the development of interpretable classifiers for multivariate time series and the demonstration of a novel feature ranking method with sliding window-based sub-interval ranking. the primary contribution of our work is to bridge the gap between complex, less understandable black-box models used for high-dimensional data and the exploration of relevant sub-intervals from multivariate time series, specifically in the context of solar flare forecasting. our findings demonstrate that our sliding-window time series forest classifier performs effectively in solar flare prediction (with a true skill statistic of over 85\%) while also pinpointing the most crucial features and sub-intervals for a given learning task.",,2024-02-05,,"['anli ji', 'berkay aydin']"
2402.03485,attention meets post-hoc interpretability: a mathematical perspective,stat.ml cs.cl cs.lg,"attention-based architectures, in particular transformers, are at the heart of a technological revolution. interestingly, in addition to helping obtain state-of-the-art results on a wide range of applications, the attention mechanism intrinsically provides meaningful insights on the internal behavior of the model. can these insights be used as explanations? debate rages on. in this paper, we mathematically study a simple attention-based architecture and pinpoint the differences between post-hoc and attention-based explanations. we show that they provide quite different results, and that, despite their limitations, post-hoc methods are capable of capturing more useful insights than merely examining the attention weights.",,2024-02-05,,"['gianluigi lopardo', 'frederic precioso', 'damien garreau']"
2402.03502,how does unlabeled data provably help out-of-distribution detection?,cs.lg stat.ml,"using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (ood) data. harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (id) and ood data. this lack of a clean set of ood samples poses significant challenges in learning an optimal ood classifier. currently, there is a lack of research on formally understanding how unlabeled data helps ood detection. this paper bridges the gap by introducing a new learning framework sal (separate and learn) that offers both strong theoretical guarantees and empirical effectiveness. the framework separates candidate outliers from the unlabeled data and then trains an ood classifier using the candidate outliers and the labeled id data. theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. our theory shows that sal can separate the candidate outliers with small error rates, which leads to a generalization guarantee for the learned ood classifier. empirically, sal achieves state-of-the-art performance on common benchmarks, reinforcing our theoretical insights. code is publicly available at https://github.com/deeplearning-wisc/sal.",,2024-02-05,,"['xuefeng du', 'zhen fang', 'ilias diakonikolas', 'yixuan li']"
2402.03527,consistent validation for predictive methods in spatial settings,stat.ml cs.lg stat.me,"spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors. determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. this mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. in the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. we show that classical and covariate-shift methods can fail this check. we instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data at hand. we prove that our proposal passes our check. and we demonstrate its advantages empirically on simulated and real data.",,2024-02-05,,"['david r. burt', 'yunyi shen', 'tamara broderick']"
2402.03540,regulation games for trustworthy machine learning,cs.lg cs.gt stat.ml,"existing work on trustworthy machine learning (ml) often concentrates on individual aspects of trust, such as fairness or privacy. additionally, many techniques overlook the distinction between those who train ml models and those responsible for assessing their trustworthiness. to address these issues, we propose a framework that views trustworthy ml as a multi-objective multi-agent optimization problem. this naturally lends itself to a game-theoretic formulation we call regulation games. we illustrate a particular game instance, the specgame in which we model the relationship between an ml model builder and fairness and privacy regulators. regulators wish to design penalties that enforce compliance with their specification, but do not want to discourage builders from participation. seeking such socially optimal (i.e., efficient for all agents) solutions to the game, we introduce paretoplay. this novel equilibrium search algorithm ensures that agents remain on the pareto frontier of their objectives and avoids the inefficiencies of other equilibria. simulating specgame through paretoplay can provide policy guidance for ml regulation. for instance, we show that for a gender classification application, regulators can enforce a differential privacy budget that is on average 4.0 lower if they take the initiative to specify their desired guarantee first.",,2024-02-05,,"['mohammad yaghini', 'patty liu', 'franziska boenisch', 'nicolas papernot']"
2402.03587,effective acquisition functions for active correlation clustering,cs.lg stat.ml,"correlation clustering is a powerful unsupervised learning paradigm that supports positive and negative similarities. in this paper, we assume the similarities are not known in advance. instead, we employ active learning to iteratively query similarities in a cost-efficient way. in particular, we develop three effective acquisition functions to be used in this setting. one is based on the notion of inconsistency (i.e., when similarities violate the transitive property). the remaining two are based on information-theoretic quantities, i.e., entropy and information gain.",,2024-02-05,,"['linus aronsson', 'morteza haghir chehreghani']"
2402.03614,bayesian factorised granger-causal graphs for multivariate time-series   data,cs.lg stat.ml,"we study the problem of automatically discovering granger causal relations from observational multivariate time-series data. vector autoregressive (var) models have been time-tested for this problem, including bayesian variants and more recent developments using deep neural networks. most existing var methods for granger causality use sparsity-inducing penalties/priors or post-hoc thresholds to interpret their coefficients as granger causal graphs. instead, we propose a new bayesian var model with a hierarchical graph prior over binary granger causal graphs, separately from the var coefficients. we develop an efficient algorithm to infer the posterior over binary granger causal graphs. our method provides better uncertainty quantification, has less hyperparameters, and achieves better performance than competing approaches, especially on sparse multivariate time-series data.",,2024-02-05,,"['he zhao', 'edwin v. bonilla']"
2402.03655,operator svd with neural networks via nested low-rank approximation,cs.lg cs.na math.na stat.ml,"computing eigenvalue decomposition (evd) of a given linear operator, or finding its leading eigenvalues and eigenfunctions, is a fundamental task in many machine learning and scientific computing problems. for high-dimensional eigenvalue problems, training neural networks to parameterize the eigenfunctions is considered as a promising alternative to the classical numerical linear algebra techniques. this paper proposes a new optimization framework based on the low-rank approximation characterization of a truncated singular value decomposition, accompanied by new techniques called nesting for learning the top-$l$ singular values and singular functions in the correct order. the proposed method promotes the desired orthogonality in the learned functions implicitly and efficiently via an unconstrained optimization formulation, which is easy to solve with off-the-shelf gradient-based optimization algorithms. we demonstrate the effectiveness of the proposed optimization framework for use cases in computational physics and machine learning.",,2024-02-05,,"['j. jon ryu', 'xiangxiang xu', 'h. s. melihcan erol', 'yuheng bu', 'lizhong zheng', 'gregory w. wornell']"
2402.03664,efficient solvers for partial gromov-wasserstein,cs.lg stat.ml,"the partial gromov-wasserstein (pgw) problem facilitates the comparison of measures with unequal masses residing in potentially distinct metric spaces, thereby enabling unbalanced and partial matching across these spaces. in this paper, we demonstrate that the pgw problem can be transformed into a variant of the gromov-wasserstein problem, akin to the conversion of the partial optimal transport problem into an optimal transport problem. this transformation leads to two new solvers, mathematically and computationally equivalent, based on the frank-wolfe algorithm, that provide efficient solutions to the pgw problem. we further establish that the pgw problem constitutes a metric for metric measure spaces. finally, we validate the effectiveness of our proposed solvers in terms of computation time and performance on shape-matching and positive-unlabeled learning problems, comparing them against existing baselines.",,2024-02-05,,"['yikun bai', 'rocio diaz martin', 'hengrong du', 'ashkan shahbazi', 'soheil kolouri']"
2402.03687,pard: permutation-invariant autoregressive diffusion for graph   generation,cs.lg stat.ml,"graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. we introduce pard, a permutation-invariant auto regressive diffusion model that integrates diffusion models with autoregressive methods. pard harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. with this partial order, pard generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. to ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with ppgn. like gpt, we extend the higher-order graph transformer to support parallel training of all blocks. without any extra features, pard achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like moses containing 1.9m molecules.",,2024-02-05,,"['lingxiao zhao', 'xueying ding', 'leman akoglu']"
2402.03698,estimating the local learning coefficient at scale,cs.lg stat.ml,"the \textit{local learning coefficient} (llc) is a principled way of quantifying model complexity, originally derived in the context of bayesian statistics using singular learning theory (slt). several methods are known for numerically estimating the local learning coefficient, but so far these methods have not been extended to the scale of modern deep learning architectures or data sets. using a method developed in {\tt arxiv:2308.12108 [stat.ml]} we empirically show how the llc may be measured accurately and self-consistently for deep linear networks (dlns) up to 100m parameters. we also show that the estimated llc has the rescaling invariance that holds for the theoretical quantity.",,2024-02-05,,"['zach furman', 'edmund lau']"
2402.03701,improving and unifying discrete&continuous-time discrete denoising   diffusion,cs.lg stat.ml,"discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. although discrete-time discrete diffusion has been established for a while, only recently campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. however, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. in this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. in addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distribution, including different noise distributions for multi-element objects. experiments show that our proposed usd3 (for unified simplified discrete denoising diffusion) outperform all sota baselines on established datasets. we open-source our unified code at https://github.com/lingxiaoshawn/usd3.",,2024-02-05,,"['lingxiao zhao', 'xueying ding', 'lijun yu', 'leman akoglu']"
2402.03724,statistical test for anomaly detections by variational auto-encoders,stat.ml cs.lg,"in this study, we consider the reliability assessment of anomaly detection (ad) using variational autoencoder (vae). over the last decade, vae-based ad has been actively studied in various perspective, from method development to applied research. however, when the results of ads are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to ensure the reliability of the detected anomalies. in this study, we propose the vae-ad test as a method for quantifying the statistical reliability of vae-based ad within the framework of statistical testing. using the vae-ad test, the reliability of the anomaly regions detected by a vae can be quantified in the form of p-values. this means that if an anomaly is declared when the p-value is below a certain threshold, it is possible to control the probability of false detection to a desired level. since the vae-ad test is constructed based on a new statistical inference framework called selective inference, its validity is theoretically guaranteed in finite samples. to demonstrate the validity and effectiveness of the proposed vae-ad test, numerical experiments on artificial data and applications to brain image analysis are conducted.",,2024-02-06,,"['daiki miwa', 'tomohiro shiraishi', 'vo nguyen le duy', 'teruyuki katsuoka', 'ichiro takeuchi']"
2402.03726,learning granger causality from instance-wise self-attentive hawkes   processes,cs.lg stat.ml,"we address the problem of learning granger causality from asynchronous, interdependent, multi-type event sequences. in particular, we are interested in discovering instance-level causal structures in an unsupervised manner. instance-level causality identifies causal relationships among individual events, providing more fine-grained information for decision-making. existing work in the literature either requires strong assumptions, such as linearity in the intensity function, or heuristically defined model parameters that do not necessarily meet the requirements of granger causality. we propose instance-wise self-attentive hawkes processes (isahp), a novel deep learning framework that can directly infer the granger causality at the event instance level. isahp is the first neural point process model that meets the requirements of granger causality. it leverages the self-attention mechanism of the transformer to align with the principles of granger causality. we empirically demonstrate that isahp is capable of discovering complex instance-level causal structures that cannot be handled by classical models. we also show that isahp achieves state-of-the-art performance in proxy tasks involving type-level causal discovery and instance-level event type prediction.",,2024-02-06,2024-02-29,"['dongxia wu', 'tsuyoshi idé', 'aurélie lozano', 'georgios kollias', 'jiří navrátil', 'naoki abe', 'yi-an ma', 'rose yu']"
2402.03737,differentially private high dimensional bandits,cs.lg cs.cr cs.sy eess.sy math.oc stat.ml,"we consider a high-dimensional stochastic contextual linear bandit problem when the parameter vector is $s_{0}$-sparse and the decision maker is subject to privacy constraints under both central and local models of differential privacy. we present privatelasso, a differentially private lasso bandit algorithm. privatelasso is based on two sub-routines: (i) a sparse hard-thresholding-based privacy mechanism and (ii) an episodic thresholding rule for identifying the support of the parameter $\theta$. we prove minimax private lower bounds and establish privacy and utility guarantees for privatelasso for the central model under standard assumptions.",,2024-02-06,,['apurv shukla']
2402.03779,eero: early exit with reject option for efficient classification with   limited budget,stat.ml cs.lg,"the increasing complexity of advanced machine learning models requires innovative approaches to manage computational resources effectively. one such method is the early exit strategy, which allows for adaptive computation by providing a mechanism to shorten the processing path for simpler data instances. in this paper, we propose eero, a new methodology to translate the problem of early exiting to a problem of using multiple classifiers with reject option in order to better select the exiting head for each instance. we calibrate the probabilities of exiting at the different heads using aggregation with exponential weights to guarantee a fixed budget .we consider factors such as bayesian risk, budget constraints, and head-specific budget consumption. experimental results, conducted using a resnet-18 model and a convnext architecture on cifar and imagenet datasets, demonstrate that our method not only effectively manages budget allocation but also enhances accuracy in overthinking scenarios.",,2024-02-06,,"['florian valade', 'mohamed hebiri', 'paul gay']"
2402.03809,combining additivity and active subspaces for high-dimensional gaussian   process modeling,math.oc stat.ml,"gaussian processes are a widely embraced technique for regression and classification due to their good prediction accuracy, analytical tractability and built-in capabilities for uncertainty quantification. however, they suffer from the curse of dimensionality whenever the number of variables increases. this challenge is generally addressed by assuming additional structure in theproblem, the preferred options being either additivity or low intrinsic dimensionality. our contribution for high-dimensional gaussian process modeling is to combine them with a multi-fidelity strategy, showcasing the advantages through experiments on synthetic functions and datasets.",,2024-02-06,,"['mickael binois', 'victor picheny']"
2402.03819,theoretical and experimental study of smote: limitations and comparisons   of rebalancing strategies,stat.ml cs.lg,"synthetic minority oversampling technique (smote) is a common rebalancing strategy for handling imbalanced data sets. asymptotically, we prove that smote (with default parameter) regenerates the original distribution by simply copying the original minority samples. we also prove that smote density vanishes near the boundary of the support of the minority distribution, therefore justifying the common borderline smote strategy. then we introduce two new smote-related strategies, and compare them with state-of-the-art rebalancing procedures. we show that rebalancing strategies are only required when the data set is highly imbalanced. for such data sets, smote, our proposals, or undersampling procedures are the best strategies.",,2024-02-06,,"['abdoulaye sakho', 'erwan scornet', 'emmanuel malherbe']"
2402.03839,random features models: a way to study the success of naive imputation,math.st stat.ml stat.th,"constant (naive) imputation is still widely used in practice as this is a first easy-to-use technique to deal with missing data. yet, this simple method could be expected to induce a large bias for prediction purposes, as the imputed input may strongly differ from the true underlying data. however, recent works suggest that this bias is low in the context of high-dimensional linear predictors when data is supposed to be missing completely at random (mcar). this paper completes the picture for linear predictors by confirming the intuition that the bias is negligible and that surprisingly naive imputation also remains relevant in very low dimension.to this aim, we consider a unique underlying random features model, which offers a rigorous framework for studying predictive performances, whilst the dimension of the observed features varies.building on these theoretical results, we establish finite-sample bounds on stochastic gradient (sgd) predictors applied to zero-imputed data, a strategy particularly well suited for large-scale learning.if the mcar assumption appears to be strong, we show that similar favorable behaviors occur for more complex missing data scenarios.",,2024-02-06,,"['alexis ayme', 'claire boyer', 'aymeric dieuleveut', 'erwan scornet']"
2402.03870,less than one percent of words would be affected by gender-inclusive   language in german press texts,cs.cl stat.ap,"research on gender and language is tightly knitted to social debates on gender equality and non-discriminatory language use. psycholinguistic scholars have made significant contributions in this field. however, corpus-based studies that investigate these matters within the context of language use are still rare. in our study, we address the question of how much textual material would actually have to be changed if non-gender-inclusive texts were rewritten to be gender-inclusive. this quantitative measure is an important empirical insight, as a recurring argument against the use of gender-inclusive german is that it supposedly makes written texts too long and complicated. it is also argued that gender-inclusive language has negative effects on language learners. however, such effects are only likely if gender-inclusive texts are very different from those that are not gender-inclusive. in our corpus-linguistic study, we manually annotated german press texts to identify the parts that would have to be changed. our results show that, on average, less than 1% of all tokens would be affected by gender-inclusive language. this small proportion calls into question whether gender-inclusive german presents a substantial barrier to understanding and learning the language, particularly when we take into account the potential complexities of interpreting masculine generics.",,2024-02-06,,"['carolin müller-spitzer', 'samira ochs', 'alexander koplenig', 'jan-oliver rüdiger', 'sascha wolfer']"
2402.03883,a framework for bilevel optimization on riemannian manifolds,math.oc cs.lg stat.ml,"bilevel optimization has seen an increasing presence in various domains of applications. in this work, we propose a framework for solving bilevel optimization problems where variables of both lower and upper level problems are constrained on riemannian manifolds. we provide several hypergradient estimation strategies on manifolds and study their estimation error. we provide convergence and complexity analysis for the proposed hypergradient descent algorithm on manifolds. we also extend the developments to stochastic bilevel optimization and to the use of general retraction. we showcase the utility of the proposed framework on various applications.",,2024-02-06,,"['andi han', 'bamdev mishra', 'pratik jawanpuria', 'akiko takeda']"
2402.03901,batch universal prediction,cs.it cs.lg math.it stat.ml,"large language models (llms) have recently gained much popularity due to their surprising ability at generating human-like english sentences. llms are essentially predictors, estimating the probability of a sequence of words given the past. therefore, it is natural to evaluate their performance from a universal prediction perspective. in order to do that fairly, we introduce the notion of batch regret as a modification of the classical average regret, and we study its asymptotical value for add-constant predictors, in the case of memoryless sources and first-order markov sources.",,2024-02-06,,"['marco bondaschi', 'michael gastpar']"
2402.03915,learning metrics that maximise power for accelerated a/b-tests,cs.lg cs.ir stat.ap stat.ml,"online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. a north star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an a/b-test can be considered superior. north star metrics are typically delayed and insensitive. as a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-ii errors (i.e. false negatives) are prevalent.   we propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the north star. we show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-ii errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. we collect such datasets from two social media applications with over 160 million monthly active users each, totalling over 153 a/b-pairs. empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the north star. alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the north star requires, significantly reducing the cost of experimentation.",,2024-02-06,,"['olivier jeunen', 'aleksei ustimenko']"
2402.03941,discovery of the hidden world with large language models,cs.lg cs.ai stat.me,"science originates with discovering new causal knowledge from a combination of known facts and observations. traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. however, the causal variables are usually unavailable in a wide range of real-world applications. the rise of large language models (llms) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. therefore, we introduce coat: causal representation assistant. coat incorporates llms as a factor proposer that extracts the potential causal factors from unstructured data. moreover, llms can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. the annotated data will be fed to a causal learning module (e.g., the fci algorithm) that provides both rigorous explanations of the data, as well as useful feedback to further improve the extraction of causal factors by llms. we verify the effectiveness of coat in uncovering the underlying causal system with two case studies of review rating analysis and neuropathic diagnosis.",,2024-02-06,,"['chenxi liu', 'yongqiang chen', 'tongliang liu', 'mingming gong', 'james cheng', 'bo han', 'kun zhang']"
2402.03954,mixed matrix completion in complex survey sampling under heterogeneous   missingness,stat.me stat.ml,"modern surveys with large sample sizes and growing mixed-type questionnaires require robust and scalable analysis methods. in this work, we consider recovering a mixed dataframe matrix, obtained by complex survey sampling, with entries following different canonical exponential distributions and subject to heterogeneous missingness. to tackle this challenging task, we propose a two-stage procedure: in the first stage, we model the entry-wise missing mechanism by logistic regression, and in the second stage, we complete the target parameter matrix by maximizing a weighted log-likelihood with a low-rank constraint. we propose a fast and scalable estimation algorithm that achieves sublinear convergence, and the upper bound for the estimation error of the proposed method is rigorously derived. experimental results support our theoretical claims, and the proposed estimator shows its merits compared to other existing methods. the proposed method is applied to analyze the national health and nutrition examination survey data.",,2024-02-06,,"['xiaojun mao', 'hengfang wang', 'zhonglei wang', 'shu yang']"
2402.03982,on convergence of adam for stochastic optimization under relaxed   assumptions,math.oc cs.lg stat.ml,"the adaptive momentum estimation (adam) algorithm is highly effective in training various deep learning tasks. despite this, there's limited theoretical understanding for adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. in this paper, we study vanilla adam under these challenging conditions. we introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-gaussian noise. we show that adam can find a stationary point with a $\mathcal{o}(\text{poly}(\log t)/\sqrt{t})$ rate in high probability under this general noise model where $t$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. more importantly, we reveal that adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the stochastic gradient descent under the same conditions. we also provide a probabilistic convergence result for adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to more accurately capture the smooth property of many practical objective functions.",,2024-02-06,,"['yusu hong', 'junhong lin']"
2402.03985,a bias-variance decomposition for ensembles over multiple synthetic   datasets,cs.lg stat.ml,"recent studies have highlighted the benefits of generating multiple synthetic datasets for supervised learning, from increased accuracy to more effective model selection and uncertainty estimation. these benefits have clear empirical support, but the theoretical understanding of them is currently very light. we seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets. our theory predicts multiple synthetic datasets to be especially beneficial for high-variance downstream predictors, and yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and brier score. we investigate how our theory works in practice by evaluating the performance of an ensemble over many synthetic datasets for several real datasets and downstream predictors. the results follow our theory, showing that our insights are also practically relevant.",,2024-02-06,,"['ossi räisä', 'antti honkela']"
2402.03990,subsampling is not magic: why large batch sizes work for differentially   private stochastic optimisation,stat.ml cs.cr cs.lg,"we study the effect of the batch size to the total gradient variance in differentially private stochastic gradient descent (dp-sgd), seeking a theoretical explanation for the usefulness of large batch sizes. as dp-sgd is the basis of modern dp deep learning, its properties have been widely studied, and recent works have empirically found large batch sizes to be beneficial. however, theoretical explanations of this benefit are currently heuristic at best. we first observe that the total gradient variance in dp-sgd can be decomposed into subsampling-induced and noise-induced variances. we then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size. the remaining subsampling-induced variance decreases with larger batch sizes, so large batches reduce the effective total gradient variance. we confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find that outside the asymptotic regime, the total gradient variance decreases even more with large batch sizes. we also find a sufficient condition that implies that large batch sizes similarly reduce effective dp noise variance for one iteration of dp-sgd.",,2024-02-06,,"['ossi räisä', 'joonas jälkö', 'antti honkela']"
2402.03991,neural rank collapse: weight decay and small within-class variability   yield low-rank bias,cs.lg cs.na math.na stat.ml,"recent work in deep learning has shown strong empirical and theoretical evidence of an implicit low-rank bias: weight matrices in deep networks tend to be approximately low-rank and removing relatively small singular values during training or from available trained models may significantly reduce model size while maintaining or even improving model performance. however, the majority of the theoretical investigations around low-rank bias in neural networks deal with oversimplified deep linear networks. in this work, we consider general networks with nonlinear activations and the weight decay parameter, and we show the presence of an intriguing neural rank collapse phenomenon, connecting the low-rank bias of trained networks with networks' neural collapse properties: as the weight decay parameter grows, the rank of each layer in the network decreases proportionally to the within-class variability of the hidden-space embeddings of the previous layers. our theoretical findings are supported by a range of experimental evaluations illustrating the phenomenon.",,2024-02-06,,"['emanuele zangrando', 'piero deidda', 'simone brugiapaglia', 'nicola guglielmi', 'francesco tudisco']"
2402.03994,gradient sketches for training data attribution and studying the loss   landscape,cs.lg stat.ml,"random projections or sketches of gradients and hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry. two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the hessian (to analyze the training dynamics), where one needs to store multiple hessian vector products. while sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks. motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms. we demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-trained language models.",,2024-02-06,,['andrea schioppa']
2402.04010,efficient availability attacks against supervised and contrastive   learning simultaneously,cs.lg stat.ml,"availability attacks can prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and making unlearnable examples before release. ideally, the obtained unlearnability prevents algorithms from training usable models. when supervised learning (sl) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (cl) algorithms to bypass the protection. through evaluation, we have found that most of the existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection. different from recent methods based on contrastive error minimization, we employ contrastive-like data augmentations in supervised error minimization or maximization frameworks to obtain attacks effective for both sl and cl. our proposed aue and aap attacks achieve state-of-the-art worst-case unlearnability across sl and cl algorithms with less computation consumption, showcasing prospects in real-world applications.",,2024-02-06,,"['yihan wang', 'yifan zhu', 'xiao-shan gao']"
2402.04012,quantized approximately orthogonal recurrent neural networks,cs.ne cs.lg eess.sp math.st stat.th,"orthogonal recurrent neural networks (ornns) are an appealing option for learning tasks involving time series with long-term dependencies, thanks to their simplicity and computational stability. however, these networks often require a substantial number of parameters to perform well, which can be prohibitive in power-constrained environments, such as compact devices. one approach to address this issue is neural network quantization. the construction of such networks remains an open problem, acknowledged for its inherent instability.in this paper, we explore the quantization of the recurrent and input weight matrices in ornns, leading to quantized approximately orthogonal rnns (qornns). we investigate one post-training quantization (ptq) strategy and three quantization-aware training (qat) algorithms that incorporate orthogonal constraints and quantized weights. empirical results demonstrate the advantages of employing qat over ptq. the most efficient model achieves results similar to state-of-the-art full-precision ornn and lstm on a variety of standard benchmarks, even with 3-bits quantization.",,2024-02-05,,"['armand foucault', 'franck mamalet', 'françois malgouyres']"
2402.04022,a general theory for kernel packets: from state space model to compactly   supported basis,stat.ml cs.lg,"it is well known that the state space (ss) model formulation of a gaussian process (gp) can lower its training and prediction time both to o(n) for n data points. we prove that an $m$-dimensional ss model formulation of gp is equivalent to a concept we introduce as the general right kernel packet (kp): a transformation for the gp covariance function $k$ such that $\sum_{i=0}^{m}a_id_t^{(j)}k(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j \leq m-1$, and $m+1$ consecutive points $t_i$, where ${d}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. we extend this idea to the backward ss model formulation of the gp, leading to the concept of the left kp for next $m$ consecutive points: $\sum_{i=0}^{m}b_i{d}_t^{(j)}k(t,t_{m+i})=0$ for any $t\geq t_{2m}$. by combining both left and right kps, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported kp functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and $j=0,\cdots,m-1$. kps further reduce the prediction time of gp to o(log n) or even o(1), can be applied to more general problems involving the derivative of gps, and have multi-dimensional generalization for scattered data.",,2024-02-06,2024-02-08,"['liang ding', 'rui tuo']"
2402.04038,pac-bayesian adversarially robust generalization bounds for graph neural   network,stat.ml cs.lg,"graph neural networks (gnns) have gained popularity for various graph-related tasks. however, similar to deep neural networks, gnns are also vulnerable to adversarial attacks. empirical studies have shown that adversarially robust generalization has a pivotal role in establishing effective defense algorithms against adversarial attacks. in this paper, we contribute by providing adversarially robust generalization bounds for two kinds of popular gnns, graph convolutional network (gcn) and message passing graph neural network, using the pac-bayesian framework. our result reveals that spectral norm of the diffusion matrix on the graph and spectral norm of the weights as well as the perturbation factor govern the robust generalization bounds of both models. our bounds are nontrivial generalizations of the results developed in (liao et al., 2020) from the standard setting to adversarial setting while avoiding exponential dependence of the maximum node degree. as corollaries, we derive better pac-bayesian robust generalization bounds for gcn in the standard setting, which improve the bounds in (liao et al., 2020) by avoiding exponential dependence on the maximum node degree.",,2024-02-06,,"['tan sun', 'junhong lin']"
2402.04054,more flexible pac-bayesian meta-learning by learning learning algorithms,cs.lg stat.ml,"we introduce a new framework for studying meta-learning methods using pac-bayesian theory. its main advantage over previous work is that it allows for more flexibility in how the transfer of knowledge between tasks is realized. for previous approaches, this could only happen indirectly, by means of learning prior distributions over models. in contrast, the new generalization bounds that we prove express the process of meta-learning much more directly as learning the learning algorithm that should be used for future tasks. the flexibility of our framework makes it suitable to analyze a wide range of meta-learning mechanisms and even design new mechanisms. other than our theoretical contributions we also show empirically that our framework improves the prediction quality in practical meta-learning mechanisms.",,2024-02-06,,"['hossein zakerinia', 'amin behjati', 'christoph h. lampert']"
2402.04082,an optimal house price prediction algorithm: xgboost,cs.lg cs.ai stat.ap stat.me,"an accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending. it is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood. meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers. to this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables. we made use of the housing dataset of ames city in iowa, usa to compare support vector regressor, random forest regressor, xgboost, multilayer perceptron and multiple linear regression algorithms for house price prediction. afterwards, we identified the key factors that influence housing costs. our results show that xgboost is the best performing model for house price prediction.",10.3390/analytics3010003,2024-02-06,,"['hemlata sharma', 'hitesh harsora', 'bayode ogunleye']"
2402.04084,provably learning a multi-head attention layer,cs.lg cs.ds stat.ml,"the multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models. given a sequence length $k$, attention matrices $\mathbf{\theta}_1,\ldots,\mathbf{\theta}_m\in\mathbb{r}^{d\times d}$, and projection matrices $\mathbf{w}_1,\ldots,\mathbf{w}_m\in\mathbb{r}^{d\times d}$, the corresponding multi-head attention layer $f: \mathbb{r}^{k\times d}\to \mathbb{r}^{k\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\mathbf{x}\in\mathbb{r}^{k\times d}$ via $f(\mathbf{x}) \triangleq \sum^m_{i=1} \mathrm{softmax}(\mathbf{x}\mathbf{\theta}_i\mathbf{x}^\top)\mathbf{x}\mathbf{w}_i$. in this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - provided $\{\mathbf{w}_i, \mathbf{\theta}_i\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{o(m^3)}$-time algorithm that learns $f$ to small error given random labeled examples drawn uniformly from $\{\pm 1\}^{k\times d}$.   - we prove computational lower bounds showing that in the worst case, exponential dependence on $m$ is unavoidable.   we focus on boolean $\mathbf{x}$ to mimic the discrete nature of tokens in large language models, though our techniques naturally extend to standard continuous settings, e.g. gaussian. our algorithm, which is centered around using examples to sculpt a convex body containing the unknown parameters, is a significant departure from existing provable algorithms for learning feedforward networks, which predominantly exploit algebraic and rotation invariance properties of the gaussian distribution. in contrast, our analysis is more flexible as it primarily relies on various upper and lower tail bounds for the input distribution and ""slices"" thereof.",,2024-02-06,,"['sitan chen', 'yuanzhi li']"
2402.04088,the use of a large language model for cyberbullying detection,cs.cl cs.ai cs.lg stat.ap,"the dominance of social media has added to the channels of bullying for perpetrators. unfortunately, cyberbullying (cb) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. this opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. several machine learning (ml) algorithms have been proposed for this purpose. however, their performances are not consistent due to high class imbalance and generalisation issues. in recent years, large language models (llms) like bert and roberta have achieved state-of-the-art (sota) results in several natural language processing (nlp) tasks. unfortunately, the llms have not been applied extensively for cb detection. in our paper, we explored the use of these models for cyberbullying (cb) detection. we have prepared a new dataset (d2) from existing studies (formspring and twitter). our experimental results for dataset d1 and d2 showed that roberta outperformed other models.",10.3390/analytics2030038,2024-02-06,,"['bayode ogunleye', 'babitha dharmaraj']"
2402.04103,an exploration of clustering algorithms for customer segmentation in the   uk retail market,cs.lg cs.ai stat.ap stat.co,"recently, peoples awareness of online purchases has significantly risen. this has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour. retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation. customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability. in this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry. to achieve this, we employed a uk-based online retail dataset obtained from the uci machine learning repository. the retail dataset consists of 541,909 customer records and eight features. our study adopted the rfm (recency, frequency, and monetary) framework to quantify customer values. thereafter, we compared several state-of-the-art (sota) clustering algorithms, namely, k-means clustering, the gaussian mixture model (gmm), density-based spatial clustering of applications with noise (dbscan), agglomerative clustering, and balanced iterative reducing and clustering using hierarchies (birch). the results showed the gmm outperformed other approaches, with a silhouette score of 0.80.",10.3390/analytics2040042,2024-02-06,,"['jeen mary john', 'olamilekan shobayo', 'bayode ogunleye']"
2402.04114,scafflsa: quantifying and eliminating heterogeneity bias in federated   linear stochastic approximation and temporal difference learning,stat.ml cs.lg math.oc,"in this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (fedlsa) algorithm. we explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm. we show that the communication complexity of fedlsa scales polynomially with the desired precision $\epsilon$, which limits the benefits of federation. to overcome this, we propose scafflsa, a novel variant of fedlsa, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity. we apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements.",,2024-02-06,,"['paul mangold', 'sergey samsonov', 'safwan labbi', 'ilya levin', 'reda alami', 'alexey naumov', 'eric moulines']"
2402.04146,interpretable multi-source data fusion through latent variable gaussian   process,stat.ml cs.lg,"with the advent of artificial intelligence (ai) and machine learning (ml), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). the proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. a high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. however, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. towards resolving this issue, a multi-source data fusion framework based on latent variable gaussian process (lvgp) is proposed. the individual data sources are tagged as a characteristic categorical variable that are mapped into a physically interpretable latent space, allowing the development of source-aware data fusion modeling. additionally, a dissimilarity metric based on the latent variables of lvgp is introduced to study and understand the differences in the sources of data. the proposed approach is demonstrated on and analyzed through two mathematical (representative parabola problem, 2d ackley function) and two materials science (design of fecral and smcofe alloys) case studies. from the case studies, it is observed that compared to using single-source and source unaware ml models, the proposed multi-source data fusion framework can provide better predictions for sparse-data problems, interpretability regarding the sources, and enhanced modeling capabilities by taking advantage of the correlations and relationships among different sources.",,2024-02-06,2024-02-16,"['sandipp krishnan ravi', 'yigitcan comlek', 'wei chen', 'arjun pathak', 'vipul gupta', 'rajnikant umretiya', 'andrew hoffman', 'ghanshyam pilania', 'piyush pandita', 'sayan ghosh', 'nathaniel mckeever', 'liping wang']"
2402.04161,attention with markov: a framework for principled analysis of   transformers via markov chains,cs.lg cs.cl cs.it math.it stat.ml,"in recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. a key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. to shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of markov chains. inspired by the markovianity of natural languages, we model the data as a markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. in particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data characteristics and the transformer architecture. backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. we further investigate these findings in the broader context of higher order markov chains and deeper architectures, and outline open problems in this arena. code is available at \url{https://github.com/bond1995/markov}.",,2024-02-06,,"['ashok vardhan makkuva', 'marco bondaschi', 'adway girish', 'alliot nagle', 'martin jaggi', 'hyeji kim', 'michael gastpar']"
2402.04177,scaling laws for downstream task performance of large language models,cs.cl cs.lg stat.ml,"scaling laws provide important insights that can guide the design of large language models (llms). existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. however, in transfer learning settings, in which llms are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. in this work, we study the scaling behavior in a transfer learning setting, where llms are finetuned for machine translation tasks. specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and bleu score. our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior. with sufficient alignment, both downstream cross-entropy and bleu score improve monotonically with more pretraining data. in such cases, we show that it is possible to predict the downstream bleu score with good accuracy using a log-law. however, there are also cases where moderate misalignment causes the bleu score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves. by analyzing these observations, we provide new practical insights for choosing appropriate pretraining data.",,2024-02-06,,"['berivan isik', 'natalia ponomareva', 'hussein hazimeh', 'dimitris paparas', 'sergei vassilvitskii', 'sanmi koyejo']"
2402.04211,variational shapley network: a probabilistic approach to self-explaining   shapley values with uncertainty quantification,cs.lg stat.ml,"shapley values have emerged as a foundational tool in machine learning (ml) for elucidating model decision-making processes. despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations. we introduce a novel, self-explaining method that simplifies the computation of shapley values significantly, requiring only a single forward pass. recognizing the deterministic treatment of shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations. unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a novel masked neural network architecture. evaluations on simulated and real datasets underscore our technique's robust predictive and explanatory performance.",,2024-02-06,,"['mert ketenci', 'iñigo urteaga', 'victor alfonso rodriguez', 'noémie elhadad', 'adler perotte']"
2402.04341,causalmetar: an r package for performing causally interpretable   meta-analyses,stat.me stat.co,"researchers would often like to leverage data from a collection of sources (e.g., primary studies in a meta-analysis) to estimate causal effects in a target population of interest. however, traditional meta-analytic methods do not produce causally interpretable estimates for a well-defined target population. in this paper, we present the causalmetar r package, which implements efficient and robust methods to estimate causal effects in a given internal or external target population using multi-source data. the package includes estimators of average and subgroup treatment effects for the entire target population. to produce efficient and robust estimates of causal effects, the package implements doubly robust and non-parametric efficient estimators and supports using flexible data-adaptive (e.g., machine learning techniques) methods and cross-fitting techniques to estimate the nuisance models (e.g., the treatment model, the outcome model). we describe the key features of the package and demonstrate how to use the package through an example.",,2024-02-06,,"['guanbo wang', 'sean mcgrath', 'yi lian', 'issa dahabreh']"
2402.04355,pqmass: probabilistic assessment of the quality of generative models   using probability mass estimation,stat.ml cs.ai cs.lg stat.me,"we propose a comprehensive sample-based method for assessing the quality of generative models. the proposed approach enables the estimation of the probability that two sets of samples are drawn from the same distribution, providing a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models trained on the same dataset. this comparison can be conducted by dividing the space into non-overlapping regions and comparing the number of data samples in each region. the method only requires samples from the generative model and the test data. it is capable of functioning directly on high-dimensional data, obviating the need for dimensionality reduction. significantly, the proposed method does not depend on assumptions regarding the density of the true distribution, and it does not rely on training or fitting any auxiliary models. instead, it focuses on approximating the integral of the density (probability mass) across various sub-regions within the data space.",,2024-02-06,,"['pablo lemos', 'sammy sharief', 'nikolay malkin', 'laurence perreault-levasseur', 'yashar hezaveh']"
2402.04376,scaling laws for learning with real and surrogate data,cs.lg cs.ai stat.ml,"collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. one may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. blurring distinctions, we refer to such data as `surrogate data'.   we define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. our main findings are: $(i)$ integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ in order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ the test error of models trained on mixtures of real and surrogate data is well described by a scaling law. this can be used to predict the optimal weighting and the gain from surrogate data.",,2024-02-06,,"['ayush jain', 'andrea montanari', 'eren sasoglu']"
2402.04384,denoising diffusion probabilistic models in six simple steps,cs.lg stat.ml,"denoising diffusion probabilistic models (ddpms) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. despite their ubiquity it is hard to find an introduction to ddpms which is simple, comprehensive, clean and clear. the compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the ddpm and the rationale of the steps that are presented is often omitted to save space. moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. on the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. in this note, we distill down the formulation of the ddpm into six simple steps each of which comes with a clear rationale. we assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, gaussian distributions, maximum likelihood estimation, and deep learning.",,2024-02-06,2024-02-10,"['richard e. turner', 'cristiana-diana diaconu', 'stratis markou', 'aliaksandra shysheya', 'andrew y. k. foong', 'bruno mlodozeniec']"
2402.04398,learning from time series under temporal label noise,cs.lg cs.ai stat.ml,"many sequential classification tasks are affected by label noise that varies over time. such noise can cause label quality to improve, worsen, or periodically change over time. we first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. in this setting, multiple labels are recorded in sequence while being corrupted by a time-dependent noise function. we first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. we then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data. we show that our methods lead to state-of-the-art performance in the presence of diverse temporal label noise functions using real and synthetic data.",,2024-02-06,,"['sujay nagaraj', 'walter gerych', 'sana tonekaboni', 'anna goldenberg', 'berk ustun', 'thomas hartvigsen']"
2402.04433,fast online changepoint detection,stat.me econ.em stat.ml,"we study online changepoint detection in the context of a linear regression model. we propose a class of heavily weighted statistics based on the cusum process of the regression residuals, which are specifically designed to ensure timely detection of breaks occurring early on during the monitoring horizon. we subsequently propose a class of composite statistics, constructed using different weighing schemes; the decision rule to mark a changepoint is based on the largest statistic across the various weights, thus effectively working like a veto-based voting mechanism, which ensures fast detection irrespective of the location of the changepoint. our theory is derived under a very general form of weak dependence, thus being able to apply our tests to virtually all time series encountered in economics, medicine, and other applied sciences. monte carlo simulations show that our methodologies are able to control the procedure-wise type i error, and have short detection delays in the presence of breaks.",,2024-02-06,,"['fabrizio ghezzi', 'eduardo rossi', 'lorenzo trapani']"
2402.04436,continuous multidimensional scaling,stat.ml cs.lg,"multidimensional scaling (mds) is the act of embedding proximity information about a set of $n$ objects in $d$-dimensional euclidean space. as originally conceived by the psychometric community, mds was concerned with embedding a fixed set of proximities associated with a fixed set of objects. modern concerns, e.g., that arise in developing asymptotic theories for statistical inference on random graphs, more typically involve studying the limiting behavior of a sequence of proximities associated with an increasing set of objects. standard results from the theory of point-to-set maps imply that, if $n$ is fixed and a sequence of proximities converges, then the limit of the embedded structures is the embedded structure of the limiting proximities. but what if $n$ increases? it then becomes necessary to reformulate mds so that the entire sequence of embedding problems can be viewed as a sequence of optimization problems in a fixed space. we present such a reformulation and derive some consequences.",,2024-02-06,2024-02-08,"['michael w. trosset', 'carey e. priebe']"
2402.04440,exploring higher-order neural network node interactions with total   correlation,cs.lg stat.ml,"in domains such as ecological systems, collaborations, and the human brain the variables interact in complex ways. yet accurately characterizing higher-order variable interactions (hois) is a difficult problem that is further exacerbated when the hois change across the data. to solve this problem we propose a new method called local correlation explanation (corex) to capture hois at a local scale by first clustering data points based on their proximity on the data manifold. we then use a multivariate version of the mutual information called the total correlation, to construct a latent factor representation of the data within each cluster to learn the local hois. we use local corex to explore hois in synthetic and real world data to extract hidden insights about the data structure. lastly, we demonstrate local corex's suitability to explore and interpret the inner workings of trained neural networks.",,2024-02-06,,"['thomas kerby', 'teresa white', 'kevin moon']"
2402.04489,de-amplifying bias from differential privacy in language model   fine-tuning,cs.lg cs.cr cs.cy stat.me,"fairness and privacy are two important values machine learning (ml) practitioners often seek to operationalize in models. fairness aims to reduce model bias for social/demographic sub-groups. privacy via differential privacy (dp) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. the trade-offs between privacy and fairness goals of trustworthy ml pose a challenge to those wishing to address both. we show that dp amplifies gender, racial, and religious bias when fine-tuning large language models (llms), producing models more biased than ones fine-tuned without dp. we find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. through the case of binary gender bias, we demonstrate that counterfactual data augmentation (cda), a known method for addressing bias, also mitigates bias amplification by dp. as a consequence, dp and cda together can be used to fine-tune models while maintaining both fairness and privacy.",,2024-02-06,,"['sanjari srivastava', 'piotr mardziel', 'zhikhun zhang', 'archana ahlawat', 'anupam datta', 'john c mitchell']"
2402.04493,a primal-dual algorithm for offline constrained reinforcement learning   with low-rank mdps,stat.ml cs.lg,"offline reinforcement learning (rl) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset. offline rl with low-rank mdps or general function approximation has been widely studied recently, but existing algorithms with sample complexity $o(\epsilon^{-2})$ for finding an $\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient. in this paper, we propose a primal dual algorithm for offline rl with low-rank mdps in the discounted infinite-horizon setting. our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $o(\epsilon^{-2})$ with partial data coverage assumption. this improves upon a recent work that requires $o(\epsilon^{-4})$ samples. moreover, our algorithm extends the previous work to the offline constrained rl setting by supporting constraints on additional reward signals.",,2024-02-06,,"['kihyuk hong', 'ambuj tewari']"
2402.04494,grandmaster-level chess without search,cs.lg cs.ai stat.ml,"the recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. this paper investigates the impact of training at scale for chess. unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270m parameter transformer model with supervised learning on a dataset of 10 million chess games. we annotate each board in the dataset with action-values provided by the powerful stockfish 16 engine, leading to roughly 15 billion data points. our largest model reaches a lichess blitz elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. we also show that our model outperforms alphazero's policy and value networks (without mcts) and gpt-3.5-turbo-instruct. a systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale. to validate our results, we perform an extensive series of ablations of design choices and hyperparameters.",,2024-02-06,,"['anian ruoss', 'grégoire delétang', 'sourabh medapati', 'jordi grau-moya', 'li kevin wenliang', 'elliot catt', 'john reid', 'tim genewein']"
2402.04516,generalized sobolev transport for probability measures on a graph,stat.ml cs.lg,"we study the optimal transport (ot) problem for measures supported on a graph metric space. recently, le et al. (2022) leverage the graph structure and propose a variant of ot, namely sobolev transport (st), which yields a closed-form expression for a fast computation. however, st is essentially coupled with the $l^p$ geometric structure within its definition which makes it nontrivial to utilize st for other prior structures. in contrast, the classic ot has the flexibility to adapt to various geometric structures by modifying the underlying cost function. an important instance is the orlicz-wasserstein (ow) which moves beyond the $l^p$ structure by leveraging the \emph{orlicz geometric structure}. comparing to the usage of standard $p$-order wasserstein, ow remarkably helps to advance certain machine learning approaches. nevertheless, ow brings up a new challenge on its computation due to its two-level optimization formulation. in this work, we leverage a specific class of convex functions for orlicz structure to propose the generalized sobolev transport (gst). gst encompasses the st as its special case, and can be utilized for prior structures beyond the $l^p$ geometry. in connection with the ow, we show that one only needs to simply solve a univariate optimization problem to compute the gst, unlike the complex two-level optimization problem in ow. we empirically illustrate that gst is several-order faster than the ow. moreover, we provide preliminary evidences on the advantages of gst for document classification and for several tasks in topological data analysis.",,2024-02-06,,"['tam le', 'truyen nguyen', 'kenji fukumizu']"
2402.04520,on computational limits of modern hopfield models: a fine-grained   complexity analysis,cs.lg cs.ai stat.ml,"we investigate the computational limits of the memory retrieval dynamics of modern hopfield models from the fine-grained complexity analysis. our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern hopfield models based on the norm of patterns. specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. only below this criterion, sub-quadratic (efficient) variants of the modern hopfield model exist, assuming the strong exponential time hypothesis (seth). to showcase our theory, we provide a formal example of efficient constructions of modern hopfield models using low-rank approximation when the efficient criterion holds. this includes a derivation of a lower bound on the computational time, scaling linearly with $\max\{$# of stored memory patterns, length of input query sequence$\}$. in addition, we prove its memory retrieval error bound and exponential memory capacity.",,2024-02-06,2024-02-14,"['jerry yao-chieh hu', 'thomas lin', 'zhao song', 'han liu']"
2402.04550,riemann-lebesgue forest for regression,stat.ml cs.lg,"we propose a novel ensemble method called riemann-lebesgue forest (rlf) for regression. the core idea of rlf is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. with this idea in mind, we develop a new tree learner named riemann-lebesgue tree which has a chance to split the node from response $y$ or a direction in feature space $\mathbf{x}$ at each non-terminal node. we generalize the asymptotic performance of rlf under different parameter settings mainly through hoeffding decomposition \cite{vaart} and stein's method \cite{chen2010normalab}. when the underlying function $y=f(\mathbf{x})$ follows an additive regression model, rlf is consistent with the argument from \cite{scornet2014consistencyor}. the competitive performance of rlf against original random forest \cite{breiman2001randomf} is demonstrated by experiments in simulation data and real world datasets.",,2024-02-06,,"['tian qin', 'wei-min huang']"
2402.04582,dimensionality reduction can be used as a surrogate model for   high-dimensional forward uncertainty quantification,stat.ap stat.ml,"we introduce a method to construct a stochastic surrogate model from the results of dimensionality reduction in forward uncertainty quantification. the hypothesis is that the high-dimensional input augmented by the output of a computational model admits a low-dimensional representation. this assumption can be met by numerous uncertainty quantification applications with physics-based computational models. the proposed approach differs from a sequential application of dimensionality reduction followed by surrogate modeling, as we ""extract"" a surrogate model from the results of dimensionality reduction in the input-output space. this feature becomes desirable when the input space is genuinely high-dimensional. the proposed method also diverges from the probabilistic learning on manifold, as a reconstruction mapping from the feature space to the input-output space is circumvented. the final product of the proposed method is a stochastic simulator that propagates a deterministic input into a stochastic output, preserving the convenience of a sequential ""dimensionality reduction + gaussian process regression"" approach while overcoming some of its limitations. the proposed method is demonstrated through two uncertainty quantification problems characterized by high-dimensional input uncertainties.",,2024-02-06,,"['jungho kim', 'sang-ri yi', 'ziqi wang']"
2402.04650,an analysis of the noise schedule for score-based generative models,math.st stat.ml stat.th,"score-based generative models (sgms) aim at estimating a target data distribution by learning score functions using only noise-perturbed samples from the target. recent literature has focused extensively on assessing the error between the target and estimated distributions, gauging the generative quality through the kullback-leibler (kl) divergence and wasserstein distances. all existing results have been obtained so far for time-homogeneous speed of the noise schedule. under mild assumptions on the data distribution, we establish an upper bound for the kl divergence between the target and the estimated distributions, explicitly depending on any time-dependent noise schedule. assuming that the score is lipschitz continuous, we provide an improved error bound in wasserstein distance, taking advantage of favourable underlying contraction mechanisms. we also propose an algorithm to automatically tune the noise schedule using the proposed upper bound. we illustrate empirically the performance of the noise schedule optimization in comparison to standard choices in the literature.",,2024-02-07,,"['stanislas strasman', 'antonio ocello', 'claire boyer', 'sylvain le corff', 'vincent lemaire']"
2402.04674,hyperparameter tuning for causal inference with double machine learning:   a simulation study,econ.em stat.ml,"proper hyperparameter tuning is essential for achieving optimal performance of modern machine learning (ml) methods in predictive tasks. while there is an extensive literature on tuning ml learners for prediction, there is only little guidance available on tuning ml learners for causal machine learning and how to select among different ml learners. in this paper, we empirically assess the relationship between the predictive performance of ml methods and the resulting causal estimation based on the double machine learning (dml) approach by chernozhukov et al. (2018). dml relies on estimating so-called nuisance parameters by treating them as supervised learning problems and using them as plug-in estimates to solve for the (causal) parameter. we conduct an extensive simulation study using data from the 2019 atlantic causal inference conference data challenge. we provide empirical insights on the role of hyperparameter tuning and other practical decisions for causal estimation with dml. first, we assess the importance of data splitting schemes for tuning ml learners within double machine learning. second, we investigate how the choice of ml methods and hyperparameters, including recent automl frameworks, impacts the estimation performance for a causal parameter of interest. third, we assess to what extent the choice of a particular causal model, as characterized by incorporated parametric assumptions, can be based on predictive performance metrics.",,2024-02-07,,"['philipp bach', 'oliver schacht', 'victor chernozhukov', 'sven klaassen', 'martin spindler']"
2402.04691,learning operators with stochastic gradient descent in general hilbert   spaces,stat.ml cs.lg math.fa math.st stat.th,"this study investigates leveraging stochastic gradient descent (sgd) to learn operators between general hilbert spaces. we propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity. under these conditions, we establish upper bounds for convergence rates of the sgd algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the sgd algorithm. it is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning. we show that the sgd estimator will converge to the best linear approximation of the nonlinear target operator. moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel hilbert spaces yields new convergence results, thereby refining the conclusions of existing literature.",,2024-02-07,2024-02-13,"['lei shi', 'jia-qi yang']"
2402.04692,from explained variance of correlated components to pca without   orthogonality constraints,stat.ml cs.lg,"block principal component analysis (block pca) of a data matrix a, where loadings z are determined by maximization of az 2 over unit norm orthogonal loadings, is difficult to use for the design of sparse pca by 1 regularization, due to the difficulty of taking care of both the orthogonality constraint on loadings and the non differentiable 1 penalty. our objective in this paper is to relax the orthogonality constraint on loadings by introducing new objective functions expvar(y) which measure the part of the variance of the data matrix a explained by correlated components y = az. so we propose first a comprehensive study of mathematical and numerical properties of expvar(y) for two existing definitions zou et al. [2006], shen and huang [2008] and four new definitions. then we show that only two of these explained variance are fit to use as objective function in block pca formulations for a rid of orthogonality constraints.",,2024-02-07,,"['marie chavent', 'guy chavent']"
2402.04740,non-parametric estimation of multi-dimensional marked hawkes processes,stat.ml cs.lg q-fin.cp q-fin.st,"an extension of the hawkes process, the marked hawkes process distinguishes itself by featuring variable jump size across each event, in contrast to the constant jump size observed in a hawkes process without marks. while extensive literature has been dedicated to the non-parametric estimation of both the linear and non-linear hawkes process, there remains a significant gap in the literature regarding the marked hawkes process. in response to this, we propose a methodology for estimating the conditional intensity of the marked hawkes process. we introduce two distinct models: \textit{shallow neural hawkes with marks}- for hawkes processes with excitatory kernels and \textit{neural network for non-linear hawkes with marks}- for non-linear hawkes processes. both these approaches take the past arrival times and their corresponding marks as the input to obtain the arrival intensity. this approach is entirely non-parametric, preserving the interpretability associated with the marked hawkes process. to validate the efficacy of our method, we subject the method to synthetic datasets with known ground truth. additionally, we apply our method to model cryptocurrency order book data, demonstrating its applicability to real-world scenarios.",,2024-02-07,,"['sobin joseph', 'shashi jain']"
2402.04751,asymptotic dynamics of alternating minimization for non-convex   optimization,math.oc cond-mat.dis-nn stat.ml,"this study investigates the asymptotic dynamics of alternating minimization applied to optimize a bilinear non-convex function with normally distributed covariates. we employ the replica method from statistical physics in a multi-step approach to precisely trace the algorithm's evolution. our findings indicate that the dynamics can be described effectively by a two--dimensional discrete stochastic process, where each step depends on all previous time steps, revealing a memory dependency in the procedure. the theoretical framework developed in this work is broadly applicable for the analysis of various iterative algorithms, extending beyond the scope of alternating minimization.",,2024-02-07,,"['koki okajima', 'takashi takahashi']"
2402.04777,a fast score-based search algorithm for maximal ancestral graphs using   entropy,stat.ml cs.lg math.st stat.th,"\emph{maximal ancestral graph} (mags) is a class of graphical model that extend the famous \emph{directed acyclic graph} in the presence of latent confounders. most score-based approaches to learn the unknown mag from empirical data rely on bic score which suffers from instability and heavy computations. we propose to use the framework of imsets \citep{studeny2006probabilistic} to score mags using empirical entropy estimation and the newly proposed \emph{refined markov property} \citep{hu2023towards}. our graphical search procedure is similar to \citet{claassen2022greedy} but improved from our theoretical results. we show that our search algorithm is polynomial in number of nodes by restricting degree, maximal head size and number of discriminating paths. in simulated experiment, our algorithm shows superior performance compared to other state of art mag learning algorithms.",,2024-02-07,,"['zhongyi hu', 'robin evans']"
2402.04859,conditionality principle under unconstrained randomness,math.st stat.me stat.th,"a very simple example demonstrates that fisher's application of the conditionality principle to regression (""fixed $x$ regression""), endorsed by sprott and many other followers, makes prediction impossible in the context of statistical learning theory. on the other hand, relaxing the requirement of conditionality makes it possible via, e.g., conformal prediction.",,2024-02-07,,['vladimir vovk']
2402.04875,on provable length and compositional generalization,cs.lg cs.cl stat.ml,"length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. in this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.",,2024-02-07,2024-02-24,"['kartik ahuja', 'amin mansouri']"
2402.04885,a unified gaussian process for branching and nested hyperparameter   optimization,stat.ml cs.ai cs.lg,"choosing appropriate hyperparameters plays a crucial role in the success of neural networks as hyper-parameters directly control the behavior and performance of the training algorithms. to obtain efficient tuning, bayesian optimization methods based on gaussian process (gp) models are widely used. despite numerous applications of bayesian optimization in deep learning, the existing methodologies are developed based on a convenient but restrictive assumption that the tuning parameters are independent of each other. however, tuning parameters with conditional dependence are common in practice. in this paper, we focus on two types of them: branching and nested parameters. nested parameters refer to those tuning parameters that exist only within a particular setting of another tuning parameter, and a parameter within which other parameters are nested is called a branching parameter. to capture the conditional dependence between branching and nested parameters, a unified bayesian optimization framework is proposed. the sufficient conditions are rigorously derived to guarantee the validity of the kernel function, and the asymptotic convergence of the proposed optimization framework is proven under the continuum-armed-bandit setting. based on the new gp model, which accounts for the dependent structure among input variables through a new kernel function, higher prediction accuracy and better optimization efficiency are observed in a series of synthetic simulations and real data applications of neural networks. sensitivity analysis is also performed to provide insights into how changes in hyperparameter values affect prediction accuracy.",,2024-01-19,,"['jiazhao zhang', 'ying hung', 'chung-ching lin', 'zicheng liu']"
2402.04922,voronoi candidates for bayesian optimization,stat.ml cs.lg,"bayesian optimization (bo) offers an elegant approach for efficiently optimizing black-box functions. however, acquisition criteria demand their own challenging inner-optimization, which can induce significant overhead. many practical bo methods, particularly in high dimension, eschew a formal, continuous optimization of the acquisition function and instead search discretely over a finite set of space-filling candidates. here, we propose to use candidates which lie on the boundary of the voronoi tessellation of the current design points, so they are equidistant to two or more of them. we discuss strategies for efficient implementation by directly sampling the voronoi boundary without explicitly generating the tessellation, thus accommodating large designs in high dimension. on a battery of test problems optimized via gaussian processes with expected improvement, our proposed approach significantly improves the execution time of a multi-start continuous search without a loss in accuracy.",,2024-02-07,,"['nathan wycoff', 'john w. smith', 'annie s. booth', 'robert b. gramacy']"
2402.04933,a bayesian approach to online learning for contextual restless bandits   with applications to public health,cs.lg stat.ap,"restless multi-armed bandits (rmabs) are used to model sequential resource allocation in public health intervention programs. in these settings, the underlying transition dynamics are often unknown a priori, requiring online reinforcement learning (rl). however, existing methods in online rl for rmabs cannot incorporate properties often present in real-world public health applications, such as contextual information and non-stationarity. we present bayesian learning for contextual rmabs (bcor), an online rl approach for rmabs that novelly combines techniques in bayesian modeling with thompson sampling to flexibly model a wide range of complex rmab settings, such as contextual and non-stationary rmabs. a key contribution of our approach is its ability to leverage shared information within and between arms to learn unknown rmab transition dynamics quickly in budget-constrained settings with relatively short time horizons. empirically, we show that bcor achieves substantially higher finite-sample performance than existing approaches over a range of experimental settings, including one constructed from a real-world public health campaign in india.",,2024-02-07,,"['biyonka liang', 'lily xu', 'aparna taneja', 'milind tambe', 'lucas janson']"
2402.04980,asymptotics of feature learning in two-layer networks after one   gradient-step,stat.ml cond-mat.dis-nn cs.lg,"in this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. leveraging a connection from (ba et al., 2022) with a non-linear spiked matrix model and recent progress on gaussian universality (dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate. we characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime. to our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\eta=\theta_{d}(d)$, beyond perturbative finite width corrections of the conjugate and neural tangent kernels.",,2024-02-07,,"['hugo cui', 'luca pesce', 'yatin dandi', 'florent krzakala', 'yue m. lu', 'lenka zdeborová', 'bruno loureiro']"
2402.04997,generative flows on discrete state-spaces: enabling multimodal flows   with applications to protein co-design,stat.ml cs.lg q-bio.qm,"combining discrete and continuous data is an important capability for generative models. we present discrete flow models (dfms), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems. our key insight is that the discrete equivalent of continuous space flow matching can be realized using continuous time markov chains. dfms benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. we utilize our dfms method to build a multimodal flow-based modeling framework. we apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or structure.",,2024-02-07,,"['andrew campbell', 'jason yim', 'regina barzilay', 'tom rainforth', 'tommi jaakkola']"
2402.05013,compression of structured data with autoencoders: provable benefit of   nonlinearities and depth,cs.lg cs.it math.it stat.ml,"autoencoders are a prominent model in many empirical branches of machine learning and lossy data compression. however, basic theoretical questions remain unanswered even in a shallow two-layer setting. in particular, to what degree does a shallow autoencoder capture the structure of the underlying data distribution? for the prototypical case of the 1-bit compression of sparse gaussian data, we prove that gradient descent converges to a solution that completely disregards the sparse structure of the input. namely, the performance of the algorithm is the same as if it was compressing a gaussian source - with no sparsity. for general data distributions, we give evidence of a phase transition phenomenon in the shape of the gradient descent minimizer, as a function of the data sparsity: below the critical sparsity level, the minimizer is a rotation taken uniformly at random (just like in the compression of non-sparse data); above the critical sparsity, the minimizer is the identity (up to a permutation). finally, by exploiting a connection with approximate message passing algorithms, we show how to improve upon gaussian performance for the compression of sparse data: adding a denoising function to a shallow architecture already reduces the loss provably, and a suitable multi-layer decoder leads to a further improvement. we validate our findings on image datasets, such as cifar-10 and mnist.",,2024-02-07,,"['kevin kögler', 'alexander shevchenko', 'hamed hassani', 'marco mondelli']"
2402.05052,causal representation learning from multiple distributions: a general   setting,cs.lg stat.ml,"in many problems, the measured variables (e.g., image pixels) are just mathematical functions of the hidden causal variables (e.g., the underlying concepts or objects). for the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the hidden causal variables $z_i$ and their causal relations represented by graph $\mathcal{g}_z$. this problem has recently been known as causal representation learning. this paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes. we aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions. we show that under the sparsity constraint on the recovered graph over the latent variables and suitable sufficient change conditions on the causal influences, interestingly, one can recover the moralized graph of the underlying directed acyclic graph, and the recovered latent variables and their relations are related to the underlying causal model in a specific, nontrivial way. in some cases, each latent variable can even be recovered up to component-wise transformations. experimental results verify our theoretical claims.",,2024-02-07,,"['kun zhang', 'shaoan xie', 'ignavier ng', 'yujia zheng']"
2402.05071,extending the reach of first-order algorithms for nonconvex min-max   problems with cohypomonotonicity,math.oc cs.lg stat.ml,"we focus on constrained, $l$-smooth, nonconvex-nonconcave min-max problems either satisfying $\rho$-cohypomonotonicity or admitting a solution to the $\rho$-weakly minty variational inequality (mvi), where larger values of the parameter $\rho>0$ correspond to a greater degree of nonconvexity. these problem classes include examples in two player reinforcement learning, interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail. it has been conjectured that first-order methods can tolerate value of $\rho$ no larger than $\frac{1}{l}$, but existing results in the literature have stagnated at the tighter requirement $\rho < \frac{1}{2l}$. with a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak mvi conditions for $\rho < \frac{1}{l}$. the algorithms we analyze are inexact variants of halpern and krasnosel'ski\u{\i}-mann (km) iterations. we also provide algorithms and complexity guarantees in the stochastic case with the same range on $\rho$. our main insight for the improvements in the convergence analyses is to harness the recently proposed ""conic nonexpansiveness"" property of operators. as byproducts, we provide a refined analysis for inexact halpern iteration and propose a stochastic km iteration with a multilevel monte carlo estimator.",,2024-02-07,,"['ahmet alacaoglu', 'donghwan kim', 'stephen j. wright']"
2402.05098,on diffusion models for amortized inference: benchmarking and improving   stochastic control and sampling,cs.lg stat.ml,"we study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. we benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. we also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. our code for the sampling methods and benchmarks studied is made public at https://github.com/gfnorg/gfn-diffusion as a base for future work on diffusion models for amortized inference.",,2024-02-07,2024-02-13,"['marcin sendera', 'minsu kim', 'sarthak mittal', 'pablo lemos', 'luca scimeca', 'jarrid rector-brooks', 'alexandre adam', 'yoshua bengio', 'nikolay malkin']"
2402.05101,tighter generalisation bounds via interpolation,stat.ml cs.lg,"this paper contains a recipe for deriving new pac-bayes generalisation bounds based on the $(f, \gamma)$-divergence, and, in addition, presents pac-bayes generalisation bounds where we interpolate between a series of probability divergences (including but not limited to kl, wasserstein, and total variation), making the best out of many worlds depending on the posterior distributions properties. we explore the tightness of these bounds and connect them to earlier results from statistical learning, which are specific cases. we also instantiate our bounds as training objectives, yielding non-trivial guarantees and practical performances.",,2024-02-07,,"['paul viallard', 'maxime haddouche', 'umut şimşekli', 'benjamin guedj']"
2402.05145,online learning approach for survival analysis,cs.lg physics.data-an stat.ml,"we introduce an online mathematical framework for survival analysis, allowing real time adaptation to dynamic environments and censored data. this framework enables the estimation of event time distributions through an optimal second order online convex optimization algorithm-online newton step (ons). this approach, previously unexplored, presents substantial advantages, including explicit algorithms with non-asymptotic convergence guarantees. moreover, we analyze the selection of ons hyperparameters, which depends on the exp-concavity property and has a significant influence on the regret bound. we propose a stochastic approach that guarantees logarithmic stochastic regret for ons. additionally, we introduce an adaptive aggregation method that ensures robustness in hyperparameter selection while maintaining fast regret bounds. the findings of this paper can extend beyond the survival analysis field, and are relevant for any case characterized by poor exp-concavity and unstable ons. finally, these assertions are illustrated by simulation experiments.",,2024-02-07,,"['camila fernandez', 'pierre gaillard', 'joseph de vilmarest', 'olivier wintenberger']"
2402.05173,towards understanding inductive bias in transformers: a view from   infinity,cs.lg cond-mat.dis-nn stat.ml,"we study inductive bias in transformers in the infinitely over-parameterized gaussian process limit and argue transformers tend to be biased towards more permutation symmetric functions in sequence space. we show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens. we present a simplified transformer block and solve the model at the limit, including accurate predictions for the learning curves and network outputs. we show that in common setups, one can derive tight bounds in the form of a scaling law for the learnability as a function of the context length. finally, we argue wikitext dataset, does indeed possess a degree of permutation symmetry.",,2024-02-07,,"['itay lavie', 'guy gur-ari', 'zohar ringel']"
2402.05187,meta-learning the mirror map in policy mirror descent,stat.ml cs.lg math.oc,"policy mirror descent (pmd) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. these algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. despite its popularity, the exploration of pmd's full potential is limited, with the majority of research focusing on a particular mirror map -- namely, the negative entropy -- which gives rise to the renowned natural policy gradient (npg) method. it remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences pmd's efficacy. in our work, we conduct empirical investigations to show that the conventional mirror map choice (npg) often yields less-than-optimal outcomes across several standard benchmark environments. by applying a meta-learning approach, we identify more efficient mirror maps that enhance performance, both on average and in terms of best performance achieved along the training trajectory. we analyze the characteristics of these learned mirror maps and reveal shared traits among certain settings. our results suggest that mirror maps have the potential to be adaptable across various environments, raising questions about how to best match a mirror map to an environment's structure and characteristics.",,2024-02-07,,"['carlo alfano', 'sebastian towers', 'silvia sapora', 'chris lu', 'patrick rebeschini']"
2402.05203,bellman conformal inference: calibrating prediction intervals for time   series,cs.lg stat.ml,"we introduce bellman conformal inference (bci), a framework that wraps around any time series forecasting models and provides approximately calibrated prediction intervals. unlike existing methods, bci is able to leverage multi-step ahead forecasts and explicitly optimize the average interval lengths by solving a one-dimensional stochastic control problem (scp) at each time step. in particular, we use the dynamic programming algorithm to find the optimal policy for the scp. we prove that bci achieves long-term coverage under arbitrary distribution shifts and temporal dependence, even with poor multi-step ahead forecasts. we find empirically that bci avoids uninformative intervals that have infinite lengths and generates substantially shorter prediction intervals in multiple applications when compared with existing methods.",,2024-02-07,2024-02-09,"['zitong yang', 'emmanuel candès', 'lihua lei']"
2402.05220,on parameter estimation in deviated gaussian mixture of experts,stat.ml cs.lg,"we consider the parameter estimation problem in the deviated gaussian mixture of experts in which the data are generated from $(1 - \lambda^{\ast}) g_0(y| x)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast} f(y|(a_{i}^{\ast})^{\top}x+b_i^{\ast},\sigma_{i}^{\ast})$, where $x, y$ are respectively a covariate vector and a response variable, $g_{0}(y|x)$ is a known function, $\lambda^{\ast} \in [0, 1]$ is true but unknown mixing proportion, and $(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$ for $1 \leq i \leq k^{\ast}$ are unknown parameters of the gaussian mixture of experts. this problem arises from the goodness-of-fit test when we would like to test whether the data are generated from $g_{0}(y|x)$ (null hypothesis) or they are generated from the whole mixture (alternative hypothesis). based on the algebraic structure of the expert functions and the distinguishability between $g_0$ and the mixture part, we construct novel voronoi-based loss functions to capture the convergence rates of maximum likelihood estimation (mle) for our models. we further demonstrate that our proposed loss functions characterize the local convergence rates of parameter estimation more accurately than the generalized wasserstein, a loss function being commonly used for estimating parameters in the gaussian mixture of experts.",,2024-02-07,,"['huy nguyen', 'khai nguyen', 'nhat ho']"
2402.05271,gradient descent induces alignment between weights and the empirical ntk   for deep non-linear networks,stat.ml cs.ai cs.lg,"understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the neural feature ansatz (nfa). however, the reason these quantities become correlated during training is poorly understood. in this work, we explain the emergence of this correlation. we identify that the nfa is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. we establish that the nfa introduced in prior works is driven by a centered nfa that isolates this alignment. we show that the speed of nfa development can be predicted analytically at early training times in terms of simple statistics of the inputs and labels. finally, we introduce a simple intervention to increase nfa correlation at any given layer, which dramatically improves the quality of features learned.",,2024-02-07,,"['daniel beaglehole', 'ioannis mitliagkas', 'atish agarwala']"
2402.05330,classification under nuisance parameters and generalized label shift in   likelihood-free inference,stat.ml cs.lg,"an open scientific challenge is how to classify events with reliable measures of uncertainty, when we have a mechanistic model of the data-generating process but the distribution over both labels and latent nuisance parameters is different between train and target data. we refer to this type of distributional shift as generalized label shift (gls). direct classification using observed data $\mathbf{x}$ as covariates leads to biased predictions and invalid uncertainty estimates of labels $y$. we overcome these biases by proposing a new method for robust uncertainty quantification that casts classification as a hypothesis testing problem under nuisance parameters. the key idea is to estimate the classifier's receiver operating characteristic (roc) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under gls. our method effectively endows a pre-trained classifier with domain adaptation capabilities and returns valid prediction sets while maintaining high power. we demonstrate its performance on two challenging scientific problems in biology and astroparticle physics with data from realistic mechanistic models.",,2024-02-07,,"['luca masserano', 'alex shen', 'michele doro', 'tommaso dorigo', 'rafael izbicki', 'ann b. lee']"
2402.05379,tradeoffs of diagonal fisher information matrix estimators,cs.lg stat.ml,"the fisher information matrix characterizes the local geometry in the parameter space of neural networks. it elucidates insightful theories and useful tools to understand and optimize neural networks. given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. we examine two such estimators, whose accuracy and sample complexity depend on their associated variances. we derive bounds of the variances and instantiate them in regression and classification networks. we navigate trade-offs of both estimators based on analytical and numerical studies. we find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the fisher information.",,2024-02-07,,"['alexander soen', 'ke sun']"
2402.05401,adaptive activation functions for predictive modeling with sparse   experimental data,cs.lg cs.ne stat.ml,"a pivotal aspect in the design of neural networks lies in selecting activation functions, crucial for introducing nonlinear structures that capture intricate input-output patterns. while the effectiveness of adaptive or trainable activation functions has been studied in domains with ample data, like image classification problems, significant gaps persist in understanding their influence on classification accuracy and predictive uncertainty in settings characterized by limited data availability. this research aims to address these gaps by investigating the use of two types of adaptive activation functions. these functions incorporate shared and individual trainable parameters per hidden layer and are examined in three testbeds derived from additive manufacturing problems containing fewer than one hundred training instances. our investigation reveals that adaptive activation functions, such as exponential linear unit (elu) and softplus, with individual trainable parameters, result in accurate and confident prediction models that outperform fixed-shape activation functions and the less flexible method of using identical trainable activation functions in a hidden layer. therefore, this work presents an elegant way of facilitating the design of adaptive neural networks in scientific and engineering problems.",,2024-02-07,,"['farhad pourkamali-anaraki', 'tahamina nasrin', 'robert e. jensen', 'amy m. peterson', 'christopher j. hansen']"
2402.05439,learning uncertainty-aware temporally-extended actions,cs.lg stat.ml,"in reinforcement learning, temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions. however, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated. this issue often negates the advantages of action repetition. to address this, we propose a novel algorithm named uncertainty-aware temporal extension (ute). ute employs ensemble methods to accurately measure uncertainty during action extension. this feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs. we demonstrate the effectiveness of ute through experiments in gridworld and atari 2600 environments. our findings show that ute outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhancing policy learning efficiency.",,2024-02-08,,"['joongkyu lee', 'seung joon park', 'yunhao tang', 'min-hwan oh']"
2402.05451,low-degree phase transitions for detecting a planted clique in sublinear   time,cs.ds cs.cc stat.ml,"we consider the problem of detecting a planted clique of size $k$ in a random graph on $n$ vertices. when the size of the clique exceeds $\theta(\sqrt{n})$, polynomial-time algorithms for detection proliferate. we study faster -- namely, sublinear time -- algorithms in the high-signal regime when $k = \theta(n^{1/2 + \delta})$, for some $\delta > 0$. to this end, we consider algorithms that non-adaptively query a subset $m$ of entries of the adjacency matrix and then compute a low-degree polynomial function of the revealed entries. we prove a computational phase transition for this class of non-adaptive low-degree algorithms: under the scaling $\lvert m \rvert = \theta(n^{\gamma})$, the clique can be detected when $\gamma > 3(1/2 - \delta)$ but not when $\gamma < 3(1/2 - \delta)$. as a result, the best known runtime for detecting a planted clique, $\widetilde{o}(n^{3(1/2-\delta)})$, cannot be improved without looking beyond the non-adaptive low-degree class.   our proof of the lower bound -- based on bounding the conditional low-degree likelihood ratio -- reveals further structure in non-adaptive detection of a planted clique. using (a bound on) the conditional low-degree likelihood ratio as a potential function, we show that for every non-adaptive query pattern, there is a highly structured query pattern of the same size that is at least as effective.",,2024-02-08,,"['jay mardia', 'kabir aladin verchand', 'alexander s. wein']"
2402.05479,a comparison of the effects of different methodologies on the statistics   learning profiles of prospective primary education teachers from a gender   perspective,stat.me,"over the last decades,it has been shown that teaching and learning statistics is complex, regardless of the teaching methodology. this research presents the different learning profiles identified in a group of future primary education (pe) teachers during the study of the statistics blockdepending on the methodology used and gender, where the sample consists of 132 students in the third year of the pe undergraduate degree in theuniversity of the basque country(universidad del pa\'is vasco/euskal herriko unibertsitatea, upv/ehu). to determine the profiles, a cluster analysis technique has been used, where the main variables to determine them are, on the one hand, their statistical competence development and, on the other hand, the evolutionof their attitude towards statistics. in order to better understand the nature of the profiles obtained, the type of teaching methodology used to work on the statistics block has been taken into account. this comparison is based on the fact that the sample is divided into two groups: one has worked with a project based learning (pbl) methodology,while the other has worked with a methodology in which theoretical explanations and typically decontextualized exercises predominate. among the results obtained,three differentiated profiles areobserved, highlighting the proportion of students with an advantageous profile in the group where pbl is included.with regard to gender, the results show that women's attitudes towardstatistics evolvedmore positively than men's after the sessions devoted to statistics in the pbl group.",10.22342/jme.v14i4.pp741-756,2024-02-08,,"['j. anasagasti', 'a. berciano', 'a. izagirre']"
2402.05525,differentially private model-based offline reinforcement learning,cs.lg cs.ai cs.cr stat.ml,"we address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. to achieve this, we introduce dp-morl, an mbrl algorithm coming with differential privacy guarantees. a private model of the environment is first learned from offline data using dp-fedavg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. we empirically show that dp-morl enables the training of private rl agents from offline data and we furthermore outline the price of privacy in this setting.",,2024-02-08,,"['alexandre rio', 'merwan barlier', 'igor colin', 'albert thomas']"
2402.05543,machine learning applied to omics data,q-bio.gn cs.lg stat.ap,"in this chapter we illustrate the use of some machine learning techniques in the context of omics data. more precisely, we review and evaluate the use of random forest and penalized multinomial logistic regression for integrative analysis of genomics and immunomics in pancreatic cancer. furthermore, we propose the use of association rules with predictive purposes to overcome the low predictive power of the previously mentioned models. finally, we apply the reviewed methods to a real data set from tcga made of 107 tumoral pancreatic samples and 117,486 germline snps, showing the good performance of the proposed methods to predict the immunological infiltration in pancreatic cancer.",10.1007/978-3-031-32729-2_2,2024-02-08,,"['aida calviño', 'almudena moreno-ribera', 'silvia pineda']"
2402.05569,hypergraph node classification with graph neural networks,cs.lg cs.ai eess.sp stat.ml,"hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. the success of graph neural networks (gnns) reveals the capability of neural networks to process data with pairwise interactions. this inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (hypergnns). gnns and hypergnns are typically considered distinct since they are designed for data on different geometric topologies. however, in this paper, we theoretically demonstrate that, in the context of node classification, most hypergnns can be approximated using a gnn with a weighted clique expansion of the hypergraph. this leads to wce-gnn, a simple and efficient framework comprising a gnn and a weighted clique expansion (wce), for hypergraph node classification. experiments on nine real-world hypergraph node classification benchmarks showcase that wce-gnn demonstrates not only higher classification accuracy compared to state-of-the-art hypergnns, but also superior memory and runtime efficiency.",,2024-02-08,,"['bohan tang', 'zexi liu', 'keyue jiang', 'siheng chen', 'xiaowen dong']"
2402.05639,nonparametric instrumental variable regression through stochastic   approximate gradients,stat.ml cs.lg,"this paper proposes sagd-iv, a novel framework for conducting nonparametric instrumental variable (npiv) regression by employing stochastic approximate gradients to minimize the projected populational risk. instrumental variables (ivs) are widely used in econometrics to address estimation problems in the presence of unobservable confounders, and the machine learning community has devoted significant effort to improving existing methods and devising new ones in the npiv setting, which is known to be an ill-posed linear inverse problem. we provide theoretical support for our algorithm and further exemplify its competitive performance through empirical experiments. furthermore, we address, with promising results, the case of binary outcomes, which has not received as much attention from the community as its continuous counterpart.",,2024-02-08,,"['caio peixoto', 'yuri saporito', 'yuri fonseca']"
2402.05674,a high dimensional model for adversarial training: geometry and   trade-offs,stat.ml cond-mat.dis-nn cs.lg,"this work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha = n / d$. we introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses. our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. in particular, we unveil the existence of directions which can be defended without penalising accuracy. finally, we show the advantage of defending non-robust features during training, identifying a uniform protection as an inherently effective defence mechanism.",,2024-02-08,,"['kasimir tanner', 'matteo vilucchio', 'bruno loureiro', 'florent krzakala']"
2402.05696,fixed width treelike neural networks capacity analysis -- generic   activations,stat.ml cond-mat.dis-nn cs.it cs.lg math.it math.pr,"we consider the capacity of \emph{treelike committee machines} (tcm) neural networks. relying on random duality theory (rdt), \cite{stojnictcmspnncaprdt23} recently introduced a generic framework for their capacity analysis. an upgrade based on the so-called \emph{partially lifted} rdt (pl rdt) was then presented in \cite{stojnictcmspnncapliftedrdt23}. both lines of work focused on the networks with the most typical, \emph{sign}, activations. here, on the other hand, we focus on networks with other, more general, types of activations and show that the frameworks of \cite{stojnictcmspnncaprdt23,stojnictcmspnncapliftedrdt23} are sufficiently powerful to enable handling of such scenarios as well. in addition to the standard \emph{linear} activations, we uncover that particularly convenient results can be obtained for two very commonly used activations, namely, the \emph{quadratic} and \emph{rectified linear unit (relu)} ones. in more concrete terms, for each of these activations, we obtain both the rdt and pl rdt based memory capacities upper bound characterization for \emph{any} given (even) number of the hidden layer neurons, $d$. in the process, we also uncover the following two, rather remarkable, facts: 1) contrary to the common wisdom, both sets of results show that the bounding capacity decreases for large $d$ (the width of the hidden layer) while converging to a constant value; and 2) the maximum bounding capacity is achieved for the networks with precisely \textbf{\emph{two}} hidden layer neurons! moreover, the large $d$ converging values are observed to be in excellent agrement with the statistical physics replica theory based predictions.",,2024-02-08,,['mihailo stojnic']
2402.05715,collaborative non-parametric two-sample testing,stat.ml cs.lg,"this paper addresses the multiple two-sample test problem in a graph-structured setting, which is a common scenario in fields such as spatial statistics and neuroscience. each node $v$ in fixed graph deals with a two-sample testing problem between two node-specific probability density functions (pdfs), $p_v$ and $q_v$. the goal is to identify nodes where the null hypothesis $p_v = q_v$ should be rejected, under the assumption that connected nodes would yield similar test outcomes. we propose the non-parametric collaborative two-sample testing (ctst) framework that efficiently leverages the graph structure and minimizes the assumptions over $p_v$ and $q_v$. our methodology integrates elements from f-divergence estimation, kernel methods, and multitask learning. we use synthetic experiments and a real sensor network detecting seismic activity to demonstrate that ctst outperforms state-of-the-art non-parametric statistical tests that apply at each node independently, hence disregard the geometry of the problem.",,2024-02-08,,"['alejandro de la concha', 'nicolas vayatis', 'argyris kalogeratos']"
2402.05718,remedi: corrective transformations for improved neural entropy   estimation,stat.ml cs.lg,"information theoretic quantities play a central role in machine learning. the recent surge in the complexity of data and models has increased the demand for accurate estimation of these quantities. however, as the dimension grows the estimation presents significant challenges, with existing methods struggling already in relatively low dimensions. to address this issue, in this work, we introduce $\texttt{remedi}$ for efficient and accurate estimation of differential entropy, a fundamental information theoretic quantity. the approach combines the minimization of the cross-entropy for simple, adaptive base models and the estimation of their deviation, in terms of the relative entropy, from the data density. our approach demonstrates improvement across a broad spectrum of estimation tasks, encompassing entropy estimation on both synthetic and natural data. further, we extend important theoretical consistency results to a more generalized setting required by our approach. we illustrate how the framework can be naturally extended to information theoretic supervised learning models, with a specific focus on the information bottleneck approach. it is demonstrated that the method delivers better accuracy compared to the existing methods in information bottleneck. in addition, we explore a natural connection between $\texttt{remedi}$ and generative modeling using rejection sampling and langevin dynamics.",,2024-02-08,,"['viktor nilsson', 'anirban samaddar', 'sandeep madireddy', 'pierre nyquist']"
2402.05719,exact capacity of the \emph{wide} hidden layer treelike neural networks   with generic activations,stat.ml cond-mat.dis-nn cs.it cs.lg math.it math.pr,"recent progress in studying \emph{treelike committee machines} (tcm) neural networks (nn) in \cite{stojnictcmspnncaprdt23,stojnictcmspnncapliftedrdt23,stojnictcmspnncapdiffactrdt23} showed that the random duality theory (rdt) and its a \emph{partially lifted}(pl rdt) variant are powerful tools that can be used for very precise networks capacity analysis. here, we consider \emph{wide} hidden layer networks and uncover that certain aspects of numerical difficulties faced in \cite{stojnictcmspnncapdiffactrdt23} miraculously disappear. in particular, we employ recently developed \emph{fully lifted} (fl) rdt to characterize the \emph{wide} ($d\rightarrow \infty$) tcm nets capacity. we obtain explicit, closed form, capacity characterizations for a very generic class of the hidden layer activations. while the utilized approach significantly lowers the amount of the needed numerical evaluations, the ultimate fl rdt usefulness and success still require a solid portion of the residual numerical work. to get the concrete capacity values, we take four very famous activations examples: \emph{\textbf{relu}}, \textbf{\emph{quadratic}}, \textbf{\emph{erf}}, and \textbf{\emph{tanh}}. after successfully conducting all the residual numerical work for all of them, we uncover that the whole lifting mechanism exhibits a remarkably rapid convergence with the relative improvements no better than $\sim 0.1\%$ happening already on the 3-rd level of lifting. as a convenient bonus, we also uncover that the capacity characterizations obtained on the first and second level of lifting precisely match those obtained through the statistical physics replica theory methods in \cite{zavpeh21} for the generic and in \cite{balmalzech19} for the relu activations.",,2024-02-08,,['mihailo stojnic']
2402.05724,model-based rl for mean-field games is not statistically harder than   single-agent rl,cs.lg cs.ai cs.gt stat.ml,"we study the sample complexity of reinforcement learning (rl) in mean-field games (mfgs) with model-based function approximation that requires strategic exploration to find a nash equilibrium policy. we introduce the partial model-based eluder dimension (p-mbed), a more effective notion to characterize the model class complexity. notably, p-mbed measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the mbed proposed by \citet{huang2023statistical}. we contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~p-mbed. crucially, our results reveal that, under the basic realizability and lipschitz continuity assumptions, \emph{learning nash equilibrium in mfgs is no more statistically challenging than solving a logarithmic number of single-agent rl problems}. we further extend our results to multi-type mfgs, generalizing from conventional mfgs and involving multiple types of agents. this extension implies statistical tractability of a broader class of markov games through the efficacy of mean-field approximation. finally, inspired by our theoretical algorithm, we present a heuristic approach with improved computational efficiency and empirically demonstrate its effectiveness.",,2024-02-08,,"['jiawei huang', 'niao he', 'andreas krause']"
2402.05738,implicit bias and fast convergence rates for self-attention,cs.lg math.oc stat.ml,"self-attention, the core mechanism of transformers, distinguishes them from traditional neural networks and drives their outstanding performance. towards developing the fundamental optimization principles of self-attention, we investigate the implicit bias of gradient descent (gd) in training a self-attention layer with fixed linear decoder in binary classification. drawing inspiration from the study of gd in linear logistic regression over separable data, recent work demonstrates that as the number of iterations $t$ approaches infinity, the key-query matrix $w_t$ converges locally (with respect to the initialization direction) to a hard-margin svm solution $w_{mm}$. our work enhances this result in four aspects. firstly, we identify non-trivial data settings for which convergence is provably global, thus shedding light on the optimization landscape. secondly, we provide the first finite-time convergence rate for $w_t$ to $w_{mm}$, along with quantifying the rate of sparsification in the attention map. thirdly, through an analysis of normalized gd and polyak step-size, we demonstrate analytically that adaptive step-size rules can accelerate the convergence of self-attention. additionally, we remove the restriction of prior work on a fixed linear decoder. our results reinforce the implicit-bias perspective of self-attention and strengthen its connections to implicit-bias in linear logistic regression, despite the intricate non-convex nature of the former.",,2024-02-08,,"['bhavya vasudeva', 'puneesh deora', 'christos thrampoulidis']"
2402.05758,latent variable model for high-dimensional point process with structured   missingness,cs.lg stat.ml,"longitudinal data are important in numerous fields, such as healthcare, sociology and seismology, but real-world datasets present notable challenges for practitioners because they can be high-dimensional, contain structured missingness patterns, and measurement time points can be governed by an unknown stochastic process. while various solutions have been suggested, the majority of them have been designed to account for only one of these challenges. in this work, we propose a flexible and efficient latent-variable model that is capable of addressing all these limitations. our approach utilizes gaussian processes to capture temporal correlations between samples and their associated missingness masks as well as to model the underlying point process. we construct our model as a variational autoencoder together with deep neural network parameterised encoder and decoder models, and develop a scalable amortised variational inference approach for efficient model training. we demonstrate competitive performance using both simulated and real datasets.",,2024-02-08,,"['maksim sinelnikov', 'manuel haussmann', 'harri lähdesmäki']"
2402.05787,how do transformers perform in-context autoregressive learning?,stat.ml cs.lg,"transformers have achieved state-of-the-art performance in language modeling tasks. however, the reasons behind their tremendous success are still unclear. in this paper, towards a better understanding, we train a transformer model on a simple next token prediction task, where sequences are generated as a first-order autoregressive process $s_{t+1} = w s_t$. we show how a trained transformer predicts the next token by first learning $w$ in-context, then applying a prediction mapping. we call the resulting procedure in-context autoregressive learning. more precisely, focusing on commuting orthogonal matrices $w$, we first show that a trained one-layer linear transformer implements one step of gradient descent for the minimization of an inner objective function, when considering augmented tokens. when the tokens are not augmented, we characterize the global minima of a one-layer diagonal linear multi-head transformer. importantly, we exhibit orthogonality between heads and show that positional encoding captures trigonometric relations in the data. on the experimental side, we consider the general case of non-commuting orthogonal matrices and generalize our theoretical findings.",,2024-02-08,,"['michael e. sander', 'raja giryes', 'taiji suzuki', 'mathieu blondel', 'gabriel peyré']"
2402.05802,unsupervised discovery of clinical disease signatures using   probabilistic independence,cs.lg stat.ap stat.ml,"insufficiently precise diagnosis of clinical disease is likely responsible for many treatment failures, even for common conditions and treatments. with a large enough dataset, it may be possible to use unsupervised machine learning to define clinical disease patterns more precisely. we present an approach to learning these patterns by using probabilistic independence to disentangle the imprint on the medical record of causal latent sources of disease. we inferred a broad set of 2000 clinical signatures of latent sources from 9195 variables in 269,099 electronic health records. the learned signatures produced better discrimination than the original variables in a lung cancer prediction task unknown to the inference algorithm, predicting 3-year malignancy in patients with no history of cancer before a solitary lung nodule was discovered. more importantly, the signatures' greater explanatory power identified pre-nodule signatures of apparently undiagnosed cancer in many of those patients.",,2024-02-08,,"['thomas a. lasko', 'john m. still', 'thomas z. li', 'marco barbero mota', 'william w. stead', 'eric v. strobl', 'bennett a. landman', 'fabien maldonado']"
2402.05806,on calibration and conformal prediction of deep classifiers,cs.lg stat.ml,"in many classification applications, the prediction of a deep neural network (dnn) based classifier needs to be accompanied with some confidence indication. two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (cp): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. while in practice both types of indications can be desired, so far the interplay between them has not been investigated. toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent cp methods. we start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive cp methods: it frequently leads to larger prediction sets. then, we turn to theoretically analyze this behavior. we reveal several mathematical properties of the procedure, according to which we provide a reasoning for the phenomenon. our study suggests that it may be worthwhile to utilize adaptive cp methods, chosen for their enhanced conditional coverage, based on softmax values prior to (or after canceling) temperature scaling calibration.",,2024-02-08,,"['lahav dabah', 'tom tirer']"
2402.05835,how much is unseen depends chiefly on information about the seen,cs.lg cs.ne stat.ml,"it might seem counter-intuitive at first: we find that, in expectation, the proportion of data points in an unknown population-that belong to classes that do not appear in the training data-is almost entirely determined by the number $f_k$ of classes that do appear in the training data the same number of times. while in theory we show that the difference of the induced estimator decays exponentially in the size of the sample, in practice the high variance prevents us from using it directly for an estimator of the sample coverage. however, our precise characterization of the dependency between $f_k$'s induces a large search space of different representations of the expected value, which can be deterministically instantiated as estimators. hence, we turn to optimization and develop a genetic algorithm that, given only the sample, searches for an estimator with minimal mean-squared error (mse). in our experiments, our genetic algorithm discovers estimators that have a substantially smaller mse than the state-of-the-art good-turing estimator. this holds for over 96% of runs when there are at least as many samples as classes. our estimators' mse is roughly 80% of the good-turing estimator's.",,2024-02-08,,"['seongmin lee', 'marcel böhme']"
2402.05862,let your graph do the talking: encoding structured data for llms,cs.lg cs.ai cs.si stat.ml,"how can we best encode structured data into sequential form for use in large language models (llms)? in this work, we introduce a parameter-efficient method to explicitly represent structured data for llms. our method, graphtoken, learns an encoding function to extend prompts with explicit structured information. unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. we show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the graphqa benchmark.",,2024-02-08,,"['bryan perozzi', 'bahare fatemi', 'dustin zelle', 'anton tsitsulin', 'mehran kazemi', 'rami al-rfou', 'jonathan halcrow']"
2402.05876,federated offline reinforcement learning: collaborative single-policy   coverage suffices,cs.lg cs.ma stat.ml,"offline reinforcement learning (rl), which seeks to learn an optimal policy using offline data, has garnered significant interest due to its potential in critical applications where online data collection is infeasible or expensive. this work explores the benefit of federated learning for offline rl, aiming at collaboratively leveraging offline datasets at multiple agents. focusing on finite-horizon episodic tabular markov decision processes (mdps), we design fedlcb-q, a variant of the popular model-free q-learning algorithm tailored for federated offline rl. fedlcb-q updates local q-functions at agents with novel learning rate schedules and aggregates them at a central server using importance averaging and a carefully designed pessimistic penalty term. our sample complexity analysis reveals that, with appropriately chosen parameters and synchronization schedules, fedlcb-q achieves linear speedup in terms of the number of agents without requiring high-quality datasets at individual agents, as long as the local datasets collectively cover the state-action space visited by the optimal policy, highlighting the power of collaboration in the federated setting. in fact, the sample complexity almost matches that of the single-agent counterpart, as if all the data are stored at a central location, up to polynomial factors of the horizon length. furthermore, fedlcb-q is communication-efficient, where the number of communication rounds is only linear with respect to the horizon length up to logarithmic factors.",,2024-02-08,,"['jiin woo', 'laixi shi', 'gauri joshi', 'yuejie chi']"
2402.05878,prior-dependent allocations for bayesian fixed-budget best-arm   identification in structured bandits,stat.ml cs.lg,"we study the problem of bayesian fixed-budget best-arm identification (bai) in structured bandits. we propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment. we provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical bai. our key contribution is introducing new proof methods that result in tighter bounds for multi-armed bai compared to existing methods. we extensively compare our approach to other fixed-budget bai methods, demonstrating its consistent and robust performance in various settings. our work improves our understanding of bayesian fixed-budget bai in structured bandits and highlights the effectiveness of our approach in practical scenarios.",,2024-02-08,,"['nicolas nguyen', 'imad aouali', 'andrás györgy', 'claire vernade']"
2402.05928,sharp rates in dependent learning theory: avoiding sample size deflation   for the square loss,cs.lg stat.ml,"in this work, we study statistical learning with dependent ($\beta$-mixing) data and square loss in a hypothesis class $\mathscr{f}\subset l_{\psi_p}$ where $\psi_p$ is the norm $\|f\|_{\psi_p} \triangleq \sup_{m\geq 1} m^{-1/p} \|f\|_{l^m} $ for some $p\in [2,\infty]$. our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data. absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated \emph{multiplicatively} by the mixing time of the underlying covariates process. we show that whenever the topologies of $l^2$ and $\psi_p$ are comparable on our hypothesis class $\mathscr{f}$ -- that is, $\mathscr{f}$ is a weakly sub-gaussian class: $\|f\|_{\psi_p} \lesssim \|f\|_{l^2}^\eta$ for some $\eta\in (0,1]$ -- the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. our result holds whether the problem is realizable or not and we refer to this as a \emph{near mixing-free rate}, since direct dependence on mixing is relegated to an additive higher order term. we arrive at our result by combining the above notion of a weakly sub-gaussian class with mixed tail generic chaining. this combination allows us to compute sharp, instance-optimal rates for a wide range of problems. %our approach, reliant on mixed tail generic chaining, allows us to obtain sharp, instance-optimal rates. examples that satisfy our framework include sub-gaussian linear regression, more general smoothly parameterized function classes, finite hypothesis classes, and bounded smoothness classes.",,2024-02-08,,"['ingvar ziemann', 'stephen tu', 'george j. pappas', 'nikolai matni']"
2402.06004,memory-efficient vision transformers: an activation-aware mixed-rank   compression strategy,cs.cv cs.ai stat.ml,"as vision transformers (vits) increasingly set new benchmarks in computer vision, their practical deployment on inference engines is often hindered by their significant memory bandwidth and (on-chip) memory footprint requirements. this paper addresses this memory limitation by introducing an activation-aware model compression methodology that uses selective low-rank weight tensor approximations of different layers to reduce the parameter count of vits. the key idea is to decompose the weight tensors into a sum of two parameter-efficient tensors while minimizing the error between the product of the input activations with the original weight tensor and the product of the input activations with the approximate tensor sum. this approximation is further refined by adopting an efficient layer-wise error compensation technique that uses the gradient of the layer's output loss. the combination of these techniques achieves excellent results while it avoids being trapped in a shallow local minimum early in the optimization process and strikes a good balance between the model compression and output accuracy. notably, the presented method significantly reduces the parameter count of deit-b by 60% with less than 1% accuracy drop on the imagenet dataset, overcoming the usual accuracy degradation seen in low-rank approximations. in addition to this, the presented compression technique can compress large deit/vit models to have about the same model size as smaller deit/vit variants while yielding up to 1.8% accuracy gain. these results highlight the efficacy of our approach, presenting a viable solution for embedding vits in memory-constrained environments without compromising their performance.",,2024-02-08,,"['seyedarmin azizi', 'mahdi nazemi', 'massoud pedram']"
2402.06010,npsvc++: nonparallel classifiers encounter representation learning,cs.lg stat.ml,"this paper focuses on a specific family of classifiers called nonparallel support vector classifiers (npsvcs). different from typical classifiers, the training of an npsvc involves the minimization of multiple objectives, resulting in the potential concerns of feature suboptimality and class dependency. consequently, no effective learning scheme has been established to improve npsvcs' performance through representation learning, especially deep learning. to break this bottleneck, we develop npsvc++ based on multi-objective optimization, enabling the end-to-end learning of npsvc and its features. by pursuing pareto optimality, npsvc++ theoretically ensures feature optimality across classes, hence effectively overcoming the two issues above. a general learning procedure via duality optimization is proposed, based on which we provide two applicable instances, k-npsvc++ and d-npsvc++. the experiments show their superiority over the existing methods and verify the efficacy of npsvc++.",,2024-02-08,,"['junhong zhang', 'zhihui lai', 'jie zhou', 'guangfei liang']"
2402.06019,checking the sufficiently scattered condition using a global non-convex   optimization software,cs.lg eess.sp math.oc stat.ml,"the sufficiently scattered condition (ssc) is a key condition in the study of identifiability of various matrix factorization problems, including nonnegative, minimum-volume, symmetric, simplex-structured, and polytopic matrix factorizations. the ssc allows one to guarantee that the computed matrix factorization is unique/identifiable, up to trivial ambiguities. however, this condition is np-hard to check in general. in this paper, we show that it can however be checked in a reasonable amount of time in realistic scenarios, when the factorization rank is not too large. this is achieved by formulating the problem as a non-convex quadratic optimization problem over a bounded set. we use the global non-convex optimization software gurobi, and showcase the usefulness of this code on synthetic data sets and on real-world hyperspectral images.",,2024-02-08,,"['nicolas gillis', 'robert luce']"
2402.06031,an operator learning perspective on parameter-to-observable maps,cs.lg math.st stat.ml stat.th,"computationally efficient surrogates for parametrized physical models play a crucial role in science and engineering. operator learning provides data-driven surrogates that map between function spaces. however, instead of full-field measurements, often the available data are only finite-dimensional parametrizations of model inputs or finite observables of model outputs. building off of fourier neural operators, this paper introduces the fourier neural mappings (fnms) framework that is able to accommodate such finite-dimensional inputs and outputs. the paper develops universal approximation theorems for the method. moreover, in many applications the underlying parameter-to-observable (pto) map is defined implicitly through an infinite-dimensional operator, such as the solution operator of a partial differential equation. a natural question is whether it is more data-efficient to learn the pto map end-to-end or first learn the solution operator and subsequently compute the observable from the full-field solution. a theoretical analysis of bayesian nonparametric regression of linear functionals, which is of independent interest, suggests that the end-to-end approach can actually have worse sample complexity. extending beyond the theory, numerical results for the fnm approximation of three nonlinear pto maps demonstrate the benefits of the operator learning perspective that this paper adopts.",,2024-02-08,,"['daniel zhengyu huang', 'nicholas h. nelsen', 'margaret trautner']"
2402.06121,iterated denoising energy matching for sampling from boltzmann densities,cs.lg stat.ml,"efficiently generating statistically independent samples from an unnormalized probability distribution, such as equilibrium samples of many-body systems, is a foundational problem in science. in this paper, we propose iterated denoising energy matching (idem), an iterative algorithm that uses a novel stochastic score matching objective leveraging solely the energy function and its gradient -- and no data samples -- to train a diffusion-based sampler. specifically, idem alternates between (i) sampling regions of high model density from a diffusion-based sampler and (ii) using these samples in our stochastic matching objective to further improve the sampler. idem is scalable to high dimensions as the inner matching objective, is simulation-free, and requires no mcmc samples. moreover, by leveraging the fast mode mixing behavior of diffusion, idem smooths out the energy landscape enabling efficient exploration and learning of an amortized sampler. we evaluate idem on a suite of tasks ranging from standard synthetic energy functions to invariant $n$-body particle systems. we show that the proposed approach achieves state-of-the-art performance on all metrics and trains $2-5\times$ faster, which allows it to be the first method to train using energy on the challenging $55$-particle lennard-jones system.",,2024-02-08,,"['tara akhound-sadegh', 'jarrid rector-brooks', 'avishek joey bose', 'sarthak mittal', 'pablo lemos', 'cheng-hao liu', 'marcin sendera', 'siamak ravanbakhsh', 'gauthier gidel', 'yoshua bengio', 'nikolay malkin', 'alexander tong']"
2402.06122,"peeking with peak: sequential, nonparametric composite hypothesis tests   for means of multiple data streams",stat.me cs.lg stat.ml,"we propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. our proposed method, \emph{peeking with expectation-based averaged capital} (peak), builds upon the testing-as-betting framework and provides a non-asymptotic $\alpha$-level test across any stopping time. peak is computationally tractable and efficiently rejects hypotheses that are incorrect across all potential distributions that satisfy our nonparametric assumption, enabling joint composite hypothesis testing on multiple streams of data. we numerically validate our theoretical findings under the best arm identification and threshold identification in the bandit setting, illustrating both the competitive performance and the computational efficiency of our method against state-of-the-art testing methods.",,2024-02-08,2024-02-16,"['brian cho', 'kyra gan', 'nathan kallus']"
2402.06151,potec: off-policy learning for large action spaces via two-stage policy   decomposition,stat.ml cs.lg,"we study off-policy learning (opl) of contextual bandit policies in large discrete action spaces where existing methods -- most of which rely crucially on reward-regression models or importance-weighted policy gradients -- fail due to excessive bias or variance. to overcome these issues in opl, we propose a novel two-stage algorithm, called policy optimization via two-stage policy decomposition (potec). it leverages clustering in the action space and learns two different policies via policy- and regression-based approaches, respectively. in particular, we derive a novel low-variance gradient estimator that enables to learn a first-stage policy for cluster selection efficiently via a policy-based approach. to select a specific action within the cluster sampled by the first-stage policy, potec uses a second-stage policy derived from a regression-based approach within each cluster. we show that a local correctness condition, which only requires that the regression model preserves the relative expected reward differences of the actions within each cluster, ensures that our policy-gradient estimator is unbiased and the second-stage policy is optimal. we also show that potec provides a strict generalization of policy- and regression-based approaches and their associated assumptions. comprehensive experiments demonstrate that potec provides substantial improvements in opl effectiveness particularly in large and structured action spaces.",,2024-02-08,,"['yuta saito', 'jihan yao', 'thorsten joachims']"
2402.06160,improved evidential deep learning via a mixture of dirichlet   distributions,cs.lg stat.ml,"this paper explores a modern predictive uncertainty estimation approach, called evidential deep learning (edl), in which a single neural network model is trained to learn a meta distribution over the predictive distribution by minimizing a specific objective function. despite their strong empirical performance, recent studies by bengs et al. identify a fundamental pitfall of the existing methods: the learned epistemic uncertainty may not vanish even in the infinite-sample limit. we corroborate the observation by providing a unifying view of a class of widely used objectives from the literature. our analysis reveals that the edl methods essentially train a meta distribution by minimizing a certain divergence measure between the distribution and a sample-size-independent target distribution, resulting in spurious epistemic uncertainty. grounded in theoretical principles, we propose learning a consistent target distribution by modeling it with a mixture of dirichlet distributions and learning via variational inference. afterward, a final meta distribution model distills the learned uncertainty from the target model. experimental results across various uncertainty-based downstream tasks demonstrate the superiority of our proposed method, and illustrate the practical implications arising from the consistency and inconsistency of learned epistemic uncertainty.",,2024-02-08,,"['j. jon ryu', 'maohao shen', 'soumya ghosh', 'yuheng bu', 'prasanna sattigeri', 'subhro das', 'gregory w. wornell']"
2402.06162,wasserstein proximal operators describe score-based generative models   and resolve memorization,stat.ml cs.lg,"we focus on the fundamental mathematical structure of score-based generative models (sgms). we first formulate sgms in terms of the wasserstein proximal operator (wpo) and demonstrate that, via mean-field games (mfgs), the wpo formulation reveals mathematical structure that describes the inductive bias of diffusion and score-based models. in particular, mfgs yield optimality conditions in the form of a pair of coupled partial differential equations: a forward-controlled fokker-planck (fp) equation, and a backward hamilton-jacobi-bellman (hjb) equation. via a cole-hopf transformation and taking advantage of the fact that the cross-entropy can be related to a linear functional of the density, we show that the hjb equation is an uncontrolled fp equation. second, with the mathematical structure at hand, we present an interpretable kernel-based model for the score function which dramatically improves the performance of sgms in terms of training samples and training time. in addition, the wpo-informed kernel model is explicitly constructed to avoid the recently studied memorization effects of score-based generative models. the mathematical form of the new kernel-based models in combination with the use of the terminal condition of the mfg reveals new explanations for the manifold learning and generalization properties of sgms, and provides a resolution to their memorization effects. finally, our mathematically informed, interpretable kernel-based model suggests new scalable bespoke neural network architectures for high-dimensional applications.",,2024-02-08,,"['benjamin j. zhang', 'siting liu', 'wuchen li', 'markos a. katsoulakis', 'stanley j. osher']"
2402.06173,smc is all you need: parallel strong scaling,stat.ml cs.lg stat.co,"in the general framework of bayesian inference, the target distribution can only be evaluated up-to a constant of proportionality. classical consistent bayesian methods such as sequential monte carlo (smc) and markov chain monte carlo (mcmc) have unbounded time complexity requirements. we develop a fully parallel sequential monte carlo (psmc) method which provably delivers parallel strong scaling, i.e. the time complexity (and per-node memory) remains bounded if the number of asynchronous processes is allowed to grow. more precisely, the psmc has a theoretical convergence rate of mse$ = o(1/nr)$, where $n$ denotes the number of communicating samples in each processor and $r$ denotes the number of processors. in particular, for suitably-large problem-dependent $n$, as $r \rightarrow \infty$ the method converges to infinitesimal accuracy mse$=o(\varepsilon^2)$ with a fixed finite time-complexity cost$=o(1)$ and with no efficiency leakage, i.e. computational complexity cost$=o(\varepsilon^{-2})$. a number of bayesian inference problems are taken into consideration to compare the psmc and mcmc methods.",,2024-02-08,,"['xinzhu liang', 'sanjaya lohani', 'joseph m. lukens', 'brian t. kirby', 'thomas a. searles', 'kody j. h. law']"
2402.06223,revealing multimodal contrastive representation learning through latent   partial causal models,cs.lg cs.cv stat.ml,"multimodal contrastive representation learning methods have proven successful across a range of domains, partly due to their ability to generate meaningful shared representations of complex phenomena. to enhance the depth of analysis and understanding of these acquired representations, we introduce a unified causal model specifically designed for multimodal data. by examining this model, we show that multimodal contrastive representation learning excels at identifying latent coupled variables within the proposed unified model, up to linear or permutation transformations resulting from different assumptions. our findings illuminate the potential of pre-trained multimodal models, eg, clip, in learning disentangled representations through a surprisingly simple yet highly effective tool: linear independent component analysis. experiments demonstrate the robustness of our findings, even when the assumptions are violated, and validate the effectiveness of the proposed method in learning disentangled representations.",,2024-02-09,,"['yuhang liu', 'zhen zhang', 'dong gong', 'biwei huang', 'mingming gong', 'anton van den hengel', 'kun zhang', 'javen qinfeng shi']"
2402.06276,safe active learning for time-series modeling with gaussian processes,cs.lg stat.ml,"learning time-series models is useful for many applications, such as simulation and forecasting. in this study, we consider the problem of actively learning time-series models while taking given safety constraints into account. for time-series modeling we employ a gaussian process with a nonlinear exogenous input structure. the proposed approach generates data appropriate for time series model learning, i.e. input and output trajectories, by dynamically exploring the input space. the approach parametrizes the input trajectory as consecutive trajectory sections, which are determined stepwise given safety requirements and past observations. we analyze the proposed algorithm and evaluate it empirically on a technical application. the results show the effectiveness of our approach in a realistic technical use case.",,2024-02-09,,"['christoph zimmer', 'mona meister', 'duy nguyen-tuong']"
2402.06293,probabilistic forecasting of irregular time series via conditional flows,cs.lg stat.ml,"probabilistic forecasting of irregularly sampled multivariate time series with missing values is an important problem in many fields, including health care, astronomy, and climate. state-of-the-art methods for the task estimate only marginal distributions of observations in single channels and at single timepoints, assuming a fixed-shape parametric distribution. in this work, we propose a novel model, profiti, for probabilistic forecasting of irregularly sampled time series with missing values using conditional normalizing flows. the model learns joint distributions over the future values of the time series conditioned on past observations and queried channels and times, without assuming any fixed shape of the underlying distribution. as model components, we introduce a novel invertible triangular attention layer and an invertible non-linear activation function on and onto the whole real line. we conduct extensive experiments on four datasets and demonstrate that the proposed model provides $4$ times higher likelihood over the previously best model.",,2024-02-09,,"['vijaya krishna yalavarthi', 'randolf scholz', 'stefan born', 'lars schmidt-thieme']"
2402.06320,particle denoising diffusion sampler,stat.ml cs.lg stat.co,"denoising diffusion models have become ubiquitous for generative modeling. the core idea is to transport the data distribution to a gaussian by using a diffusion. approximate samples from the data distribution are then obtained by estimating the time-reversal of this diffusion using score matching ideas. we follow here a similar strategy to sample from unnormalized probability densities and compute their normalizing constants. however, the time-reversed diffusion is here simulated by using an original iterative particle scheme relying on a novel score matching loss. contrary to standard denoising diffusion models, the resulting particle denoising diffusion sampler (pdds) provides asymptotically consistent estimates under mild assumptions. we demonstrate pdds on multimodal and high dimensional sampling tasks.",,2024-02-09,,"['angus phillips', 'hai-dang dau', 'michael john hutchinson', 'valentin de bortoli', 'george deligiannidis', 'arnaud doucet']"
2402.06323,how uniform random weights induce non-uniform bias: typical   interpolating neural networks generalize with narrow teachers,cs.lg stat.ml,"background. a main theoretical puzzle is why over-parameterized neural networks (nns) generalize well when trained to zero loss (i.e., so they interpolate the data). usually, the nn is trained with stochastic gradient descent (sgd) or one of its variants. however, recent empirical work examined the generalization of a random nn that interpolates the data: the nn was sampled from a seemingly uniform prior over the parameters, conditioned on that the nn perfectly classifying the training set. interestingly, such a nn sample typically generalized as well as sgd-trained nns.   contributions. we prove that such a random nn interpolator typically generalizes well if there exists an underlying narrow ``teacher nn"" that agrees with the labels. specifically, we show that such a `flat' prior over the nn parametrization induces a rich prior over the nn functions, due to the redundancy in the nn structure. in particular, this creates a bias towards simpler functions, which require less relevant parameters to represent -- enabling learning with a sample complexity approximately proportional to the complexity of the teacher (roughly, the number of non-redundant parameters), rather than the student's.",,2024-02-09,,"['gon buzaglo', 'itamar harel', 'mor shpigel nacson', 'alon brutzkus', 'nathan srebro', 'daniel soudry']"
2402.06348,fairness of exposure in online restless multi-armed bandits,cs.lg stat.ml,"restless multi-armed bandits (rmabs) generalize the multi-armed bandits where each arm exhibits markovian behavior and transitions according to their transition dynamics. solutions to rmab exist for both offline and online cases. however, they do not consider the distribution of pulls among the arms. studies have shown that optimal policies lead to unfairness, where some arms are not exposed enough. existing works in fairness in rmabs focus heavily on the offline case, which diminishes their application in real-world scenarios where the environment is largely unknown. in the online scenario, we propose the first fair rmab framework, where each arm receives pulls in proportion to its merit. we define the merit of an arm as a function of its stationary reward distribution. we prove that our algorithm achieves sublinear fairness regret in the single pull case $o(\sqrt{t\ln t})$, with $t$ being the total number of episodes. empirically, we show that our algorithm performs well in the multi-pull scenario as well.",,2024-02-09,,"['archit sood', 'shweta jain', 'sujit gujar']"
2402.06380,optimal estimation of gaussian (poly)trees,cs.lg stat.ml,"we develop optimal algorithms for learning undirected gaussian trees and directed gaussian polytrees from data. we consider both problems of distribution learning (i.e. in kl distance) and structure learning (i.e. exact recovery). the first approach is based on the chow-liu algorithm, and learns an optimal tree-structured distribution efficiently. the second approach is a modification of the pc algorithm for polytrees that uses partial correlation as a conditional independence tester for constraint-based structure learning. we derive explicit finite-sample guarantees for both approaches, and show that both approaches are optimal by deriving matching lower bounds. additionally, we conduct numerical experiments to compare the performance of various algorithms, providing further insights and empirical evidence.",,2024-02-09,,"['yuhao wang', 'ming gao', 'wai ming tai', 'bryon aragam', 'arnab bhattacharyya']"
2402.06386,boosting-based sequential meta-tree ensemble construction for improved   decision trees,stat.ml cs.lg,"a decision tree is one of the most popular approaches in machine learning fields. however, it suffers from the problem of overfitting caused by overly deepened trees. then, a meta-tree is recently proposed. it solves the problem of overfitting caused by overly deepened trees. moreover, the meta-tree guarantees statistical optimality based on bayes decision theory. therefore, the meta-tree is expected to perform better than the decision tree. in contrast to a single decision tree, it is known that ensembles of decision trees, which are typically constructed boosting algorithms, are more effective in improving predictive performance. thus, it is expected that ensembles of meta-trees are more effective in improving predictive performance than a single meta-tree, and there are no previous studies that construct multiple meta-trees in boosting. therefore, in this study, we propose a method to construct multiple meta-trees using a boosting approach. through experiments with synthetic and benchmark datasets, we conduct a performance comparison between the proposed methods and the conventional methods using ensembles of decision trees. furthermore, while ensembles of decision trees can cause overfitting as well as a single decision tree, experiments confirmed that ensembles of meta-trees can prevent overfitting due to the tree depth.",,2024-02-09,,"['ryota maniwa', 'naoki ichijo', 'yuta nakahara', 'toshiyasu matsushima']"
2402.06388,on the convergence rate of the stochastic gradient descent (sgd) and   application to a modified policy gradient for the multi armed bandit,stat.ml cs.ai cs.ds cs.lg cs.na math.na,we present a self-contained proof of the convergence rate of the stochastic gradient descent (sgd) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient multi-armed bandit (mab) with $l2$ regularization.,,2024-02-09,,"['stefana anita', 'gabriel turinici']"
2402.06412,improving the worst-case bidirectional communication complexity for   nonconvex distributed optimization under function similarity,math.oc cs.lg stat.ml,"effective communication between the server and workers plays a key role in distributed optimization. in this paper, we focus on optimizing the server-to-worker communication, uncovering inefficiencies in prevalent downlink compression approaches. considering first the pure setup where the uplink communication costs are negligible, we introduce marina-p, a novel method for downlink compression, employing a collection of correlated compressors. theoretical analyses demonstrates that marina-p with permutation compressors can achieve a server-to-worker communication complexity improving with the number of workers, thus being provably superior to existing algorithms. we further show that marina-p can serve as a starting point for extensions such as methods supporting bidirectional compression. we introduce m3, a method combining marina-p with uplink compression and a momentum step, achieving bidirectional compression with provable improvements in total communication complexity as the number of workers increases. theoretical findings align closely with empirical experiments, underscoring the efficiency of the proposed algorithms.",,2024-02-09,,"['kaja gruntkowska', 'alexander tyurin', 'peter richtárik']"
2402.06434,where is the truth? the risk of getting confounded in a continual world,cs.lg stat.ml,"a dataset is confounded if it is most easily solved via a spurious correlation which fails to generalize to new data. we will show that, in a continual learning setting where confounders may vary in time across tasks, the resulting challenge far exceeds the standard forgetting problem normally considered. in particular, we derive mathematically the effect of such confounders on the space of valid joint solutions to sets of confounded tasks. interestingly, our theory predicts that for many such continual datasets, spurious correlations are easily ignored when the tasks are trained on jointly, but it is far harder to avoid confounding when they are considered sequentially. we construct such a dataset and demonstrate empirically that standard continual learning methods fail to ignore confounders, while training jointly on all tasks is successful. our continually confounded dataset, concon, is based on clevr images and demonstrates the need for continual learning methods with more robust behavior with respect to confounding.",,2024-02-09,,"['florian peter busch', 'roshni kamath', 'rupert mitchell', 'wolfgang stammer', 'kristian kersting', 'martin mundt']"
2402.06461,sequential flow straightening for generative modeling,cs.lg cs.cv stat.ml,"straightening the probability flow of the continuous-time generative models, such as diffusion models or flow-based models, is the key to fast sampling through the numerical solvers, existing methods learn a linear path by directly generating the probability path the joint distribution between the noise and data distribution. one key reason for the slow sampling speed of the ode-based solvers that simulate these generative models is the global truncation error of the ode solver, caused by the high curvature of the ode trajectory, which explodes the truncation error of the numerical solvers in the low-nfe regime. to address this challenge, we propose a novel method called seqrf, a learning technique that straightens the probability flow to reduce the global truncation error and hence enable acceleration of sampling and improve the synthesis quality. in both theoretical and empirical studies, we first observe the straightening property of our seqrf. through empirical evaluations via seqrf over flow-based generative models, we achieve surpassing results on cifar-10, celeba-$64 \times 64$, and lsun-church datasets.",,2024-02-09,2024-02-14,"['jongmin yoon', 'juho lee']"
2402.06525,flexible infinite-width graph convolutional networks and the importance   of representation learning,stat.ml cs.lg,"a common theoretical approach to understanding neural networks is to take an infinite-width limit, at which point the outputs become gaussian process (gp) distributed. this is known as a neural network gaussian process (nngp). however, the nngp kernel is fixed, and tunable only through a small number of hyperparameters, eliminating any possibility of representation learning. this contrasts with finite-width nns, which are often believed to perform well precisely because they are able to learn representations. thus in simplifying nns to make them theoretically tractable, nngps may eliminate precisely what makes them work well (representation learning). this motivated us to understand whether representation learning is necessary in a range of graph classification tasks. we develop a precise tool for this task, the graph convolutional deep kernel machine. this is very similar to an nngp, in that it is an infinite width limit and uses kernels, but comes with a `knob' to control the amount of representation learning. we found that representation learning is necessary (in the sense that it gives dramatic performance improvements) in graph classification tasks and heterophilous node classification tasks, but not in homophilous node classification tasks.",,2024-02-09,,"['ben anson', 'edward milsom', 'laurence aitchison']"
2402.06535,bandit convex optimisation,math.oc cs.lg stat.ml,"bandit convex optimisation is a fundamental framework for studying zeroth-order convex optimisation. these notes cover the many tools used for this problem, including cutting plane methods, interior point methods, continuous exponential weights, gradient descent and online newton step. the nuances between the many assumptions and setups are explained. although there is not much truly new here, some existing tools are applied in novel ways to obtain new algorithms. a few bounds are improved in minor ways.",,2024-02-09,,['tor lattimore']
2402.06578,on the universality of coupling-based normalizing flows,cs.lg stat.ml,"we present a novel theoretical framework for understanding the expressive power of coupling-based normalizing flows such as realnvp. despite their prevalence in scientific applications, a comprehensive understanding of coupling flows remains elusive due to their restricted architectures. existing theorems fall short as they require the use of arbitrarily ill-conditioned neural networks, limiting practical applicability. additionally, we demonstrate that these constructions inherently lead to volume-preserving flows, a property which we show to be a fundamental constraint for expressivity. we propose a new distributional universality theorem for coupling-based normalizing flows, which overcomes several limitations of prior work. our results support the general wisdom that the coupling architecture is expressive and provide a nuanced view for choosing the expressivity of coupling functions, bridging a gap between empirical results and theoretical understanding.",,2024-02-09,,"['felix draxler', 'stefan wahl', 'christoph schnörr', 'ullrich köthe']"
2402.06614,the complexity of sequential prediction in dynamical systems,cs.lg stat.ml,"we study the problem of learning to predict the next state of a dynamical system when the underlying evolution function is unknown. unlike previous work, we place no parametric assumptions on the dynamical system, and study the problem from a learning theory perspective. we define new combinatorial measures and dimensions and show that they quantify the optimal mistake and regret bounds in the realizable and agnostic setting respectively.",,2024-02-09,,"['vinod raman', 'unique subedi', 'ambuj tewari']"
2402.06756,convergence of gradient descent with small initialization for   unregularized matrix completion,cs.lg math.oc stat.ml,"we study the problem of symmetric matrix completion, where the goal is to reconstruct a positive semidefinite matrix $\rm{x}^\star \in \mathbb{r}^{d\times d}$ of rank-$r$, parameterized by $\rm{u}\rm{u}^{\top}$, from only a subset of its observed entries. we show that the vanilla gradient descent (gd) with small initialization provably converges to the ground truth $\rm{x}^\star$ without requiring any explicit regularization. this convergence result holds true even in the over-parameterized scenario, where the true rank $r$ is unknown and conservatively over-estimated by a search rank $r'\gg r$. the existing results for this problem either require explicit regularization, a sufficiently accurate initial point, or exact knowledge of the true rank $r$.   in the over-parameterized regime where $r'\geq r$, we show that, with $\widetilde\omega(dr^9)$ observations, gd with an initial point $\|\rm{u}_0\| \leq \epsilon$ converges near-linearly to an $\epsilon$-neighborhood of $\rm{x}^\star$. consequently, smaller initial points result in increasingly accurate solutions. surprisingly, neither the convergence rate nor the final accuracy depends on the over-parameterized search rank $r'$, and they are only governed by the true rank $r$. in the exactly-parameterized regime where $r'=r$, we further enhance this result by proving that gd converges at a faster rate to achieve an arbitrarily small accuracy $\epsilon>0$, provided the initial point satisfies $\|\rm{u}_0\| = o(1/d)$. at the crux of our method lies a novel weakly-coupled leave-one-out analysis, which allows us to establish the global convergence of gd, extending beyond what was previously possible using the classical leave-one-out analysis.",,2024-02-09,,"['jianhao ma', 'salar fattahi']"
2402.06763,"scalable kernel logistic regression with nystr\""om approximation:   theoretical analysis and application to discrete choice modelling",cs.lg stat.ml,"the application of kernel-based machine learning (ml) techniques to discrete choice modelling using large datasets often faces challenges due to memory requirements and the considerable number of parameters involved in these models. this complexity hampers the efficient training of large-scale models. this paper addresses these problems of scalability by introducing the nystr\""om approximation for kernel logistic regression (klr) on large datasets. the study begins by presenting a theoretical analysis in which: i) the set of klr solutions is characterised, ii) an upper bound to the solution of klr with nystr\""om approximation is provided, and finally iii) a specialisation of the optimisation algorithms to nystr\""om klr is described. after this, the nystr\""om klr is computationally validated. four landmark selection methods are tested, including basic uniform sampling, a k-means sampling strategy, and two non-uniform methods grounded in leverage scores. the performance of these strategies is evaluated using large-scale transport mode choice datasets and is compared with traditional methods such as multinomial logit (mnl) and contemporary ml techniques. the study also assesses the efficiency of various optimisation techniques for the proposed nystr\""om klr model. the performance of gradient descent, momentum, adam, and l-bfgs-b optimisation methods is examined on these datasets. among these strategies, the k-means nystr\""om klr approach emerges as a successful solution for applying klr to large datasets, particularly when combined with the l-bfgs-b and adam optimisation methods. the results highlight the ability of this strategy to handle datasets exceeding 200,000 observations while maintaining robust performance.",,2024-02-09,,"['josé ángel martín-baos', 'ricardo garcía-ródenas', 'luis rodriguez-benitez', 'michel bierlaire']"
2402.06884,low-rank approximation of structural redundancy for self-supervised   learning,stat.ml cs.lg,"we study the data-generating mechanism for reconstructive ssl to shed light on its effectiveness. with an infinite amount of labeled samples, we provide a sufficient and necessary condition for perfect linear approximation. the condition reveals a full-rank component that preserves the label classes of y, along with a redundant component. motivated by the condition, we propose to approximate the redundant component by a low-rank factorization and measure the approximation quality by introducing a new quantity $\epsilon_s$, parameterized by the rank of factorization s. we incorporate $\epsilon_s$ into the excess risk analysis under both linear regression and ridge regression settings, where the latter regularization approach is to handle scenarios when the dimension of the learned features is much larger than the number of labeled samples n for downstream tasks. we design three stylized experiments to compare ssl with supervised learning under different settings to support our theoretical findings.",,2024-02-09,,"['kang du', 'yu xiang']"
2402.06886,principled penalty-based methods for bilevel reinforcement learning and   rlhf,cs.lg math.oc stat.ml,"bilevel optimization has been recently applied to many machine learning tasks. however, their applications have been restricted to the supervised learning setting, where static objective functions with benign structures are considered. but bilevel problems such as incentive design, inverse reinforcement learning (rl), and rl from human feedback (rlhf) are often modeled as dynamic objective functions that go beyond the simple static objective structures, which pose significant challenges of using existing bilevel solutions. to tackle this new class of bilevel problems, we introduce the first principled algorithmic framework for solving bilevel rl problems through the lens of penalty formulation. we provide theoretical studies of the problem landscape and its penalty-based (policy) gradient algorithms. we demonstrate the effectiveness of our algorithms via simulations in the stackelberg markov game, rl from human feedback and incentive design.",,2024-02-09,,"['han shen', 'zhuoran yang', 'tianyi chen']"
2402.06923,cochceps-augment: a novel self-supervised contrastive learning using   cochlear cepstrum-based masking for speech emotion recognition,eess.as cs.lg cs.sd eess.sp stat.ml,"self-supervised learning (ssl) for automated speech recognition in terms of its emotional content, can be heavily degraded by the presence noise, affecting the efficiency of modeling the intricate temporal and spectral informative structures of speech. recently, ssl on large speech datasets, as well as new audio-specific ssl proxy tasks, such as, temporal and frequency masking, have emerged, yielding superior performance compared to classic approaches drawn from the image augmentation domain. our proposed contribution builds upon this successful paradigm by introducing cochceps-augment, a novel bio-inspired masking augmentation task for self-supervised contrastive learning of speech representations. specifically, we utilize the newly introduced bio-inspired cochlear cepstrogram (ccgram) to derive noise robust representations of input speech, that are then further refined through a self-supervised learning scheme. the latter employs simclr to generate contrastive views of a ccgram through masking of its angle and quefrency dimensions. our experimental approach and validations on the emotion recognition k-emocon benchmark dataset, for the first time via a speaker-independent approach, features unsupervised pre-training, linear probing and fine-tuning. our results potentiate cochceps-augment to serve as a standard tool in speech emotion recognition analysis, showing the added value of incorporating bio-inspired masking as an informative augmentation task for self-supervision. our code for implementing cochceps-augment will be made available at: https://github.com/gianniszgs/cochcepsaugment.",,2024-02-10,,"['ioannis ziogas', 'hessa alfalahi', 'ahsan h. khandoker', 'leontios j. hadjileontiadis']"
2402.06940,efficient incremental belief updates using weighted virtual observations,stat.ml cs.lg,"we present an algorithmic solution to the problem of incremental belief updating in the context of monte carlo inference in bayesian statistical models represented by probabilistic programs. given a model and a sample-approximated posterior, our solution constructs a set of weighted observations to condition the model such that inference would result in the same posterior. this problem arises e.g. in multi-level modelling, incremental inference, inference in presence of privacy constraints. first, a set of virtual observations is selected, then, observation weights are found through a computationally efficient optimization procedure such that the reconstructed posterior coincides with or closely approximates the original posterior. we implement and apply the solution to a number of didactic examples and case studies, showing efficiency and robustness of our approach. the provided reference implementation is agnostic to the probabilistic programming language or the inference algorithm, and can be applied to most mainstream probabilistic programming environments.",,2024-02-10,,['david tolpin']
2402.06963,tree ensembles for contextual bandits,cs.lg cs.ai stat.ml,"we propose a novel framework for contextual multi-armed bandits based on tree ensembles. our framework integrates two widely used bandit methods, upper confidence bound and thompson sampling, for both standard and combinatorial settings. we demonstrate the effectiveness of our framework via several experimental studies, employing xgboost, a popular tree ensemble method. compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks.",,2024-02-10,,"['hannes nilsson', 'rikard johansson', 'niklas åkerblom', 'morteza haghir chehreghani']"
2402.07025,generalization error of graph neural networks in the mean-field regime,stat.ml cs.it cs.lg math.it,"this work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. we explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. we establish upper bounds with a convergence rate of $o(1/n)$, where $n$ is the number of graph samples. these upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our understanding of their performance.",,2024-02-10,,"['gholamali aminian', 'yixuan he', 'gesine reinert', 'łukasz szpruch', 'samuel n. cohen']"
2402.07048,logistic-beta processes for modeling dependent random probabilities with   beta marginals,stat.me stat.ml,"the beta distribution serves as a canonical tool for modeling probabilities and is extensively used in statistics and machine learning, especially in the field of bayesian nonparametrics. despite its widespread use, there is limited work on flexible and computationally convenient stochastic process extensions for modeling dependent random probabilities. we propose a novel stochastic process called the logistic-beta process, whose logistic transformation yields a stochastic process with common beta marginals. similar to the gaussian process, the logistic-beta process can model dependence on both discrete and continuous domains, such as space or time, and has a highly flexible dependence structure through correlation kernels. moreover, its normal variance-mean mixture representation leads to highly effective posterior inference algorithms. the flexibility and computational benefits of logistic-beta processes are demonstrated through nonparametric binary regression simulation studies. furthermore, we apply the logistic-beta process in modeling dependent dirichlet processes, and illustrate its application and benefits through bayesian density regression problems in a toxicology study.",,2024-02-10,,"['changwoo j. lee', 'alessandro zito', 'huiyan sang', 'david b. dunson']"
2402.07052,understanding the training speedup from sampling with approximate losses,cs.lg stat.ml,"it is well known that selecting samples with large losses/gradients can significantly reduce the number of training steps. however, the selection overhead is often too high to yield any meaningful gains in terms of overall training time. in this work, we focus on the greedy approach of selecting samples with large \textit{approximate losses} instead of exact losses in order to reduce the selection overhead. for smooth convex losses, we show that such a greedy strategy can converge to a constant factor of the minimum value of the average loss in fewer iterations than the standard approach of random selection. we also theoretically quantify the effect of the approximation level. we then develop sift which uses early exiting to obtain approximate losses with an intermediate layer's representations for sample selection. we evaluate sift on the task of training a 110m parameter 12-layer bert base model and show significant gains (in terms of training hours and number of backpropagation steps) without any optimized implementation over vanilla training. for e.g., to reach 64% validation accuracy, sift with exit at the first layer takes ~43 hours compared to ~57 hours of vanilla training.",,2024-02-10,,"['rudrajit das', 'xi chen', 'bertram ieong', 'parikshit bansal', 'sujay sanghavi']"
2402.07062,fast ucb-type algorithms for stochastic bandits with heavy and super   heavy symmetric noise,cs.lg math.oc stat.ml,"in this study, we propose a new method for constructing ucb-type algorithms for stochastic multi-armed bandits based on general convex optimization methods with an inexact oracle. we derive the regret bounds corresponding to the convergence rates of the optimization methods. we propose a new algorithm clipped-sgd-ucb and show, both theoretically and empirically, that in the case of symmetric noise in the reward, we can achieve an $o(\log t\sqrt{kt\log t})$ regret bound instead of $o\left (t^{\frac{1}{1+\alpha}} k^{\frac{\alpha}{1+\alpha}} \right)$ for the case when the reward distribution satisfies $\mathbb{e}_{x \in d}[|x|^{1+\alpha}] \leq \sigma^{1+\alpha}$ ($\alpha \in (0, 1])$, i.e. perform better than it is assumed by the general lower bound for bandits with heavy-tails. moreover, the same bound holds even when the reward distribution does not have the expectation, that is, when $\alpha<0$.",,2024-02-10,,"['yuriy dorn', 'aleksandr katrutsa', 'ilgam latypov', 'andrey pudovikov']"
2402.07082,refined sample complexity for markov games with independent linear   function approximation,cs.lg cs.gt stat.ml,"markov games (mg) is an important model for multi-agent reinforcement learning (marl). it was long believed that the ""curse of multi-agents"" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (daskalakis et al., 2023; cui et al., 2023; wang et al., 2023. while these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $o(t^{-1/4})$ or brought a polynomial dependency on the number of actions $a_{\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (dai et al., 2023). this paper first refines the `avlpr` framework by wang et al. (2023), with an insight of *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. when specialized to mgs with independent linear function approximations, we propose novel *action-dependent bonuses* to cover occasionally extreme estimation errors. with the help of state-of-the-art techniques from the single-agent rl literature, we give the first algorithm that tackles the curse of multi-agents, attains the optimal $o(t^{-1/2})$ convergence rate, and avoids $\text{poly}(a_{\max})$ dependency simultaneously.",,2024-02-10,,"['yan dai', 'qiwen cui', 'simon s. du']"
2402.07084,"codpy: a python library for numerics, machine learning, and statistics",math.na cs.na math.st stat.co stat.th,"this monograph offers an introduction to a collection of numerical algorithms implemented in the library codpy (an acronym that stands for the curse of dimensionality in python), which has found widespread applications across various areas, including machine learning, statistics, and computational physics. we develop here a strategy based on the theory of reproducing kernel hilbert spaces (rkhs) and the theory of optimal transport. initially designed for mathematical finance, this library has since been enhanced and broadened to be applicable to problems arising in engineering and industry. in order to present the general principles and techniques employed in codpy and its applications, we have structured this monograph into two main parts. first of all, we focus on the fundamental principles of kernel-based representations of data and solutions, also that the presentation therein is supplemented with illustrative examples only. next, we discuss the application of these principles to many classes of concrete problems, spanning from the numerical approximation of partial differential equations to (supervised, unsupervised) machine learning, extending to generative methods with a focus on stochastic aspects.",,2024-02-10,,"['philippe g. lefloch', 'jean-marc mercier', 'shohruh miryusupov']"
2402.07087,self-correcting self-consuming loops for generative model training,cs.lg cs.ai cs.cv stat.ml,"as synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates ""self-consuming loops"" which may lead to training instability or even collapse, unless certain conditions are met. our paper aims to stabilize self-consuming generative model training. our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. we then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. we empirically validate the effectiveness of self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids model collapse, even when the ratio of synthetic data to real data is as high as 100%.",,2024-02-10,,"['nate gillman', 'michael freeman', 'daksh aggarwal', 'chia-hong hsu', 'calvin luo', 'yonglong tian', 'chen sun']"
2402.07114,towards quantifying the preconditioning effect of adam,cs.lg cs.na math.na math.oc stat.ml,"there is a notable dearth of results characterizing the preconditioning effect of adam and showing how it may alleviate the curse of ill-conditioning -- an issue plaguing gradient descent (gd). in this work, we perform a detailed analysis of adam's preconditioning effect for quadratic functions and quantify to what extent adam can mitigate the dependence on the condition number of the hessian. our key finding is that adam can suffer less from the condition number but at the expense of suffering a dimension-dependent quantity. specifically, for a $d$-dimensional quadratic with a diagonal hessian having condition number $\kappa$, we show that the effective condition number-like quantity controlling the iteration complexity of adam without momentum is $\mathcal{o}(\min(d, \kappa))$. for a diagonally dominant hessian, we obtain a bound of $\mathcal{o}(\min(d \sqrt{d \kappa}, \kappa))$ for the corresponding quantity. thus, when $d < \mathcal{o}(\kappa^p)$ where $p = 1$ for a diagonal hessian and $p = 1/3$ for a diagonally dominant hessian, adam can outperform gd (which has an $\mathcal{o}(\kappa)$ dependence). on the negative side, our results suggest that adam can be worse than gd for a sufficiently non-diagonal hessian even if $d \ll \mathcal{o}(\kappa^{1/3})$; we corroborate this with empirical evidence. finally, we extend our analysis to functions satisfying per-coordinate lipschitz smoothness and a modified version of the polyak-\l ojasiewicz condition.",,2024-02-11,,"['rudrajit das', 'naman agarwal', 'sujay sanghavi', 'inderjit s. dhillon']"
2402.07131,resampling methods for private statistical inference,stat.ml cs.cr cs.lg stat.me,"we consider the task of constructing confidence intervals with differential privacy. we propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little'' bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. for a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. we empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.",,2024-02-11,,"['karan chadha', 'john duchi', 'rohith kuditipudi']"
2402.07160,pasoa- particle based bayesian optimal adaptive design,stat.ml cs.lg stat.co stat.me,"we propose a new procedure named pasoa, for bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference. the sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and sequential monte carlo (smc) samplers to maximise the expected information gain (eig). as larger information gains are obtained for larger distances between successive posterior distributions, this eig objective may worsen classical smc performance. to handle this issue, tempering is proposed to have both a large information gain and an accurate smc sampling, that we show is crucial for performance. this novel combination of stochastic optimization and tempered smc allows to jointly handle design optimization and parameter inference. we provide a proof that the obtained optimal design estimators benefit from some consistency property. numerical experiments confirm the potential of the approach, which outperforms other recent existing procedures.",,2024-02-11,,"['jacopo iollo', 'christophe heinkelé', 'pierre alliez', 'florence forbes']"
2402.07189,improving lsh via tensorized random projection,stat.ml cs.ds cs.lg,"locality sensitive hashing (lsh) is a fundamental algorithmic toolkit used by data scientists for approximate nearest neighbour search problems that have been used extensively in many large scale data processing applications such as near duplicate detection, nearest neighbour search, clustering, etc. in this work, we aim to propose faster and space efficient locality sensitive hash functions for euclidean distance and cosine similarity for tensor data. typically, the naive approach for obtaining lsh for tensor data involves first reshaping the tensor into vectors, followed by applying existing lsh methods for vector data $e2lsh$ and $srp$. however, this approach becomes impractical for higher order tensors because the size of the reshaped vector becomes exponential in the order of the tensor. consequently, the size of lsh parameters increases exponentially. to address this problem, we suggest two methods for lsh for euclidean distance and cosine similarity, namely $cp-e2lsh$, $tt-e2lsh$, and $cp-srp$, $tt-srp$, respectively, building on $cp$ and tensor train $(tt)$ decompositions techniques. our approaches are space efficient and can be efficiently applied to low rank $cp$ or $tt$ tensors. we provide a rigorous theoretical analysis of our proposal on their correctness and efficacy.",,2024-02-11,,"['bhisham dev verma', 'rameshwar pratap']"
2402.07193,the implicit bias of gradient noise: a symmetry perspective,cs.lg math.oc stat.ml,"we characterize the learning dynamics of stochastic gradient descent (sgd) when continuous symmetry exists in the loss function, where the divergence between sgd and gradient descent is dramatic. we show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. for one class of symmetry, sgd naturally converges to solutions that have a balanced and aligned gradient noise. for the other class of symmetry, sgd will almost always diverge. then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. we demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, matrix factorization, and the use of warmup.",,2024-02-11,,"['liu ziyin', 'mingze wang', 'lei wu']"
2402.07211,towards fast stochastic sampling in diffusion generative models,cs.lg stat.ml,"diffusion models suffer from slow sample generation at inference time. despite recent efforts, improving the sampling efficiency of stochastic samplers for diffusion models remains a promising direction. we propose splitting integrators for fast stochastic sampling in pre-trained diffusion models in augmented spaces. commonly used in molecular dynamics, splitting-based integrators attempt to improve sampling efficiency by cleverly alternating between numerical updates involving the data, auxiliary, or noise variables. however, we show that a naive application of splitting integrators is sub-optimal for fast sampling. consequently, we propose several principled modifications to naive splitting samplers for improving sampling efficiency and denote the resulting samplers as reduced splitting integrators. in the context of phase space langevin diffusion (psld) [pandey \& mandt, 2023] on cifar-10, our stochastic sampler achieves an fid score of 2.36 in only 100 network function evaluations (nfe) as compared to 2.63 for the best baselines.",,2024-02-11,2024-02-13,"['kushagra pandey', 'maja rudolph', 'stephan mandt']"
2402.07240,thresholded oja does sparse pca?,math.st cs.lg stat.ml stat.th,"we consider the problem of sparse principal component analysis (pca) when the ratio $d/n \rightarrow c > 0$. there has been a lot of work on optimal rates on sparse pca in the offline setting, where all the data is available for multiple passes. in contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $o(d)$ storage and $o(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error. we show that a simple algorithm that thresholds and renormalizes the output of oja's algorithm (the oja vector) obtains a near-optimal error rate. this is very surprising because, without thresholding, the oja vector has a large error. our analysis centers around bounding the entries of the unnormalized oja vector, which involves the projection of a product of independent random matrices on a random initial vector. this is nontrivial and novel since previous analyses of oja's algorithm and matrix products have been done when the trace of the population covariance matrix is bounded while in our setting, this quantity can be as large as $n$.",,2024-02-11,2024-02-20,"['syamantak kumar', 'purnamrita sarkar']"
2402.07248,depth separations in neural networks: separating the dimension from the   accuracy,cs.lg stat.ml,"we prove an exponential separation between depth 2 and depth 3 neural networks, when approximating an $\mathcal{o}(1)$-lipschitz target function to constant accuracy, with respect to a distribution with support in $[0,1]^{d}$, assuming exponentially bounded weights. this addresses an open problem posed in \citet{safran2019depth}, and proves that the curse of dimensionality manifests in depth 2 approximation, even in cases where the target function can be represented efficiently using depth 3. previously, lower bounds that were used to separate depth 2 from depth 3 required that at least one of the lipschitz parameter, target accuracy or (some measure of) the size of the domain of approximation scale polynomially with the input dimension, whereas we fix the former two and restrict our domain to the unit hypercube. our lower bound holds for a wide variety of activation functions, and is based on a novel application of an average- to worst-case random self-reducibility argument, to reduce the problem to threshold circuits lower bounds.",,2024-02-11,,"['itay safran', 'daniel reichman', 'paul valiant']"
2402.07296,estimating the mixing coefficients of geometrically ergodic markov   processes,math.st stat.ml stat.th,"we propose methods to estimate the individual $\beta$-mixing coefficients of a real-valued geometrically ergodic markov process from a single sample-path $x_0,x_1, \dots,x_n$. under standard smoothness conditions on the densities, namely, that the joint density of the pair $(x_0,x_m)$ for each $m$ lies in a besov space $b^s_{1,\infty}(\mathbb r^2)$ for some known $s>0$, we obtain a rate of convergence of order $\mathcal{o}(\log(n) n^{-[s]/(2[s]+2)})$ for the expected error of our estimator in this case\footnote{we use $[s]$ to denote the integer part of the decomposition $s=[s]+\{s\}$ of $s \in (0,\infty)$ into an integer term and a {\em strictly positive} remainder term $\{s\} \in (0,1]$.}. we complement this result with a high-probability bound on the estimation error, and further obtain analogues of these bounds in the case where the state-space is finite. naturally no density assumptions are required in this setting; the expected error rate is shown to be of order $\mathcal o(\log(n) n^{-1/2})$.",,2024-02-11,,"['steffen grünewälder', 'azadeh khaleghi']"
2402.07307,self-consistent conformal prediction,stat.ml cs.lg stat.me,"in decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. inspired by this perspective, we introduce self-consistent conformal prediction, which yields both venn-abers calibrated predictions and conformal prediction intervals that are valid conditional on actions prompted by model predictions. our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.",,2024-02-11,,"['lars van der laan', 'ahmed m. alaa']"
2402.07309,hyperbert: mixing hypergraph-aware layers with language models for node   classification on text-attributed hypergraphs,cs.lg cs.cl stat.ml,"hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. however, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. to overcome these challenges, we explore ways to further augment a pretrained bert model with specialized hypergraph-aware layers for the task of node classification. such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. in this paper, we propose a new architecture, hyperbert, a mixed text-hypergraph model which simultaneously models hypergraph relational structure while maintaining the high-quality text encoding capabilities of a pre-trained bert. notably, hyperbert presents results that achieve a new state-of-the-art on five challenging text-attributed hypergraph node classification benchmarks.",,2024-02-11,2024-02-13,"['adrián bazaga', 'pietro liò', 'gos micklem']"
2402.07314,a theoretical analysis of nash learning from human feedback under   general kl-regularized preference,cs.lg stat.ml,"reinforcement learning from human feedback (rlhf) learns from the preference signal provided by a probabilistic preference model, which takes a prompt and two responses as input, and produces a score indicating the preference of one response against another. so far, the most popular rlhf paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage. however, the existence of a reward function is a strong assumption and the reward-based rlhf is limited in expressivity and cannot capture the real-world complicated human preference.   in this work, we provide theoretical insights for a recently proposed learning paradigm, nash learning from human feedback (nlhf), which considered a general preference model and formulated the alignment process as a game between two competitive llms. the learning objective is to find a policy that consistently generates responses preferred over any competing policy while staying close to the initial model. the objective is defined as the nash equilibrium (ne) of the kl-regularized preference model. we aim to make the first attempt to study the theoretical learnability of the kl-regularized nlhf by considering both offline and online settings. for the offline learning from a pre-collected dataset, we propose algorithms that are efficient under suitable coverage conditions of the dataset. for batch online learning from iterative interactions with a preference oracle, our proposed algorithm enjoys a finite sample guarantee under the structural condition of the underlying preference model. our results connect the new nlhf paradigm with traditional rl theory, and validate the potential of reward-model-free learning under general preference.",,2024-02-11,,"['chenlu ye', 'wei xiong', 'yuheng zhang', 'nan jiang', 'tong zhang']"
2402.07340,random geometric graph alignment with graph neural networks,cs.lg cs.it cs.si math.it math.pr math.st stat.ml stat.th,"we characterize the performance of graph neural networks for graph alignment problems in the presence of vertex feature information. more specifically, given two graphs that are independent perturbations of a single random geometric graph with noisy sparse features, the task is to recover an unknown one-to-one mapping between the vertices of the two graphs. we show under certain conditions on the sparsity and noise level of the feature vectors, a carefully designed one-layer graph neural network can with high probability recover the correct alignment between the vertices with the help of the graph structure. we also prove that our conditions on the noise level are tight up to logarithmic factors. finally we compare the performance of the graph neural network to directly solving an assignment problem on the noisy vertex features. we demonstrate that when the noise level is at least constant this direct matching fails to have perfect recovery while the graph neural network can tolerate noise level growing as fast as a power of the size of the graph.",,2024-02-11,,"['suqi liu', 'morgane austern']"
2402.07341,noise-adaptive confidence sets for linear bandits and application to   bayesian optimization,stat.ml cs.lg,"adapting to a priori unknown noise level is a very important but challenging problem in sequential decision-making as efficient exploration typically requires knowledge of the noise level, which is often loosely specified. we report significant progress in addressing this issue in linear bandits in two respects. first, we propose a novel confidence set that is `semi-adaptive' to the unknown sub-gaussian parameter $\sigma_*^2$ in the sense that the (normalized) confidence width scales with $\sqrt{d\sigma_*^2 + \sigma_0^2}$ where $d$ is the dimension and $\sigma_0^2$ is the specified sub-gaussian parameter (known) that can be much larger than $\sigma_*^2$. this is a significant improvement over $\sqrt{d\sigma_0^2}$ of the standard confidence set of abbasi-yadkori et al. (2011), especially when $d$ is large. we show that this leads to an improved regret bound in linear bandits. second, for bounded rewards, we propose a novel variance-adaptive confidence set that has a much improved numerical performance upon prior art. we then apply this confidence set to develop, as we claim, the first practical variance-adaptive linear bandit algorithm via an optimistic approach, which is enabled by our novel regret analysis technique. both of our confidence sets rely critically on `regret equality' from online learning. our empirical evaluation in bayesian optimization tasks shows that our algorithms demonstrate better or comparable performance compared to existing methods.",,2024-02-11,,"['kwang-sung jun', 'jungtaek kim']"
2402.07355,sampling from the mean-field stationary distribution,math.st cs.lg stat.ml stat.th,"we study the complexity of sampling from the stationary distribution of a mean-field sde, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term. our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field sde via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. this leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.",,2024-02-11,2024-02-18,"['yunbum kook', 'matthew s. zhang', 'sinho chewi', 'murat a. erdogdu', 'mufan bill li']"
2402.07357,regression trees for fast and adaptive prediction intervals,stat.ml cs.lg,"predictive models make mistakes. hence, there is a need to quantify the uncertainty associated with their predictions. conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions. new conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation. although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model. this paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees. our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage. we create this partition by training regression trees and random forests on conformity scores. our proposal is versatile, as it applies to various conformity scores and prediction settings and demonstrates superior scalability and performance compared to established baselines in simulated and real-world datasets. we provide a python package clover that implements our methods using the standard scikit-learn interface.",,2024-02-11,2024-02-13,"['luben m. c. cabezas', 'mateus p. otto', 'rafael izbicki', 'rafael b. stern']"
2402.07391,replicability is asymptotically free in multi-armed bandits,stat.ml cs.lg,"this work is motivated by the growing demand for reproducible machine learning. we study the stochastic multi-armed bandit problem. in particular, we consider a replicable algorithm that ensures, with high probability, that the algorithm's sequence of actions is not affected by the randomness inherent in the dataset. we observe that existing algorithms require $o(1/\rho^2)$ times more regret than nonreplicable algorithms, where $\rho$ is the level of nonreplication. however, we demonstrate that this additional cost is unnecessary when the time horizon $t$ is sufficiently large for a given $\rho$, provided that the magnitude of the confidence bounds is chosen carefully. we introduce an explore-then-commit algorithm that draws arms uniformly before committing to a single arm. additionally, we examine a successive elimination algorithm that eliminates suboptimal arms at the end of each phase. to ensure the replicability of these algorithms, we incorporate randomness into their decision-making processes. we extend the use of successive elimination to the linear bandit problem as well. for the analysis of these algorithms, we propose a principled approach to limiting the probability of nonreplication. this approach elucidates the steps that existing research has implicitly followed. furthermore, we derive the first lower bound for the two-armed replicable bandit problem, which implies the optimality of the proposed algorithms up to a $\log\log t$ factor for the two-armed case.",,2024-02-11,,"['junpei komiyama', 'shinji ito', 'yuichi yoshida', 'souta koshino']"
2402.07407,conformal predictive programming for chance constrained optimization,eess.sy cs.lg cs.sy math.oc stat.ml,"motivated by the advances in conformal prediction (cp), we propose conformal predictive programming (cpp), an approach to solve chance constrained optimization (cco) problems, i.e., optimization problems with nonlinear constraint functions affected by arbitrary random parameters. cpp utilizes samples from these random parameters along with the quantile lemma -- which is central to cp -- to transform the cco problem into a deterministic optimization problem. we then present two tractable reformulations of cpp by: (1) writing the quantile as a linear program along with its kkt conditions (cpp-kkt), and (2) using mixed integer programming (cpp-mip). cpp comes with marginal probabilistic feasibility guarantees for the cco problem that are conceptually different from existing approaches, e.g., the sample approximation and the scenario approach. while we explore algorithmic similarities with the sample approximation approach, we emphasize that the strength of cpp is that it can easily be extended to incorporate different variants of cp. to illustrate this, we present robust conformal predictive programming to deal with distribution shifts in the uncertain parameters of the cco problem.",,2024-02-11,,"['yiqi zhao', 'xinyi yu', 'jyotirmoy v. deshmukh', 'lars lindemann']"
2402.07419,conditional generative models are sufficient to sample from any causal   effect estimand,cs.lg cs.ai stat.me stat.ml,"causal inference from observational data has recently found many applications in machine learning. while sound and complete algorithms exist to compute causal effects, many of these algorithms require explicit access to conditional likelihoods over the observational distribution, which is difficult to estimate in the high-dimensional regime, such as with images. to alleviate this issue, researchers have approached the problem by simulating causal relations with neural models and obtained impressive results. however, none of these existing approaches can be applied to generic scenarios such as causal graphs on image data with latent confounders, or obtain conditional interventional samples. in this paper, we show that any identifiable causal effect given an arbitrary causal graph can be computed through push-forward computations of conditional generative models. based on this result, we devise a diffusion-based approach to sample from any (conditional) interventional distribution on image data. to showcase our algorithm's performance, we conduct experiments on a colored mnist dataset having both the treatment ($x$) and the target variables ($y$) as images and obtain interventional samples from $p(y|do(x))$. as an application of our algorithm, we evaluate two large conditional generative models that are pre-trained on the celeba dataset by analyzing the strength of spurious correlations and the level of disentanglement they achieve.",,2024-02-12,,"['md musfiqur rahman', 'matt jordan', 'murat kocaoglu']"
2402.07445,top-$k$ ranking with a monotone adversary,stat.ml cs.it cs.lg math.it math.st stat.th,"in this paper, we address the top-$k$ ranking problem with a monotone adversary. we consider the scenario where a comparison graph is randomly generated and the adversary is allowed to add arbitrary edges. the statistician's goal is then to accurately identify the top-$k$ preferred items based on pairwise comparisons derived from this semi-random comparison graph. the main contribution of this paper is to develop a weighted maximum likelihood estimator (mle) that achieves near-optimal sample complexity, up to a $\log^2(n)$ factor, where n denotes the number of items under comparison. this is made possible through a combination of analytical and algorithmic innovations. on the analytical front, we provide a refined $\ell_\infty$ error analysis of the weighted mle that is more explicit and tighter than existing analyses. it relates the $\ell_\infty$ error with the spectral properties of the weighted comparison graph. motivated by this, our algorithmic innovation involves the development of an sdp-based approach to reweight the semi-random graph and meet specified spectral properties. additionally, we propose a first-order method based on the matrix multiplicative weight update (mmwu) framework. this method efficiently solves the resulting sdp in nearly-linear time relative to the size of the semi-random comparison graph.",,2024-02-12,,"['yuepeng yang', 'antares chen', 'lorenzo orecchia', 'cong ma']"
2402.07453,bandit-feedback online multiclass classification: variants and tradeoffs,cs.lg stat.ml,"consider the domain of multiclass classification within the adversarial online setting. what is the price of relying on bandit feedback as opposed to full information? to what extent can an adaptive adversary amplify the loss compared to an oblivious one? to what extent can a randomized learner reduce the loss compared to a deterministic one? we study these questions in the mistake bound model and provide nearly tight answers.   we demonstrate that the optimal mistake bound under bandit feedback is at most $o(k)$ times higher than the optimal mistake bound in the full information case, where $k$ represents the number of labels. this bound is tight and provides an answer to an open question previously posed and studied by daniely and helbertal ['13] and by long ['17, '20], who focused on deterministic learners.   moreover, we present nearly optimal bounds of $\tilde{\theta}(k)$ on the gap between randomized and deterministic learners, as well as between adaptive and oblivious adversaries in the bandit feedback setting. this stands in contrast to the full information scenario, where adaptive and oblivious adversaries are equivalent, and the gap in mistake bounds between randomized and deterministic learners is a constant multiplicative factor of $2$.   in addition, our results imply that in some cases the optimal randomized mistake bound is approximately the square-root of its deterministic parallel. previous results show that this is essentially the smallest it can get.",,2024-02-12,,"['yuval filmus', 'steve hanneke', 'idan mehalel', 'shay moran']"
2402.07458,on the distance from calibration in sequential prediction,cs.lg cs.ds stat.ml,"we study a sequential binary prediction setting where the forecaster is evaluated in terms of the calibration distance, which is defined as the $l_1$ distance between the predicted values and the set of predictions that are perfectly calibrated in hindsight. this is analogous to a calibration measure recently proposed by b{\l}asiok, gopalan, hu and nakkiran (stoc 2023) for the offline setting. the calibration distance is a natural and intuitive measure of deviation from perfect calibration, and satisfies a lipschitz continuity property which does not hold for many popular calibration measures, such as the $l_1$ calibration error and its variants.   we prove that there is a forecasting algorithm that achieves an $o(\sqrt{t})$ calibration distance in expectation on an adversarially chosen sequence of $t$ binary outcomes. at the core of this upper bound is a structural result showing that the calibration distance is accurately approximated by the lower calibration distance, which is a continuous relaxation of the former. we then show that an $o(\sqrt{t})$ lower calibration distance can be achieved via a simple minimax argument and a reduction to online learning on a lipschitz class.   on the lower bound side, an $\omega(t^{1/3})$ calibration distance is shown to be unavoidable, even when the adversary outputs a sequence of independent random bits, and has an additional ability to early stop (i.e., to stop producing random bits and output the same bit in the remaining steps). interestingly, without this early stopping, the forecaster can achieve a much smaller calibration distance of $\mathrm{polylog}(t)$.",,2024-02-12,,"['mingda qiao', 'letian zheng']"
2402.07465,score-based physics-informed neural networks for high-dimensional   fokker-planck equations,cs.lg cs.ai cs.na math.ds math.na stat.ml,"the fokker-planck (fp) equation is a foundational pde in stochastic processes. however, curse of dimensionality (cod) poses challenge when dealing with high-dimensional fp pdes. although monte carlo and vanilla physics-informed neural networks (pinns) have shown the potential to tackle cod, both methods exhibit numerical errors in high dimensions when dealing with the probability density function (pdf) associated with brownian motion. the point-wise pdf values tend to decrease exponentially as dimension increases, surpassing the precision of numerical simulations and resulting in substantial errors. moreover, due to its massive sampling, monte carlo fails to offer fast sampling. modeling the logarithm likelihood (ll) via vanilla pinns transforms the fp equation into a difficult hjb equation, whose error grows rapidly with dimension. to this end, we propose a novel approach utilizing a score-based solver to fit the score function in sdes. the score function, defined as the gradient of the ll, plays a fundamental role in inferring ll and pdf and enables fast sde sampling. three fitting methods, score matching (sm), sliced sm (ssm), and score-pinn, are introduced. the proposed score-based sde solver operates in two stages: first, employing sm, ssm, or score-pinn to acquire the score; and second, solving the ll via an ode using the obtained score. comparative evaluations across these methods showcase varying trade-offs. the proposed method is evaluated across diverse sdes, including anisotropic ou processes, geometric brownian, and brownian with varying eigenspace. we also test various distributions, including gaussian, log-normal, laplace, and cauchy. the numerical results demonstrate the score-based sde solver's stability, speed, and performance across different settings, solidifying its potential as a solution to cod for high-dimensional fp equations.",,2024-02-12,,"['zheyuan hu', 'zhongqiang zhang', 'george em karniadakis', 'kenji kawaguchi']"
2402.07514,physics-informed machine learning as a kernel method,cs.ai math.st stat.th,"physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models. in this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency. we prove that for linear differential priors, the problem can be formulated as a kernel regression task. taking advantage of kernel theory, we derive convergence rates for the minimizer of the regularized risk and show that it converges at least at the sobolev minimax rate. however, faster rates can be achieved, depending on the physical error. this principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators.",,2024-02-12,,"['nathan doumèche', 'francis bach', 'claire boyer', 'gérard biau']"
2402.07521,a step towards the integration of machine learning and small area   estimation,stat.me econ.em stat.ml,"the use of machine-learning techniques has grown in numerous research areas. currently, it is also widely used in statistics, including the official statistics for data collection (e.g. satellite imagery, web scraping and text mining, data cleaning, integration and imputation) but also for data analysis. however, the usage of these methods in survey sampling including small area estimation is still very limited. therefore, we propose a predictor supported by these algorithms which can be used to predict any population or subpopulation characteristics based on cross-sectional and longitudinal data. machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means that they have very good properties in case of strong departures from the classic assumptions. therefore, we analyse the performance of our proposal under a different set-up, in our opinion of greater importance in real-life surveys. we study only small departures from the assumed model, to show that our proposal is a good alternative in this case as well, even in comparison with optimal methods under the model. what is more, we propose the method of the accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods, where the accuracy is measured as in survey sampling practice. the solution of this problem is indicated in the literature as one of the key issues in integration of these approaches. the simulation studies are based on a real, longitudinal dataset, freely available from the polish local data bank, where the prediction problem of subpopulation characteristics in the last period, with ""borrowing strength"" from other subpopulations and time periods, is considered.",,2024-02-12,,"['tomasz żądło', 'adam chwila']"
2402.07568,weisfeiler-leman at the margin: when more expressivity matters,cs.lg cs.dm cs.ne stat.ml,"the weisfeiler-leman algorithm ($1$-wl) is a well-studied heuristic for the graph isomorphism problem. recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (mpnns) and being effective as a graph kernel. despite its success, $1$-wl faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive mpnn and kernel architectures. however, the relationship between enhanced expressivity and improved generalization performance remains unclear. here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. moreover, we focus on augmenting $1$-wl and mpnns with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. in addition, we show that gradient flow pushes the mpnn's weights toward the maximum margin solution. further, we introduce variations of expressive $1$-wl-based kernel and mpnn architectures with provable generalization properties. our empirical study confirms the validity of our theoretical findings.",,2024-02-12,,"['billy j. franks', 'christopher morris', 'ameya velingker', 'floris geerts']"
2402.07588,rethinking scaling laws for learning in strategic environments,cs.gt cs.lg stat.ml,"the deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. as models get deployed in a variety of real world scenarios, they inevitably face strategic environments. in this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. we find that strategic interactions can break the conventional view of scaling laws$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). we show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\unicode{x2013}$one can achieve strictly better equilibrium outcomes. motivated by these examples, we then propose a new paradigm for model-selection in games wherein an agent seeks to choose amongst different model classes to use as their action set in a game.",,2024-02-12,2024-02-21,"['tinashe handina', 'eric mazumdar']"
2402.07598,near-minimax-optimal distributional reinforcement learning with a   generative model,cs.lg stat.ml,"we propose a new algorithm for model-based distributional reinforcement learning (rl), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of zhang et al. (2023). our analysis provides new theoretical results on categorical approaches to distributional rl, and also introduces a new distributional bellman equation, the stochastic categorical cdf bellman equation, which we expect to be of independent interest. we also provide an experimental study comparing several model-based distributional rl algorithms, with several takeaways for practitioners.",,2024-02-12,,"['mark rowland', 'li kevin wenliang', 'rémi munos', 'clare lyle', 'yunhao tang', 'will dabney']"
2402.07613,global optimality under amenable symmetry constraints,math.st cs.lg stat.ml stat.th,"we ask whether there exists a function or measure that (1) minimizes a given convex functional or risk and (2) satisfies a symmetry property specified by an amenable group of transformations. examples of such symmetry properties are invariance, equivariance, or quasi-invariance. our results draw on old ideas of stein and le cam and on approximate group averages that appear in ergodic theorems for amenable groups. a class of convex sets known as orbitopes in convex analysis emerges as crucial, and we establish properties of such orbitopes in nonparametric settings. we also show how a simple device called a cocycle can be used to reduce different forms of symmetry to a single problem. as applications, we obtain results on invariant kernel mean embeddings and a monge-kantorovich theorem on optimality of transport plans under symmetry constraints. we also explain connections to the hunt-stein theorem on invariant tests.",,2024-02-12,,['peter orbanz']
2402.07626,stochastic gradient flow dynamics of test risk and its exact solution   for weak features,stat.ml cond-mat.dis-nn cs.lg,"we investigate the test risk of continuous-time stochastic gradient flow dynamics in learning theory. using a path integral formulation we provide, in the regime of a small learning rate, a general formula for computing the difference between test risk curves of pure gradient and stochastic gradient flows. we apply the general theory to a simple model of weak features, which displays the double descent phenomenon, and explicitly compute the corrections brought about by the added stochastic term in the dynamics, as a function of time and model parameters. the analytical results are compared to simulations of discrete-time stochastic gradient descent and show good agreement.",,2024-02-12,,"['rodrigo veiga', 'anastasia remizova', 'nicolas macris']"
2402.07684,towards a foundation model for brain age prediction using covariance   neural networks,q-bio.qm cs.lg stat.ap,"brain age is the estimate of biological age derived from neuroimaging datasets using machine learning algorithms. increasing brain age with respect to chronological age can reflect increased vulnerability to neurodegeneration and cognitive decline. in this paper, we study neurovnn, based on covariance neural networks, as a paradigm for foundation model for the brain age prediction application. neurovnn is pre-trained as a regression model on healthy population to predict chronological age using cortical thickness features and fine-tuned to estimate brain age in different neurological contexts. importantly, neurovnn adds anatomical interpretability to brain age and has a `scale-free' characteristic that allows its transference to datasets curated according to any arbitrary brain atlas. our results demonstrate that neurovnn can extract biologically plausible brain age estimates in different populations, as well as transfer successfully to datasets of dimensionalities distinct from that for the dataset used to train neurovnn.",,2024-02-12,,"['saurabh sihag', 'gonzalo mateos', 'alejandro ribeiro']"
2402.07712,model collapse demystified: the case of regression,cs.lg cs.ai stat.ml,"in the era of large language models like chatgpt, the phenomenon of ""model collapse"" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. in this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. we also propose a simple strategy based on adaptive regularization to mitigate model collapse. our theoretical results are validated with experiments.",,2024-02-12,,"['elvis dohmatob', 'yunzhen feng', 'julia kempe']"
2402.07717,efficient reductions between some statistical models,math.st cs.it math.it math.pr stat.me stat.ml stat.th,"we study the problem of approximately transforming a sample from a source statistical model to a sample from a target statistical model without knowing the parameters of the source model, and construct several computationally efficient such reductions between statistical experiments. in particular, we provide computationally efficient procedures that approximately reduce uniform, erlang, and laplace location models to general target families. we illustrate our methodology by establishing nonasymptotic reductions between some canonical high-dimensional problems, spanning mixtures of experts, phase retrieval, and signal denoising. notably, the reductions are structure preserving and can accommodate missing data. we also point to a possible application in transforming one differentially private mechanism to another.",,2024-02-12,,"['mengqi lou', 'guy bresler', 'ashwin pananjady']"
2402.07723,generalization bounds for heavy-tailed sdes through the fractional   fokker-planck equation,stat.ml cs.lg,"understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. while illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed sdes which do not contain any nontrivial information theoretic terms. to achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional fokker-planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed sde). in addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to prior art. our results further identify a phase transition phenomenon, which suggests that heavy tails can be either beneficial or harmful depending on the problem structure. we support our theory with experiments conducted in a variety of settings.",,2024-02-12,,"['benjamin dupuis', 'umut şimşekli']"
2402.07735,graph structure inference with bam: introducing the bilinear attention   mechanism,stat.ml cs.lg,"in statistics and machine learning, detecting dependencies in datasets is a central challenge. we propose a novel neural network model for supervised graph structure learning, i.e., the process of learning a mapping between observational data and their underlying dependence structure. the model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. by leveraging structural equation models and employing randomly generated multivariate chebyshev polynomials for the simulation of training data, our method demonstrates robust generalizability across both linear and various types of non-linear dependencies. we introduce a novel bilinear attention mechanism (bam) for explicit processing of dependency information, which operates on the level of covariance matrices of transformed data and respects the geometry of the manifold of symmetric positive definite matrices. empirical evaluation demonstrates the robustness of our method in detecting a wide range of dependencies, excelling in undirected graph estimation and proving competitive in completed partially directed acyclic graph estimation through a novel two-step approach.",,2024-02-12,2024-02-13,"['philipp froehlich', 'heinz koeppl']"
2402.07747,optimal score estimation via empirical bayes smoothing,math.st stat.ml stat.th,"we study the problem of estimating the score function of an unknown probability distribution $\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions. assuming that $\rho^*$ is subgaussian and has a lipschitz-continuous score function $s^*$, we establish the optimal rate of $\tilde \theta(n^{-\frac{2}{d+4}})$ for this estimation problem under the loss function $\|\hat s - s^*\|^2_{l^2(\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. leveraging key insights in empirical bayes theory as well as a new convergence rate of smoothed empirical distribution in hellinger distance, we show that a regularized score estimator based on a gaussian kernel attains this rate, shown optimal by a matching minimax lower bound. we also discuss the implication of our theory on the sample complexity of score-based generative models.",,2024-02-12,,"['andre wibisono', 'yihong wu', 'kaylee yingxi yang']"
2402.07762,scalable structure learning for sparse context-specific causal systems,stat.ml cs.lg math.co,"several approaches to graphically representing context-specific relations among jointly distributed categorical variables have been proposed, along with structure learning algorithms. while existing optimization-based methods have limited scalability due to the large number of context-specific models, the constraint-based methods are more prone to error than even constraint-based dag learning algorithms since more relations must be tested. we present a hybrid algorithm for learning context-specific models that scales to hundreds of variables while testing no more constraints than standard dag learning algorithms. scalable learning is achieved through a combination of an order-based mcmc algorithm and sparsity assumptions analogous to those typically invoked for dag models. to implement the method, we solve a special case of an open problem recently posed by alon and balogh. the method is shown to perform well on synthetic data and real world examples, in terms of both accuracy and scalability.",,2024-02-12,,"['felix leopoldo rios', 'alex markham', 'liam solus']"
2402.07802,towards a mathematical theory for consistency training in diffusion   models,stat.ml cs.it cs.lg math.it math.st stat.th,"consistency models, which were proposed to mitigate the high computational overhead during the sampling phase of diffusion models, facilitate single-step sampling while attaining state-of-the-art empirical performance. when integrated into the training phase, consistency models attempt to train a sequence of consistency functions capable of mapping any point at any time step of the diffusion process to its starting point. despite the empirical success, a comprehensive theoretical understanding of consistency training remains elusive. this paper takes a first step towards establishing theoretical underpinnings for consistency models. we demonstrate that, in order to generate samples within $\varepsilon$ proximity to the target in distribution (measured by some wasserstein metric), it suffices for the number of steps in consistency learning to exceed the order of $d^{5/2}/\varepsilon$, with $d$ the data dimension. our theory offers rigorous insights into the validity and efficacy of consistency models, illuminating their utility in downstream inference tasks.",,2024-02-12,,"['gen li', 'zhihan huang', 'yuting wei']"
2402.07821,on computationally efficient multi-class calibration,cs.lg cs.cc cs.ds math.st stat.ml stat.th,"consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. in this work, we study the following foundational question: are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$? prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.   our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. projected smooth calibration gives strong guarantees for all downstream decision makers who want to use the predictor for binary classification problems of the form: does the label belong to a subset $t \subseteq [k]$: e.g. is this an image of an animal? it ensures that the probabilities predicted by summing the probabilities assigned to labels in $t$ are close to some perfectly calibrated binary predictor for that task. we also show that natural strengthenings of our definition are computationally hard to achieve: they run into information theoretic barriers or computational intractability. underlying both our upper and lower bounds is a tight connection that we prove between multi-class calibration and the well-studied problem of agnostic learning in the (standard) binary prediction setting.",,2024-02-12,,"['parikshit gopalan', 'lunjia hu', 'guy n. rothblum']"
2402.07846,generative modeling of discrete joint distributions by e-geodesic flow   matching on assignment manifolds,cs.lg stat.ml,"this paper introduces a novel generative model for discrete distributions based on continuous normalizing flows on the submanifold of factorizing discrete measures. integration of the flow gradually assigns categories and avoids issues of discretizing the latent continuous model like rounding, sample truncation etc. general non-factorizing discrete distributions capable of representing complex statistical dependencies of structured discrete data, can be approximated by embedding the submanifold into a the meta-simplex of all joint discrete distributions and data-driven averaging. efficient training of the generative model is demonstrated by matching the flow of geodesics of factorizing discrete distributions. various experiments underline the approach's broad applicability.",,2024-02-12,,"['bastian boll', 'daniel gonzalez-alvarado', 'christoph schnörr']"
2402.07875,implicit bias of policy gradient in linear quadratic control:   extrapolation to unseen initial states,cs.lg cs.ai cs.sy eess.sy stat.ml,"in modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. this implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). there, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. this paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. focusing on the fundamental linear quadratic regulator (lqr) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. experiments corroborate our theory, and demonstrate its conclusions on problems beyond lqr, where systems are non-linear and controllers are neural networks. we hypothesize that real-world optimal control may be greatly improved by developing methods for informed selection of initial states to train on.",,2024-02-12,,"['noam razin', 'yotam alexander', 'edo cohen-karlik', 'raja giryes', 'amir globerson', 'nadav cohen']"
2402.08010,which frequencies do cnns need? emergent bottleneck structure in feature   learning,cs.lg cs.ai stat.ml,"we describe the emergence of a convolution bottleneck (cbn) structure in cnns, where the network uses its first few layers to transform the input representation into a representation that is supported only along a few frequencies and channels, before using the last few layers to map back to the outputs. we define the cbn rank, which describes the number and type of frequencies that are kept inside the bottleneck, and partially prove that the parameter norm required to represent a function $f$ scales as depth times the cbn rank $f$. we also show that the parameter norm depends at next order on the regularity of $f$. we show that any network with almost optimal parameter norm will exhibit a cbn structure in both the weights and - under the assumption that the network is stable under large learning rate - the activations, which motivates the common practice of down-sampling; and we verify that the cbn results still hold with down-sampling. finally we use the cbn structure to interpret the functions learned by cnns on a number of tasks.",,2024-02-12,,"['yuxiao wen', 'arthur jacot']"
2402.08018,nearest neighbour score estimators for diffusion generative models,cs.lg cs.cv stat.ml,"score function estimation is the cornerstone of both training and sampling from diffusion generative models. despite this fact, the most commonly used estimators are either biased neural network approximations or high variance monte carlo estimators based on the conditional score. we introduce a novel nearest neighbour score function estimator which utilizes multiple samples from the training set to dramatically decrease estimator variance. we leverage our low variance estimator in two compelling applications. training consistency models with our estimator, we report a significant increase in both convergence speed and sample quality. in diffusion models, we show that our estimator can replace a learned network for probability-flow ode integration, opening promising new avenues of future research.",,2024-02-12,,"['matthew niedoba', 'dylan green', 'saeid naderiparizi', 'vasileios lioutas', 'jonathan wilder lavington', 'xiaoxuan liang', 'yunpeng liu', 'ke zhang', 'setareh dabiri', 'adam ścibior', 'berend zwartsenberg', 'frank wood']"
2402.08077,diffeomorphic measure matching with kernels for generative modeling,stat.ml cs.lg math.ds stat.co,"this article presents a general framework for the transport of probability measures towards minimum divergence generative modeling and sampling using ordinary differential equations (odes) and reproducing kernel hilbert spaces (rkhss), inspired by ideas from diffeomorphic matching and image registration. a theoretical analysis of the proposed method is presented, giving a priori error bounds in terms of the complexity of the model, the number of samples in the training set, and model misspecification. an extensive suite of numerical experiments further highlights the properties, strengths, and weaknesses of the method and extends its applicability to other tasks, such as conditional simulation and inference.",,2024-02-12,,"['biraj pandey', 'bamdad hosseini', 'pau batlle', 'houman owhadi']"
2402.08082,score-based generative models break the curse of dimensionality in   learning a family of sub-gaussian probability distributions,stat.ml cs.lg,"while score-based generative models (sgms) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. in this paper, we analyze the approximation and generalization of sgms in learning a family of sub-gaussian probability distributions. we introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard gaussian measure. we prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. we illustrate our theory through examples, which include certain mixtures of gaussians. an essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is interesting in its own right.",,2024-02-12,2024-02-23,"['frank cole', 'yulong lu']"
2402.08095,convergence analysis of discrete diffusion model: exact implementation   through uniformization,stat.ml cs.lg,"diffusion models have achieved huge empirical success in data generation tasks. recently, some efforts have been made to adapt the framework of diffusion models to discrete state space, providing a more natural approach for modeling intrinsically discrete data, such as language and graphs. this is achieved by formulating both the forward noising process and the corresponding reversed process as continuous time markov chains (ctmcs). in this paper, we investigate the theoretical properties of the discrete diffusion model. specifically, we introduce an algorithm leveraging the uniformization of continuous markov chains, implementing transitions on random time points. under reasonable assumptions on the learning of the discrete score function, we derive total variation distance and kl divergence guarantees for sampling from any distribution on a hypercube. our results align with state-of-the-art achievements for diffusion models in $\mathbb{r}^d$ and further underscore the advantages of discrete diffusion models in comparison to the $\mathbb{r}^d$ setting.",,2024-02-12,2024-02-14,"['hongrui chen', 'lexing ying']"
2402.08097,an accelerated gradient method for simple bilevel optimization with   convex lower-level problem,math.oc cs.lg stat.ml,"in this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem. we present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set. we measure the performance of our method in terms of suboptimality and infeasibility errors and provide non-asymptotic convergence guarantees for both error criteria. specifically, when the feasible set is compact, we show that our method requires at most $\mathcal{o}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$ iterations to find a solution that is $\epsilon_f$-suboptimal and $\epsilon_g$-infeasible. moreover, under the additional assumption that the lower-level objective satisfies the $r$-th h\""olderian error bound, we show that our method achieves an iteration complexity of $\mathcal{o}(\max\{\epsilon_{f}^{-\frac{2r-1}{2r}},\epsilon_{g}^{-\frac{2r-1}{2r}}\})$, which matches the optimal complexity of single-level convex constrained optimization when $r=1$.",,2024-02-12,,"['jincheng cao', 'ruichen jiang', 'erfan yazdandoost hamedani', 'aryan mokhtari']"
2402.08105,learning cartesian product graphs with laplacian constraints,cs.lg stat.ml,"graph laplacian learning, also known as network topology inference, is a problem of great interest to multiple communities. in gaussian graphical models (gm), graph learning amounts to endowing covariance selection with the laplacian structure. in graph signal processing (gsp), it is essential to infer the unobserved graph from the outputs of a filtering system. in this paper, we study the problem of learning cartesian product graphs under laplacian constraints. the cartesian graph product is a natural way for modeling higher-order conditional dependencies and is also the key for generalizing gsp to multi-way tensors. we establish statistical consistency for the penalized maximum likelihood estimation (mle) of a cartesian product laplacian, and propose an efficient algorithm to solve the problem. we also extend our method for efficient joint graph learning and imputation in the presence of structural missing values. experiments on synthetic and real-world datasets demonstrate that our method is superior to previous gsp and gm methods.",,2024-02-12,,"['changhao shi', 'gal mishne']"
2402.08164,on limitations of the transformer architecture,stat.ml cs.ai cs.lg,"what are the root causes of hallucinations in large language models (llms)? we use communication complexity to prove that the transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. we also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for llms are unlikely to be solvable by transformers, for large enough instances and assuming that certain well accepted conjectures in the field of computational complexity are true.",,2024-02-12,2024-02-26,"['binghui peng', 'srini narayanan', 'christos papadimitriou']"
2402.08182,variational continual test-time adaptation,cs.lg stat.ml,"the prior drift is crucial in continual test-time adaptation (ctta) methods that only use unlabeled test data, as it can cause significant error propagation. in this paper, we introduce vcotta, a variational bayesian approach to measure uncertainties in ctta. at the source stage, we transform a pre-trained deterministic model into a bayesian neural network (bnn) via a variational warm-up strategy, injecting uncertainties into the model. during the testing time, we employ a mean-teacher update strategy using variational inference for the student model and exponential moving average for the teacher model. our novel approach updates the student model by combining priors from both the source and teacher models. the evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the kullback-leibler (kl) divergence of the prior mixture. experimental results on three datasets demonstrate the method's effectiveness in mitigating prior drift within the ctta framework.",,2024-02-12,,"['fan lyu', 'kaile du', 'yuyang li', 'hanyu zhao', 'zhang zhang', 'guangcan liu', 'liang wang']"
2402.08193,gaussian ensemble belief propagation for efficient inference in   high-dimensional systems,cs.lg stat.ml,"efficient inference in high-dimensional models remains a central challenge in machine learning. this paper introduces the gaussian ensemble belief propagation (genbp) algorithm, a fusion of the ensemble kalman filter and gaussian belief propagation (gabp) methods. genbp updates ensembles by passing low-rank local messages in a graphical model structure. this combination inherits favourable qualities from each method. ensemble techniques allow genbp to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. the use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. genbp is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. this scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. genbp can be applied to general problem structures, including jointly learning system parameters, observation parameters, and latent state variables.",,2024-02-12,,"['dan mackinlay', 'russell tsuchida', 'dan pagendam', 'petra kuhnert']"
2402.08201,off-policy evaluation in markov decision processes under weak   distributional overlap,stat.ml cs.lg,"doubly robust methods hold considerable promise for off-policy evaluation in markov decision processes (mdps) under sequential ignorability: they have been shown to converge as $1/\sqrt{t}$ with the horizon $t$, to be statistically efficient in large samples, and to allow for modular implementation where preliminary estimation tasks can be executed using standard reinforcement learning techniques. existing results, however, make heavy use of a strong distributional overlap assumption whereby the stationary distributions of the target policy and the data-collection policy are within a bounded factor of each other -- and this assumption is typically only credible when the state space of the mdp is bounded. in this paper, we re-visit the task of off-policy evaluation in mdps under a weaker notion of distributional overlap, and introduce a class of truncated doubly robust (tdr) estimators which we find to perform well in this setting. when the distribution ratio of the target and data-collection policies is square-integrable (but not necessarily bounded), our approach recovers the large-sample behavior previously established under strong distributional overlap. when this ratio is not square-integrable, tdr is still consistent but with a slower-than-$1/\sqrt{t}$; furthermore, this rate of convergence is minimax over a class of mdps defined only using mixing conditions. we validate our approach numerically and find that, in our experiments, appropriate truncation plays a major role in enabling accurate off-policy evaluation when strong distributional overlap does not hold.",,2024-02-12,,"['mohammad mehrabi', 'stefan wager']"
2402.08229,causal discovery under off-target interventions,cs.lg cs.ds stat.me stat.ml,"causal graph discovery is a significant problem with applications across various disciplines. however, with observational data alone, the underlying causal graph can only be recovered up to its markov equivalence class, and further assumptions or interventions are necessary to narrow down the true graph. this work addresses the causal discovery problem under the setting of stochastic interventions with the natural goal of minimizing the number of interventions performed. we propose the following stochastic intervention model which subsumes existing adaptive noiseless interventions in the literature while capturing scenarios such as fat-hand interventions and crispr gene knockouts: any intervention attempt results in an actual intervention on a random subset of vertices, drawn from a distribution dependent on attempted action. under this model, we study the two fundamental problems in causal discovery of verification and search and provide approximation algorithms with polylogarithmic competitive ratios and provide some preliminary experimental results.",,2024-02-13,,"['davin choo', 'kirankumar shiragur', 'caroline uhler']"
2402.08283,classification using global and local mahalanobis distances,stat.me stat.ml,"we propose a novel semi-parametric classifier based on mahalanobis distances of an observation from the competing classes. our tool is a generalized additive model with the logistic link function that uses these distances as features to estimate the posterior probabilities of the different classes. while popular parametric classifiers like linear and quadratic discriminant analyses are mainly motivated by the normality of the underlying distributions, the proposed classifier is more flexible and free from such parametric assumptions. since the densities of elliptic distributions are functions of mahalanobis distances, this classifier works well when the competing classes are (nearly) elliptic. in such cases, it often outperforms popular nonparametric classifiers, especially when the sample size is small compared to the dimension of the data. to cope with non-elliptic and possibly multimodal distributions, we propose a local version of the mahalanobis distance. subsequently, we propose another classifier based on a generalized additive model that uses the local mahalanobis distances as features. this nonparametric classifier usually performs like the mahalanobis distance based semiparametric classifier when the underlying distributions are elliptic, but outperforms it for several non-elliptic and multimodal distributions. we also investigate the behaviour of these two classifiers in high dimension, low sample size situations. a thorough numerical study involving several simulated and real datasets demonstrate the usefulness of the proposed classifiers in comparison to many state-of-the-art methods.",,2024-02-13,,"['annesha ghosh', 'anil k. ghosh', 'rita saharay', 'soham sarkar']"
2402.08321,exploration by optimization with hybrid regularizers: logarithmic regret   with adversarial robustness in partial monitoring,cs.lg stat.ml,"partial monitoring is a generic framework of online decision-making problems with limited observations. to make decisions from such limited observations, it is necessary to find an appropriate distribution for exploration. recently, a powerful approach for this purpose, exploration by optimization (exo), was proposed, which achieves the optimal bounds in adversarial environments with follow-the-regularized-leader for a wide range of online decision-making problems. however, a naive application of exo in stochastic environments significantly degrades regret bounds. to resolve this problem in locally observable games, we first establish a novel framework and analysis for exo with a hybrid regularizer. this development allows us to significantly improve the existing regret bounds of best-of-both-worlds (bobw) algorithms, which achieves nearly optimal bounds both in stochastic and adversarial environments. in particular, we derive a stochastic regret bound of $o(\sum_{a \neq a^*} k^2 m^2 \log t / \delta_a)$, where $k$, $m$, and $t$ are the numbers of actions, observations and rounds, $a^*$ is an optimal action, and $\delta_a$ is the suboptimality gap for action $a$. this bound is roughly $\theta(k^2 \log t)$ times smaller than existing bobw bounds. in addition, for globally observable games, we provide a new bobw algorithm with the first $o(\log t)$ stochastic bound.",,2024-02-13,,"['taira tsuchiya', 'shinji ito', 'junya honda']"
2402.08328,dual likelihood for causal inference under structure uncertainty,stat.me math.st stat.th,"knowledge of the underlying causal relations is essential for inferring the effect of interventions in complex systems. in a widely studied approach, structural causal models postulate noisy functional relations among interacting variables, where the underlying causal structure is then naturally represented by a directed graph whose edges indicate direct causal dependencies. in the typical application, this underlying causal structure must be learned from data, and thus, the remaining structure uncertainty needs to be incorporated into causal inference in order to draw reliable conclusions. in recent work, test inversions provide an ansatz to account for this data-driven model choice and, therefore, combine structure learning with causal inference. in this article, we propose the use of dual likelihood to greatly simplify the treatment of the involved testing problem. indeed, dual likelihood leads to a closed-form solution for constructing confidence regions for total causal effects that rigorously capture both sources of uncertainty: causal structure and numerical size of nonzero effects. the proposed confidence regions can be computed with a bottom-up procedure starting from sink nodes. to render the causal structure identifiable, we develop our ideas in the context of linear causal relations with equal error variances.",,2024-02-13,,"['david strieder', 'mathias drton']"
2402.08344,implicit bias in noisy-sgd: with applications to differentially private   training,stat.ml cs.lg,"training deep neural networks (dnns) with small batches using stochastic gradient descent (sgd) yields superior test performance compared to larger batches. the specific noise structure inherent to sgd is known to be responsible for this implicit bias. dp-sgd, used to ensure differential privacy (dp) in dnns' training, adds gaussian noise to the clipped gradients. surprisingly, large-batch training still results in a significant decrease in performance, which poses an important challenge because strong dp guarantees necessitate the use of massive batches. we first show that the phenomenon extends to noisy-sgd (dp-sgd without clipping), suggesting that the stochasticity (and not the clipping) is the cause of this implicit bias, even with additional isotropic gaussian noise. we theoretically analyse the solutions obtained with continuous versions of noisy-sgd for the linear least square and diagonal linear network settings, and reveal that the implicit bias is indeed amplified by the additional noise. thus, the performance issues of large-batch dp-sgd training are rooted in the same underlying principles as sgd, offering hope for potential improvements in large batch training strategies.",,2024-02-13,,"['tom sander', 'maxime sylvestre', 'alain durmus']"
2402.08412,interacting particle systems on networks: joint inference of the network   and the interaction kernel,stat.ml cs.lg math.ds math.st stat.th,"modeling multi-agent systems on networks is a fundamental challenge in a wide variety of disciplines. we jointly infer the weight matrix of the network and the interaction kernel, which determine respectively which agents interact with which others and the rules of such interactions from data consisting of multiple trajectories. the estimator we propose leads naturally to a non-convex optimization problem, and we investigate two approaches for its solution: one is based on the alternating least squares (als) algorithm; another is based on a new algorithm named operator regression with alternating least squares (orals). both algorithms are scalable to large ensembles of data trajectories. we establish coercivity conditions guaranteeing identifiability and well-posedness. the als algorithm appears statistically efficient and robust even in the small data regime but lacks performance and convergence guarantees. the orals estimator is consistent and asymptotically normal under a coercivity condition. we conduct several numerical experiments ranging from kuramoto particle systems on networks to opinion dynamics in leader-follower models.",,2024-02-13,,"['quanjun lang', 'xiong wang', 'fei lu', 'mauro maggioni']"
2402.08425,transfer operators from batches of unpaired points via entropic   transport kernels,stat.ml cs.lg math.ds,"in this paper, we are concerned with estimating the joint probability of random variables $x$ and $y$, given $n$ independent observation blocks $(\boldsymbol{x}^i,\boldsymbol{y}^i)$, $i=1,\ldots,n$, each of $m$ samples $(\boldsymbol{x}^i,\boldsymbol{y}^i) = \bigl((x^i_j, y^i_{\sigma^i(j)}) \bigr)_{j=1}^m$, where $\sigma^i$ denotes an unknown permutation of i.i.d. sampled pairs $(x^i_j,y_j^i)$, $j=1,\ldots,m$. this means that the internal ordering of the $m$ samples within an observation block is not known. we derive a maximum-likelihood inference functional, propose a computationally tractable approximation and analyze their properties. in particular, we prove a $\gamma$-convergence result showing that we can recover the true density from empirical approximations as the number $n$ of blocks goes to infinity. using entropic optimal transport kernels, we model a class of hypothesis spaces of density functions over which the inference functional can be minimized. this hypothesis class is particularly suited for approximate inference of transfer operators from data. we solve the resulting discrete minimization problem by a modification of the emml algorithm to take addional transition probability constraints into account and prove the convergence of this algorithm. proof-of-concept examples demonstrate the potential of our method.",,2024-02-13,,"['florian beier', 'hancheng bi', 'clément sarrazin', 'bernhard schmitzer', 'gabriele steidl']"
2402.08493,sparsity via sparse group $k$-max regularization,cs.lg stat.ml,"for the linear inverse problem with sparsity constraints, the $l_0$ regularized problem is np-hard, and existing approaches either utilize greedy algorithms to find almost-optimal solutions or to approximate the $l_0$ regularization with its convex counterparts. in this paper, we propose a novel and concise regularization, namely the sparse group $k$-max regularization, which can not only simultaneously enhance the group-wise and in-group sparsity, but also casts no additional restraints on the magnitude of variables in each group, which is especially important for variables at different scales, so that it approximate the $l_0$ norm more closely. we also establish an iterative soft thresholding algorithm with local optimality conditions and complexity analysis provided. through numerical experiments on both synthetic and real-world datasets, we verify the effectiveness and flexibility of the proposed method.",,2024-02-13,,"['qinghua tao', 'xiangming xi', 'jun xu', 'johan a. k. suykens']"
2402.08508,a pac-bayesian link between generalisation and flat minima,stat.ml cs.lg,"modern machine learning usually involves predictors in the overparametrised setting (number of trained parameters greater than dataset size), and their training yield not only good performances on training data, but also good generalisation capacity. this phenomenon challenges many theoretical results, and remains an open problem. to reach a better understanding, we provide novel generalisation bounds involving gradient terms. to do so, we combine the pac-bayes toolbox with poincar\'e and log-sobolev inequalities, avoiding an explicit dependency on dimension of the predictor space. our results highlight the positive influence of \emph{flat minima} (being minima with a neighbourhood nearly minimising the learning problem as well) on generalisation performances, involving directly the benefits of the optimisation phase.",,2024-02-13,,"['maxime haddouche', 'paul viallard', 'umut simsekli', 'benjamin guedj']"
2402.08530,a distributional analogue to the successor representation,cs.lg cs.ai stat.ml,"this paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. analogous to how the successor representation (sr) describes the expected consequences of behaving according to a given policy, our distributional successor measure (sm) describes the distributional consequences of this behaviour. we formulate the distributional sm as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. moreover, we propose an algorithm that learns the distributional sm from data by minimizing a two-level maximum mean discrepancy. key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. as an illustration of the usefulness of the distributional sm, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible.",,2024-02-13,,"['harley wiltzer', 'jesse farebrother', 'arthur gretton', 'yunhao tang', 'andré barreto', 'will dabney', 'marc g. bellemare', 'mark rowland']"
2402.08539,intelligent diagnosis of alzheimer's disease based on machine learning,cs.lg stat.ap,"this study is based on the alzheimer's disease neuroimaging initiative (adni) dataset and aims to explore early detection and disease progression in alzheimer's disease (ad). we employ innovative data preprocessing strategies, including the use of the random forest algorithm to fill missing data and the handling of outliers and invalid data, thereby fully mining and utilizing these limited data resources. through spearman correlation coefficient analysis, we identify some features strongly correlated with ad diagnosis. we build and test three machine learning models using these features: random forest, xgboost, and support vector machine (svm). among them, the xgboost model performs the best in terms of diagnostic performance, achieving an accuracy of 91%. overall, this study successfully overcomes the challenge of missing data and provides valuable insights into early detection of alzheimer's disease, demonstrating its unique research value and practical significance.",,2024-02-13,,"['mingyang li', 'hongyu liu', 'yixuan li', 'zejun wang', 'yuan yuan', 'honglin dai']"
2402.08543,theoretical analysis of leave-one-out cross validation for   non-differentiable penalties under high-dimensional settings,math.st stat.ml stat.th,"despite a large and significant body of recent work focused on estimating the out-of-sample risk of regularized models in the high dimensional regime, a theoretical understanding of this problem for non-differentiable penalties such as generalized lasso and nuclear norm is missing. in this paper we resolve this challenge. we study this problem in the proportional high dimensional regime where both the sample size n and number of features p are large, and n/p and the signal-to-noise ratio (per observation) remain finite. we provide finite sample upper bounds on the expected squared error of leave-one-out cross-validation (lo) in estimating the out-of-sample risk. the theoretical framework presented here provides a solid foundation for elucidating empirical findings that show the accuracy of lo.",,2024-02-13,2024-02-14,"['haolin zou', 'arnab auddy', 'kamiar rahnama rad', 'arian maleki']"
2402.08602,globally-optimal greedy experiment selection for active sequential   estimation,math.st stat.me stat.ml stat.th,"motivated by modern applications such as computerized adaptive testing, sequential rank aggregation, and heterogeneous data source selection, we study the problem of active sequential estimation, which involves adaptively selecting experiments for sequentially collected data. the goal is to design experiment selection rules for more accurate model estimation. greedy information-based experiment selection methods, optimizing the information gain for one-step ahead, have been employed in practice thanks to their computational convenience, flexibility to context or task changes, and broad applicability. however, statistical analysis is restricted to one-dimensional cases due to the problem's combinatorial nature and the seemingly limited capacity of greedy algorithms, leaving the multidimensional problem open.   in this study, we close the gap for multidimensional problems. in particular, we propose adopting a class of greedy experiment selection methods and provide statistical analysis for the maximum likelihood estimator following these selection rules. this class encompasses both existing methods and introduces new methods with improved numerical efficiency. we prove that these methods produce consistent and asymptotically normal estimators. additionally, within a decision theory framework, we establish that the proposed methods achieve asymptotic optimality when the risk measure aligns with the selection rule. we also conduct extensive numerical studies on both simulated and real data to illustrate the efficacy of the proposed methods.   from a technical perspective, we devise new analytical tools to address theoretical challenges. these analytical tools are of independent theoretical interest and may be reused in related problems involving stochastic approximation and sequential designs.",,2024-02-13,,"['xiaoou li', 'hongru zhao']"
2402.08616,adjustment identification distance: a gadjid for causal structure   learning,stat.ml cs.lg stat.me,"evaluating graphs learned by causal discovery algorithms is difficult: the number of edges that differ between two graphs does not reflect how the graphs differ with respect to the identifying formulas they suggest for causal effects. we introduce a framework for developing causal distances between graphs which includes the structural intervention distance for directed acyclic graphs as a special case. we use this framework to develop improved adjustment-based distances as well as extensions to completed partially directed acyclic graphs and causal orders. we develop polynomial-time reachability algorithms to compute the distances efficiently. in our package gadjid (open source at https://github.com/causaldisco/gadjid), we provide implementations of our distances; they are orders of magnitude faster than the structural intervention distance and thereby provide a success metric for causal discovery that scales to graph sizes that were previously prohibitive.",,2024-02-13,,"['leonard henckel', 'theo würtzen', 'sebastian weichwald']"
2402.08621,a generalized approach to online convex optimization,cs.lg math.oc stat.ml,"in this paper, we analyze the problem of online convex optimization in different settings. we show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization. we also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound. we further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries. we use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds. our framework allows us to analyze online optimization in various settings, such full-information feedback, bandit feedback, stochastic regret, adversarial regret and various forms of non-stationary regret. using our analysis, we provide the first efficient projection-free online convex optimization algorithm using linear optimization oracles.",,2024-02-13,,"['mohammad pedramfar', 'vaneet aggarwal']"
2402.08667,target score matching,cs.lg stat.co stat.ml,"denoising score matching estimates the score of a noised version of a target distribution by minimizing a regression loss and is widely used to train the popular class of denoising diffusion models. a well known limitation of denoising score matching, however, is that it yields poor estimates of the score at low noise levels. this issue is particularly unfavourable for problems in the physical sciences and for monte carlo sampling tasks for which the score of the clean original target is known. intuitively, estimating the score of a slightly noised version of the target should be a simple task in such cases. in this paper, we address this shortcoming and show that it is indeed possible to leverage knowledge of the target score. we present a target score identity and corresponding target score matching regression loss which allows us to obtain score estimates admitting favourable properties at low noise levels.",,2024-02-13,,"['valentin de bortoli', 'michael hutchinson', 'peter wirnsberger', 'arnaud doucet']"
2402.08687,fuzzy clustering of circular time series based on a new dependence   measure with applications to wind data,stat.ap cs.lg,"time series clustering is an essential machine learning task with applications in many disciplines. while the majority of the methods focus on time series taking values on the real line, very few works consider time series defined on the unit circle, although the latter objects frequently arise in many applications. in this paper, the problem of clustering circular time series is addressed. to this aim, a distance between circular series is introduced and used to construct a clustering procedure. the metric relies on a new measure of serial dependence considering circular arcs, thus taking advantage of the directional character inherent to the series range. since the dynamics of the series may vary over the time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees. the resulting clustering algorithm is able to group series generated from similar stochastic processes, reaching accurate results with series coming from a broad variety of models. an extensive simulation study shows that the proposed method outperforms several alternative techniques, besides being computationally efficient. two interesting applications involving time series of wind direction in saudi arabia highlight the potential of the proposed approach.",,2024-01-26,,"['ángel lópez-oriona', 'ying sun', 'rosa m. crujeiras']"
2402.08711,"correction to ""wasserstein distance estimates for the distributions of   numerical approximations to ergodic stochastic differential equations""",stat.ml cs.lg cs.na math.na math.pr,"a method for analyzing non-asymptotic guarantees of numerical discretizations of ergodic sdes in wasserstein-2 distance is presented by sanz-serna and zygalakis in ``wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations"". they analyze the ubu integrator which is strong order two and only requires one gradient evaluation per step, resulting in desirable non-asymptotic guarantees, in particular $\mathcal{o}(d^{1/4}\epsilon^{-1/2})$ steps to reach a distance of $\epsilon > 0$ in wasserstein-2 distance away from the target distribution. however, there is a mistake in the local error estimates in sanz-serna and zygalakis (2021), in particular, a stronger assumption is needed to achieve these complexity estimates. this note reconciles the theory with the dimension dependence observed in practice in many applications of interest.",,2024-02-13,2024-02-15,"['daniel paulin', 'peter a. whalley']"
2402.08799,projection-free online convex optimization with time-varying constraints,cs.lg math.oc stat.ml,"we consider the setting of online convex optimization with adversarial time-varying constraints in which actions must be feasible w.r.t. a fixed constraint set, and are also required on average to approximately satisfy additional time-varying constraints. motivated by scenarios in which the fixed feasible set (hard constraint) is difficult to project on, we consider projection-free algorithms that access this set only through a linear optimization oracle (loo). we present an algorithm that, on a sequence of length $t$ and using overall $t$ calls to the loo, guarantees $\tilde{o}(t^{3/4})$ regret w.r.t. the losses and $o(t^{7/8})$ constraints violation (ignoring all quantities except for $t$) . in particular, these bounds hold w.r.t. any interval of the sequence. we also present a more efficient algorithm that requires only first-order oracle access to the soft constraints and achieves similar bounds w.r.t. the entire sequence. we extend the latter to the setting of bandit feedback and obtain similar bounds (as a function of $t$) in expectation.",,2024-02-13,,"['dan garber', 'ben kretzu']"
2402.08808,depth separation in norm-bounded infinite-width neural networks,cs.lg stat.ml,"we study depth separation in infinite-width neural networks, where complexity is controlled by the overall squared $\ell_2$-norm of the weights (sum of squares of all weights in the network). whereas previous depth separation results focused on separation in terms of width, such results do not give insight into whether depth determines if it is possible to learn a network that generalizes well even when the network width is unbounded. here, we study separation in terms of the sample complexity required for learnability. specifically, we show that there are functions that are learnable with sample complexity polynomial in the input dimension by norm-controlled depth-3 relu networks, yet are not learnable with sub-exponential sample complexity by norm-controlled depth-2 relu networks (with any value for the norm). we also show that a similar statement in the reverse direction is not possible: any function learnable with polynomial sample complexity by a norm-controlled depth-2 relu network with infinite width is also learnable with polynomial sample complexity by a norm-controlled depth-3 relu network.",,2024-02-13,,"['suzanna parkinson', 'greg ongie', 'rebecca willett', 'ohad shamir', 'nathan srebro']"
2402.08818,corridor geometry in gradient-based optimization,stat.ml cs.lg math.oc,"we characterize regions of a loss surface as corridors when the continuous curves of steepest descent -- the solutions of the gradient flow -- become straight lines. we show that corridors provide insights into gradient-based optimization, since corridors are exactly the regions where gradient descent and the gradient flow follow the same trajectory, while the loss decreases linearly. as a result, inside corridors there are no implicit regularization effects or training instabilities that have been shown to occur due to the drift between gradient descent and the gradient flow. using the loss linear decrease on corridors, we devise a learning rate adaptation scheme for gradient descent; we call this scheme corridor learning rate (clr). the clr formulation coincides with a special case of polyak step-size, discovered in the context of convex optimization. the polyak step-size has been shown recently to have also good convergence properties for neural networks; we further confirm this here with results on cifar-10 and imagenet.",,2024-02-13,,"['benoit dherin', 'mihaela rosca']"
2402.08845,feature attribution with necessity and sufficiency via dual-stage   perturbation test for causal explanation,cs.lg stat.me,"we investigate the problem of explainability in machine learning.to address this problem, feature attribution methods (fams) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.however, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.in order to enhance the ability of fams to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (pns) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.our approach, feature attribution with necessity and sufficiency (fans), computes the pns via a perturbation test involving two stages (factual and interventional).in practice, to generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution.finally, we combine fans and gradient-based optimization to extract the subset with the largest pns.we demonstrate that fans outperforms existing feature attribution methods on six benchmarks.",,2024-02-13,,"['xuexin chen', 'ruichu cai', 'zhengting huang', 'yuxuan zhu', 'julien horwood', 'zhifeng hao', 'zijian li', 'jose miguel hernandez-lobato']"
2402.08847,space-time bridge-diffusion,stat.ml cs.lg,"in this study, we introduce a novel method for generating new synthetic samples that are independent and identically distributed (i.i.d.) from high-dimensional real-valued probability distributions, as defined implicitly by a set of ground truth (gt) samples. central to our method is the integration of space-time mixing strategies that extend across temporal and spatial dimensions. our methodology is underpinned by three interrelated stochastic processes designed to enable optimal transport from an easily tractable initial probability distribution to the target distribution represented by the gt samples: (a) linear processes incorporating space-time mixing that yield gaussian conditional probability densities, (b) their bridge-diffusion analogs that are conditioned to the initial and final state vectors, and (c) nonlinear stochastic processes refined through score-matching techniques. the crux of our training regime involves fine-tuning the nonlinear model, and potentially the linear models - to align closely with the gt data. we validate the efficacy of our space-time diffusion approach with numerical experiments, laying the groundwork for more extensive future theory and experiments to fully authenticate the method, particularly providing a more efficient (possibly simulation-free) inference.",,2024-02-13,,"['hamidreza behjoo', 'michael chertkov']"
2402.08856,approximation of relation functions and attention mechanisms,cs.lg stat.ml,"inner products of neural network feature maps arises in a wide variety of machine learning frameworks as a method of modeling relations between inputs. this work studies the approximation properties of inner products of neural networks. it is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions. in the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator. in both cases, a bound is obtained on the number of neurons required to achieve a given accuracy of approximation. in the symmetric case, the function class can be identified with kernels of reproducing kernel hilbert spaces, whereas in the asymmetric case the function class can be identified with kernels of reproducing kernel banach spaces. finally, these approximation results are applied to analyzing the attention mechanism underlying transformers, showing that any retrieval mechanism defined by an abstract preorder can be approximated by attention through its inner product relations. this result uses the debreu representation theorem in economics to represent preference relations in terms of utility functions.",,2024-02-13,,"['awni altabaa', 'john lafferty']"
2402.08871,position paper: challenges and opportunities in topological deep   learning,cs.lg stat.ml,"topological deep learning (tdl) is a rapidly evolving field that uses topological features to understand and design deep learning models. this paper posits that tdl may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. to this end, this paper discusses open problems in tdl, ranging from practical benefits to theoretical foundations. for each problem, it outlines potential solutions and future research opportunities. at the same time, this paper serves as an invitation to the scientific community to actively participate in tdl research to unlock the potential of this emerging field.",,2024-02-13,,"['theodore papamarkou', 'tolga birdal', 'michael bronstein', 'gunnar carlsson', 'justin curry', 'yue gao', 'mustafa hajij', 'roland kwitt', 'pietro liò', 'paolo di lorenzo', 'vasileios maroulas', 'nina miolane', 'farzana nasrin', 'karthikeyan natesan ramamurthy', 'bastian rieck', 'simone scardapane', 'michael t. schaub', 'petar veličković', 'bei wang', 'yusu wang', 'guo-wei wei', 'ghada zamzmi']"
2402.08922,the mirrored influence hypothesis: efficient data influence estimation   by harnessing forward passes,cs.lg stat.ml,"large-scale black-box models have become ubiquitous across numerous applications. understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. these approaches face obvious computational challenges when scaled up to large datasets and models.   in this paper, we introduce and explore the mirrored influence hypothesis, highlighting a reciprocal nature of influence between training and test data. specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. through both empirical and theoretical validations, we demonstrate the wide applicability of our hypothesis. inspired by this, we introduce a new method for estimating the influence of training data, which requires calculating gradients for specific test samples, paired with a forward pass for each training point. this approach can capitalize on the common asymmetry in scenarios where the number of test samples under concurrent examination is much smaller than the scale of the training dataset, thus gaining a significant improvement in efficiency compared to existing approaches.   we demonstrate the applicability of our method across a range of scenarios, including data attribution in diffusion models, data leakage detection, analysis of memorization, mislabeled data detection, and tracing behavior in language models. our code will be made available at https://github.com/ruoxi-jia-group/forward-inf.",,2024-02-13,,"['myeongseob ko', 'feiyang kang', 'weiyan shi', 'ming jin', 'zhou yu', 'ruoxi jia']"
2402.08929,second order methods for bandit optimization and control,cs.lg stat.ml,"bandit convex optimization (bco) is a general framework for online decision making under uncertainty. while tight regret bounds for general convex losses have been established, existing algorithms achieving these bounds have prohibitive computational costs for high dimensional data.   in this paper, we propose a simple and practical bco algorithm inspired by the online newton step algorithm. we show that our algorithm achieves optimal (in terms of horizon) regret bounds for a large class of convex functions that we call $\kappa$-convex. this class contains a wide range of practically relevant loss functions including linear, quadratic, and generalized linear models. in addition to optimal regret, this method is the most efficient known algorithm for several well-studied applications including bandit logistic regression.   furthermore, we investigate the adaptation of our second-order bandit algorithm to online convex optimization with memory. we show that for loss functions with a certain affine structure, the extended algorithm attains optimal regret. this leads to an algorithm with optimal regret for bandit lqr/lqg problems under a fully adversarial noise model, thereby resolving an open question posed in \citep{gradu2020non} and \citep{sun2023optimal}.   finally, we show that the more general problem of bco with (non-affine) memory is harder. we derive a $\tilde{\omega}(t^{2/3})$ regret lower bound, even under the assumption of smooth and quadratic losses.",,2024-02-13,,"['arun suggala', 'y. jennifer sun', 'praneeth netrapalli', 'elad hazan']"
2402.08991,towards robust model-based reinforcement learning against adversarial   corruption,stat.ml cs.lg,"this study tackles the challenges of adversarial corruption in model-based reinforcement learning (rl), where the transition dynamics can be corrupted by an adversary. existing studies on corruption-robust rl mostly focus on the setting of model-free rl, where robust least-square regression is often employed for value function estimation. however, these techniques cannot be directly applied to model-based rl. in this paper, we focus on model-based rl and take the maximum likelihood estimation (mle) approach to learn transition model. our work encompasses both online and offline settings. in the online setting, we introduce an algorithm called corruption-robust optimistic mle (cr-omle), which leverages total-variation (tv)-based information ratios as uncertainty weights for mle. we prove that cr-omle achieves a regret of $\tilde{\mathcal{o}}(\sqrt{t} + c)$, where $c$ denotes the cumulative corruption level after $t$ episodes. we also prove a lower bound to show that the additive dependence on $c$ is optimal. we extend our weighting technique to the offline setting, and propose an algorithm named corruption-robust pessimistic mle (cr-pmle). under a uniform coverage condition, cr-pmle exhibits suboptimality worsened by $\mathcal{o}(c/n)$, nearly matching the lower bound. to the best of our knowledge, this is the first work on corruption-robust model-based rl algorithms with provable guarantees.",,2024-02-14,2024-02-14,"['chenlu ye', 'jiafan he', 'quanquan gu', 'tong zhang']"
2402.08992,variance reduction and low sample complexity in stochastic optimization   via proximal point method,math.oc cs.lg stat.ml,"this paper proposes a stochastic proximal point method to solve a stochastic convex composite optimization problem. high probability results in stochastic optimization typically hinge on restrictive assumptions on the stochastic gradient noise, for example, sub-gaussian distributions. assuming only weak conditions such as bounded variance of the stochastic gradient, this paper establishes a low sample complexity to obtain a high probability guarantee on the convergence of the proposed method. additionally, a notable aspect of this work is the development of a subroutine to solve the proximal subproblem, which also serves as a novel technique for variance reduction.",,2024-02-14,,['jiaming liang']
2402.08998,nearly minimax optimal regret for learning linear mixture stochastic   shortest path,cs.lg stat.ml,"we study the stochastic shortest path (ssp) problem with a linear mixture transition kernel, where an agent repeatedly interacts with a stochastic environment and seeks to reach certain goal state while minimizing the cumulative cost. existing works often assume a strictly positive lower bound of the cost function or an upper bound of the expected length for the optimal policy. in this paper, we propose a new algorithm to eliminate these restrictive assumptions. our algorithm is based on extended value iteration with a fine-grained variance-aware confidence set, where the variance is estimated recursively from high-order moments. our algorithm achieves an $\tilde{\mathcal o}(db_*\sqrt{k})$ regret bound, where $d$ is the dimension of the feature mapping in the linear transition kernel, $b_*$ is the upper bound of the total cumulative cost for the optimal policy, and $k$ is the number of episodes. our regret upper bound matches the $\omega(db_*\sqrt{k})$ lower bound of linear mixture ssps in min et al. (2022), which suggests that our algorithm is nearly minimax optimal.",,2024-02-14,,"['qiwei di', 'jiafan he', 'dongruo zhou', 'quanquan gu']"
2402.09018,neural operators meet energy-based theory: operator learning for   hamiltonian and dissipative pdes,stat.ml cs.lg,"the operator learning has received significant attention in recent years, with the aim of learning a mapping between function spaces. prior works have proposed deep neural networks (dnns) for learning such a mapping, enabling the learning of solution operators of partial differential equations (pdes). however, these works still struggle to learn dynamics that obeys the laws of physics. this paper proposes energy-consistent neural operators (enos), a general framework for learning solution operators of pdes that follows the energy conservation or dissipation law from observed solution trajectories. we introduce a novel penalty function inspired by the energy-based theory of physics for training, in which the energy functional is modeled by another dnn, allowing one to bias the outputs of the dnn-based solution operators to ensure energetic consistency without explicit pdes. experiments on multiple physical systems show that eno outperforms existing dnn models in predicting solutions from data, especially in super-resolution settings.",,2024-02-14,,"['yusuke tanaka', 'takaharu yaguchi', 'tomoharu iwata', 'naonori ueda']"
2402.09033,cross-temporal forecast reconciliation at digital platforms with machine   learning,econ.em stat.ap stat.me stat.ml,"platform businesses operate on a digital core and their decision making requires high-dimensional accurate forecast streams at different levels of cross-sectional (e.g., geographical regions) and temporal aggregation (e.g., minutes to days). it also necessitates coherent forecasts across all levels of the hierarchy to ensure aligned decision making across different planning units such as pricing, product, controlling and strategy. given that platform data streams feature complex characteristics and interdependencies, we introduce a non-linear hierarchical forecast reconciliation method that produces cross-temporal reconciled forecasts in a direct and automated way through the use of popular machine learning methods. the method is sufficiently fast to allow forecast-based high-frequency decision making that platforms require. we empirically test our framework on a unique, large-scale streaming dataset from a leading on-demand delivery platform in europe.",,2024-02-14,,"['jeroen rombouts', 'marie ternes', 'ines wilms']"
2402.09122,mixed-output gaussian process latent variable models,stat.ml cs.lg,"this work develops a bayesian non-parametric approach to signal separation where the signals may vary according to latent variables. our key contribution is to augment gaussian process latent variable models (gplvms) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations. our framework allows the use of a range of priors for the weights of each observation. this flexibility enables us to represent use cases including sum-to-one constraints for estimating fractional makeup, and binary weights for classification. our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample. to demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temperatures, a simulated data set for identifying flow configuration through a pipe, and a data set for determining the type of rock from its reflectance.",,2024-02-14,,"['james odgers', 'chrysoula kappatou', 'ruth misener', 'sarah filippi']"
2402.09201,better-than-kl pac-bayes bounds,cs.lg stat.ml,"let $f(\theta, x_1),$ $ \dots,$ $ f(\theta, x_n)$ be a sequence of random elements, where $f$ is a fixed scalar function, $x_1, \dots, x_n$ are independent random variables (data), and $\theta$ is a random parameter distributed according to some data-dependent posterior distribution $p_n$. in this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence. an example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network where $f$ is a loss function. classically, this problem is approached through a pac-bayes analysis where, in addition to the posterior, we choose a prior distribution which captures our belief about the inductive bias of the learning problem. then, the key quantity in pac-bayes concentration bounds is a divergence that captures the complexity of the learning problem where the de facto standard choice is the kl divergence. however, the tightness of this choice has rarely been questioned.   in this paper, we challenge the tightness of the kl-divergence-based bounds by showing that it is possible to achieve a strictly tighter bound. in particular, we demonstrate new high-probability pac-bayes bounds with a novel and better-than-kl divergence that is inspired by zhang et al. (2022). our proof is inspired by recent advances in regret analysis of gambling algorithms, and its use to derive concentration inequalities. our result is first-of-its-kind in that existing pac-bayes bounds with non-kl divergences are not known to be strictly better than kl. thus, we believe our work marks the first step towards identifying optimal rates of pac-bayes bounds.",,2024-02-14,,"['ilja kuzborskij', 'kwang-sung jun', 'yulian wu', 'kyoungseok jang', 'francesco orabona']"
2402.09226,directional convergence near small initializations and saddles in   two-homogeneous neural networks,cs.lg math.oc stat.ml,"this paper examines gradient flow dynamics of two-homogeneous neural networks for small initializations, where all weights are initialized near the origin. for both square and logistic losses, it is shown that for sufficiently small initializations, the gradient flow dynamics spend sufficient time in the neighborhood of the origin to allow the weights of the neural network to approximately converge in direction to the karush-kuhn-tucker (kkt) points of a neural correlation function that quantifies the correlation between the output of the neural network and corresponding labels in the training data set. for square loss, it has been observed that neural networks undergo saddle-to-saddle dynamics when initialized close to the origin. motivated by this, this paper also shows a similar directional convergence among weights of small magnitude in the neighborhood of certain saddle points.",,2024-02-14,,"['akshay kumar', 'jarvis haupt']"
2402.09236,learning interpretable concepts: unifying causal representation learning   and foundation models,cs.lg cs.ai math.st stat.ml stat.th,"to build intelligent machine learning systems, there are two broad approaches. one approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. the other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. in this work, we relate these two approaches and study how to learn human-interpretable concepts from data. weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. experiments on synthetic data and large language models show the utility of our unified approach.",,2024-02-14,,"['goutham rajendran', 'simon buchholz', 'bryon aragam', 'bernhard schölkopf', 'pradeep ravikumar']"
2402.09328,connecting algorithmic fairness to quality dimensions in machine   learning in official statistics and survey production,stat.ml cs.lg stat.me,"national statistical organizations (nsos) increasingly draw on machine learning (ml) to improve the timeliness and cost-effectiveness of their products. when introducing ml solutions, nsos must ensure that high standards with respect to robustness, reproducibility, and accuracy are upheld as codified, e.g., in the quality framework for statistical algorithms (qf4sa; yung et al. 2022). at the same time, a growing body of research focuses on fairness as a pre-condition of a safe deployment of ml to prevent disparate social impacts in practice. however, fairness has not yet been explicitly discussed as a quality aspect in the context of the application of ml at nsos. we employ yung et al. (2022)'s qf4sa quality framework and present a mapping of its quality dimensions to algorithmic fairness. we thereby extend the qf4sa framework in several ways: we argue for fairness as its own quality dimension, we investigate the interaction of fairness with other dimensions, and we explicitly address data, both on its own and its interaction with applied methodology. in parallel with empirical illustrations, we show how our mapping can contribute to methodology in the domains of official statistics, algorithmic fairness, and trustworthy machine learning.",,2024-02-14,,"['patrick oliver schenk', 'christoph kern']"
2402.09373,loss shaping constraints for long-term time series forecasting,cs.lg stat.ml,"several applications in time series forecasting require predicting multiple steps ahead. despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. we observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. that is, optimising performance on average can lead to undesirably large errors at specific time-steps. in this work, we present a constrained learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. we call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show that despite its non-convexity, the resulting problem has a bounded duality gap. we propose a practical primal-dual algorithm to tackle it, and demonstrate that the proposed approach exhibits competitive average performance in time series forecasting benchmarks, while shaping the distribution of errors across the predicted window.",,2024-02-14,,"['ignacio hounie', 'javier porras-valenzuela', 'alejandro ribeiro']"
2402.09401,reinforcement learning from human feedback with active queries,cs.lg cs.ai cs.cl math.oc stat.ml,"aligning large language models (llm) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (rlhf). despite their superior performance, current rlhf approaches often require a large amount of human-labelled preference data, which is expensive to collect. in this paper, inspired by the success of active learning, we address this problem by proposing query-efficient rlhf methods. we first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (appo) algorithm with an $\tilde{o}(d^2/\delta)$ regret bound and an $\tilde{o}(d^2/\delta^2)$ query complexity, where $d$ is the dimension of feature space and $\delta$ is the sub-optimality gap over all the contexts. we then propose adpo, a practical version of our algorithm based on direct preference optimization (dpo) and apply it to fine-tuning llms. our experiments show that adpo, while only making about half of queries for human preference, matches the performance of the state-of-the-art dpo method.",,2024-02-14,,"['kaixuan ji', 'jiafan he', 'quanquan gu']"
2402.09456,optimistic thompson sampling for no-regret learning in unknown games,cs.lg cs.ai cs.gt stat.ml,"this work tackles the complexities of multi-player scenarios in \emph{unknown games}, where the primary challenge lies in navigating the uncertainty of the environment through bandit feedback alongside strategic decision-making. we introduce thompson sampling (ts)-based algorithms that exploit the information of opponents' actions and reward structures, leading to a substantial reduction in experimental budgets -- achieving over tenfold improvements compared to conventional approaches. notably, our algorithms demonstrate that, given specific reward structures, the regret bound depends logarithmically on the total action space, significantly alleviating the curse of multi-player. furthermore, we unveil the \emph{optimism-then-noregret} (otn) framework, a pioneering methodology that seamlessly incorporates our advancements with established algorithms, showcasing its utility in practical scenarios such as traffic routing and radar sensing in the real world.",,2024-02-07,2024-02-24,"['yingru li', 'liangqi liu', 'wenqiang pu', 'hao liang', 'zhi-quan luo']"
2402.09467,optimal thresholding linear bandit,stat.ml cs.lg,we study a novel pure exploration problem: the $\epsilon$-thresholding bandit problem (tbp) with fixed confidence in stochastic linear bandits. we prove a lower bound for the sample complexity and extend an algorithm designed for best arm identification in the linear case to tbp that is asymptotically optimal.,,2024-02-11,,"['eduardo ochoa rivera', 'ambuj tewari']"
2402.09469,fourier circuits in neural networks: unlocking the potential of large   language models in mathematical reasoning and modular arithmetic,cs.lg stat.ml,"in the evolving landscape of machine learning, a pivotal challenge lies in deciphering the internal representations harnessed by neural networks and transformers. building on recent progress toward comprehending how networks execute distinct target functions, our study embarks on an exploration of the underlying reasons behind networks adopting specific computational strategies. we direct our focus to the complex algebraic learning task of modular addition involving $k$ inputs. our research presents a thorough analytical characterization of the features learned by stylized one-hidden layer neural networks and one-layer transformers in addressing this task.   a cornerstone of our theoretical framework is the elucidation of how the principle of margin maximization shapes the features adopted by one-hidden layer neural networks. let $p$ denote the modulus, $d_p$ denote the dataset of modular arithmetic with $k$ inputs and $m$ denote the network width. we demonstrate that a neuron count of $ m \geq 2^{2k-2} \cdot (p-1) $, these networks attain a maximum $ l_{2,k+1} $-margin on the dataset $ d_p $. furthermore, we establish that each hidden-layer neuron aligns with a specific fourier spectrum, integral to solving modular addition problems.   by correlating our findings with the empirical observations of similar studies, we contribute to a deeper comprehension of the intrinsic computational mechanisms of neural networks. furthermore, we observe similar computational mechanisms in the attention matrix of the transformer. this research stands as a significant stride in unraveling their operation complexities, particularly in the realm of complex algebraic tasks.",,2024-02-12,,"['jiuxiang gu', 'chenyang li', 'yingyu liang', 'zhenmei shi', 'zhao song', 'tianyi zhou']"
2402.09470,rolling diffusion models,cs.lg stat.ml,"diffusion models have recently been increasingly applied to temporal data such as video, fluid mechanics simulations, or climate data. these methods generally treat subsequent frames equally regarding the amount of noise in the diffusion process. this paper explores rolling diffusion: a new approach that uses a sliding window denoising process. it ensures that the diffusion process progressively corrupts through time by assigning more noise to frames that appear later in a sequence, reflecting greater uncertainty about the future as the generation process unfolds. empirically, we show that when the temporal dynamics are complex, rolling diffusion is superior to standard diffusion. in particular, this result is demonstrated in a video prediction task using the kinetics-600 video dataset and in a chaotic fluid dynamics forecasting experiment.",,2024-02-12,,"['david ruhe', 'jonathan heek', 'tim salimans', 'emiel hoogeboom']"
2402.09473,one-for-many counterfactual explanations by column generation,cs.lg stat.ml,"in this paper, we consider the problem of generating a set of counterfactual explanations for a group of instances, with the one-for-many allocation rule, where one explanation is allocated to a subgroup of the instances. for the first time, we solve the problem of minimizing the number of explanations needed to explain all the instances, while considering sparsity by limiting the number of features allowed to be changed collectively in each explanation. a novel column generation framework is developed to efficiently search for the explanations. our framework can be applied to any black-box classifier, like neural networks. compared with a simple adaptation of a mixed-integer programming formulation from the literature, the column generation framework dominates in terms of scalability, computational performance and quality of the solutions.",,2024-02-12,,"['andrea lodi', 'jasone ramírez-ayerbe']"
2402.09483,oracle-efficient differentially private learning with public data,stat.ml cs.cr cs.lg,"due to statistical lower bounds on the learnability of many function classes under privacy constraints, there has been recent interest in leveraging public data to improve the performance of private learning algorithms. in this model, algorithms must always guarantee differential privacy with respect to the private samples while also ensuring learning guarantees when the private data distribution is sufficiently close to that of the public data. previous work has demonstrated that when sufficient public, unlabelled data is available, private learning can be made statistically tractable, but the resulting algorithms have all been computationally inefficient. in this work, we present the first computationally efficient, algorithms to provably leverage public data to learn privately whenever a function class is learnable non-privately, where our notion of computational efficiency is with respect to the number of calls to an optimization oracle for the function class. in addition to this general result, we provide specialized algorithms with improved sample complexities in the special cases when the function class is convex or when the task is binary classification.",,2024-02-13,,"['adam block', 'mark bun', 'rathin desai', 'abhishek shetty', 'steven wu']"
2402.09553,statistical and machine learning models for predicting fire and other   emergency events,cs.ai cs.lg stat.ml,"emergency events in a city cause considerable economic loss to individuals, their families, and the community. accurate and timely prediction of events can help the emergency fire and rescue services in preparing for and mitigating the consequences of emergency events. in this paper, we present a systematic development of predictive models for various types of emergency events in the city of edmonton, canada. we present methods for (i) data collection and dataset development; (ii) descriptive analysis of each event type and its characteristics at different spatiotemporal levels; (iii) feature analysis and selection based on correlation coefficient analysis and feature importance analysis; and (iv) development of prediction models for the likelihood of occurrence of each event type at different temporal and spatial resolutions. we analyze the association of event types with socioeconomic and demographic data at the neighborhood level, identify a set of predictors for each event type, and develop predictive models with negative binomial regression. we conduct evaluations at neighborhood and fire station service area levels. our results show that the models perform well for most of the event types with acceptable prediction errors for weekly and monthly periods. the evaluation shows that the prediction accuracy is consistent at the level of the fire station, so the predictions can be used in management by fire rescue service departments for planning resource allocation for these time periods. we also examine the impact of the covid-19 pandemic on the occurrence of events and on the accuracy of event predictor models. our findings show that covid-19 had a significant impact on the performance of the event prediction models.",,2024-02-14,,"['dilli prasad sharma', 'nasim beigi-mohammadi', 'hongxiang geng', 'dawn dixon', 'rob madro', 'phil emmenegger', 'carlos tobar', 'jeff li', 'alberto leon-garcia']"
2402.09560,distribution-free rates in neyman-pearson classification,cs.lg stat.ml,"we consider the problem of neyman-pearson classification which models unbalanced classification settings where error w.r.t. a distribution $\mu_1$ is to be minimized subject to low error w.r.t. a different distribution $\mu_0$. given a fixed vc class $\mathcal{h}$ of classifiers to be minimized over, we provide a full characterization of possible distribution-free rates, i.e., minimax rates over the space of all pairs $(\mu_0, \mu_1)$. the rates involve a dichotomy between hard and easy classes $\mathcal{h}$ as characterized by a simple geometric condition, a three-points-separation condition, loosely related to vc dimension.",,2024-02-14,,"['mohammadreza m. kalan', 'samory kpotufe']"
2402.09598,mcmc-driven learning,stat.ml cs.lg math.st stat.co stat.th,"this paper is intended to appear as a chapter for the handbook of markov chain monte carlo. the goal of this chapter is to unify various problems at the intersection of markov chain monte carlo (mcmc) and machine learning$\unicode{x2014}$which includes black-box variational inference, adaptive mcmc, normalizing flow construction and transport-assisted mcmc, surrogate-likelihood mcmc, coreset construction for mcmc with big data, markov chain gradient descent, markovian score climbing, and more$\unicode{x2014}$within one common framework. by doing so, the theory and methods developed for each may be translated and generalized.",,2024-02-14,,"['alexandre bouchard-côté', 'trevor campbell', 'geoff pleiss', 'nikola surjanovic']"
2402.09600,low-rank graph contrastive learning for node classification,cs.lg cs.si stat.ml,"graph neural networks (gnns) have been widely used to learn node representations and with outstanding performance on various tasks such as node classification. however, noise, which inevitably exists in real-world graph data, would considerably degrade the performance of gnns revealed by recent studies. in this work, we propose a novel and robust gnn encoder, low-rank graph contrastive learning (lr-gcl). our method performs transductive node classification in two steps. first, a low-rank gcl encoder named lr-gcl is trained by prototypical contrastive learning with low-rank regularization. next, using the features produced by lr-gcl, a linear transductive classification algorithm is used to classify the unlabeled nodes in the graph. our lr-gcl is inspired by the low frequency property of the graph data and its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning. to the best of our knowledge, our theoretical result is among the first to theoretically demonstrate the advantage of low-rank learning in graph contrastive learning supported by strong empirical performance. extensive experiments on public benchmarks demonstrate the superior performance of lr-gcl and the robustness of the learned node representations. the code of lr-gcl is available at \url{https://anonymous.4open.science/r/low-rank_graph_contrastive_learning-64a6/}.",,2024-02-14,,"['yancheng wang', 'yingzhen yang']"
2402.09608,"exact, fast and expressive poisson point processes via squared neural   families",cs.lg stat.ml,"we introduce squared neural poisson point processes (sneppps) by parameterising the intensity function by the squared norm of a two layer neural network. when the hidden layer is fixed and the second layer has a single neuron, our approach resembles previous uses of squared gaussian process or kernel methods, but allowing the hidden layer to be learnt allows for additional flexibility. in many cases of interest, the integrated intensity function admits a closed form and can be computed in quadratic time in the number of hidden neurons. we enumerate a far more extensive number of such cases than has previously been discussed. our approach is more memory and time efficient than naive implementations of squared or exponentiated kernel methods or gaussian processes. maximum likelihood and maximum a posteriori estimates in a reparameterisation of the final layer of the intensity function can be obtained by solving a (strongly) convex optimisation problem using projected gradient descent. we demonstrate sneppps on real, and synthetic benchmarks, and provide a software implementation. https://github.com/russelltsuchida/snefy",,2024-02-14,,"['russell tsuchida', 'cheng soon ong', 'dino sejdinovic']"
2402.09623,conformalized adaptive forecasting of heterogeneous trajectories,stat.ml cs.lg,"this paper presents a new conformal method for generating simultaneous forecasting bands guaranteed to cover the entire path of a new random trajectory with sufficiently high probability. prompted by the need for dependable uncertainty estimates in motion planning applications where the behavior of diverse objects may be more or less unpredictable, we blend different techniques from online conformal prediction of single and multiple time series, as well as ideas for addressing heteroscedasticity in regression. this solution is both principled, providing precise finite-sample guarantees, and effective, often leading to more informative predictions than prior methods.",,2024-02-14,,"['yanfei zhou', 'lars lindemann', 'matteo sesia']"
2402.09698,combining evidence across filtrations,stat.me cs.lg math.pr math.st stat.ml stat.th,"in anytime-valid sequential inference, it is known that any admissible inference procedure must be based on test martingales and their composite generalization, called e-processes, which are nonnegative processes whose expectation at any arbitrary stopping time is upper-bounded by one. an e-process quantifies the accumulated evidence against a composite null hypothesis over a sequence of outcomes. this paper studies methods for combining e-processes that are computed using different information sets, i.e., filtrations, for a null hypothesis. even though e-processes constructed on the same filtration can be combined effortlessly (e.g., by averaging), e-processes constructed on different filtrations cannot be combined as easily because their validity in a coarser filtration does not translate to validity in a finer filtration. we discuss three concrete examples of such e-processes in the literature: exchangeability tests, independence tests, and tests for evaluating and comparing forecasts with lags. our main result establishes that these e-processes can be lifted into any finer filtration using adjusters, which are functions that allow betting on the running maximum of the accumulated wealth (thereby insuring against the loss of evidence). we also develop randomized adjusters that can improve the power of the resulting sequential inference procedure.",,2024-02-14,,"['yo joong choe', 'aaditya ramdas']"
2402.09723,best arm identification for prompt learning under a limited budget,stat.ml cs.ai cs.cl cs.lg,"the remarkable instruction-following capability of large language models (llms) has sparked a growing interest in automatically learning suitable prompts. however, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing llm and evaluating the responses) has not been considered. to overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (bai-fb) in multi-armed bandits (mab). based on this connection, a general framework triple (best arm identification for prompt learning) is proposed to harness the power of bai-fb in prompt learning systematically. unique characteristics of prompt learning further lead to two embedding-based enhancements of triple by exploiting the ideas of clustering and function approximation. extensive experiments on multiple well-adopted tasks using both gpt 3.5 and llama2 demonstrate the significant performance improvement of triple over the previous baselines while satisfying the limited budget constraints.",,2024-02-15,2024-02-20,"['chengshuai shi', 'kun yang', 'jing yang', 'cong shen']"
2402.09754,robust svd made easy: a fast and reliable algorithm for large-scale data   analysis,stat.ml cs.lg math.st stat.th,"the singular value decomposition (svd) is a crucial tool in machine learning and statistical data analysis. however, it is highly susceptible to outliers in the data matrix. existing robust svd algorithms often sacrifice speed for robustness or fail in the presence of only a few outliers. this study introduces an efficient algorithm, called spherically normalized svd, for robust svd approximation that is highly insensitive to outliers, computationally scalable, and provides accurate approximations of singular vectors. the proposed algorithm achieves remarkable speed by utilizing only two applications of a standard reduced-rank svd algorithm to appropriately scaled data, significantly outperforming competing algorithms in computation times. to assess the robustness of the approximated singular vectors and their subspaces against data contamination, we introduce new notions of breakdown points for matrix-valued input, including row-wise, column-wise, and block-wise breakdown points. theoretical and empirical analyses demonstrate that our algorithm exhibits higher breakdown points compared to standard svd and its modifications. we empirically validate the effectiveness of our approach in applications such as robust low-rank approximation and robust principal component analysis of high-dimensional microarray datasets. overall, our study presents a highly efficient and robust solution for svd approximation that overcomes the limitations of existing algorithms in the presence of outliers.",,2024-02-15,,"['sangil han', 'kyoowon kim', 'sungkyu jung']"
2402.09758,extrapolation-aware nonparametric statistical inference,stat.me math.st stat.ml stat.th,"we define extrapolation as any type of statistical inference on a conditional function (e.g., a conditional expectation or conditional quantile) evaluated outside of the support of the conditioning variable. this type of extrapolation occurs in many data analysis applications and can invalidate the resulting conclusions if not taken into account. while extrapolating is straightforward in parametric models, it becomes challenging in nonparametric models. in this work, we extend the nonparametric statistical model to explicitly allow for extrapolation and introduce a class of extrapolation assumptions that can be combined with existing inference techniques to draw extrapolation-aware conclusions. the proposed class of extrapolation assumptions stipulate that the conditional function attains its minimal and maximal directional derivative, in each direction, within the observed support. we illustrate how the framework applies to several statistical applications including prediction and uncertainty quantification. we furthermore propose a consistent estimation procedure that can be used to adjust existing nonparametric estimates to account for extrapolation by providing lower and upper extrapolation bounds. the procedure is empirically evaluated on both simulated and real-world data.",,2024-02-15,,"['niklas pfister', 'peter bühlmann']"
2402.09796,closed-form filtering for non-linear systems,stat.ml cs.lg cs.ro,"sequential bayesian filtering aims to estimate the current state distribution of a hidden markov model, given the past observations. the problem is well-known to be intractable for most application domains, except in notable cases such as the tabular setting or for linear dynamical systems with gaussian noise. in this work, we propose a new class of filters based on gaussian psd models, which offer several advantages in terms of density approximation and computational efficiency. we show that filtering can be efficiently performed in closed form when transitions and observations are gaussian psd models. when the transition and observations are approximated by gaussian psd models, we show that our proposed estimator enjoys strong theoretical guarantees, with estimation error that depends on the quality of the approximation and is adaptive to the regularity of the transition probabilities. in particular, we identify regimes in which our proposed filter attains a tv $\epsilon$-error with memory and computational complexity of $o(\epsilon^{-1})$ and $o(\epsilon^{-3/2})$ respectively, including the offline learning step, in contrast to the $o(\epsilon^{-2})$ complexity of sampling methods such as particle filtering.",,2024-02-15,,"['théophile cantelobre', 'carlo ciliberto', 'benjamin guedj', 'alessandro rudi']"
2402.09802,criterion collapse and loss distribution control,stat.ml cs.lg,"in this work, we consider the notion of ""criterion collapse,"" in which optimization of one metric implies optimality in another, with a particular focus on conditions for collapse into error probability minimizers under a wide variety of learning criteria, ranging from dro and oce risks (cvar, tilted erm) to non-monotonic criteria underlying recent ascent-descent algorithms explored in the literature (flooding, softad). we show how collapse in the context of losses with a bernoulli distribution goes far beyond existing results for cvar and dro, then expand our scope to include surrogate losses, showing conditions where monotonic criteria such as tilted erm cannot avoid collapse, whereas non-monotonic alternatives can.",,2024-02-15,,['matthew j. holland']
2402.09807,two trust region type algorithms for solving nonconvex-strongly concave   minimax problems,math.oc cs.lg stat.ml,"in this paper, we propose a minimax trust region (minimax-tr) algorithm and a minimax trust region algorithm with contractions and expansions(minimax-trace) algorithm for solving nonconvex-strongly concave minimax problems. both algorithms can find an $(\epsilon, \sqrt{\epsilon})$-second order stationary point(ssp) within $\mathcal{o}(\epsilon^{-1.5})$ iterations, which matches the best well known iteration complexity.",,2024-02-15,,"['tongliang yao', 'zi xu']"
2402.09849,recommendations for baselines and benchmarking approximate gaussian   processes,cs.lg stat.ml,"gaussian processes (gps) are a mature and widely-used component of the ml toolbox. one of their desirable qualities is automatic hyperparameter selection, which allows for training without user intervention. however, in many realistic settings, approximations are typically needed, which typically do require tuning. we argue that this requirement for tuning complicates evaluation, which has led to a lack of a clear recommendations on which method should be used in which situation. to address this, we make recommendations for comparing gp approximations based on a specification of what a user should expect from a method. in addition, we develop a training procedure for the variational method of titsias [2009] that leaves no choices to the user, and show that this is a strong baseline that meets our specification. we conclude that benchmarking according to our suggestions gives a clearer view of the current state of the field, and uncovers problems that are still open that future papers should address.",,2024-02-15,,"['sebastian w. ober', 'artem artemev', 'marcel wagenländer', 'rudolfs grobins', 'mark van der wilk']"
2402.09891,predictors from causal features do not generalize better to new domains,cs.lg stat.ml,"we study how well machine learning models trained on causal features generalize across domains. we consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. for each prediction task, we select features that have a causal influence on the target of prediction. our goal is to test the hypothesis that models trained on causal features generalize better across domains. without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features. if the goal is to generalize to new domains, practitioners might as well train the best possible model on all available features.",,2024-02-15,,"['vivian y. nastl', 'moritz hardt']"
2402.09941,fedlion: faster adaptive federated optimization with fewer communication,cs.lg cs.ai stat.ml,"in federated learning (fl), a framework to train machine learning models across distributed data, well-known algorithms like fedavg tend to have slow convergence rates, resulting in high communication costs during training. to address this challenge, we introduce fedlion, an adaptive federated optimization algorithm that seamlessly incorporates key elements from the recently proposed centralized adaptive algorithm, lion (chen et al. 2o23), into the fl framework. through comprehensive evaluations on two widely adopted fl benchmarks, we demonstrate that fedlion outperforms previous state-of-the-art adaptive algorithms, including fafed (wu et al. 2023) and fedda. moreover, thanks to the use of signed gradients in local training, fedlion substantially reduces data transmission requirements during uplink communication when compared to existing adaptive algorithms, further reducing communication costs. last but not least, this work also includes a novel theoretical analysis, showcasing that fedlion attains faster convergence rate than established fl algorithms like fedavg.",,2024-02-15,,"['zhiwei tang', 'tsung-hui chang']"
2402.09970,accelerating parallel sampling of diffusion models,cs.lg stat.ml,"diffusion models have emerged as state-of-the-art generative models for image generation. however, sampling from diffusion models is usually time-consuming due to the inherent autoregressive nature of their sampling process. in this work, we propose a novel approach that accelerates the sampling of diffusion models by parallelizing the autoregressive process. specifically, we reformulate the sampling process as solving a system of triangular nonlinear equations through fixed-point iteration. with this innovative formulation, we explore several systematic techniques to further reduce the iteration steps required by the solving process. applying these techniques, we introduce parataa, a universal and training-free parallel sampling algorithm that can leverage extra computational and memory resources to increase the sampling speed. our experiments demonstrate that parataa can decrease the inference steps required by common sequential sampling algorithms such as ddim and ddpm by a factor of 4~14 times. notably, when applying parataa with 100 steps ddim for stable diffusion, a widely-used text-to-image diffusion model, it can produce the same images as the sequential sampling in only 7 inference steps.",,2024-02-15,,"['zhiwei tang', 'jiasheng tang', 'hao luo', 'fan wang', 'tsung-hui chang']"
2402.10028,diffusion models meet contextual bandits with large action spaces,cs.lg cs.ai stat.ml,"efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. in this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion thompson sampling (dts). both theoretical and algorithmic foundations are developed for dts, and empirical evaluation also shows its favorable performance.",,2024-02-15,,['imad aouali']
2402.10062,optimal parameter and neuron pruning for out-of-distribution detection,cs.lg stat.ml,"for a machine learning model deployed in real world scenarios, the ability of detecting out-of-distribution (ood) samples is indispensable and challenging. most existing ood detection methods focused on exploring advanced training skills or training-free tricks to prevent the model from yielding overconfident confidence score for unknown samples. the training-based methods require expensive training cost and rely on ood samples which are not always available, while most training-free methods can not efficiently utilize the prior information from the training data. in this work, we propose an \textbf{o}ptimal \textbf{p}arameter and \textbf{n}euron \textbf{p}runing (\textbf{opnp}) approach, which aims to identify and remove those parameters and neurons that lead to over-fitting. the main method is divided into two steps. in the first step, we evaluate the sensitivity of the model parameters and neurons by averaging gradients over all training samples. in the second step, the parameters and neurons with exceptionally large or close to zero sensitivities are removed for prediction. our proposal is training-free, compatible with other post-hoc methods, and exploring the information from all training data. extensive experiments are performed on multiple ood detection tasks and model architectures, showing that our proposed opnp consistently outperforms the existing methods by a large margin.",,2024-02-04,,"['chao chen', 'zhihang fu', 'kai liu', 'ze chen', 'mingyuan tao', 'jieping ye']"
2402.10065,how much does each datapoint leak your privacy? quantifying the   per-datum membership leakage,cs.lg cs.cr math.st stat.ml stat.th,"we study the per-datum membership inference attacks (mias), where an attacker aims to infer whether a fixed target datum has been included in the input dataset of an algorithm and thus, violates privacy. first, we define the membership leakage of a datum as the advantage of the optimal adversary targeting to identify it. then, we quantify the per-datum membership leakage for the empirical mean, and show that it depends on the mahalanobis distance between the target datum and the data-generating distribution. we further assess the effect of two privacy defences, i.e. adding gaussian noise and sub-sampling. we quantify exactly how both of them decrease the per-datum membership leakage. our analysis builds on a novel proof technique that combines an edgeworth expansion of the likelihood ratio test and a lindeberg-feller central limit theorem. our analysis connects the existing likelihood ratio and scalar product attacks, and also justifies different canary selection strategies used in the privacy auditing literature. finally, our experiments demonstrate the impacts of the leakage score, the sub-sampling ratio and the noise scale on the per-datum membership leakage as indicated by the theory.",,2024-02-15,,"['achraf azize', 'debabrota basu']"
2402.10126,"exchangeability, prediction and predictive modeling in bayesian   statistics",math.st stat.th,"prediction is a central problem in statistics, and there is currently a renewed interest for the so-called predictive approach in bayesian statistics. what is the latter about? one has to return on foundational concepts, which we do in this paper, moving from the role of exchangeability and reviewing forms of partial exchangeability for more structured data, with the aim of discussing their use and implications in bayesian statistics. there we show the underlying concept that, in bayesian statistics, a predictive rule is meant as a learning rule - how one conveys past information to information on future events. this concept has implications on the use of exchangeability and generally invests all statistical problems, also in inference. it applies to classic contexts and to less explored situations, such as the use of predictive algorithms that can be read as bayesian learning rules. the paper offers a historical overview, but also includes a few new results, presents some recent developments and poses some open questions.",,2024-02-15,,"['sandra fortini', 'sonia petrone']"
2402.10127,nonlinear spiked covariance matrices and signal propagation in deep   neural networks,stat.ml cs.lg math.pr math.st stat.th,"many recent works have studied the eigenvalue spectrum of the conjugate kernel (ck) defined by the nonlinear feature map of a feedforward neural network. however, existing results only establish weak convergence of the empirical eigenvalue distribution, and fall short of providing precise quantitative characterizations of the ''spike'' eigenvalues and eigenvectors that often capture the low-dimensional signal structure of the learning problem. in this work, we characterize these signal eigenvalues and eigenvectors for a nonlinear version of the spiked covariance model, including the ck as a special case. using this general result, we give a quantitative description of how spiked eigenstructure in the input data propagates through the hidden layers of a neural network with random weights. as a second application, we study a simple regime of representation learning where the weight matrix develops a rank-one signal component over training and characterize the alignment of the target function with the spike eigenvector of the ck on test data.",,2024-02-15,,"['zhichao wang', 'denny wu', 'zhou fan']"
2402.10198,unlocking the potential of transformers in time series forecasting with   sharpness-aware minimization and channel-wise attention,cs.lg stat.ml,"transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. to better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. we further identify the attention of transformers as being responsible for this low generalization capacity. building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. we empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. in particular, samformer surpasses the current state-of-the-art model tsmixer by 14.33% on average, while having ~4 times fewer parameters. the code is available at https://github.com/romilbert/samformer.",,2024-02-15,2024-02-19,"['romain ilbert', 'ambroise odonnat', 'vasilii feofanov', 'aladin virmaux', 'giuseppe paolo', 'themis palpanas', 'ievgen redko']"
2402.10210,self-play fine-tuning of diffusion models for text-to-image generation,cs.lg cs.ai cs.cl cs.cv stat.ml,"fine-tuning diffusion models remains an underexplored frontier in generative artificial intelligence (genai), especially when compared with the remarkable progress made in fine-tuning large language models (llms). while cutting-edge diffusion models such as stable diffusion (sd) and sdxl rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. recently, reinforcement learning (rl) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (""winner"" and ""loser"" images) for each text prompt. in this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (spin-diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. our approach offers an alternative to conventional supervised fine-tuning and rl strategies, significantly improving both model performance and alignment. our experiments on the pick-a-pic dataset reveal that spin-diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. by the second iteration, it exceeds the performance of rlhf-based methods across all metrics, achieving these results with less data.",,2024-02-15,,"['huizhuo yuan', 'zixiang chen', 'kaixuan ji', 'quanquan gu']"
2402.10227,"correlational lagrangian schr\""odinger bridge: learning dynamics with   population-level regularization",cs.lg stat.ml,"accurate modeling of system dynamics holds intriguing potential in broad scientific fields including cytodynamics and fluid mechanics. this task often presents significant challenges when (i) observations are limited to cross-sectional samples (where individual trajectories are inaccessible for learning), and moreover, (ii) the behaviors of individual particles are heterogeneous (especially in biological systems due to biodiversity). to address them, we introduce a novel framework dubbed correlational lagrangian schr\""odinger bridge (clsb), aiming to seek for the evolution ""bridging"" among cross-sectional observations, while regularized for the minimal population ""cost"". in contrast to prior methods relying on \textit{individual}-level regularizers for all particles \textit{homogeneously} (e.g. restraining individual motions), clsb operates at the population level admitting the heterogeneity nature, resulting in a more generalizable modeling in practice. to this end, our contributions include (1) a new class of population regularizers capturing the temporal variations in multivariate relations, with the tractable formulation derived, (2) three domain-informed instantiations based on genetic co-expression stability, and (3) an integration of population regularizers into data-driven generative models as constrained optimization, and a numerical solution, with further extension to conditional generative models. empirically, we demonstrate the superiority of clsb in single-cell sequencing data analyses such as simulating cell development over time and predicting cellular responses to drugs of varied doses.",,2024-02-04,,"['yuning you', 'ruida zhou', 'yang shen']"
2402.10232,"simple, unified analysis of johnson-lindenstrauss with applications",stat.ml cs.ds cs.lg math.pr,"we present a simple and unified analysis of the johnson-lindenstrauss (jl) lemma, a cornerstone in the field of dimensionality reduction critical for managing high-dimensional data. our approach not only simplifies the understanding but also unifies various constructions under the jl framework, including spherical, binary-coin, sparse jl, gaussian and sub-gaussian models. this simplification and unification make significant strides in preserving the intrinsic geometry of data, essential across diverse applications from streaming algorithms to reinforcement learning. notably, we deliver the first rigorous proof of the spherical construction's effectiveness and provide a general class of sub-gaussian constructions within this simplified framework. at the heart of our contribution is an innovative extension of the hanson-wright inequality to high dimensions, complete with explicit constants. by employing simple yet powerful probabilistic tools and analytical techniques, such as an enhanced diagonalization process, our analysis not only solidifies the jl lemma's theoretical foundation by removing an independence assumption but also extends its practical reach, showcasing its adaptability and importance in contemporary computational algorithms.",,2024-02-10,2024-02-27,['yingru li']
2402.10252,online control of linear systems with unbounded and degenerate noise,eess.sy cs.lg cs.sy math.oc stat.ml,"this paper investigates the problem of controlling a linear system under possibly unbounded and degenerate noise with unknown cost functions, known as an online control problem. in contrast to the existing work, which assumes the boundedness of noise, we reveal that for convex costs, an $ \widetilde{o}(\sqrt{t}) $ regret bound can be achieved even for unbounded noise, where $ t $ denotes the time horizon. moreover, when the costs are strongly convex, we establish an $ o({\rm poly} (\log t)) $ regret bound without the assumption that noise covariance is non-degenerate, which has been required in the literature. the key ingredient in removing the rank assumption on noise is a system transformation associated with the noise covariance. this simultaneously enables the parameter reduction of an online control algorithm.",,2024-02-15,,"['kaito ito', 'taira tsuchiya']"
2402.10282,information capacity regret bounds for bandits with mediator feedback,cs.lg stat.ml,"this work addresses the mediator feedback problem, a bandit game where the decision set consists of a number of policies, each associated with a probability distribution over a common space of outcomes. upon choosing a policy, the learner observes an outcome sampled from its distribution and incurs the loss assigned to this outcome in the present round. we introduce the policy set capacity as an information-theoretic measure for the complexity of the policy set. adopting the classical exp4 algorithm, we provide new regret bounds depending on the policy set capacity in both the adversarial and the stochastic settings. for a selection of policy set families, we prove nearly-matching lower bounds, scaling similarly with the capacity. we also consider the case when the policies' distributions can vary between rounds, thus addressing the related bandits with expert advice problem, which we improve upon its prior results. additionally, we prove a lower bound showing that exploiting the similarity between the policies is not possible in general under linear bandit feedback. finally, for a full-information variant, we provide a regret bound scaling with the information radius of the policy set.",,2024-02-15,,"['khaled eldowa', 'nicolò cesa-bianchi', 'alberto maria metelli', 'marcello restelli']"
2402.10289,thompson sampling in partially observable contextual bandits,stat.ml cs.lg,"contextual bandits constitute a classical framework for decision-making under uncertainty. in this setting, the goal is to learn the arms of highest reward subject to contextual information, while the unknown reward parameters of each arm need to be learned by experimenting that specific arm. accordingly, a fundamental problem is that of balancing exploration (i.e., pulling different arms to learn their parameters), versus exploitation (i.e., pulling the best arms to gain reward). to study this problem, the existing literature mostly considers perfectly observed contexts. however, the setting of partial context observations remains unexplored to date, despite being theoretically more general and practically more versatile. we study bandit policies for learning to select optimal arms based on the data of observations, which are noisy linear functions of the unobserved context vectors. our theoretical analysis shows that the thompson sampling policy successfully balances exploration and exploitation. specifically, we establish the followings: (i) regret bounds that grow poly-logarithmically with time, (ii) square-root consistency of parameter estimation, and (iii) scaling of the regret with other quantities including dimensions and number of arms. extensive numerical experiments with both real and synthetic data are presented as well, corroborating the efficacy of thompson sampling. to establish the results, we introduce novel martingale techniques and concentration inequalities to address partially observed dependent random variables generated from unspecified distributions, and also leverage problem-dependent information to sharpen probabilistic bounds for time-varying suboptimality gaps. these techniques pave the road towards studying other decision-making problems with contextual information as well as partial observations.",,2024-02-15,,"['hongju park', 'mohamad kazem shirani faradonbeh']"
2402.10291,an evaluation of real-time adaptive sampling change point detection   algorithm using kcusum,cs.lg stat.ml,"detecting abrupt changes in real-time data streams from scientific simulations presents a challenging task, demanding the deployment of accurate and efficient algorithms. identifying change points in live data stream involves continuous scrutiny of incoming observations for deviations in their statistical characteristics, particularly in high-volume data scenarios. maintaining a balance between sudden change detection and minimizing false alarms is vital. many existing algorithms for this purpose rely on known probability distributions, limiting their feasibility. in this study, we introduce the kernel-based cumulative sum (kcusum) algorithm, a non-parametric extension of the traditional cumulative sum (cusum) method, which has gained prominence for its efficacy in online change point detection under less restrictive conditions. kcusum splits itself by comparing incoming samples directly with reference samples and computes a statistic grounded in the maximum mean discrepancy (mmd) non-parametric framework. this approach extends kcusum's pertinence to scenarios where only reference samples are available, such as atomic trajectories of proteins in vacuum, facilitating the detection of deviations from the reference sample without prior knowledge of the data's underlying distribution. furthermore, by harnessing mmd's inherent random-walk structure, we can theoretically analyze kcusum's performance across various use cases, including metrics like expected delay and mean runtime to false alarms. finally, we discuss real-world use cases from scientific simulations such as nwchem codar and protein folding data, demonstrating kcusum's practical effectiveness in online change point detection.",,2024-02-15,,"['vijayalakshmi saravanan', 'perry siehien', 'shinjae yoo', 'hubertus van dam', 'thomas flynn', 'christopher kelly', 'khaled z ibrahim']"
2402.10357,efficient sampling on riemannian manifolds via langevin mcmc,math.st cs.lg math.pr stat.co stat.ml stat.th,"we study the task of efficiently sampling from a gibbs distribution $d \pi^* = e^{-h} d {vol}_g$ over a riemannian manifold $m$ via (geometric) langevin mcmc; this algorithm involves computing exponential maps in random gaussian directions and is efficiently implementable in practice. the key to our analysis of langevin mcmc is a bound on the discretization error of the geometric euler-murayama scheme, assuming $\nabla h$ is lipschitz and $m$ has bounded sectional curvature. our error bound matches the error of euclidean euler-murayama in terms of its stepsize dependence. combined with a contraction guarantee for the geometric langevin diffusion under kendall-cranston coupling, we prove that the langevin mcmc iterates lie within $\epsilon$-wasserstein distance of $\pi^*$ after $\tilde{o}(\epsilon^{-2})$ steps, which matches the iteration complexity for euclidean langevin mcmc. our results apply in general settings where $h$ can be nonconvex and $m$ can have negative ricci curvature. under additional assumptions that the riemannian curvature tensor has bounded derivatives, and that $\pi^*$ satisfies a $cd(\cdot,\infty)$ condition, we analyze the stochastic gradient version of langevin mcmc, and bound its iteration complexity by $\tilde{o}(\epsilon^{-2})$ as well.",,2024-02-15,,"['xiang cheng', 'jingzhao zhang', 'suvrit sra']"
2402.10360,learnability is a compact property,cs.lg cs.cc cs.ds cs.lo stat.ml,"recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard zfc axioms of set theory. furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem.   on the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. how can these results be reconciled? more precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations?   we demonstrate that the difficulty of supervised learning with metric losses admits a tight finite characterization. in particular, we prove that the sample complexity of learning a hypothesis class can be detected by examining its finite projections. for realizable and agnostic learning with respect to a wide class of proper loss functions, we demonstrate an exact compactness result: a class is learnable with a given sample complexity precisely when the same is true of all its finite projections. for realizable learning with improper loss functions, we show that exact compactness of sample complexity can fail, and provide matching upper and lower bounds of a factor of 2 on the extent to which such sample complexities can differ. we conjecture that larger gaps are possible for the agnostic case.   at the heart of our technical work is a compactness result concerning assignments of variables that maintain a class of functions below a target value, which generalizes hall's classic matching theorem and may be of independent interest.",,2024-02-15,,"['julian asilis', 'siddartha devic', 'shaddin dughmi', 'vatsal sharan', 'shang-hua teng']"
2402.10421,recurrent neural networks for multivariate loss reserving and risk   capital analysis,stat.ap,"reserves comprise most of the liabilities of a property and casualty (p&c) company and are actuaries' best estimate for unpaid future claims. notably, the reserves for different lines of business (lob) are related, as there may be dependence between events related to claims. there have been parametric and non-parametric methods in the actuarial industry for loss reserving; only a few tools have been developed to use the recurrent neural network (rnn) for multivariate loss reserving and risk capital analyses. this paper aims to study rnn methods to model dependence between loss triangles and develop predictive distribution for reserves using machine learning. thus, we create an rnn model to capture dependence between lobs by extending the deep triangle (dt) model from kuo (2019). in the extended deep triangle (edt), we use the incremental paid loss from two lobs as input and the symmetric squared loss of two lobs as the loss function. then, we extend generative adversarial networks (gans) by transforming the two loss triangles into a tabular format and generating synthetic loss triangles to obtain the predictive distribution for reserves. to illustrate our method, we apply and calibrate these methods on personal and commercial automobile lines from a large us p&c insurance company and compare the results with copula regression models. the results show that the edt model performs better than the copula regression models in predicting total loss reserve. in addition, with the obtained predictive distribution for reserves, we show that risk capitals calculated from edt combined with gan are smaller than that of the copula regression models, which implies a more considerable diversification benefit. finally, these findings are also confirmed in a simulation study.",,2024-02-15,,"['pengfei cai', 'anas abdallah', 'pratheepa jeganathan']"
2402.10429,fixed confidence best arm identification in the bayesian setting,stat.ml cs.lg,"we consider the fixed-confidence best arm identification (fc-bai) problem in the bayesian setting. this problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior. most studies on the fc-bai problem have been conducted in the frequentist setting, where the bandit model is predetermined before the game starts. we show that the traditional fc-bai algorithms studied in the frequentist setting, such as track-and-stop and top-two algorithms, result in arbitrary suboptimal performances in the bayesian setting. we also prove a lower bound of the expected number of samples in the bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. simulations verify the theoretical results.",,2024-02-15,,"['kyoungseok jang', 'junpei komiyama', 'kazutoshi yamazaki']"
2402.10445,collaborative learning with different labeling functions,cs.lg cs.ds stat.ml,"we study a variant of collaborative pac learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   we show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. we give a learning algorithm based on empirical risk minimization (erm) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the vc dimension of this augmented class.   in terms of the computational efficiency, we show that erm on the augmented hypothesis class is np-hard, which gives evidence against the existence of computationally efficient learners in general. on the positive side, for two special cases, we give learners that are both sample- and computationally-efficient.",,2024-02-15,2024-02-20,"['yuyang deng', 'mingda qiao']"
2402.10456,generative modeling for tabular data via penalized optimal transport   network,stat.ml cs.lg stat.ap stat.me,"the task of precisely learning the probability distribution of rows within tabular data and producing authentic synthetic samples is both crucial and non-trivial. wasserstein generative adversarial network (wgan) marks a notable improvement in generative modeling, addressing the challenges faced by its predecessor, generative adversarial network. however, due to the mixed data types and multimodalities prevalent in tabular data, the delicate equilibrium between the generator and discriminator, as well as the inherent instability of wasserstein distance in high dimensions, wgan often fails to produce high-fidelity samples. to this end, we propose potnet (penalized optimal transport network), a generative deep neural network based on a novel, robust, and interpretable marginally-penalized wasserstein (mpw) loss. potnet can effectively model tabular data containing both categorical and continuous features. moreover, it offers the flexibility to condition on a subset of features. we provide theoretical justifications for the motivation behind the mpw loss. we also empirically demonstrate the effectiveness of our proposed method on four different benchmarks across a variety of real-world and simulated datasets. our proposed model achieves orders of magnitude speedup during the sampling stage compared to state-of-the-art generative models for tabular data, thereby enabling efficient large-scale synthetic data generation.",,2024-02-16,,"['wenhui sophia lu', 'chenyang zhong', 'wing hung wong']"
2402.10470,theoretical understanding of learning from adversarial perturbations,cs.lg cs.cv stat.ml,"it is not fully understood why adversarial examples can deceive neural networks and transfer between different networks. to elucidate this, several studies have hypothesized that adversarial perturbations, while appearing as noises, contain class features. this is supported by empirical evidence showing that networks trained on mislabeled adversarial examples can still generalize well to correctly labeled test samples. however, a theoretical understanding of how perturbations include class features and contribute to generalization is limited. in this study, we provide a theoretical framework for understanding learning from perturbations using a one-hidden-layer network trained on mutually orthogonal samples. our results highlight that various adversarial perturbations, even perturbations of a few pixels, contain sufficient class features for generalization. moreover, we reveal that the decision boundary when learning from perturbations matches that from standard samples except for specific regions under mild conditions. the code is available at https://github.com/s-kumano/learning-from-adversarial-perturbations.",,2024-02-16,,"['soichiro kumano', 'hiroshi kera', 'toshihiko yamasaki']"
2402.10474,one-bit quantization and sparsification for multiclass linear   classification via regularized regression,cs.lg stat.ml,"we study the use of linear regression for multiclass classification in the over-parametrized regime where some of the training data is mislabeled. in such scenarios it is necessary to add an explicit regularization term, $\lambda f(w)$, for some convex function $f(\cdot)$, to avoid overfitting the mislabeled data. in our analysis, we assume that the data is sampled from a gaussian mixture model with equal class sizes, and that a proportion $c$ of the training labels is corrupted for each class. under these assumptions, we prove that the best classification performance is achieved when $f(\cdot) = \|\cdot\|^2_2$ and $\lambda \to \infty$. we then proceed to analyze the classification errors for $f(\cdot) = \|\cdot\|_1$ and $f(\cdot) = \|\cdot\|_\infty$ in the large $\lambda$ regime and notice that it is often possible to find sparse and one-bit solutions, respectively, that perform almost as well as the one corresponding to $f(\cdot) = \|\cdot\|_2^2$.",,2024-02-16,,"['reza ghane', 'danil akhtiamov', 'babak hassibi']"
2402.10482,understanding self-distillation and partial label learning in   multi-class classification with label noise,cs.lg stat.ml,"self-distillation (sd) is the process of training a student model using the outputs of a teacher model, with both models sharing the same architecture. our study theoretically examines sd in multi-class classification with cross-entropy loss, exploring both multi-round sd and sd with refined teacher outputs, inspired by partial label learning (pll). by deriving a closed-form solution for the student model's outputs, we discover that sd essentially functions as label averaging among instances with high feature correlations. initially beneficial, this averaging helps the model focus on feature clusters correlated with a given instance for predicting the label. however, it leads to diminishing performance with increasing distillation rounds. additionally, we demonstrate sd's effectiveness in label noise scenarios and identify the label corruption condition and minimum number of distillation rounds needed to achieve 100% classification accuracy. our study also reveals that one-step distillation with refined teacher outputs surpasses the efficacy of multi-step sd using the teacher's direct output in high noise rate regimes.",,2024-02-16,,"['hyeonsu jeong', 'hye won chung']"
2402.10504,resilience of the quadratic littlewood-offord problem,math.pr cs.it cs.lg math.co math.it stat.ml,"we study the statistical resilience of high-dimensional data. our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic radamecher chaos $\boldsymbol{\xi}^{\mathsf{t}} m \boldsymbol{\xi}$, where $m$ is a fixed (high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal rademacher vector. specifically, we pursue the question of how many adversarial sign-flips can $\boldsymbol{\xi}$ sustain without ""inflating"" $\sup_{x\in \mathbb{r}} \mathbb{p} \left\{\boldsymbol{\xi}^{\mathsf{t}} m \boldsymbol{\xi} = x\right\}$ and thus ""de-smooth"" the original distribution resulting in a more ""grainy"" and adversarially biased distribution. our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear rademacher chaos; these are shown to be asymptotically tight across key regimes.",,2024-02-16,,"['elad aigner-horev', 'daniel rozenberg', 'roi weiss']"
2402.10574,nowcasting with mixed frequency data using gaussian processes,econ.em stat.ml,we propose and discuss bayesian machine learning methods for mixed data sampling (midas) regressions. this involves handling frequency mismatches with restricted and unrestricted midas variants and specifying functional relationships between many predictors and the dependent variable. we use gaussian processes (gp) and bayesian additive regression trees (bart) as flexible extensions to linear penalized estimation. in a nowcasting and forecasting exercise we focus on quarterly us output growth and inflation in the gdp deflator. the new models leverage macroeconomic big data in a computationally efficient way and offer gains in predictive accuracy along several dimensions.,,2024-02-16,,"['niko hauzenberger', 'massimiliano marcellino', 'michael pfarrhofer', 'anna stelzer']"
2402.10592,optimizing adaptive experiments: a unified approach to regret   minimization and best-arm identification,cs.lg econ.em stat.ml,"practitioners conducting adaptive experiments often encounter two competing priorities: reducing the cost of experimentation by effectively assigning treatments during the experiment itself, and gathering information swiftly to conclude the experiment and implement a treatment across the population. currently, the literature is divided, with studies on regret minimization addressing the former priority in isolation, and research on best-arm identification focusing solely on the latter. this paper proposes a unified model that accounts for both within-experiment performance and post-experiment outcomes. we then provide a sharp theory of optimal performance in large populations that unifies canonical results in the literature. this unification also uncovers novel insights. for example, the theory reveals that familiar algorithms, like the recently proposed top-two thompson sampling algorithm, can be adapted to optimize a broad class of objectives by simply adjusting a single scalar parameter. in addition, the theory reveals that enormous reductions in experiment duration can sometimes be achieved with minimal impact on both within-experiment and post-experiment regret.",,2024-02-16,,"['chao qin', 'daniel russo']"
2402.10677,performance gaps in multi-view clustering under the nested matrix-tensor   model,stat.ml cs.lg math.pr,"we study the estimation of a planted signal hidden in a recently introduced nested matrix-tensor model, which is an extension of the classical spiked rank-one tensor model, motivated by multi-view clustering. prior work has theoretically examined the performance of a tensor-based approach, which relies on finding a best rank-one approximation, a problem known to be computationally hard. a tractable alternative approach consists in computing instead the best rank-one (matrix) approximation of an unfolding of the observed tensor data, but its performance was hitherto unknown. we quantify here the performance gap between these two approaches, in particular by deriving the precise algorithmic threshold of the unfolding approach and demonstrating that it exhibits a bbp-type transition behavior. this work is therefore in line with recent contributions which deepen our understanding of why tensor-based methods surpass matrix-based methods in handling structured tensor data.",,2024-02-16,,"['hugo lebeau', 'mohamed el amine seddik', 'josé henrique de morais goulart']"
2402.10723,conformalized credal set predictors,stat.ml cs.lg,"credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution. in machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction. however, the design of methods for learning credal set predictors remains a challenging problem. in this paper, we make use of conformal prediction for this purpose. more specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions. since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution). we demonstrate the applicability of our method to natural language inference, a highly ambiguous natural language task where it is common to obtain multiple annotations per example.",,2024-02-16,,"['alireza javanmardi', 'david stutz', 'eyke hüllermeier']"
2402.10727,predictive uncertainty quantification via risk decompositions for   strictly proper scoring rules,stat.ml cs.lg,"distinguishing sources of predictive uncertainty is of crucial importance in the application of forecasting models across various domains. despite the presence of a great variety of proposed uncertainty measures, there are no strict definitions to disentangle them. furthermore, the relationship between different measures of uncertainty quantification remains somewhat unclear. in this work, we introduce a general framework, rooted in statistical reasoning, which not only allows the creation of new uncertainty measures but also clarifies their interrelations. our approach leverages statistical risk to distinguish aleatoric and epistemic uncertainty components and utilizes proper scoring rules to quantify them. to make it practically tractable, we propose an idea to incorporate bayesian reasoning into this framework and discuss the properties of the proposed approximation.",,2024-02-16,,"['nikita kotelevskii', 'maxim panov']"
2402.10758,stochastic localization via iterative posterior sampling,stat.ml cs.lg stat.co,"building upon score-based learning, new interest in stochastic localization techniques has recently emerged. in these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. this work contributes to fill this gap. we consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. we provide a complete methodology, $\textit{stochastic localization via iterative posterior sampling}$ (slips), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. our scheme is based on a markov chain monte carlo estimation of the denoiser and comes with detailed practical guidelines. we illustrate the benefits and applicability of slips on several benchmarks, including gaussian mixtures in increasing dimensions, bayesian logistic regression and a high-dimensional field system from statistical-mechanics.",,2024-02-16,,"['louis grenioux', 'maxence noble', 'marylou gabrié', 'alain oliviero durmus']"
2402.10774,error feedback reloaded: from quadratic to arithmetic mean of smoothness   constants,cs.lg cs.ai math.oc stat.ml,"error feedback (ef) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed gd or sgd) when these are enhanced with greedy communication compression techniques such as topk. while ef was proposed almost a decade ago (seide et al., 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore. in this work we study a modern form of error feedback called ef21 (richtarik et al., 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. in particular, while the theoretical communication complexity of ef21 depends on the quadratic mean of certain smoothness parameters, we improve this dependence to their arithmetic mean, which is always smaller, and can be substantially smaller, especially in heterogeneous data regimes. we take the reader on a journey of our discovery process. starting with the idea of applying ef21 to an equivalent reformulation of the underlying problem which (unfortunately) requires (often impractical) machine cloning, we continue to the discovery of a new weighted version of ef21 which can (fortunately) be executed without any cloning, and finally circle back to an improved analysis of the original ef21 method. while this development applies to the simplest form of ef21, our approach naturally extends to more elaborate variants involving stochastic gradients and partial participation. further, our technique improves the best-known theory of ef21 in the rare features regime (richtarik et al., 2023). finally, we validate our theoretical findings with suitable experiments.",,2024-02-16,,"['peter richtárik', 'elnur gasanov', 'konstantin burlachenko']"
2402.10797,blackjax: composable bayesian inference in jax,cs.ms cs.lg stat.co stat.ml,"blackjax is a library implementing sampling and variational inference algorithms commonly used in bayesian computation. it is designed for ease of use, speed, and modularity by taking a functional approach to the algorithms' implementation. blackjax is written in python, using jax to compile and run numppy-like samplers and variational methods on cpus, gpus, and tpus. the library integrates well with probabilistic programming languages by working directly with the (un-normalized) target log density function. blackjax is intended as a collection of low-level, composable implementations of basic statistical 'atoms' that can be combined to perform well-defined bayesian inference, but also provides high-level routines for ease of use. it is designed for users who need cutting-edge methods, researchers who want to create complex sampling methods, and people who want to learn how these work.",,2024-02-16,2024-02-22,"['alberto cabezas', 'adrien corenflos', 'junpeng lao', 'rémi louf', 'antoine carnec', 'kaustubh chaudhari', 'reuben cohn-gordon', 'jeremie coullon', 'wei deng', 'sam duffield', 'gerardo durán-martín', 'marcin elantkowski', 'dan foreman-mackey', 'michele gregori', 'carlos iguaran', 'ravin kumar', 'martin lysy', 'kevin murphy', 'juan camilo orduz', 'karm patel', 'xi wang', 'rob zinkov']"
2402.10810,double duality: variational primal-dual policy optimization for   constrained reinforcement learning,cs.lg math.oc stat.ml,"we study the constrained convex markov decision process (mdp), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint. designing algorithms for a constrained convex mdp faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure. in this work, we present a model-based algorithm, variational primal-dual policy optimization (vpdpo), in which lagrangian and fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization. moreover, the primal variables are updated by model-based value iteration following the principle of optimism in the face of uncertainty (ofu), while the dual variables are updated by gradient ascent. moreover, by embedding the visitation measure into a finite-dimensional space, we can handle large state spaces by incorporating function approximation. two notable examples are (1) kernelized nonlinear regulators and (2) low-rank mdps. we prove that with an optimistic planning oracle, our algorithm achieves sublinear regret and constraint violation in both cases and can attain the globally optimal policy of the original constrained problem.",,2024-02-16,,"['zihao li', 'boyi liu', 'zhuoran yang', 'zhaoran wang', 'mengdi wang']"
2402.10818,trading off consistency and dimensionality of convex surrogates for the   mode,cs.lg stat.ml,"in multiclass classification over $n$ outcomes, the outcomes must be embedded into the reals with dimension at least $n-1$ in order to design a consistent surrogate loss that leads to the ""correct"" classification, regardless of the data distribution. for large $n$, such as in information retrieval and structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is often intractable. we investigate ways to trade off surrogate loss dimension, the number of problem instances, and restricting the region of consistency in the simplex for multiclass classification. following past work, we examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space. we show that full-dimensional subsets of the simplex exist around each point mass distribution for which consistency holds, but also, with less than $n-1$ dimensions, there exist distributions for which a phenomenon called hallucination occurs, which is when the optimal report under the surrogate loss is an outcome with zero probability. looking towards application, we derive a result to check if consistency holds under a given polytope embedding and low-noise assumption, providing insight into when to use a particular embedding. we provide examples of embedding $n = 2^{d}$ outcomes into the $d$-dimensional unit cube and $n = d!$ outcomes into the $d$-dimensional permutahedron under low-noise assumptions. finally, we demonstrate that with multiple problem instances, we can learn the mode with $\frac{n}{2}$ dimensions over the whole simplex.",,2024-02-16,,"['enrique nueve', 'bo waggoner', 'dhamma kimpara', 'jessie finocchiaro']"
2402.11020,proximal causal inference for conditional separable effects,stat.me,"scientists often pose questions about treatment effects on outcomes conditional on a post-treatment event. however, defining, identifying, and estimating causal effects conditional on post-treatment events requires care, even in perfectly executed randomized experiments. recently, the conditional separable effect (cse) was proposed as an interventionist estimand, corresponding to scientifically meaningful questions in these settings. however, while being a single-world estimand, which can be queried experimentally, existing identification results for the cse require no unmeasured confounding between the outcome and post-treatment event. this assumption can be violated in many applications. in this work, we address this concern by developing new identification and estimation results for the cse in the presence of unmeasured confounding. we establish nonparametric identification of the cse in both observational and experimental settings when certain proxy variables are available for hidden common causes of the post-treatment event and outcome. we characterize the efficient influence function for the cse under a semiparametric model of the observed data law in which nuisance functions are a priori unrestricted. moreover, we develop a consistent, asymptotically linear, and locally semiparametric efficient estimator of the cse using modern machine learning theory. we illustrate our framework with simulation studies and a real-world cancer therapy trial.",,2024-02-16,,"['chan park', 'mats stensrud', 'eric tchetgen tchetgen']"
2402.11025,training bayesian neural networks with sparse subspace variational   inference,cs.lg stat.ml,"bayesian neural networks (bnns) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. sparse bnns have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by post-training compression of dense bnns. the dilemma of how to cut down massive training costs remains, particularly given the requirement to learn about the uncertainty. to solve this challenge, we introduce sparse subspace variational inference (ssvi), the first fully sparse bnn framework that maintains a consistently highly sparse bayesian model throughout the training and inference phases. starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters. while basis selection is characterized as a non-differentiable problem, we approximate the optimal solution with a removal-and-addition strategy, guided by novel criteria based on weight distribution statistics. our extensive experiments show that ssvi sets new benchmarks in crafting sparse bnns, achieving, for instance, a 10-20x compression in model size with under 3\% performance drop, and up to 20x flops reduction during training compared with dense vi training. remarkably, ssvi also demonstrates enhanced robustness to hyperparameters, reducing the need for intricate tuning in vi and occasionally even surpassing vi-trained dense bnns on both accuracy and uncertainty metrics.",,2024-02-16,,"['junbo li', 'zichen miao', 'qiang qiu', 'ruqi zhang']"
2402.11039,robustness to subpopulation shift with domain label noise via   regularized annotation of domains,cs.lg stat.ml,"existing methods for last layer retraining that aim to optimize worst-group accuracy (wga) rely heavily on well-annotated groups in the training data. we show, both in theory and practice, that annotation-based data augmentations using either downsampling or upweighting for wga are susceptible to domain annotation noise, and in high-noise regimes approach the wga of a model trained with vanilla empirical risk minimization. we introduce regularized annotation of domains (rad) in order to train robust last layer classifiers without the need for explicit domain annotations. our results show that rad is competitive with other recently proposed domain annotation-free techniques. most importantly, rad outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data for several publicly available datasets.",,2024-02-16,,"['nathan stromberg', 'rohan ayyagari', 'monica welfert', 'sanmi koyejo', 'lalitha sankar']"
2402.11052,building trees for probabilistic prediction via scoring rules,stat.me stat.ml,decision trees built with data remain in widespread use for nonparametric prediction. predicting probability distributions is preferred over point predictions when uncertainty plays a prominent role in analysis and decision-making. we study modifying a tree to produce nonparametric predictive distributions. we find the standard method for building trees may not result in good predictive distributions and propose changing the splitting criteria for trees to one based on proper scoring rules. analysis of both simulated data and several real datasets demonstrates that using these new splitting criteria results in trees with improved predictive properties considering the entire predictive distribution.,,2024-02-16,,"['sara shashaani', 'ozge surer', 'matthew plumlee', 'seth guikema']"
2402.11120,dart: a principled approach to adversarially robust unsupervised domain   adaptation,cs.lg cs.cv stat.ml,"distribution shifts and adversarial examples are two major challenges for deploying machine learning models. while these challenges have been studied individually, their combination is an important topic that remains relatively under-explored. in this work, we study the problem of adversarial robustness under a common setting of distribution shift - unsupervised domain adaptation (uda). specifically, given a labeled source domain $d_s$ and an unlabeled target domain $d_t$ with related but different distributions, the goal is to obtain an adversarially robust model for $d_t$. the absence of target domain labels poses a unique challenge, as conventional adversarial robustness defenses cannot be directly applied to $d_t$. to address this challenge, we first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence. motivated by this bound, we develop a novel unified defense framework called divergence aware adversarial training (dart), which can be used in conjunction with a variety of standard uda methods; e.g., dann [ganin and lempitsky, 2015]. dart is applicable to general threat models, including the popular $\ell_p$-norm model, and does not require heuristic regularizers or architectural changes. we also release domainrobust: a testbed for evaluating robustness of uda models to adversarial attacks. domainrobust consists of 4 multi-domain benchmark datasets (with 46 source-target pairs) and 7 meta-algorithms with a total of 11 variants. our large-scale experiments demonstrate that on average, dart significantly enhances model robustness on all benchmarks compared to the state of the art, while maintaining competitive standard accuracy. the relative improvement in robustness from dart reaches up to 29.2% on the source-target domain pairs considered.",,2024-02-16,,"['yunjuan wang', 'hussein hazimeh', 'natalia ponomareva', 'alexey kurakin', 'ibrahim hammoud', 'raman arora']"
2402.11133,two-sample hypothesis testing for large random graphs of unequal size,stat.me,"two-sample hypothesis testing for large graphs is popular in cognitive science, probabilistic machine learning and artificial intelligence. while numerous methods have been proposed in the literature to address this problem, less attention has been devoted to scenarios involving graphs of unequal size or situations where there are only one or a few samples of graphs. in this article, we propose a frobenius test statistic tailored for small sample sizes and unequal-sized random graphs to test whether they are generated from the same model or not. our approach involves an algorithm for generating bootstrapped adjacency matrices from estimated community-wise edge probability matrices, forming the basis of the frobenius test statistic. we derive the asymptotic distribution of the proposed test statistic and validate its stability and efficiency in detecting minor differences in underlying models through simulations. furthermore, we explore its application to fmri data where we are able to distinguish brain activity patterns when subjects are exposed to sentences and pictures for two different stimuli and the control group.",,2024-02-16,,"['xin jin', 'kit chan', 'ian barnett', 'riddhi pratim ghosh']"
2402.11134,functional partial least-squares: optimal rates and adaptation,math.st econ.em stat.co stat.me stat.ml stat.th,"we consider the functional linear regression model with a scalar response and a hilbert space-valued predictor, a well-known ill-posed inverse problem. we propose a new formulation of the functional partial least-squares (pls) estimator related to the conjugate gradient method. we shall show that the estimator achieves the (nearly) optimal convergence rate on a class of ellipsoids and we introduce an early stopping rule which adapts to the unknown degree of ill-posedness. some theoretical and simulation comparison between the estimator and the principal component regression estimator is provided.",,2024-02-16,,"['andrii babii', 'marine carrasco', 'idriss tsafack']"
2402.11156,"efficient low-rank matrix estimation, experimental design, and   arm-set-dependent low-rank bandits",stat.ml cs.lg,"we study low-rank matrix trace regression and the related problem of low-rank matrix bandits. assuming access to the distribution of the covariates, we propose a novel low-rank matrix estimation method called lowpopart and provide its recovery guarantee that depends on a novel quantity denoted by b(q) that characterizes the hardness of the problem, where q is the covariance matrix of the measurement distribution. we show that our method can provide tighter recovery guarantees than classical nuclear norm penalized least squares (koltchinskii et al., 2011) in several problems. to perform efficient estimation with a limited number of measurements from an arbitrarily given measurement set a, we also propose a novel experimental design criterion that minimizes b(q) with computational efficiency. we leverage our novel estimator and design of experiments to derive two low-rank linear bandit algorithms for general arm sets that enjoy improved regret upper bounds. this improves over previous works on low-rank bandits, which make somewhat restrictive assumptions that the arm set is the unit ball or that an efficient exploration distribution is given. to our knowledge, our experimental design criterion is the first one tailored to low-rank matrix estimation beyond the naive reduction to linear regression, which can be of independent interest.",,2024-02-16,,"['kyoungseok jang', 'chicheng zhang', 'kwang-sung jun']"
2402.11179,uncertainty quantification of graph convolution neural network models of   evolving processes,cs.lg math.st physics.comp-ph stat.th,"the application of neural network models to scientific machine learning tasks has proliferated in recent years. in particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. hence there is a need to find uncertainty quantification methods that are suitable for neural networks. in this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with hamiltonian monte carlo and stein variational gradient descent and its projected variant. specifically we apply these methods to graph convolutional neural network models of evolving systems modeled with recurrent neural network and neural ordinary differential equations architectures. we show that stein variational inference is a viable alternative to monte carlo methods with some clear advantages for complex neural network models. for our exemplars, stein variational interference gave similar uncertainty profiles through time compared to hamiltonian monte carlo, albeit with generally more generous variance.projected stein variational gradient descent also produced similar uncertainty profiles to the non-projected counterpart, but large reductions in the active weight space were confounded by the stability of the neural network predictions and the convoluted likelihood landscape.",,2024-02-16,,"['jeremiah hauth', 'cosmin safta', 'xun huan', 'ravi g. patel', 'reese e. jones']"
2402.11215,adadagrad: adaptive batch size schemes for adaptive gradient methods,cs.lg math.oc stat.ml,"the choice of batch sizes in stochastic gradient optimizers is critical for model training. however, the practice of varying batch sizes throughout the training process is less explored compared to other hyperparameters. we investigate adaptive batch size strategies derived from adaptive sampling methods, traditionally applied only in stochastic gradient descent. given the significant interplay between learning rates and batch sizes, and considering the prevalence of adaptive gradient methods in deep learning, we emphasize the need for adaptive batch size strategies in these contexts. we introduce adadagrad and its scalar variant adadagradnorm, which incrementally increase batch sizes during training, while model updates are performed using adagrad and adagradnorm. we prove that adagradnorm converges with high probability at a rate of $\mathscr{o}(1/k)$ for finding a first-order stationary point of smooth nonconvex functions within $k$ iterations. adagrad also demonstrates similar convergence properties when integrated with a novel coordinate-wise variant of our adaptive batch size strategies. our theoretical claims are supported by numerical experiments on various image classification tasks, highlighting the enhanced adaptability of progressive batching protocols in deep learning and the potential of such adaptive batch size strategies with adaptive gradient optimizers in large-scale model training.",,2024-02-17,,"['tim tsz-kit lau', 'han liu', 'mladen kolar']"
2402.11228,adaptive split balancing for optimal random forest,stat.ml cs.lg math.st stat.me stat.th,"while random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios. in this study, we introduce the adaptive split balancing forest (asbf), capable of learning tree representations from data while simultaneously achieving minimax optimality under the lipschitz class. to exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the h\""older class $\mathcal{h}^{q,\beta}$ for any $q\in\mathbb{n}$ and $\beta\in(0,1]$. rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches. our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results. conversely, a less random, more balanced approach demonstrates optimality. additionally, we establish uniform upper bounds and explore the application of random forests in average treatment effect estimation problems. through simulation studies and real-data applications, we demonstrate the superior empirical performance of the proposed methods over existing random forests.",,2024-02-17,,"['yuqian zhang', 'weijie ji', 'jelena bradic']"
2402.11283,deep adaptive sampling for surrogate modeling without labeled data,math.na cs.na stat.ml,"surrogate modeling is of great practical significance for parametric differential equation systems. in contrast to classical numerical methods, using physics-informed deep learning methods to construct simulators for such systems is a promising direction due to its potential to handle high dimensionality, which requires minimizing a loss over a training set of random samples. however, the random samples introduce statistical errors, which may become the dominant errors for the approximation of low-regularity and high-dimensional problems. in this work, we present a deep adaptive sampling method for surrogate modeling ($\text{das}^2$), where we generalize the deep adaptive sampling (das) method [62] [tang, wan and yang, 2023] to build surrogate models for low-regularity parametric differential equations. in the parametric setting, the residual loss function can be regarded as an unnormalized probability density function (pdf) of the spatial and parametric variables. this pdf is approximated by a deep generative model, from which new samples are generated and added to the training set. since the new samples match the residual-induced distribution, the refined training set can further reduce the statistical error in the current approximate solution. we demonstrate the effectiveness of $\text{das}^2$ with a series of numerical experiments, including the parametric lid-driven 2d cavity flow problem with a continuous range of reynolds numbers from 100 to 1000.",,2024-02-17,,"['xili wang', 'kejun tang', 'jiayu zhai', 'xiaoliang wan', 'chao yang']"
2402.11337,learning by reconstruction produces uninformative features for   perception,cs.cv cs.ai stat.ml,"input space reconstruction is an attractive representation learning paradigm. despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. we show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. for example, the supervised tinyimagenet task with images projected onto the top subspace explaining 90\% of the pixel variance can be solved with 45\% test accuracy. using the bottom subspace instead, accounting for only 20\% of the pixel variance, reaches 55\% test accuracy. the features for perception being learned last explains the need for long training time, e.g., with masked autoencoders. learning by denoising is a popular strategy to alleviate that misalignment. we prove that while some noise strategies such as masking are indeed beneficial, others such as additive gaussian noise are not. yet, even in the case of masking, we find that the benefits vary as a function of the mask's shape, ratio, and the considered dataset. while tuning the noise strategy without knowledge of the perception task seems challenging, we provide first clues on how to detect if a noise strategy is never beneficial regardless of the perception task.",,2024-02-17,,"['randall balestriero', 'yann lecun']"
2402.11338,fair classification with partial feedback: an exploration-based   data-collection approach,cs.lg cs.ai cs.cy stat.ml,"in many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. these past observations, in turn, form training datasets for classifiers that make future predictions. however, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. we present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. for any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a ""desired"" classifier. the right exploration strategy is context-dependent; it can be chosen to improve learning guarantees and encode context-specific group fairness properties. evaluation on real-world datasets shows that this approach consistently boosts the quality of collected outcome data and improves the fraction of true positives for all groups, with only a small reduction in predictive utility.",,2024-02-17,,"['vijay keswani', 'anay mehrotra', 'l. elisa celis']"
2402.11339,expressive higher-order link prediction through hypergraph symmetry   breaking,cs.lg stat.ml,"a hypergraph consists of a set of nodes along with a collection of subsets of the nodes called hyperedges. higher-order link prediction is the task of predicting the existence of a missing hyperedge in a hypergraph. a hyperedge representation learned for higher order link prediction is fully expressive when it does not lose distinguishing power up to an isomorphism. many existing hypergraph representation learners, are bounded in expressive power by the generalized weisfeiler lehman-1 (gwl-1) algorithm, a generalization of the weisfeiler lehman-1 algorithm. however, gwl-1 has limited expressive power. in fact, induced subhypergraphs with identical gwl-1 valued nodes are indistinguishable. furthermore, message passing on hypergraphs can already be computationally expensive, especially on gpu memory. to address these limitations, we devise a preprocessing algorithm that can identify certain regular subhypergraphs exhibiting symmetry. our preprocessing algorithm runs once with complexity the size of the input hypergraph. during training, we randomly replace subhypergraphs identified by the algorithm with covering hyperedges to break symmetry. we show that our method improves the expressivity of gwl-1. our extensive experiments also demonstrate the effectiveness of our approach for higher-order link prediction on both graph and hypergraph datasets with negligible change in computation.",,2024-02-17,,"['simon zhang', 'cheng xin', 'tamal k. dey']"
2402.11345,variational entropy search for adjusting expected improvement,stat.ml cs.lg math.oc,"bayesian optimization is a widely used technique for optimizing black-box functions, with expected improvement (ei) being the most commonly utilized acquisition function in this domain. while ei is often viewed as distinct from other information-theoretic acquisition functions, such as entropy search (es) and max-value entropy search (mes), our work reveals that ei can be considered a special case of mes when approached through variational inference (vi). in this context, we have developed the variational entropy search (ves) methodology and the ves-gamma algorithm, which adapts ei by incorporating principles from information-theoretic concepts. the efficacy of ves-gamma is demonstrated across a variety of test functions and read datasets, highlighting its theoretical and practical utilities in bayesian optimization scenarios.",,2024-02-17,,"['nuojin cheng', 'stephen becker']"
2402.11365,data-driven stochastic ac-opf using gaussian processes,cs.lg math.oc stat.ml,"the thesis focuses on developing a data-driven algorithm, based on machine learning, to solve the stochastic alternating current (ac) chance-constrained (cc) optimal power flow (opf) problem. although the ac cc-opf problem has been successful in academic circles, it is highly nonlinear and computationally demanding, which limits its practical impact. the proposed approach aims to address this limitation and demonstrate its empirical efficiency through applications to multiple ieee test cases. to solve the non-convex and computationally challenging cc ac-opf problem, the proposed approach relies on a machine learning gaussian process regression (gpr) model. the full gaussian process (gp) approach is capable of learning a simple yet non-convex data-driven approximation to the ac power flow equations that can incorporate uncertain inputs. the proposed approach uses various approximations for gp-uncertainty propagation. the full gp cc-opf approach exhibits highly competitive and promising results, outperforming the state-of-the-art sample-based chance constraint approaches. to further improve the robustness and complexity/accuracy trade-off of the full gp cc-opf, a fast data-driven setup is proposed. this setup relies on the sparse and hybrid gaussian processes (gp) framework to model the power flow equations with input uncertainty.",,2024-02-17,,['mile mitrovic']
2402.11410,an elementary predictor obtaining $2\sqrt{t}$ distance to calibration,cs.lg cs.ds stat.ml,"blasiok et al. [2023] proposed distance to calibration as a natural measure of calibration error that unlike expected calibration error (ece) is continuous. recently, qiao and zheng [2024] gave a non-constructive argument establishing the existence of an online predictor that can obtain $o(\sqrt{t})$ distance to calibration in the adversarial setting, which is known to be impossible for ece. they leave as an open problem finding an explicit, efficient algorithm. we resolve this problem and give an extremely simple, efficient, deterministic algorithm that obtains distance to calibration error at most $2\sqrt{t}$.",,2024-02-17,,"['eshwar ram arunachaleswaran', 'natalie collina', 'aaron roth', 'mirah shi']"
2402.11427,optex: expediting first-order optimization with approximately   parallelized iterations,cs.lg cs.ai stat.ml,"first-order optimization (foo) algorithms are pivotal in numerous computational domains such as machine learning and signal denoising. however, their application to complex tasks like neural network training often entails significant inefficiencies due to the need for many sequential iterations for convergence. in response, we introduce first-order optimization expedited with approximately parallelized iterations (optex), the first framework that enhances the efficiency of foo by leveraging parallel computing to mitigate its iterative bottleneck. optex employs kernelized gradient estimation to make use of gradient history for future gradient prediction, enabling parallelization of iterations -- a strategy once considered impractical because of the inherent iterative dependency in foo. we provide theoretical guarantees for the reliability of our kernelized gradient estimation and the iteration complexity of sgd-based optex, confirming that estimation errors diminish to zero as historical gradients accumulate and that sgd-based optex enjoys an effective acceleration rate of $\omega(\sqrt{n})$ over standard sgd given parallelism of n. we also use extensive empirical studies, including synthetic functions, reinforcement learning tasks, and neural network training across various datasets, to underscore the substantial efficiency improvements achieved by optex.",,2024-02-17,,"['yao shu', 'jiongfeng fang', 'ying tiffany he', 'fei richard yu']"
2402.11552,empirical density estimation based on spline quasi-interpolation with   applications to copulas clustering modeling,stat.ml cs.lg cs.na math.na,"density estimation is a fundamental technique employed in various fields to model and to understand the underlying distribution of data. the primary objective of density estimation is to estimate the probability density function of a random variable. this process is particularly valuable when dealing with univariate or multivariate data and is essential for tasks such as clustering, anomaly detection, and generative modeling. in this paper we propose the mono-variate approximation of the density using spline quasi interpolation and we applied it in the context of clustering modeling. the clustering technique used is based on the construction of suitable multivariate distributions which rely on the estimation of the monovariate empirical densities (marginals). such an approximation is achieved by using the proposed spline quasi-interpolation, while the joint distributions to model the sought clustering partition is constructed with the use of copulas functions. in particular, since copulas can capture the dependence between the features of the data independently from the marginal distributions, a finite mixture copula model is proposed. the presented algorithm is validated on artificial and real datasets.",,2024-02-18,,"['cristiano tamborrino', 'antonella falini', 'francesca mazzia']"
2402.11652,doubly robust inference in causal latent factor models,econ.em cs.lg stat.me stat.ml,"this article introduces a new framework for estimating average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. the proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. we derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero gaussian distribution at a parametric rate. simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.",,2024-02-18,,"['alberto abadie', 'anish agarwal', 'raaz dwivedi', 'abhin shah']"
2402.11736,monte carlo with kernel-based gibbs measures: guarantees for   probabilistic herding,cs.lg math.pr stat.ml,"kernel herding belongs to a family of deterministic quadratures that seek to minimize the worst-case integration error over a reproducing kernel hilbert space (rkhs). in spite of strong experimental support, it has revealed difficult to prove that this worst-case error decreases at a faster rate than the standard square root of the number of quadrature nodes, at least in the usual case where the rkhs is infinite-dimensional. in this theoretical paper, we study a joint probability distribution over quadrature nodes, whose support tends to minimize the same worst-case error as kernel herding. we prove that it does outperform i.i.d. monte carlo, in the sense of coming with a tighter concentration inequality on the worst-case integration error. while not improving the rate yet, this demonstrates that the mathematical tools of the study of gibbs measures can help understand to what extent kernel herding and its variants improve on computationally cheaper methods. moreover, we provide early experimental evidence that a faster rate of convergence, though not worst-case, is likely.",,2024-02-18,,"['martin rouault', 'rémi bardenet', 'mylène maïda']"
2402.11742,"balanced data, imbalanced spectra: unveiling class disparities with   spectral imbalance",cs.lg stat.ml,"classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. this issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. in this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. to build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. we then study this phenomenon in 11 different state-of-the-art pretrained encoders and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation strategies to mitigate the issue. our work sheds light on the class-dependent effects of learning, and provides new insights into how state-of-the-art pretrained features may have unknown biases that can be diagnosed through their spectra.",,2024-02-18,,"['chiraag kaushik', 'ran liu', 'chi-heng lin', 'amrit khera', 'matthew y jin', 'wenrui ma', 'vidya muthukumar', 'eva l dyer']"
2402.11771,evaluating the effectiveness of index-based treatment allocation,cs.lg cs.ai stat.me stat.ml,"when resources are scarce, an allocation policy is needed to decide who receives a resource. this problem occurs, for instance, when allocating scarce medical resources and is often solved using modern ml methods. this paper introduces methods to evaluate index-based allocation policies -- that allocate a fixed number of resources to those who need them the most -- by using data from a randomized control trial. such policies create dependencies between agents, which render the assumptions behind standard statistical tests invalid and limit the effectiveness of estimators. addressing these challenges, we translate and extend recent ideas from the statistics literature to present an efficient estimator and methods for computing asymptotically correct confidence intervals. this enables us to effectively draw valid statistical conclusions, a critical gap in previous work. our extensive experiments validate our methodology in practical settings, while also showcasing its statistical power. we conclude by proposing and empirically verifying extensions of our methodology that enable us to reevaluate a past randomized control trial to evaluate different ml allocation policies in the context of a mhealth program, drawing previously invisible conclusions.",,2024-02-18,,"['niclas boehmer', 'yash nair', 'sanket shah', 'lucas janson', 'aparna taneja', 'milind tambe']"
2402.11789,statistical test for generated hypotheses by diffusion models,stat.ml cs.cv cs.lg,"the enhanced performance of ai has accelerated its integration into scientific research. in particular, the use of generative ai to create scientific hypotheses is promising and is increasingly being applied across various fields. however, when employing ai-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. in this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. the basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. we show the theoretical validity of the proposed statistical test and its effectiveness through numerical experiments on synthetic and brain image datasets.",,2024-02-18,,"['teruyuki katsuoka', 'tomohiro shiraishi', 'daiki miwa', 'vo nguyen le duy', 'ichiro takeuchi']"
2402.11948,mini-hes: a parallelizable second-order latent factor analysis model,cs.lg cs.ai stat.ml,"interactions among large number of entities is naturally high-dimensional and incomplete (hdi) in many big data related tasks. behavioral characteristics of users are hidden in these interactions, hence, effective representation of the hdi data is a fundamental task for understanding user behaviors. latent factor analysis (lfa) model has proven to be effective in representing hdi data. the performance of an lfa model relies heavily on its training process, which is a non-convex optimization. it has been proven that incorporating local curvature and preprocessing gradients during its training process can lead to superior performance compared to lfa models built with first-order family methods. however, with the escalation of data volume, the feasibility of second-order algorithms encounters challenges. to address this pivotal issue, this paper proposes a mini-block diagonal hessian-free (mini-hes) optimization for building an lfa model. it leverages the dominant diagonal blocks in the generalized gauss-newton matrix based on the analysis of the hessian matrix of lfa model and serves as an intermediary strategy bridging the gap between first-order and second-order optimization methods. experiment results indicate that, with mini-hes, the lfa model outperforms several state-of-the-art models in addressing missing data estimation task on multiple real hdi datasets from recommender system. (the source code of mini-hes is available at https://github.com/goallow/mini-hes)",,2024-02-19,,"['jialiang wang', 'weiling li', 'yurong zhong', 'xin luo']"
2402.11973,bayesian active learning for censored regression,cs.lg stat.ml,"bayesian active learning is based on information theoretical approaches that focus on maximising the information that new observations provide to the model parameters. this is commonly done by maximising the bayesian active learning by disagreement (bald) acquisitions function. however, we highlight that it is challenging to estimate bald when the new data points are subject to censorship, where only clipped values of the targets are observed. to address this, we derive the entropy and the mutual information for censored distributions and derive the bald objective for active learning in censored regression ($\mathcal{c}$-bald). we propose a novel modelling approach to estimate the $\mathcal{c}$-bald objective and use it for active learning in the censored setting. across a wide range of datasets and models, we demonstrate that $\mathcal{c}$-bald outperforms other bayesian active learning methods in censored regression.",,2024-02-19,,"['frederik boe hüttel', 'christoffer riis', 'filipe rodrigues', 'francisco câmara pereira']"
2402.11981,universal generalization guarantees for wasserstein distributionally   robust models,math.oc stat.ml,"distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. recent statistical analyses have proved that robust models built from wasserstein ambiguity sets have nice generalization guarantees, breaking the curse of dimensionality. however, these results are obtained in specific cases, at the cost of approximations, or under assumptions difficult to verify in practice. in contrast, we establish, in this article, exact generalization guarantees that cover all practical cases, including any transport cost function and any loss function, potentially non-convex and nonsmooth. for instance, our result applies to deep learning, without requiring restrictive assumptions. we achieve this result through a novel proof technique that combines nonsmooth analysis rationale with classical concentration results. our approach is general enough to extend to the recent versions of wasserstein/sinkhorn distributionally robust problems that involve (double) regularizations.",,2024-02-19,,"['tam le', 'jérôme malick']"
2402.12008,cluster metric sensitivity to irrelevant features,cs.lg cs.ai stat.ml,"clustering algorithms are used extensively in data analysis for data exploration and discovery. technological advancements lead to continually growth of data in terms of volume, dimensionality and complexity. this provides great opportunities in data analytics as the data can be interrogated for many different purposes. this however leads challenges, such as identification of relevant features for a given task. in supervised tasks, one can utilise a number of methods to optimise the input features for the task objective (e.g. classification accuracy). in unsupervised problems, such tools are not readily available, in part due to an inability to quantify feature relevance in unlabeled tasks. in this paper, we investigate the sensitivity of clustering performance noisy uncorrelated variables iteratively added to baseline datasets with well defined clusters. we show how different types of irrelevant variables can impact the outcome of a clustering result from $k$-means in different ways. we observe a resilience to very high proportions of irrelevant features for adjusted rand index (ari) and normalised mutual information (nmi) when the irrelevant features are gaussian distributed. for uniformly distributed irrelevant features, we notice the resilience of ari and nmi is dependent on the dimensionality of the data and exhibits tipping points between high scores and near zero. our results show that the silhouette coefficient and the davies-bouldin score are the most sensitive to irrelevant added features exhibiting large changes in score for comparably low proportions of irrelevant features regardless of underlying distribution or data scaling. as such the silhouette coefficient and the davies-bouldin score are good candidates for optimising feature selection in unsupervised clustering tasks.",,2024-02-19,,"['miles mccrory', 'spencer a. thomas']"
2402.12034,when do off-policy and on-policy policy gradient methods align?,stat.ml cs.lg,"policy gradient methods are widely adopted reinforcement learning algorithms for tasks with continuous action spaces. these methods succeeded in many application domains, however, because of their notorious sample inefficiency their use remains limited to problems where fast and accurate simulations are available. a common way to improve sample efficiency is to modify their objective function to be computable from off-policy samples without importance sampling. a well-established off-policy objective is the excursion objective. this work studies the difference between the excursion objective and the traditional on-policy objective, which we refer to as the on-off gap. we provide the first theoretical analysis showing conditions to reduce the on-off gap while establishing empirical evidence of shortfalls arising when these conditions are not met.",,2024-02-19,,"['davide mambelli', 'stephan bongers', 'onno zoeter', 'matthijs t. j. spaan', 'frans a. oliehoek']"
2402.12042,linear bandits with polylogarithmic minimax regret,cs.lg cs.ai stat.ml,"we study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector. we introduce an algorithm for this problem that exhibits a minimax regret scaling as $\log^3(t)$ in the time horizon $t$, in stark contrast the square root scaling of this regret for typical bandit algorithms. our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\lambda_{\min} ( v_t ) = \omega (\sqrt{\lambda_{\max}(v_t ) })$ for the design matrix $v_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. this allows us to tightly control the expected regret in each time step to be of the order $o(\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.",,2024-02-19,,"['josep lumbreras', 'marco tomamichel']"
2402.12232,kernel kmeans clustering splits for end-to-end unsupervised decision   trees,stat.ml cs.ai cs.lg,"trees are convenient models for obtaining explainable predictions on relatively small datasets. although there are many proposals for the end-to-end construction of such trees in supervised learning, learning a tree end-to-end for clustering without labels remains an open challenge. as most works focus on interpreting with trees the result of another clustering algorithm, we present here a novel end-to-end trained unsupervised binary tree for clustering: kauri. this method performs a greedy maximisation of the kernel kmeans objective without requiring the definition of centroids. we compare this model on multiple datasets with recent unsupervised trees and show that kauri performs identically when using a linear kernel. for other kernels, kauri often outperforms the concatenation of kernel kmeans and a cart decision tree.",,2024-02-19,,"['louis ohl', 'pierre-alexandre mattei', 'mickaël leclercq', 'arnaud droit', 'frédéric precioso']"
2402.12241,convergence of gradient descent for recurrent neural networks: a   nonasymptotic analysis,cs.lg math.oc stat.ml,"we analyze recurrent neural networks trained with gradient descent in the supervised learning setting for dynamical systems, and prove that gradient descent can achieve optimality \emph{without} massive overparameterization. our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\tau$ in terms of the sequence length $t$, sample size $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the lipschitz continuity of the activation function. remarkably, this analysis reveals that an appropriately-initialized recurrent neural network trained with $n$ samples can achieve optimality with a network size $m$ that scales only logarithmically with $n$. this sharply contrasts with the prior works that require high-order polynomial dependency of $m$ on $n$ to establish strong regularity conditions. our results are based on an explicit characterization of the class of dynamical systems that can be approximated and learned by recurrent neural networks via norm-constrained transportation mappings, and establishing local smoothness properties of the hidden state with respect to the learnable parameters.",,2024-02-19,,"['semih cayci', 'atilla eryilmaz']"
2402.12264,uncertainty quantification in fine-tuned llms using lora ensembles,cs.lg cs.ai cs.cl stat.ml,"fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. we derive principled uncertainty quantification for fine-tuned llms with posterior approximations using computationally efficient low-rank adaptation ensembles. we analyze three common multiple-choice datasets using low-rank adaptation ensembles based on mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. in particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.",,2024-02-19,,"['oleksandr balabanov', 'hampus linander']"
2402.12292,regularization by denoising: bayesian model and langevin-within-split   gibbs sampling,stat.ml cs.cv cs.lg,"this paper introduces a bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (red) paradigm. it additionally implements a monte carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact data augmentation (axda). the proposed algorithm is an approximate instance of split gibbs sampling (sgs) which embeds one langevin monte carlo step. the proposed method is applied to common imaging tasks such as deblurring, inpainting and super-resolution, demonstrating its efficacy through extensive numerical experiments. these contributions advance bayesian inference in imaging by leveraging data-driven regularization strategies within a probabilistic framework.",,2024-02-19,,"['elhadji c. faye', 'mame diarra fall', 'nicolas dobigeon']"
2402.12302,asymptotic gaussian fluctuations of eigenvectors in spectral clustering,stat.ml cs.lg math.pr,"the performance of spectral clustering relies on the fluctuations of the entries of the eigenvectors of a similarity matrix, which has been left uncharacterized until now. in this letter, it is shown that the signal $+$ noise structure of a general spike random matrix model is transferred to the eigenvectors of the corresponding gram kernel matrix and the fluctuations of their entries are gaussian in the large-dimensional regime. this clt-like result was the last missing piece to precisely predict the classification performance of spectral clustering. the proposed proof is very general and relies solely on the rotational invariance of the noise. numerical experiments on synthetic and real data illustrate the universality of this phenomenon.",,2024-02-19,,"['hugo lebeau', 'florent chatelain', 'romain couillet']"
2402.12331,generating survival interpretable trajectories and data,cs.lg cs.ai stat.ml,"a new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed. it solves three tasks. first, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the beran estimator. second, the model generates additional data based on a given training set that would supplement the original dataset. third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event. the trajectory can be viewed as a type of the counterfactual explanation. the proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder. the model also determines the censored indicators of new generated data by solving a classification task. the paper demonstrates the efficiency and properties of the proposed model using numerical experiments on synthetic and real datasets. the code of the algorithm implementing the proposed model is publicly available.",,2024-02-19,,"['andrei v. konstantinov', 'stanislav r. kirpichenko', 'lev v. utkin']"
2402.12336,robust clip: unsupervised adversarial fine-tuning of vision embeddings   for robust large vision-language models,cs.lg cs.ai cs.cv stat.ml,"multi-modal foundation models like openflamingo, llava, and gpt-4 are increasingly used for various real-world tasks. prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. these attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. the clip model, or one of its variants, is used as a frozen vision encoder in many vision-language models (vlms), e.g. llava and openflamingo. we propose an unsupervised adversarial fine-tuning scheme to obtain a robust clip vision encoder, which yields robustness on all vision down-stream tasks (vlms, zero-shot classification) that rely on clip. in particular, we show that stealth-attacks on users of vlms by a malicious third party providing manipulated images are no longer possible once one replaces the original clip model with our robust one. no retraining or fine-tuning of the vlm is required. the code and robust models are available at https://github.com/chs20/robustvlm",,2024-02-19,,"['christian schlarmann', 'naman deep singh', 'francesco croce', 'matthias hein']"
2402.12354,lora+: efficient low rank adaptation of large models,cs.lg cs.ai cs.cl stat.ml,"in this paper, we show that low rank adaptation (lora) as originally introduced in hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). this is due to the fact that adapter matrices a and b in lora are updated with the same learning rate. using scaling arguments for large width networks, we demonstrate that using the same learning rate for a and b does not allow efficient feature learning. we then show that this suboptimality of lora can be corrected simply by setting different learning rates for the lora adapter matrices a and b with a well-chosen ratio. we call this proposed algorithm lora$+$. in our extensive experiments, lora$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2x speedup), at the same computational cost as lora.",,2024-02-19,,"['soufiane hayou', 'nikhil ghosh', 'bin yu']"
2402.12397,multi-class temporal logic neural networks,stat.ml cs.lg,"time-series data can represent the behaviors of autonomous systems, such as drones and self-driving cars. the problem of binary and multi-class classification has received a lot of attention in this field. neural networks represent a popular approach to classifying data; however, they lack interpretability, which poses a significant challenge in extracting meaningful information from them. signal temporal logic (stl) is a formalism to describe the properties of timed behaviors. we propose a method that combines all of the above: neural networks that represent stl specifications for multi-class classification of time-series data. we offer two key contributions: 1) we introduce a notion of margin for multi-class classification, and 2) we introduce the use of stl-based attributes for enhancing the interpretability of the results. we evaluate our method on two datasets and compare with state-of-the-art baselines.",,2024-02-16,,"['danyang li', 'roberto tron']"
2402.12400,estimating the age-conditioned average treatment effects curves: an   application for assessing load-management strategies in the nba,stat.ap cs.lg,"in the realm of competitive sports, understanding the performance dynamics of athletes, represented by the age curve (showing progression, peak, and decline), is vital. our research introduces a novel framework for quantifying age-specific treatment effects, enhancing the granularity of performance trajectory analysis. firstly, we propose a methodology for estimating the age curve using game-level data, diverging from traditional season-level data approaches, and tackling its inherent complexities with a meta-learner framework that leverages advanced machine learning models. this approach uncovers intricate non-linear patterns missed by existing methods. secondly, our framework enables the identification of causal effects, allowing for a detailed examination of age curves under various conditions. by defining the age-conditioned treatment effect (acte), we facilitate the exploration of causal relationships regarding treatment impacts at specific ages. finally, applying this methodology to study the effects of rest days on performance metrics, particularly across different ages, offers valuable insights into load management strategies' effectiveness. our findings underscore the importance of tailored rest periods, highlighting their positive impact on athlete performance and suggesting a reevaluation of current management practices for optimizing athlete performance.",,2024-02-17,,"['shinpei nakamura-sakai', 'laura forastiere', 'brian macdonald']"
2402.12630,fast: an optimization framework for fast additive segmentation in   transparent ml,stat.ml cs.lg,"we present fast, an optimization framework for fast additive segmentation. fast segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models. the framework leverages a novel optimization procedure to fit these models $\sim$2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines \citep{nori2019interpretml}. we also develop new feature selection algorithms in the fast framework to fit parsimonious models that perform well. through experiments and case studies, we show that fast improves the computational efficiency and interpretability of additive models.",,2024-02-19,,"['brian liu', 'rahul mazumder']"
2402.12649,bias in language models: beyond trick tests and toward ruted evaluation,cs.cl stat.ap,"bias benchmarks are a popular method for studying the negative impacts of bias in llms, yet there has been little empirical investigation of whether these benchmarks are actually indicative of how real world harm may manifest in the real world. in this work, we study the correspondence between such decontextualized ""trick tests"" and evaluations that are more grounded in realistic use and tangible {effects (i.e. ruted evaluations). we explore this correlation in the context of gender-occupation bias--a popular genre of bias evaluation. we compare three de-contextualized evaluations adapted from the current literature to three analogous ruted evaluations applied to long-form content generation. we conduct each evaluation for seven instruction-tuned llms. for the ruted evaluations, we conduct repeated trials of three text generation tasks: children's bedtime stories, user personas, and english language learning exercises. we found no correspondence between trick tests and ruted evaluations. specifically, selecting the least biased model based on the de-contextualized results coincides with selecting the model with the best performance on ruted evaluations only as often as random chance. we conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms.",,2024-02-19,,"['kristian lum', 'jacy reese anthis', 'chirag nagpal', ""alexander d'amour""]"
2402.12668,randomization can reduce both bias and variance: a case study in random   forests,stat.ml cs.lg,"we study the often overlooked phenomenon, first noted in \cite{breiman2001random}, that random forests appear to reduce bias compared to bagging. motivated by an interesting paper by \cite{mentch2020randomization}, where the authors argue that random forests reduce effective degrees of freedom and only outperform bagging ensembles in low signal-to-noise ratio (snr) settings, we explore how random forests can uncover patterns in the data missed by bagging. we empirically demonstrate that in the presence of such patterns, random forests reduce bias along with variance and increasingly outperform bagging ensembles when snr is high. our observations offer insights into the real-world success of random forests across a range of snrs and enhance our understanding of the difference between random forests and bagging ensembles with respect to the randomization injected into each split. our investigations also yield practical insights into the importance of tuning $mtry$ in random forests.",,2024-02-19,,"['brian liu', 'rahul mazumder']"
2402.12683,torchcp: a library for conformal prediction based on pytorch,cs.lg cs.cv math.st stat.th,"torchcp is a python toolbox for conformal prediction research on deep learning models. it contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). torchcp is built on pytorch (paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. the code is licensed under the lgpl license and is open-sourced at $\href{https://github.com/ml-stat-sustech/torchcp}{\text{this https url}}$.",,2024-02-19,,"['hongxin wei', 'jianguo huang']"
2402.12687,learning on manifolds without manifold learning,cs.lg stat.ml,"function approximation based on data drawn randomly from an unknown distribution is an important problem in machine learning. in contrast to the prevalent paradigm of solving this problem by minimizing a loss functional, we have given a direct one-shot construction together with optimal error bounds under the manifold assumption; i.e., one assumes that the data is sampled from an unknown sub-manifold of a high dimensional euclidean space. a great deal of research deals with obtaining information about this manifold, such as the eigendecomposition of the laplace-beltrami operator or coordinate charts, and using this information for function approximation. this two step approach implies some extra errors in the approximation stemming from basic quantities of the data in addition to the errors inherent in function approximation. in neural networks, 132:253268, 2020, we have proposed a one-shot direct method to achieve function approximation without requiring the extraction of any information about the manifold other than its dimension. however, one cannot pin down the class of approximants used in that paper.   in this paper, we view the unknown manifold as a sub-manifold of an ambient hypersphere and study the question of constructing a one-shot approximation using the spherical polynomials based on the hypersphere. our approach does not require preprocessing of the data to obtain information about the manifold other than its dimension. we give optimal rates of approximation for relatively ""rough"" functions.",,2024-02-19,,"['h. n. mhaskar', ""ryan o'dowd""]"
2402.12710,integrating active learning in causal inference with interference: a   novel approach in online experiments,stat.me cs.lg stat.ml,"in the domain of causal inference research, the prevalent potential outcomes framework, notably the rubin causal model (rcm), often overlooks individual interference and assumes independent treatment effects. this assumption, however, is frequently misaligned with the intricate realities of real-world scenarios, where interference is not merely a possibility but a common occurrence. our research endeavors to address this discrepancy by focusing on the estimation of direct and spillover treatment effects under two assumptions: (1) network-based interference, where treatments on neighbors within connected networks affect one's outcomes, and (2) non-random treatment assignments influenced by confounders. to improve the efficiency of estimating potentially complex effects functions, we introduce an novel active learning approach: active learning in causal inference with interference (aci). this approach uses gaussian process to flexibly model the direct and spillover treatment effects as a function of a continuous measure of neighbors' treatment assignment. the aci framework sequentially identifies the experimental settings that demand further data. it further optimizes the treatment assignments under the network interference structure using genetic algorithms to achieve efficient learning outcome. by applying our method to simulation data and a tencent game dataset, we demonstrate its feasibility in achieving accurate effects estimations with reduced data requirements. this aci approach marks a significant advancement in the realm of data efficiency for causal inference, offering a robust and efficient alternative to traditional methodologies, particularly in scenarios characterized by complex interference patterns.",,2024-02-19,,"['hongtao zhu', 'sizhe zhang', 'yang su', 'zhenyu zhao', 'nan chen']"
2402.12711,achieving near-optimal regret for bandit algorithms with uniform   last-iterate guarantee,cs.lg stat.ml,"existing performance measures for bandit algorithms such as regret, pac bounds, or uniform-pac (dann et al., 2017), typically evaluate the cumulative performance, while allowing the play of an arbitrarily bad arm at any finite time t. such a behavior can be highly detrimental in high-stakes applications. this paper introduces a stronger performance measure, the uniform last-iterate (uli) guarantee, capturing both cumulative and instantaneous performance of bandit algorithms. specifically, uli characterizes the instantaneous performance since it ensures that the per-round regret of the played arm is bounded by a function, monotonically decreasing w.r.t. (large) round t, preventing revisits to bad arms when sufficient samples are available. we demonstrate that a near-optimal uli guarantee directly implies near-optimal cumulative performance across aforementioned performance measures. to examine the achievability of uli in the finite arm setting, we first provide two positive results that some elimination-based algorithms and high-probability adversarial algorithms with stronger analysis or additional designs, can attain near-optimal uli guarantees. then, we also provide a negative result, indicating that optimistic algorithms cannot achieve a near-optimal uli guarantee. finally, we propose an efficient algorithm for linear bandits with infinitely many arms, which achieves the uli guarantee, given access to an optimization oracle.",,2024-02-19,,"['junyan liu', 'yunfan li', 'lin yang']"
2402.12727,diffusion posterior sampling is computationally intractable,cs.lg cs.ai math.st stat.ml stat.th,"diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. in posterior sampling, one is also given a measurement model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x \mid y)$. posterior sampling is useful for tasks such as inpainting, super-resolution, and mri reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time.   in this paper we show that posterior sampling is \emph{computationally intractable}: under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which \emph{every} algorithm takes superpolynomial time, even though \emph{unconditional} sampling is provably fast. we also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption that there are one-way functions that take exponential time to invert.",,2024-02-20,,"['shivam gupta', 'ajil jalal', 'aditya parulekar', 'eric price', 'zhiyang xun']"
2402.12762,learning under singularity: an information criterion improving wbic and   sbic,stat.ml cs.lg,"we introduce a novel information criterion (ic), termed learning under singularity (ls), designed to enhance the functionality of the widely applicable bayes information criterion (wbic) and the singular bayesian information criterion (sbic). ls is effective without regularity constraints and demonstrates stability. watanabe defined a statistical model or a learning machine as regular if the mapping from a parameter to a probability distribution is one-to-one and its fisher information matrix is positive definite. in contrast, models not meeting these conditions are termed singular. over the past decade, several information criteria for singular cases have been proposed, including wbic and sbic. wbic is applicable in non-regular scenarios but faces challenges with large sample sizes and redundant estimation of known learning coefficients. conversely, sbic is limited in its broader application due to its dependence on maximum likelihood estimates. ls addresses these limitations by enhancing the utility of both wbic and sbic. it incorporates the empirical loss from the widely applicable information criterion (waic) to represent the goodness of fit to the statistical model, along with a penalty term similar to that of sbic. this approach offers a flexible and robust method for model selection, free from regularity constraints.",,2024-02-20,2024-02-22,"['lirui liu', 'joe suzuki']"
2402.12808,learning generalization and regularization of nonhomogeneous temporal   poisson processes,cs.lg stat.ml,"the poisson process, especially the nonhomogeneous poisson process (nhpp), is an essentially important counting process with numerous real-world applications. up to date, almost all works in the literature have been on the estimation of nhpps with infinite data using non-data driven binning methods. in this paper, we formulate the problem of estimation of nhpps from finite and limited data as a learning generalization problem. we mathematically show that while binning methods are essential for the estimation of nhpps, they pose a threat of overfitting when the amount of data is limited. we propose a framework for regularized learning of nhpps with two new adaptive and data-driven binning methods that help to remove the ad-hoc tuning of binning parameters. our methods are experimentally tested on synthetic and real-world datasets and the results show their effectiveness.",,2024-02-20,,"['son nguyen van', 'hoai nguyen xuan']"
2402.12828,sgd with clipping is secretly estimating the median gradient,stat.ml cs.lg math.oc,"there are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient. for example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself. here we study sgd with robust gradient estimators based on estimating the median. we first consider computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise. we then derive iterative methods based on the stochastic proximal point method for computing the geometric median and generalizations thereof. finally we propose an algorithm estimating the median gradient across iterations, and find that several well known methods - in particular different forms of clipping - are particular cases of this framework.",,2024-02-20,,"['fabian schaipp', 'guillaume garrigos', 'umut simsekli', 'robert gower']"
2402.12868,fast rates in online convex optimization by exploiting the curvature of   feasible sets,cs.lg stat.ml,"in this paper, we explore online convex optimization (oco) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets. in online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (ftl) algorithm to achieve a logarithmic regret. this paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. we first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $o(\rho \log t)$ in stochastic environments. here, $\rho > 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. our approach, unlike existing ones, can work directly with convex loss functions, exploiting the curvature of loss functions simultaneously, and can achieve the logarithmic regret only with a local property of feasible sets. additionally, it achieves an $o(\sqrt{t})$ regret even in adversarial environments where ftl suffers an $\omega(t)$ regret, and attains an $o(\rho \log t + \sqrt{c \rho \log t})$ regret bound in corrupted stochastic environments with corruption level $c$. furthermore, by extending our analysis, we establish a regret upper bound of $o\big(t^{\frac{q-2}{2(q-1)}} (\log t)^{\frac{q}{2(q-1)}}\big)$ for $q$-uniformly convex feasible sets, where uniformly convex sets include strongly convex sets and $\ell_p$-balls for $p \in [1,\infty)$. this bound bridges the gap between the $o(\log t)$ regret bound for strongly convex sets ($q=2$) and the $o(\sqrt{t})$ regret bound for non-curved sets ($q\to\infty$).",,2024-02-20,,"['taira tsuchiya', 'shinji ito']"
2402.12875,chain of thought empowers transformers to solve inherently serial   problems,cs.lg cs.cc stat.ml,"instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (cot), is a highly effective method to improve the accuracy of large language models (llms) on arithmetics and symbolic reasoning tasks. however, the mechanism behind cot remains unclear. this work provides a theoretical understanding of the power of cot for decoder-only transformers through the lens of expressiveness. conceptually, cot empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{tc}^0$ without cot. we first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{ac}^0$, a proper subset of $ \mathsf{tc}^0$. however, with $t$ steps of cot, constant-depth transformers using constant-bit precision and $o(\log n)$ embedding size can solve any problem solvable by boolean circuits of size $t$. empirically, enabling cot dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.",,2024-02-20,,"['zhiyuan li', 'hong liu', 'denny zhou', 'tengyu ma']"
2402.12885,a bound on the maximal marginal degrees of freedom,stat.ml cs.lg,"common kernel ridge regression is expensive in memory allocation and computation time. this paper addresses low rank approximations and surrogates for kernel ridge regression, which bridge these difficulties. the fundamental contribution of the paper is a lower bound on the rank of the low dimensional approximation, which is required such that the prediction power remains reliable. the bound relates the effective dimension with the largest statistical leverage score. we characterize the effective dimension and its growth behavior with respect to the regularization parameter by involving the regularity of the kernel. this growth is demonstrated to be asymptotically logarithmic for suitably chosen kernels, justifying low-rank approximations as the nystr\""om method.",,2024-02-20,,['paul dommel']
2402.12980,efficient adjustment for complex covariates: gaining efficiency with   dope,math.st stat.me stat.ml stat.th,"covariate adjustment is a ubiquitous method used to estimate the average treatment effect (ate) from observational data. assuming a known graphical structure of the data generating model, recent results give graphical criteria for optimal adjustment, which enables efficient estimation of the ate. however, graphical approaches are challenging for high-dimensional and complex data, and it is not straightforward to specify a meaningful graphical model of non-euclidean data such as texts. we propose an general framework that accommodates adjustment for any subset of information expressed by the covariates. we generalize prior works and leverage these results to identify the optimal covariate information for efficient adjustment. this information is minimally sufficient for prediction of the outcome conditionally on treatment.   based on our theoretical results, we propose the debiased outcome-adapted propensity estimator (dope) for efficient estimation of the ate, and we provide asymptotic results for the dope under general conditions. compared to the augmented inverse propensity weighted (aipw) estimator, the dope can retain its efficiency even when the covariates are highly predictive of treatment. we illustrate this with a single-index model, and with an implementation of the dope based on neural networks, we demonstrate its performance on simulated and real data. our results show that the dope provides an efficient and robust methodology for ate estimation in various observational settings.",,2024-02-20,,"['alexander mangulad christgau', 'niels richard hansen']"
2402.13042,not all distributional shifts are equal: fine-grained robust conformal   inference,stat.me,"we introduce a fine-grained framework for uncertainty quantification of predictive models under distributional shifts. this framework distinguishes the shift in covariate distributions from that in the conditional relationship between the outcome ($y$) and the covariates ($x$). we propose to reweight the training samples to adjust for an identifiable covariate shift while protecting against worst-case conditional distribution shift bounded in an $f$-divergence ball. based on ideas from conformal inference and distributionally robust learning, we present an algorithm that outputs (approximately) valid and efficient prediction intervals in the presence of distributional shifts. as a use case, we apply the framework to sensitivity analysis of individual treatment effects with hidden confounding. the proposed methods are evaluated in simulation studies and three real data applications, demonstrating superior robustness and efficiency compared with existing benchmarks.",,2024-02-20,2024-02-25,"['jiahao ai', 'zhimei ren']"
2402.13079,mode estimation with partial feedback,stat.ml cs.ir cs.it cs.lg math.it,"the combination of lightly supervised pre-training and online fine-tuning has played a key role in recent ai developments. these new learning pipelines call for new theoretical frameworks. in this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. we show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. finally, we combine those contributions into a statistically and computationally efficient solution to our problem.",,2024-02-20,,"['charles arnal', 'vivien cabannes', 'vianney perchet']"
2402.13106,on generalization bounds for deep compound gaussian neural networks,stat.ml cs.lg eess.sp,"algorithm unfolding or unrolling is the technique of constructing a deep neural network (dnn) from an iterative algorithm. unrolled dnns often provide better interpretability and superior empirical performance over standard dnns in signal estimation tasks. an important theoretical question, which has only recently received attention, is the development of generalization error bounds for unrolled dnns. these bounds deliver theoretical and practical insights into the performance of a dnn on empirical datasets that are distinct from, but sampled from, the probability density generating the dnn training data. in this paper, we develop novel generalization error bounds for a class of unrolled dnns that are informed by a compound gaussian prior. these compound gaussian networks have been shown to outperform comparative standard and unfolded deep neural networks in compressive sensing and tomographic imaging problems. the generalization error bound is formulated by bounding the rademacher complexity of the class of compound gaussian network estimates with dudley's integral. under realistic conditions, we show that, at worst, the generalization error scales $\mathcal{o}(n\sqrt{\ln(n)})$ in the signal dimension and $\mathcal{o}(($network size$)^{3/2})$ in network size.",,2024-02-20,,"['carter lyons', 'raghu g. raj', 'margaret cheney']"
2402.13182,order-optimal regret in distributed kernel bandits using uniform   sampling with shared randomness,cs.lg cs.dc stat.ml,"we consider distributed kernel bandits where $n$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel hilbert space. each agent sequentially queries the function to obtain noisy observations at the query points. agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $t$ and aggregating over agents. we develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $n$ and $t$. the key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. working together with the sparse approximation of the gp model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.",,2024-02-20,,"['nikola pavlovic', 'sudeep salgia', 'qing zhao']"
2402.13187,testing calibration in subquadratic time,cs.lg cs.ds stat.co stat.ml,"in the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. however, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. motivated by [bghn23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. we define the problem of calibration testing from samples where given $n$ draws from a distribution $\mathcal{d}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\mathcal{d}$ is perfectly calibrated, and the case where $\mathcal{d}$ is $\varepsilon$-far from calibration.   we design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factors) in time $o(n^{1.5} \log(n))$. this improves upon state-of-the-art black-box linear program solvers requiring $\omega(n^\omega)$ time, where $\omega > 2$ is the exponent of matrix multiplication. we also develop algorithms for tolerant variants of our testing problem, and give sample complexity lower bounds for alternative calibration distances to the one considered in this work. finally, we present preliminary experiments showing that the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale to accommodate moderate sample sizes.",,2024-02-20,,"['lunjia hu', 'kevin tian', 'chutong yang']"
2402.13221,chili: chemically-informed large-scale inorganic nanomaterials dataset   for advancing graph machine learning,cs.lg stat.ml,"advances in graph machine learning (ml) have been driven by applications in chemistry as graphs have remained the most expressive representations of molecules. while early graph ml methods focused primarily on small organic molecules, recently, the scope of graph ml has expanded to include inorganic materials. modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ml methods are unable to address. moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each graph can be broad ($10$ to $10^5$). the bulk of existing graph ml focuses on characterising molecules and materials by predicting target properties with graphs as input. however, the most exciting applications of graph ml will be in their generative capabilities, which is currently not at par with other domains such as images or text.   we invite the graph ml community to address these open challenges by presenting two new chemically-informed large-scale inorganic (chili) nanomaterials datasets: a medium-scale dataset (with overall >6m nodes, >49m edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types (chili-3k) and a large-scale dataset (with overall >183m nodes, >1.2b edges) of nanomaterials generated from experimentally determined crystal structures (chili-100k). we define 11 property prediction tasks and 6 structure prediction tasks, which are of special interest for nanomaterial research. we benchmark the performance of a wide array of baseline methods and use these benchmarking results to highlight areas which need future work. to the best of our knowledge, chili-3k and chili-100k are the first open-source nanomaterial datasets of this scale -- both on the individual graph level and of the dataset as a whole -- and the only nanomaterials datasets with high structural and elemental diversity.",,2024-02-20,2024-02-21,"['ulrik friis-jensen', 'frederik l. johansen', 'andy s. anker', 'erik b. dam', 'kirsten m. ø. jensen', 'raghavendra selvan']"
2402.13285,leveraging pac-bayes theory and gibbs distributions for generalization   bounds with complexity measures,stat.ml cs.lg,"in statistical learning theory, a generalization bound usually involves a complexity measure imposed by the considered theoretical framework. this limits the scope of such bounds, as other forms of capacity measures or regularizations are used in algorithms. in this paper, we leverage the framework of disintegrated pac-bayes bounds to derive a general generalization bound instantiable with arbitrary complexity measures. one trick to prove such a result involves considering a commonly used family of distributions: the gibbs distributions. our bound stands in probability jointly over the hypothesis and the learning sample, which allows the complexity to be adapted to the generalization gap as it can be customized to fit both the hypothesis class and the task.",,2024-02-19,,"['paul viallard', 'rémi emonet', 'amaury habrard', 'emilie morvant', 'valentina zantedeschi']"
2402.13332,double machine learning for causal hybrid modeling -- applications in   the earth sciences,cs.lg stat.me,"hybrid modeling integrates machine learning with scientific knowledge with the goal of enhancing interpretability, generalization, and adherence to natural laws. nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. this paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing double machine learning (dml) to estimate causal effects. we showcase its use for the earth sciences on two problems related to carbon dioxide fluxes. in the $q_{10}$ model, we demonstrate that dml-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (dnn) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. the study emphasizes the necessity of explicitly defining causal graphs and relationships, advocating for this as a general best practice. we encourage the continued exploration of causality in hybrid models for more interpretable and trustworthy results in knowledge-guided machine learning.",,2024-02-20,,"['kai-hendrik cohrs', 'gherardo varando', 'nuno carvalhais', 'markus reichstein', 'gustau camps-valls']"
2402.13366,statistical curriculum learning: an elimination algorithm achieving an   oracle risk,cs.lg stat.ml,"we consider a statistical version of curriculum learning (cl) in a parametric prediction setting. the learner is required to estimate a target parameter vector, and can adaptively collect samples from either the target model, or other source models that are similar to the target model, but less noisy. we consider three types of learners, depending on the level of side-information they receive. the first two, referred to as strong/weak-oracle learners, receive high/low degrees of information about the models, and use these to learn. the third, a fully adaptive learner, estimates the target parameter vector without any prior information. in the single source case, we propose an elimination learning method, whose risk matches that of a strong-oracle learner. in the multiple source case, we advocate that the risk of the weak-oracle learner is a realistic benchmark for the risk of adaptive learners. we develop an adaptive multiple elimination-rounds cl algorithm, and characterize instance-dependent conditions for its risk to match that of the weak-oracle learner. we consider instance-dependent minimax lower bounds, and discuss the challenges associated with defining the class of instances for the bound. we derive two minimax lower bounds, and determine the conditions under which the performance weak-oracle learner is minimax optimal.",,2024-02-20,,"['omer cohen', 'ron meir', 'nir weinberger']"
2402.13380,toward transformers: revolutionizing the solution of mixed integer   programs with transformers,cs.ai cs.lg math.co math.oc stat.ml,"in this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the capacitated lot sizing problem (clsp). our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (mip) problem. specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the clsp. this problem is inherently dynamic, and we need to handle sequential decision making under constraints. we present an efficient algorithm in which clsp solutions are learned through a transformer neural network. the proposed post-processed transformer algorithm surpasses the state-of-the-art solver, cplex and long short-term memory (lstm) in solution time, optimal gap, and percent infeasibility over 240k benchmark clsp instances tested. after the ml model is trained, conducting inference on the model, including post-processing, reduces the mip into a linear program (lp). this transforms the ml-based algorithm, combined with an lp solver, into a polynomial-time approximation algorithm to solve a well-known np-hard problem, with almost perfect solution quality.",,2024-02-20,,"['joshua f. cooper', 'seung jin choi', 'i. esra buyuktahtakin']"
2402.13391,de-biasing the bias: methods for improving disparity assessments with   noisy group measurements,stat.me,"health care decisions are increasingly informed by clinical decision support algorithms, but these algorithms may perpetuate or increase racial and ethnic disparities in access to and quality of health care. further complicating the problem, clinical data often have missing or poor quality racial and ethnic information, which can lead to misleading assessments of algorithmic bias. we present novel statistical methods that allow for the use of probabilities of racial/ethnic group membership in assessments of algorithm performance and quantify the statistical bias that results from error in these imputed group probabilities. we propose a sensitivity analysis approach to estimating the statistical bias that allows practitioners to assess disparities in algorithm performance under a range of assumed levels of group probability error. we also prove theoretical bounds on the statistical bias for a set of commonly used fairness metrics and describe real-world scenarios where our theoretical results are likely to apply. we present a case study using imputed race and ethnicity from the bayesian improved surname geocoding (bisg) algorithm for estimation of disparities in a clinical decision support algorithm used to inform osteoporosis treatment. our novel methods allow policy makers to understand the range of potential disparities under a given algorithm even when race and ethnicity information is missing and to make informed decisions regarding the implementation of machine learning for clinical decision support.",,2024-02-20,2024-02-26,"['solvejg wastvedt', 'joshua snoke', 'denis agniel', 'julie lai', 'marc n. elliott', 'steven c. martino']"
2402.13400,the dimension of self-directed learning,stat.ml cs.lg,"understanding the self-directed learning complexity has been an important problem that has captured the attention of the online learning theory community since the early 1990s. within this framework, the learner is allowed to adaptively choose its next data point in making predictions unlike the setting in adversarial online learning.   in this paper, we study the self-directed learning complexity in both the binary and multi-class settings, and we develop a dimension, namely $sddim$, that exactly characterizes the self-directed learning mistake-bound for any concept class. the intuition behind $sddim$ can be understood as a two-player game called the ""labelling game"". armed with this two-player game, we calculate $sddim$ on a whole host of examples with notable results on axis-aligned rectangles, vc dimension $1$ classes, and linear separators. we demonstrate several learnability gaps with a central focus on self-directed learning and offline sequence learning models that include either the best or worst ordering. finally, we extend our analysis to the self-directed binary agnostic setting where we derive upper and lower bounds.",,2024-02-20,,"['pramith devulapalli', 'steve hanneke']"
2402.13410,bayesian neural networks with domain knowledge priors,cs.lg stat.ml,"bayesian neural networks (bnns) have recently gained popularity due to their ability to quantify model uncertainty. however, specifying a prior for bnns that captures relevant domain knowledge is often extremely challenging. in this work, we propose a framework for integrating general forms of domain knowledge (i.e., any knowledge that can be represented by a loss function) into a bnn prior through variational inference, while enabling computationally efficient posterior inference and sampling. specifically, our approach results in a prior over neural network weights that assigns high probability mass to models that better align with our domain knowledge, leading to posterior samples that also exhibit this behavior. we show that bnns using our proposed domain knowledge priors outperform those with standard priors (e.g., isotropic gaussian, gaussian process), successfully incorporating diverse types of prior information such as fairness, physics rules, and healthcare knowledge and achieving better predictive performance. we also present techniques for transferring the learned priors across different model architectures, demonstrating their broad utility across various settings.",,2024-02-20,,"['dylan sam', 'rattana pukdee', 'daniel p. jeong', 'yewon byun', 'j. zico kolter']"
2402.13425,investigating the histogram loss in regression,cs.lg cs.ai stat.ml,"it is becoming increasingly common in regression to train neural networks that model the entire distribution even if only the mean is required for prediction. this additional modeling often comes with performance gain and the reasons behind the improvement are not fully known. this paper investigates a recent approach to regression, the histogram loss, which involves learning the conditional distribution of the target variable by minimizing the cross-entropy between a target distribution and a flexible histogram prediction. we design theoretical and empirical analyses to determine why and when this performance gain appears, and how different components of the loss contribute to it. our results suggest that the benefits of learning distributions in this setup come from improvements in optimization rather than learning a better representation. we then demonstrate the viability of the histogram loss in common deep learning applications without a need for costly hyperparameter tuning.",,2024-02-20,,"['ehsan imani', 'kai luedemann', 'sam scholnick-hughes', 'esraa elelimy', 'martha white']"
2402.13595,a cutting plane algorithm for globally solving low dimensional k-means   clustering problems,math.oc cs.lg stat.ml,"clustering is one of the most fundamental tools in data science and machine learning, and k-means clustering is one of the most common such methods. there is a variety of approximate algorithms for the k-means problem, but computing the globally optimal solution is in general np-hard. in this paper we consider the k-means problem for instances with low dimensional data and formulate it as a structured concave assignment problem. this allows us to exploit the low dimensional structure and solve the problem to global optimality within reasonable time for large data sets with several clusters. the method builds on iteratively solving a small concave problem and a large linear programming problem. this gives a sequence of feasible solutions along with bounds which we show converges to zero optimality gap. the paper combines methods from global optimization theory to accelerate the procedure, and we provide numerical results on their performance.",,2024-02-21,,"['martin ryner', 'jan kronqvist', 'johan karlsson']"
2402.13608,convergence acceleration of markov chain monte carlo-based gradient   descent by deep unfolding,cond-mat.dis-nn cs.lg stat.ml,"this study proposes a trainable sampling-based solver for combinatorial optimization problems (cops) using a deep-learning technique called deep unfolding. the proposed solver is based on the ohzeki method that combines markov-chain monte-carlo (mcmc) and gradient descent, and its step sizes are trained by minimizing a loss function. in the training process, we propose a sampling-based gradient estimation that substitutes auto-differentiation with a variance estimation, thereby circumventing the failure of back propagation due to the non-differentiability of mcmc. the numerical results for a few cops demonstrated that the proposed solver significantly accelerated the convergence speed compared with the original ohzeki method.",,2024-02-21,,"['ryo hagiwara', 'satoshi takabe']"
2402.13622,analysis of bootstrap and subsampling in high-dimensional regularized   regression,stat.ml cond-mat.dis-nn cs.lg,"we investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks. we provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\alpha\!=\! n/d$. our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\alpha\!<\!1$ relevant to modern machine learning practice, their predictions are not consistent, even with optimal regularization.",,2024-02-21,,"['lucas clarté', 'adrien vandenbroucque', 'guillaume dalle', 'bruno loureiro', 'florent krzakala', 'lenka zdeborová']"
2402.13646,a large dimensional analysis of multi-task semi-supervised learning,stat.ml cs.lg,"this article conducts a large dimensional study of a simple yet quite versatile classification model, encompassing at once multi-task and semi-supervised learning, and taking into account uncertain labeling. using tools from random matrix theory, we characterize the asymptotics of some key functionals, which allows us on the one hand to predict the performances of the algorithm, and on the other hand to reveal some counter-intuitive guidance on how to use it efficiently. the model, powerful enough to provide good performance guarantees, is also straightforward enough to provide strong insights into its behavior.",,2024-02-21,,"['victor leger', 'romain couillet']"
2402.13662,a method for bounding tail probabilities,math.pr cs.it math.it math.st stat.ml stat.th,"we present a method for upper and lower bounding the right and the left tail probabilities of continuous random variables (rvs). for the right tail probability of rv $x$ with probability density function $f_x(x)$, this method requires first setting a continuous, positive, and strictly decreasing function $g_x(x)$ such that $-f_x(x)/g'_x(x)$ is a decreasing and increasing function, $\forall x>x_0$, which results in upper and lower bounds, respectively, given in the form $-f_x(x) g_x(x)/g'_x(x)$, $\forall x>x_0$, where $x_0$ is some point. similarly, for the upper and lower bounds on the left tail probability of $x$, this method requires first setting a continuous, positive, and strictly increasing function $g_x(x)$ such that $f_x(x)/g'_x(x)$ is an increasing and decreasing function, $\forall x<x_0$, which results in upper and lower bounds, respectively, given in the form $f_x(x) g_x(x)/g'_x(x)$, $\forall x<x_0$. we provide some examples of good candidates for the function $g_x(x)$. we also establish connections between the new bounds and markov's inequality and chernoff's bound. in addition, we provide an iterative method for obtaining ever tighter lower and upper bounds, under certain conditions. finally, we provide numerical examples, where we show the tightness of these bounds, for some chosen $g_x(x)$.",,2024-02-21,2024-02-22,['nikola zlatanov']
2402.13666,measurement uncertainty: relating the uncertainties of physical and   virtual measurements,stat.ap cs.lg math.pr,"in the context of industrially mass-manufactured products, quality management is based on physically inspecting a small sample from a large batch and reasoning about the batch's quality conformance. when complementing physical inspections with predictions from machine learning models, it is crucial that the uncertainty of the prediction is known. otherwise, the application of established quality management concepts is not legitimate. deterministic (machine learning) models lack quantification of their predictive uncertainty and are therefore unsuitable. probabilistic (machine learning) models provide a predictive uncertainty along with the prediction. however, a concise relationship is missing between the measurement uncertainty of physical inspections and the predictive uncertainty of probabilistic models in their application in quality management. here, we show how the predictive uncertainty of probabilistic (machine learning) models is related to the measurement uncertainty of physical inspections. this enables the use of probabilistic models for virtual inspections and integrates them into existing quality management concepts. thus, we can provide a virtual measurement for any quality characteristic based on the process data and achieve a 100 percent inspection rate. in the field of predictive quality, the virtual measurement is of great interest. based on our results, physical inspections with a low sampling rate can be accompanied by virtual measurements that allow an inspection rate of 100 percent. we add substantial value, especially to complex process chains, as faulty products/parts are identified promptly and upcoming process steps can be aborted.",,2024-02-21,,"['simon cramer', 'tobias müller', 'robert h. schmitt']"
2402.13728,average gradient outer product as a mechanism for deep neural collapse,cs.lg stat.ml,"deep neural collapse (dnc) refers to the surprisingly rigid structure of the data representations in the final layers of deep neural networks (dnns). though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood. in this work, we provide substantial evidence that dnc formation occurs primarily through deep feature learning with the average gradient outer product (agop). this takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model. we proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in dnns. as shown in recent work, this singular structure is highly correlated with that of the agop. we then establish experimentally and theoretically that agop induces neural collapse in a randomly initialized neural network. in particular, we demonstrate that deep recursive feature machines, a method originally introduced as an abstraction for agop feature learning in convolutional neural networks, exhibits dnc.",,2024-02-21,,"['daniel beaglehole', 'peter súkeník', 'marco mondelli', 'mikhail belkin']"
2402.13765,accuracy-preserving calibration via statistical modeling on probability   simplex,cs.lg stat.ml,"classification models based on deep neural networks (dnns) must be calibrated to measure the reliability of predictions. some recent calibration methods have employed a probabilistic model on the probability simplex. however, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy. we propose an accuracy-preserving calibration method using the concrete distribution as the probabilistic model on the probability simplex. we theoretically prove that a dnn model trained on cross-entropy loss has optimality as the parameter of the concrete distribution. we also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex. we demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks.",,2024-02-21,,"['yasushi esaki', 'akihiro nakamura', 'keisuke kawano', 'ryoko tokuhisa', 'takuro kutsuna']"
2402.13794,revisiting convergence of adagrad with relaxed assumptions,math.oc cs.lg stat.ml,"in this study, we revisit the convergence of adagrad with momentum (covering adagrad as a special case) on non-convex smooth optimization problems. we consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude. this model encompasses a broad range of noises including bounded noise, sub-gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications. our analysis yields a probabilistic convergence rate which, under the general noise, could reach at (\tilde{\mathcal{o}}(1/\sqrt{t})). this rate does not rely on prior knowledge of problem-parameters and could accelerate to (\tilde{\mathcal{o}}(1/t)) where (t) denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small. the convergence rate thus matches the lower rate for stochastic first-order methods over non-convex smooth landscape up to logarithm terms [arjevani et al., 2023]. we further derive a convergence bound for adagrad with mometum, considering the generalized smoothness where the local smoothness is controlled by a first-order function of the gradient norm.",,2024-02-21,,"['yusu hong', 'junhong lin']"
2402.13870,generative probabilistic time series forecasting and applications in   grid operations,cs.lg eess.sp stat.ap,"generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations. such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations. inspired by wiener and kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series. we show that the weak innovation sequence is bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting. the proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques.",,2024-02-21,,"['xinyi wang', 'lang tong', 'qing zhao']"
2402.13890,a unified bayesian framework for interval hypothesis testing in clinical   trials,stat.me stat.ml,"the american statistical association (asa) statement on statistical significance and p-values \cite{wasserstein2016asa} cautioned statisticians against making scientific decisions solely on the basis of traditional p-values. the statement delineated key issues with p-values, including a lack of transparency, an inability to quantify evidence in support of the null hypothesis, and an inability to measure the size of an effect or the importance of a result. in this article, we demonstrate that the interval null hypothesis framework (instead of the point null hypothesis framework), when used in tandem with bayes factor-based tests, is instrumental in circumnavigating the key issues of p-values. further, we note that specifying prior densities for bayes factors is challenging and has been a reason for criticism of bayesian hypothesis testing in existing literature. we address this by adapting bayes factors directly based on common test statistics. we demonstrate, through numerical experiments and real data examples, that the proposed bayesian interval hypothesis testing procedures can be calibrated to ensure frequentist error control while retaining their inherent interpretability. finally, we illustrate the improved flexibility and applicability of the proposed methods by providing coherent frameworks for competitive landscape analysis and end-to-end bayesian hypothesis tests in the context of reporting clinical trial outcomes.",,2024-02-21,,"['abhisek chakraborty', 'megan h. murray', 'ilya lipkovich', 'yu du']"
2402.13891,overcoming saturation in density ratio estimation by iterated   regularization,cs.lg stat.ml,"estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. in this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. to resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models.",,2024-02-21,,"['lukas gruber', 'markus holzleitner', 'johannes lehner', 'sepp hochreiter', 'werner zellinger']"
2402.13901,non-asymptotic convergence of discrete-time diffusion models: new   approach and improved rate,cs.lg eess.sp stat.ml,"the denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. in this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. in particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. we then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with lipschitz scores, gaussian mixture distributions, and distributions with bounded support. we further propose a novel accelerated sampler and show that it improves the convergence rates of the corresponding regular sampler by orders of magnitude with respect to all system parameters. for distributions with bounded support, our result improves the dimensional dependence of the previous convergence rate by orders of magnitude. our study features a novel analysis technique that constructs tilting factor representation of the convergence error and exploits tweedie's formula for handling taylor expansion power terms.",,2024-02-21,,"['yuchen liang', 'peizhong ju', 'yingbin liang', 'ness shroff']"
2402.13903,dealing with unbounded gradients in stochastic saddle-point optimization,cs.lg math.oc stat.ml,"we study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. a notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. in this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward mdp without prior knowledge of the bias span.",,2024-02-21,,"['gergely neu', 'nneka okolo']"
2402.13934,do efficient transformers really save computation?,cs.lg cs.ai cs.cl stat.ml,"as transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard transformer has become very valuable. while many efficient transformers and transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard transformer. this makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. in this paper, we aim to understand the capabilities and limitations of efficient transformers, specifically the sparse transformer and the linear transformer. we focus on their reasoning capability as exhibited by chain-of-thought (cot) prompts and follow previous works to model them as dynamic programming (dp) problems. our results show that while these models are expressive enough to solve general dp tasks, contrary to expectations, they require a model size that scales with the problem size. nonetheless, we identify a class of dp problems for which these models can be more efficient than the standard transformer. we confirm our theoretical results through experiments on representative dp tasks, adding to the understanding of efficient transformers' practical strengths and weaknesses.",,2024-02-21,,"['kai yang', 'jan ackermann', 'zhenyu he', 'guhao feng', 'bohang zhang', 'yunzhen feng', 'qiwei ye', 'di he', 'liwei wang']"
2402.13945,probabilistic neural networks (pnns) for modeling aleatoric uncertainty   in scientific machine learning,stat.ml cs.ai cs.lg,"this paper investigates the use of probabilistic neural networks (pnns) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity. unlike traditional neural networks that produce deterministic outputs, pnns generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios. contributions of this paper include the development of a probabilistic distance metric to optimize pnn architecture, and the deployment of pnns in controlled data sets as well as a practical material science case involving fiber-reinforced composites. the findings confirm that pnns effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed gaussian process regression for this purpose. specifically, in a real-world scientific machine learning context, pnns yield remarkably accurate output mean estimates with r-squared scores approaching 0.97, and their predicted intervals exhibit a high correlation coefficient of nearly 0.80, closely matching observed data intervals. hence, this research contributes to the ongoing exploration of leveraging the sophisticated representational capacity of neural networks to delineate complex input-output relationships in scientific problems.",,2024-02-21,,"['farhad pourkamali-anaraki', 'jamal f. husseini', 'scott e. stapleton']"
2402.13999,asymptotics of learning with deep structured (random) features,stat.ml cond-mat.dis-nn cs.lg math.st stat.th,"for a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large. this characterization is formulated in terms of the population covariance of the features. our work is partially motivated by the problem of learning with gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers. for such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices. we further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent.",,2024-02-21,,"['dominik schröder', 'daniil dmitriev', 'hugo cui', 'bruno loureiro']"
2402.14022,statistical validation of a deep learning algorithm for dental anomaly   detection in intraoral radiographs using paired data,eess.iv cs.cv cs.lg stat.ap,"this article describes the clinical validation study setup, statistical analysis and results for a deep learning algorithm which detects dental anomalies in intraoral radiographic images, more specifically caries, apical lesions, root canal treatment defects, marginal defects at crown restorations, periodontal bone loss and calculus. the study compares the detection performance of dentists using the deep learning algorithm to the prior performance of these dentists evaluating the images without algorithmic assistance. calculating the marginal profit and loss of performance from the annotated paired image data allows for a quantification of the hypothesized change in sensitivity and specificity. the statistical significance of these results is extensively proven using both mcnemar's test and the binomial hypothesis test. the average sensitivity increases from $60.7\%$ to $85.9\%$, while the average specificity slightly decreases from $94.5\%$ to $92.7\%$. we prove that the increase of the area under the localization roc curve (auc) is significant (from $0.60$ to $0.86$ on average), while the average auc is bounded by the $95\%$ confidence intervals ${[}0.54, 0.65{]}$ and ${[}0.82, 0.90{]}$. when using the deep learning algorithm for diagnostic guidance, the dentist can be $95\%$ confident that the average true population sensitivity is bounded by the range $79.6\%$ to $91.9\%$. the proposed paired data setup and statistical analysis can be used as a blueprint to thoroughly test the effect of a modality change, like a deep learning based detection and/or segmentation, on radiographic images.",,2024-02-01,,"['pieter van leemput', 'johannes keustermans', 'wouter mollemans']"
2402.14026,probability tools for sequential random projection,math.st cs.ds cs.it cs.na math.it math.na math.pr stat.ml stat.th,"we introduce the first probabilistic framework tailored for sequential random projection, an approach rooted in the challenges of sequential decision-making under uncertainty. the analysis is complicated by the sequential dependence and high-dimensional nature of random variables, a byproduct of the adaptive mechanisms inherent in sequential decision processes. our work features a novel construction of a stopped process, facilitating the analysis of a sequence of concentration events that are interconnected in a sequential manner. by employing the method of mixtures within a self-normalized process, derived from the stopped process, we achieve a desired non-asymptotic probability bound. this bound represents a non-trivial martingale extension of the johnson-lindenstrauss (jl) lemma, marking a pioneering contribution to the literature on random projection and sequential analysis.",,2024-02-16,2024-02-26,['yingru li']
2402.14029,partial search in a frozen network is enough to find a strong lottery   ticket,cs.lg cs.ai stat.ml,"randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (slts). recently, gadhikar et al. (2023) demonstrated theoretically and experimentally that slts can also be found within a randomly pruned source network, thus reducing the slt search space. however, this limits the search to slts that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. this paper proposes a method that reduces the slt search space by an arbitrary ratio that is independent of the desired slt sparsity. a random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the slt. indeed, the slt existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. in addition to reducing search space, the random freezing pattern can also be exploited to reduce model size in inference. furthermore, experimental results show that the proposed method finds slts with better accuracy and model size trade-off than the slts obtained from dense or randomly pruned source networks. in particular, the slt found in a frozen graph neural network achieves higher accuracy than its weight trained counterpart while reducing model size by $40.3\times$.",,2024-02-19,,"['hikari otsuka', 'daiki chijiwa', 'ángel lópez garcía-arias', 'yasuyuki okoshi', 'kazushi kawamura', 'thiem van chu', 'daichi fujiki', 'susumu takeuchi', 'masato motomura']"
2402.14080,efficient normalized conformal prediction and uncertainty quantification   for anti-cancer drug sensitivity prediction with deep regression forests,cs.lg cs.ai stat.ml,"deep learning models are being adopted and applied on various critical decision-making tasks, yet they are trained to provide point predictions without providing degrees of confidence. the trustworthiness of deep learning models can be increased if paired with uncertainty estimations. conformal prediction has emerged as a promising method to pair machine learning models with prediction intervals, allowing for a view of the model's uncertainty. however, popular uncertainty estimation methods for conformal prediction fail to provide heteroskedastic intervals that are equally accurate for all samples. in this paper, we propose a method to estimate the uncertainty of each sample by calculating the variance obtained from a deep regression forest. we show that the deep regression forest variance improves the efficiency and coverage of normalized inductive conformal prediction on a drug response prediction task.",,2024-02-21,,"['daniel nolte', 'souparno ghosh', 'ranadip pal']"
2402.14081,robust learning of noisy time series collections using stochastic   process models with motion codes,cs.lg cs.ai stat.ml,"while time series classification and forecasting problems have been extensively studied, the cases of noisy time series data with arbitrary time sequence lengths have remained challenging. each time series instance can be thought of as a sample realization of a noisy dynamical model, which is characterized by a continuous stochastic process. for many applications, the data are mixed and consist of several types of noisy time series sequences modeled by multiple stochastic processes, making the forecasting and classification tasks even more challenging. instead of regressing data naively and individually to each time series type, we take a latent variable model approach using a mixtured gaussian processes with learned spectral kernels. more specifically, we auto-assign each type of noisy time series data a signature vector called its motion code. then, conditioned on each assigned motion code, we infer a sparse approximation of the corresponding time series using the concept of the most informative timestamps. our unmixing classification approach involves maximizing the likelihood across all the mixed noisy time series sequences of varying lengths. this stochastic approach allows us to learn not only within a single type of noisy time series data but also across many underlying stochastic processes, giving us a way to learn multiple dynamical models in an integrated and robust manner. the different learned latent stochastic models allow us to generate specific sub-type forecasting. we provide several quantitative comparisons demonstrating the performance of our approach.",,2024-02-21,,"['chandrajit bajaj', 'minh nguyen']"
2402.14090,social environment design,cs.ai econ.gn q-fin.ec stat.ml,"artificial intelligence (ai) holds promise as a technology that can be used to improve government and economic policy-making. this paper proposes a new research agenda towards this end by introducing social environment design, a general framework for the use of ai for automated policy-making that connects with the reinforcement learning, econcs, and computational social choice communities. the framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through ai simulation. we highlight key open problems for future research in ai-based policy-making. by solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.",,2024-02-21,,"['edwin zhang', 'sadie zhao', 'tonghan wang', 'safwan hossain', 'henry gasztowtt', 'stephan zheng', 'david c. parkes', 'milind tambe', 'yiling chen']"
2402.14103,computational-statistical gaps for improper learning in sparse linear   regression,cs.lg cs.cc math.st stat.ml stat.th,"we study computational-statistical gaps for improper learning in sparse linear regression. more specifically, given $n$ samples from a $k$-sparse linear model in dimension $d$, we ask what is the minimum sample complexity to efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense estimate for the regression vector that achieves non-trivial prediction error on the $n$ samples. information-theoretically this can be achieved using $\theta(k \log (d/k))$ samples. yet, despite its prominence in the literature, there is no polynomial-time algorithm known to achieve the same guarantees using less than $\theta(d)$ samples without additional restrictions on the model. similarly, existing hardness results are either restricted to the proper setting, in which the estimate must be sparse as well, or only apply to specific algorithms.   we give evidence that efficient algorithms for this task require at least (roughly) $\omega(k^2)$ samples. in particular, we show that an improper learning algorithm for sparse linear regression can be used to solve sparse pca problems (with a negative spike) in their wishart form, in regimes in which efficient algorithms are widely believed to require at least $\omega(k^2)$ samples. we complement our reduction with low-degree and statistical query lower bounds for the sparse pca problems from which we reduce.   our hardness results apply to the (correlated) random design setting in which the covariates are drawn i.i.d. from a mean-zero gaussian distribution with unknown covariance.",,2024-02-21,,"['rares-darius buhai', 'jingqiu ding', 'stefan tiegel']"
2402.14145,multiply robust estimation for local distribution shifts with multiple   domains,stat.ml cs.lg stat.me,"distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another. we focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment. we propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. the method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. our method is designed to be implemented with commonly used off-the-shelf machine learning models. we establish theoretical guarantees on the generalization bound of the method on the test risk. with extensive experiments on synthetic and real datasets, we demonstrate that the proposed method substantially improves over existing alternatives in prediction accuracy and robustness on both regression and classification tasks. we also assess its effectiveness on a user city prediction dataset from a large technology company.",,2024-02-21,,"['steven wilkins-reeves', 'xu chen', 'qi ma', 'christine agarwal', 'aude hofleitner']"
2402.14220,estimating unknown population sizes using the hypergeometric   distribution,cs.lg stat.me stat.ml,"the multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories. addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the sizes of its constituent categories are unknown. here, we propose a novel solution using the hypergeometric likelihood to solve this estimation challenge, even in the presence of severe under-sampling. we develop our approach to account for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, such as with collaborative filtering, using the variational autoencoder framework. empirical data simulation demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and in its ability to learn an informative latent space. we demonstrate our method's versatility through applications in nlp, by inferring and estimating the complexity of latent vocabularies in text excerpts, and in biology, by accurately recovering the true number of gene transcripts from sparse single-cell genomics data.",,2024-02-21,,"['liam hodgson', 'danilo bzdok']"
2402.14254,a hierarchical decomposition for explaining ml performance discrepancies,cs.lg stat.ml,"machine learning (ml) algorithms can often differ in performance across domains. understanding $\textit{why}$ their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. existing methods focus on $\textit{aggregate decompositions}$ of the total performance gap into the impact of a shift in the distribution of features $p(x)$ versus the impact of a shift in the conditional distribution of the outcome $p(y|x)$; however, such coarse explanations offer only a few options for how one can close the performance gap. $\textit{detailed variable-level decompositions}$ that quantify the importance of each variable to each term in the aggregate decomposition can provide a much deeper understanding and suggest much more targeted interventions. however, existing methods assume knowledge of the full causal graph or make strong parametric assumptions. we introduce a nonparametric hierarchical framework that provides both aggregate and detailed decompositions for explaining why the performance of an ml algorithm differs across domains, without requiring causal knowledge. we derive debiased, computationally-efficient estimators, and statistical inference procedures for asymptotically valid confidence intervals.",,2024-02-21,,"['jean feng', 'harvineet singh', 'fan xia', 'adarsh subbaswamy', 'alexej gossmann']"
2402.14294,high-arity pac learning via exchangeability,cs.lg math.lo math.st stat.th,"we develop a theory of high-arity pac learning, which is statistical learning in the presence of ""structured correlation"". in this theory, hypotheses are either graphs, hypergraphs or, more generally, structures in finite relational languages, and i.i.d. sampling is replaced by sampling an induced substructure, producing an exchangeable distribution. we prove a high-arity version of the fundamental theorem of statistical learning by characterizing high-arity (agnostic) pac learnability in terms of finiteness of a purely combinatorial dimension and in terms of an appropriate version of uniform convergence.",,2024-02-22,,"['leonardo n. coregliano', 'maryanthe malliaris']"
2402.14332,from large to small datasets: size generalization for clustering   algorithm selection,cs.lg stat.ml,"in clustering algorithm selection, we are given a massive dataset and must efficiently select which clustering algorithm to use. we study this problem in a semi-supervised setting, with an unknown ground-truth clustering that we can only access through expensive oracle queries. ideally, the clustering algorithm's output will be structurally close to the ground truth. we approach this problem by introducing a notion of size generalization for clustering algorithm accuracy. we identify conditions under which we can (1) subsample the massive clustering instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance. we provide theoretical size generalization guarantees for three classic clustering algorithms: single-linkage, k-means++, and (a smoothed variant of) gonzalez's k-centers heuristic. we validate our theoretical analysis with empirical results, observing that on real-world clustering instances, we can use a subsample of as little as 5% of the data to identify which algorithm is best on the full dataset.",,2024-02-22,2024-02-25,"['vaggos chatziafratis', 'ishani karmarkar', 'ellen vitercik']"
2402.14335,hyperfast: instant classification for tabular data,cs.lg cs.ai stat.ml,"training deep learning models and performing hyperparameter tuning can be computationally demanding and time-consuming. meanwhile, traditional machine learning methods like gradient-boosting algorithms remain the preferred choice for most tabular data applications, while neural network alternatives require extensive hyperparameter tuning or work only in toy datasets under limited settings. in this paper, we introduce hyperfast, a meta-trained hypernetwork designed for instant classification of tabular data in a single forward pass. hyperfast generates a task-specific neural network tailored to an unseen dataset that can be directly used for classification inference, removing the need for training a model. we report extensive experiments with openml and genomic data, comparing hyperfast to competing tabular data neural networks, traditional ml methods, automl systems, and boosting machines. hyperfast shows highly competitive results, while being significantly faster. additionally, our approach demonstrates robust adaptability across a variety of classification tasks with little to no fine-tuning, positioning hyperfast as a strong solution for numerous applications and rapid model deployment. hyperfast introduces a promising paradigm for fast classification, with the potential to substantially decrease the computational burden of deep learning. our code, which offers a scikit-learn-like interface, along with the trained hyperfast model, can be found at https://github.com/ai-sandbox/hyperfast.",,2024-02-22,,"['david bonet', 'daniel mas montserrat', 'xavier giró-i-nieto', 'alexander g. ioannidis']"
2402.14385,winddragon: enhancing wind power forecasting with automated deep   learning,cs.lg physics.ao-ph stat.ml,"achieving net zero carbon emissions by 2050 requires the integration of increasing amounts of wind power into power grids. this energy source poses a challenge to system operators due to its variability and uncertainty. therefore, accurate forecasting of wind power is critical for grid operation and system balancing. this paper presents an innovative approach to short-term (1 to 6 hour horizon) windpower forecasting at a national level. the method leverages automated deep learning combined with numerical weather predictions wind speed maps to accurately forecast wind power.",,2024-02-22,,"['julie keisler', 'etienne le naour']"
2402.14402,global safe sequential learning via efficient knowledge transfer,cs.lg stat.ml,"sequential learning methods such as active learning and bayesian optimization select the most informative data to learn about a task. in many medical or engineering applications, the data selection is constrained by a priori unknown safety conditions. a promissing line of safe learning methods utilize gaussian processes (gps) to model the safety probability and perform data selection in areas with high safety confidence. however, accurate safety modeling requires prior knowledge or consumes data. in addition, the safety confidence centers around the given observations which leads to local exploration. as transferable source knowledge is often available in safety critical experiments, we propose to consider transfer safe sequential learning to accelerate the learning of safety. we further consider a pre-computation of source components to reduce the additional computational load that is introduced by incorporating source data. in this paper, we theoretically analyze the maximum explorable safe regions of conventional safe learning methods. furthermore, we empirically demonstrate that our approach 1) learns a task with lower data consumption, 2) globally explores multiple disjoint safe regions under guidance of the source knowledge, and 3) operates with computation comparable to conventional safe learning methods.",,2024-02-22,,"['cen-you li', 'olaf duennbier', 'marc toussaint', 'barbara rakitsch', 'christoph zimmer']"
2402.14416,algorithm-agnostic significance testing in supervised learning with   multimodal data,stat.ap,"valid statistical inference is crucial for decision-making but difficult to obtain in supervised learning with multimodal data, e.g., combinations of clinical features, genomic data, and medical images. multimodal data often warrants the use of black-box algorithms, for instance, random forests or neural networks, which impede the use of traditional variable significance tests. we address this problem by proposing the use of covariance measure tests (comets), which are calibrated and powerful tests that can be combined with any sufficiently predictive supervised learning algorithm. we apply comets to several high-dimensional, multimodal data sets to illustrate (i) variable significance testing for finding relevant mutations modulating drug-activity, (ii) modality selection for predicting survival in liver cancer patients with multiomics data, and (iii) modality selection with clinical features and medical imaging data. in all applications, comets yield results consistent with domain knowledge without requiring data-driven pre-processing which may invalidate type i error control. these novel applications with high-dimensional multimodal data corroborate prior results on the power and robustness of comets for significance testing. the comets r package and source code for reproducing all results is available at https://github.com/lucaskook/comets. all data sets used in this work are openly available.",,2024-02-22,,"['lucas kook', 'anton rask lundborg']"
2402.14423,the universe as a learning system,quant-ph physics.data-an stat.ml,"at its microscopic level, the universe follows the laws of quantum mechanics. focusing on the quantum trajectories of particles as followed from the hydrodynamical formulation of quantum mechanics, we propose that under general requirements, quantum systems follow a disrupted version of the gradient descent model, a basic machine learning algorithm, where the learning is distorted due to the self-organizing process of the quantum system. such a learning process is possible only when we assume dissipation, i.e., that the quantum system is open. the friction parameter determines the nonlinearity of the quantum system. we then provide an empirical demonstration of the proposed model.",,2024-02-22,2024-02-25,['tomer shushi']
2402.14469,reimagining anomalies: what if anomalies were normal?,cs.cv cs.lg stat.ml,"deep learning-based methods have achieved a breakthrough in image anomaly detection, but their complexity introduces a considerable challenge to understanding why an instance is predicted to be anomalous. we introduce a novel explanation method that generates multiple counterfactual examples for each anomaly, capturing diverse concepts of anomalousness. a counterfactual example is a modification of the anomaly that is perceived as normal by the anomaly detector. the method provides a high-level semantic explanation of the mechanism that triggered the anomaly detector, allowing users to explore ""what-if scenarios."" qualitative and quantitative analyses across various image datasets show that the method applied to state-of-the-art anomaly detectors can achieve high-quality semantic explanations of detectors.",,2024-02-22,,"['philipp liznerski', 'saurabh varshneya', 'ece calikus', 'sophie fellenz', 'marius kloft']"
2402.14532,a framework for variational inference of lightweight bayesian neural   networks with heteroscedastic uncertainties,cs.lg stat.ml,"obtaining heteroscedastic predictive uncertainties from a bayesian neural network (bnn) is vital to many applications. often, heteroscedastic aleatoric uncertainties are learned as outputs of the bnn in addition to the predictive means, however doing so may necessitate adding more learnable parameters to the network. in this work, we demonstrate that both the heteroscedastic aleatoric and epistemic variance can be embedded into the variances of learned bnn parameters, improving predictive performance for lightweight networks. by complementing this approach with a moment propagation approach to inference, we introduce a relatively simple framework for sampling-free variational inference suitable for lightweight bnns.",,2024-02-22,,"['david j. schodt', 'ryan brown', 'michael merritt', 'samuel park', 'delsin menolascino', 'mark a. peot']"
2402.14578,multivariate online linear regression for hierarchical forecasting,stat.ml cs.lg math.oc,"in this paper, we consider a deterministic online linear regression model where we allow the responses to be multivariate. to address this problem, we introduce multivaw, a method that extends the well-known vovk-azoury-warmuth algorithm to the multivariate setting, and show that it also enjoys logarithmic regret in time. we apply our results to the online hierarchical forecasting problem and recover an algorithm from this literature as a special case, allowing us to relax the hypotheses usually made for its analysis.",,2024-02-22,,"['massil hihat', 'guillaume garrigos', 'adeline fermanian', 'simon bussy']"
2402.14585,bandits with abstention under expert advice,cs.lg stat.ml,"we study the classic problem of prediction with expert advice under bandit feedback. our model assumes that one action, corresponding to the learner's abstention from play, has no reward or loss on every trial. we propose the cba algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical exp4 algorithm. we can view our problem as the aggregation of confidence-rated predictors when the learner has the option of abstention from play. importantly, we are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors. in the special case of specialists we achieve a novel reward bound, significantly improving previous bounds of specialistexp (treating abstention as another action). as an example application, we discuss learning unions of balls in a finite metric space. in this contextual setting, we devise an efficient implementation of cba, reducing the runtime from quadratic to almost linear in the number of contexts. preliminary experiments show that cba improves over existing bandit algorithms.",,2024-02-22,,"['stephen pasteris', 'alberto rumi', 'maximilian thiessen', 'shota saito', 'atsushi miyauchi', 'fabio vitale', 'mark herbster']"
2402.14621,latrend: a framework for clustering longitudinal data,cs.lg stat.ml,"clustering of longitudinal data is used to explore common trends among subjects over time for a numeric measurement of interest. various r packages have been introduced throughout the years for identifying clusters of longitudinal patterns, summarizing the variability in trajectories between subject in terms of one or more trends. we introduce the r package ""latrend"" as a framework for the unified application of methods for longitudinal clustering, enabling comparisons between methods with minimal coding. the package also serves as an interface to commonly used packages for clustering longitudinal data, including ""dtwclust"", ""flexmix"", ""kml"", ""lcmm"", ""mclust"", ""mixak"", and ""mixtools"". this enables researchers to easily compare different approaches, implementations, and method specifications. furthermore, researchers can build upon the standard tools provided by the framework to quickly implement new cluster methods, enabling rapid prototyping. we demonstrate the functionality and application of the latrend package on a synthetic dataset based on the therapy adherence patterns of patients with sleep apnea.",,2024-02-22,,"['niek den teuling', 'steffen pauws', 'edwin van den heuvel']"
2402.14645,sparse linear regression and lattice problems,cs.lg stat.ml,"sparse linear regression (slr) is a well-studied problem in statistics where one is given a design matrix $x\in\mathbb{r}^{m\times n}$ and a response vector $y=x\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is, $\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\widehat{\theta} \in \mathbb{r}^n$ that minimizes the mean squared prediction error $\frac{1}{m}\|x\widehat{\theta}-x\theta^*\|^2_2$. while $\ell_1$-relaxation methods such as basis pursuit, lasso, and the dantzig selector solve slr when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.   we give evidence of average-case hardness of slr w.r.t. all efficient algorithms assuming the worst-case hardness of lattice problems. specifically, we give an instance-by-instance reduction from a variant of the bounded distance decoding (bdd) problem on lattices to slr, where the condition number of the lattice basis that defines the bdd instance is directly related to the restricted eigenvalue condition of the design matrix, which characterizes some of the classical statistical-computational gaps for sparse linear regression. also, by appealing to worst-case to average-case reductions from the world of lattices, this shows hardness for a distribution of slr instances; while the design matrices are ill-conditioned, the resulting slr instances are in the identifiable regime.   furthermore, for well-conditioned (essentially) isotropic gaussian design matrices, where lasso is known to behave well in the identifiable regime, we show hardness of outputting any good solution in the unidentifiable regime where there are many solutions, assuming the worst-case hardness of standard and well-studied lattice problems.",,2024-02-22,,"['aparna gupte', 'neekon vafa', 'vinod vaikuntanathan']"
2402.14646,colora: continuous low-rank adaptation for reduced implicit neural   modeling of parameterized partial differential equations,cs.lg cs.na math.na stat.ml,"this work introduces reduced models based on continuous low rank adaptation (colora) that pre-train neural networks for a given partial differential equation and then continuously adapt low-rank weights in time to rapidly predict the evolution of solution fields at new physics parameters and new initial conditions. the adaptation can be either purely data-driven or via an equation-driven variational approach that provides galerkin-optimal approximations. because colora approximates solution fields locally in time, the rank of the weights can be kept small, which means that only few training trajectories are required offline so that colora is well suited for data-scarce regimes. predictions with colora are orders of magnitude faster than with classical methods and their accuracy and parameter efficiency is higher compared to other neural network approaches.",,2024-02-22,,"['jules berman', 'benjamin peherstorfer']"
2402.14664,bayesian off-policy evaluation and learning for large action spaces,cs.lg cs.ai stat.ml,"in interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (ope) and learning (opl) in large action spaces. we introduce a unified bayesian framework to capture these correlations through structured and informative priors. in this framework, we propose sdm, a generic bayesian approach designed for ope and opl, grounded in both algorithmic and theoretical foundations. notably, sdm leverages action correlations without compromising computational efficiency. moreover, inspired by online bayesian bandits, we introduce bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. we analyze sdm in ope and opl, highlighting the benefits of leveraging action correlations. empirical evidence showcases the strong performance of sdm.",,2024-02-22,,"['imad aouali', 'victor-emmanuel brunel', 'david rohde', 'anna korba']"
2402.14684,adaptive time series forecasting with markovian variance switching,stat.ml cs.lg math.pr,"adaptive time series forecasting is essential for prediction under regime changes. several classical methods assume linear gaussian state space model (lgssm) with variances constant in time. however, there are many real-world processes that cannot be captured by such models. we consider a state-space model with markov switching variances. such dynamical systems are usually intractable because of their computational complexity increasing exponentially with time; variational bayes (vb) techniques have been applied to this problem. in this paper, we propose a new way of estimating variances based on online learning theory; we adapt expert aggregation methods to learn the variances over time. we apply the proposed method to synthetic data and to the problem of electricity load forecasting. we show that this method is robust to misspecification and outperforms traditional expert aggregation.",,2024-02-22,,"['baptiste abélès', 'joseph de vilmarest', 'olivier wintemberger']"
2402.14703,on the curses of future and history in future-dependent value functions   for off-policy evaluation,cs.lg cs.ai stat.ml,"we study off-policy evaluation (ope) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. while such estimators exist for mdps and pomdps can be converted to history-based mdps, their estimation errors depend on the state-density ratio for mdps which becomes history ratios after conversion, an exponential object. recently, uehara et al. (2022) proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space. however, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. in this paper, we discover novel coverage assumptions tailored to the structure of pomdps, such as outcome coverage and belief coverage. these assumptions not only enable polynomial bounds on the aforementioned quantities, but also lead to the discovery of new algorithms with complementary properties.",,2024-02-22,,"['yuheng zhang', 'nan jiang']"
2402.14726,incorporating expert rules into neural networks in the framework of   concept-based learning,cs.lg cs.ai stat.ml,a problem of incorporating the expert rules into machine learning models for extending the concept-based learning is formulated in the paper. it is proposed how to combine logical rules and neural networks predicting the concept probabilities. the first idea behind the combination is to form constraints for a joint probability distribution over all combinations of concept values to satisfy the expert rules. the second idea is to represent a feasible set of probability distributions in the form of a convex polytope and to use its vertices or faces. we provide several approaches for solving the stated problem and for training neural networks which guarantee that the output probabilities of concepts would not violate the expert rules. the solution of the problem can be viewed as a way for combining the inductive and deductive learning. expert rules are used in a broader sense when any logical function that connects concepts and class labels or just concepts with each other can be regarded as a rule. this feature significantly expands the class of the proposed results. numerical examples illustrate the approaches. the code of proposed algorithms is publicly available.,,2024-02-22,,"['andrei v. konstantinov', 'lev v. utkin']"
2402.14735,how transformers learn causal structure with gradient descent,cs.lg cs.it math.it stat.ml,"the incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. however, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. to better understand this process, we introduce an in-context learning task that requires learning latent causal structure. we prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. the key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. as a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph. as a special case, when the sequences are generated from in-context markov chains, we prove that transformers learn an induction head (olsson et al., 2022). we confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures.",,2024-02-22,,"['eshaan nichani', 'alex damian', 'jason d. lee']"
2402.14758,batch and match: black-box variational inference with a score-based   divergence,stat.ml cs.ai cs.lg stat.co,"most leading implementations of black-box variational inference (bbvi) are based on optimizing a stochastic evidence lower bound (elbo). but such approaches to bbvi often converge slowly due to the high variance of their gradient estimates. in this work, we propose batch and match (bam), an alternative approach to bbvi based on a score-based divergence. notably, this score-based divergence can be optimized by a closed-form proximal update for gaussian variational families with full covariance matrices. we analyze the convergence of bam when the target distribution is gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. we also evaluate the performance of bam on gaussian and non-gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. in these experiments, we find that bam typically converges in fewer (and sometimes significantly fewer) gradient evaluations than leading implementations of bbvi based on elbo maximization.",,2024-02-22,,"['diana cai', 'chirag modi', 'loucas pillaud-vivien', 'charles c. margossian', 'robert m. gower', 'david m. blei', 'lawrence k. saul']"
2402.14759,generalising realisability in statistical learning theory under   epistemic uncertainty,cs.lg cs.ai math.st stat.th,"the purpose of this paper is to look into how central notions in statistical learning theory, such as realisability, generalise under the assumption that train and test distribution are issued from the same credal set, i.e., a convex set of probability distributions. this can be considered as a first step towards a more general treatment of statistical learning under epistemic uncertainty.",,2024-02-22,,['fabio cuzzolin']
2402.14775,localised natural causal learning algorithms for weak consistency   conditions,stat.me,"by relaxing conditions for natural structure learning algorithms, a family of constraint-based algorithms containing all exact structure learning algorithms under the faithfulness assumption, we define localised natural structure learning algorithms (lons). we also provide a set of necessary and sufficient assumptions for consistency of lons, which can be thought of as a strict relaxation of the restricted faithfulness assumption. we provide a practical lons algorithm that runs in exponential time, which is then compared with related existing structure learning algorithms, namely pc/sgs and the relatively recent sparsest permutation algorithm. simulation studies are also provided.",,2024-02-22,,"['kai z teh', 'kayvan sadeghi', 'terry soo']"
2402.14777,causal imputation for counterfactual scms: bridging graphs and latent   factor models,stat.ml cs.lg,"we consider the task of causal imputation, where we aim to predict the outcomes of some set of actions across a wide range of possible contexts. as a running example, we consider predicting how different drugs affect cells from different cell types. we study the index-only setting, where the actions and contexts are categorical variables with a finite number of possible values. even in this simple setting, a practical challenge arises, since often only a small subset of possible action-context pairs have been studied. thus, models must extrapolate to novel action-context pairs, which can be framed as a form of matrix completion with rows indexed by actions, columns indexed by contexts, and matrix entries corresponding to outcomes. we introduce a novel scm-based model class, where the outcome is expressed as a counterfactual, actions are expressed as interventions on an instrumental variable, and contexts are defined based on the initial state of the system. we show that, under a linearity assumption, this setup induces a latent factor model over the matrix of outcomes, with an additional fixed effect term. to perform causal prediction based on this model class, we introduce simple extension to the synthetic interventions estimator (agarwal et al., 2020). we evaluate several matrix completion approaches on the prism drug repurposing dataset, showing that our method outperforms all other considered matrix completion approaches.",,2024-02-22,,"['alvaro ribot', 'chandler squires', 'caroline uhler']"
2402.14781,rao-blackwellising bayesian causal inference,cs.lg cs.ai stat.me stat.ml,"bayesian causal inference, i.e., inferring a posterior over causal models for the use in downstream causal reasoning tasks, poses a hard computational inference problem that is little explored in literature. in this work, we combine techniques from order-based mcmc structure learning with recent advances in gradient-based graph learning into an effective bayesian causal inference framework. specifically, we decompose the problem of inferring the causal structure into (i) inferring a topological order over variables and (ii) inferring the parent sets for each variable. when limiting the number of parents per variable, we can exactly marginalise over the parent sets in polynomial time. we further use gaussian processes to model the unknown causal mechanisms, which also allows their exact marginalisation. this introduces a rao-blackwellization scheme, where all components are eliminated from the model, except for the causal order, for which we learn a distribution via gradient-based optimisation. the combination of rao-blackwellization with our sequential inference procedure for causal orders yields state-of-the-art on linear and non-linear additive noise benchmarks with scale-free and erdos-renyi graph structures.",,2024-02-22,,"['christian toth', 'christian knoll', 'franz pernkopf', 'robert peharz']"
2402.14925,efficient unbiased sparsification,cs.it cs.lg math.it math.st stat.th,"an unbiased $m$-sparsification of a vector $p\in \mathbb{r}^n$ is a random vector $q\in \mathbb{r}^n$ with mean $p$ that has at most $m<n$ nonzero coordinates. unbiased sparsification compresses the original vector without introducing bias; it arises in various contexts, such as in federated learning and sampling sparse probability distributions. ideally, unbiased sparsification should also minimize the expected value of a divergence function $\mathsf{div}(q,p)$ that measures how far away $q$ is from the original $p$. if $q$ is optimal in this sense, then we call it efficient. our main results describe efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable. surprisingly, the characterization for permutation-invariant divergences is robust to the choice of divergence function, in the sense that our class of optimal $q$ for squared euclidean distance coincides with our class of optimal $q$ for kullback-leibler divergence, or indeed any of a wide variety of divergences.",,2024-02-22,,"['leighton barnes', 'timothy chow', 'emma cohen', 'keith frankston', 'benjamin howard', 'fred kochman', 'daniel scheinerman', 'jeffrey vanderkam']"
2402.14951,in-context learning of a linear transformer block: benefits of the mlp   component and one-step gd initialization,stat.ml cs.cl cs.lg,"we study the \emph{in-context learning} (icl) ability of a \emph{linear transformer block} (ltb) that combines a linear attention component and a linear multi-layer perceptron (mlp) component. for icl of linear regression with a gaussian prior and a \emph{non-zero mean}, we show that ltb can achieve nearly bayes optimal icl risk. in contrast, using only linear attention must incur an irreducible additive approximation error. furthermore, we establish a correspondence between ltb and one-step gradient descent estimators with learnable initialization ($\mathsf{gd}\text{-}\mathbf{\beta}$), in the sense that every $\mathsf{gd}\text{-}\mathbf{\beta}$ estimator can be implemented by an ltb estimator and every optimal ltb estimator that minimizes the in-class icl risk is effectively a $\mathsf{gd}\text{-}\mathbf{\beta}$ estimator. finally, we show that $\mathsf{gd}\text{-}\mathbf{\beta}$ estimators can be efficiently optimized with gradient flow, despite a non-convex training objective. our results reveal that ltb achieves icl by implementing $\mathsf{gd}\text{-}\mathbf{\beta}$, and they highlight the role of mlp layers in reducing approximation error.",,2024-02-22,,"['ruiqi zhang', 'jingfeng wu', 'peter l. bartlett']"
2402.14966,smoothness adaptive hypothesis transfer learning,stat.ml cs.lg stat.me,"many existing two-phase kernel-based hypothesis transfer learning algorithms employ the same kernel regularization across phases and rely on the known smoothness of functions to obtain optimality. therefore, they fail to adapt to the varying and unknown smoothness between the target/source and their offset in practice. in this paper, we address these problems by proposing smoothness adaptive transfer learning (satl), a two-phase kernel ridge regression(krr)-based algorithm. we first prove that employing the misspecified fixed bandwidth gaussian kernel in target-only krr learning can achieve minimax optimality and derive an adaptive procedure to the unknown sobolev smoothness. leveraging these results, satl employs gaussian kernels in both phases so that the estimators can adapt to the unknown smoothness of the target/source and their offset function. we derive the minimax lower bound of the learning problem in excess risk and show that satl enjoys a matching upper bound up to a logarithmic factor. the minimax convergence rate sheds light on the factors influencing transfer dynamics and demonstrates the superiority of satl compared to non-transfer learning settings. while our main objective is a theoretical analysis, we also conduct several experiments to confirm our results.",,2024-02-22,,"['haotian lin', 'matthew reimherr']"
2402.14980,"comparative analysis of data preprocessing methods, feature selection   techniques and machine learning models for improved classification and   regression performance on imbalanced genetic data",q-bio.qm cs.lg stat.ml,"rapid advancements in genome sequencing have led to the collection of vast amounts of genomics data. researchers may be interested in using machine learning models on such data to predict the pathogenicity or clinical significance of a genetic mutation. however, many genetic datasets contain imbalanced target variables that pose challenges to machine learning models: observations are skewed/imbalanced in regression tasks or class-imbalanced in classification tasks. genetic datasets are also often high-cardinal and contain skewed predictor variables, which poses further challenges. we aimed to investigate the effects of data preprocessing, feature selection techniques, and model selection on the performance of models trained on these datasets. we measured performance with 5-fold cross-validation and compared averaged r-squared and accuracy metrics across different combinations of techniques. we found that outliers/skew in predictor or target variables did not pose a challenge to regression models. we also found that class-imbalanced target variables and skewed predictors had little to no impact on classification performance. random forest was the best model to use for imbalanced regression tasks. while our study uses a genetic dataset as an example of a real-world application, our findings can be generalized to any similar datasets.",,2024-02-22,,"['arshmeet kaur', 'morteza sarmadi']"
2402.14985,nonsmooth nonparametric regression via fractional laplacian eigenmaps,math.st stat.ml stat.th,"we develop nonparametric regression methods for the case when the true regression function is not necessarily smooth. more specifically, our approach is using the fractional laplacian and is designed to handle the case when the true regression function lies in an $l_2$-fractional sobolev space with order $s\in (0,1)$. this function class is a hilbert space lying between the space of square-integrable functions and the first-order sobolev space consisting of differentiable functions. it contains fractional power functions, piecewise constant or polynomial functions and bump function as canonical examples. for the proposed approach, we prove upper bounds on the in-sample mean-squared estimation error of order $n^{-\frac{2s}{2s+d}}$, where $d$ is the dimension, $s$ is the aforementioned order parameter and $n$ is the number of observations. we also provide preliminary empirical results validating the practical performance of the developed estimators.",,2024-02-22,,"['zhaoyang shi', 'krishnakumar balasubramanian', 'wolfgang polonik']"
2402.14987,on the performance of empirical risk minimization with smoothed data,stat.ml cs.lg,"in order to circumvent statistical and computational hardness results in sequential decision-making, recent work has considered smoothed online learning, where the distribution of data at each time is assumed to have bounded likeliehood ratio with respect to a base measure when conditioned on the history. while previous works have demonstrated the benefits of smoothness, they have either assumed that the base measure is known to the learner or have presented computationally inefficient algorithms applying only in special cases. this work investigates the more general setting where the base measure is \emph{unknown} to the learner, focusing in particular on the performance of empirical risk minimization (erm) with square loss when the data are well-specified and smooth. we show that in this setting, erm is able to achieve sublinear error whenever a class is learnable with iid data; in particular, erm achieves error scaling as $\tilde o( \sqrt{\mathrm{comp}(\mathcal f)\cdot t} )$, where $\mathrm{comp}(\mathcal f)$ is the statistical complexity of learning $\mathcal f$ with iid data. in so doing, we prove a novel norm comparison bound for smoothed data that comprises the first sharp norm comparison for dependent data applying to arbitrary, nonlinear function classes. we complement these results with a lower bound indicating that our analysis of erm is essentially tight, establishing a separation in the performance of erm between smoothed and iid data.",,2024-02-22,,"['adam block', 'alexander rakhlin', 'abhishek shetty']"
2402.14988,verifiable boosted tree ensembles,cs.lg cs.cr cs.lo stat.ml,"verifiable learning advocates for training machine learning models amenable to efficient security verification. prior research demonstrated that specific classes of decision tree ensembles -- called large-spread ensembles -- allow for robustness verification in polynomial time against any norm-based attacker. this study expands prior work on verifiable learning from basic ensemble methods (i.e., hard majority voting) to advanced boosted tree ensembles, such as those trained using xgboost or lightgbm. our formal results indicate that robustness verification is achievable in polynomial time when considering attackers based on the $l_\infty$-norm, but remains np-hard for other norm-based attackers. nevertheless, we present a pseudo-polynomial time algorithm to verify robustness against attackers based on the $l_p$-norm for any $p \in \mathbb{n} \cup \{0\}$, which in practice grants excellent performance. our experimental evaluation shows that large-spread boosted ensembles are accurate enough for practical adoption, while being amenable to efficient security verification.",,2024-02-22,,"['stefano calzavara', 'lorenzo cazzaro', 'claudio lucchese', 'giulio ermanno pibiri']"
2402.14992,tinybenchmarks: evaluating llms with fewer examples,cs.cl cs.ai cs.lg stat.ml,"the versatility of large language models (llms) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. these benchmarks consist of tens of thousands of examples making evaluation of llms very expensive. in this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an llm on several key benchmarks. for example, we show that to accurately estimate the performance of an llm on mmlu, a popular multiple-choice qa benchmark consisting of 14k examples, it is sufficient to evaluate this llm on 100 curated examples. we release evaluation tools and tiny versions of popular benchmarks: open llm leaderboard, mmlu, helm, and alpacaeval 2.0. our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.",,2024-02-22,,"['felipe maia polo', 'lucas weber', 'leshem choshen', 'yuekai sun', 'gongjun xu', 'mikhail yurochkin']"
2402.15005,comparison of machine learning classification algorithms and application   to the framingham heart study,cs.lg stat.ml,"the use of machine learning algorithms in healthcare can amplify social injustices and health inequities. while the exacerbation of biases can occur and compound during the problem selection, data collection, and outcome definition, this research pertains to some generalizability impediments that occur during the development and the post-deployment of machine learning classification algorithms. using the framingham coronary heart disease data as a case study, we show how to effectively select a probability cutoff to convert a regression model for a dichotomous variable into a classifier. we then compare the sampling distribution of the predictive performance of eight machine learning classification algorithms under four training/testing scenarios to test their generalizability and their potential to perpetuate biases. we show that both the extreme gradient boosting, and support vector machine are flawed when trained on an unbalanced dataset. we introduced and show that the double discriminant scoring of type i is the most generalizable as it consistently outperforms the other classification algorithms regardless of the training/testing scenario. finally, we introduce a methodology to extract an optimal variable hierarchy for a classification algorithm, and illustrate it on the overall, male and female framingham coronary heart disease data.",,2024-02-22,,['nabil kahouadji']
2402.15019,consistency-guided temperature scaling using style and content   information for out-of-domain calibration,cs.lg cs.ai stat.ml,"research interests in the robustness of deep neural networks against domain shifts have been rapidly increasing in recent years. most existing works, however, focus on improving the accuracy of the model, not the calibration performance which is another important requirement for trustworthy ai systems. temperature scaling (ts), an accuracy-preserving post-hoc calibration method, has been proven to be effective in in-domain settings, but not in out-of-domain (ood) due to the difficulty in obtaining a validation set for the unseen domain beforehand. in this paper, we propose consistency-guided temperature scaling (cts), a new temperature scaling strategy that can significantly enhance the ood calibration performance by providing mutual supervision among data samples in the source domains. motivated by our observation that over-confidence stemming from inconsistent sample predictions is the main obstacle to ood calibration, we propose to guide the scaling process by taking consistencies into account in terms of two different aspects -- style and content -- which are the key components that can well-represent data samples in multi-domain settings. experimental results demonstrate that our proposed strategy outperforms existing works, achieving superior ood calibration performance on various datasets. this can be accomplished by employing only the source domains without compromising accuracy, making our scheme directly applicable to various trustworthy ai systems.",,2024-02-22,,"['wonjeong choi', 'jungwuk park', 'dong-jun han', 'younghyun park', 'jaekyun moon']"
2402.15053,nonlinear bayesian optimal experimental design using logarithmic sobolev   inequalities,stat.ml cs.lg stat.me,"we study the problem of selecting $k$ experiments from a larger candidate pool, where the goal is to maximize mutual information (mi) between the selected subset and the underlying parameters. finding the exact solution is to this combinatorial optimization problem is computationally costly, not only due to the complexity of the combinatorial search but also the difficulty of evaluating mi in nonlinear/non-gaussian settings. we propose greedy approaches based on new computationally inexpensive lower bounds for mi, constructed via log-sobolev inequalities. we demonstrate that our method outperforms random selection strategies, gaussian approximations, and nested monte carlo (nmc) estimators of mi in various settings, including optimal design for nonlinear models with non-additive noise.",,2024-02-22,,"['fengyi li', 'ayoub belhadji', 'youssef marzouk']"
2402.15115,physics-constrained polynomial chaos expansion for scientific machine   learning and uncertainty quantification,stat.ml cs.lg physics.data-an,"we present a novel physics-constrained polynomial chaos expansion as a surrogate modeling method capable of performing both scientific machine learning (sciml) and uncertainty quantification (uq) tasks. the proposed method possesses a unique capability: it seamlessly integrates sciml into uq and vice versa, which allows it to quantify the uncertainties in sciml tasks effectively and leverage sciml for improved uncertainty assessment during uq-related tasks. the proposed surrogate model can effectively incorporate a variety of physical constraints, such as governing partial differential equations (pdes) with associated initial and boundary conditions constraints, inequality-type constraints (e.g., monotonicity, convexity, non-negativity, among others), and additional a priori information in the training process to supplement limited data. this ensures physically realistic predictions and significantly reduces the need for expensive computational model evaluations to train the surrogate model. furthermore, the proposed method has a built-in uncertainty quantification (uq) feature to efficiently estimate output uncertainties. to demonstrate the effectiveness of the proposed method, we apply it to a diverse set of problems, including linear/non-linear pdes with deterministic and stochastic parameters, data-driven surrogate modeling of a complex physical system, and uq of a stochastic system with parameters modeled as random fields.",,2024-02-23,,"['himanshu sharma', 'lukáš novák', 'michael d. shields']"
2402.15125,accelerating convergence of stein variational gradient descent via deep   unfolding,cs.lg stat.ml,"stein variational gradient descent (svgd) is a prominent particle-based variational inference method used for sampling a target distribution. svgd has attracted interest for application in machine-learning techniques such as bayesian inference. in this paper, we propose novel trainable algorithms that incorporate a deep-learning technique called deep unfolding,into svgd. this approach facilitates the learning of the internal parameters of svgd, thereby accelerating its convergence speed. to evaluate the proposed trainable svgd algorithms, we conducted numerical simulations of three tasks: sampling a one-dimensional gaussian mixture, performing bayesian logistic regression, and learning bayesian neural networks. the results show that our proposed algorithms exhibit faster convergence than the conventional variants of svgd.",,2024-02-23,,"['yuya kawamura', 'satoshi takabe']"
2402.15127,multi-armed bandits with abstention,cs.lg cs.it math.it stat.ml,"we introduce a novel extension of the canonical multi-armed bandit problem that incorporates an additional strategic element: abstention. in this enhanced framework, the agent is not only tasked with selecting an arm at each time step, but also has the option to abstain from accepting the stochastic instantaneous reward before observing it. when opting for abstention, the agent either suffers a fixed regret or gains a guaranteed reward. given this added layer of complexity, we ask whether we can develop efficient algorithms that are both asymptotically and minimax optimal. we answer this question affirmatively by designing and analyzing algorithms whose regrets meet their corresponding information-theoretic lower bounds. our results offer valuable quantitative insights into the benefits of the abstention option, laying the groundwork for further exploration in other online decision-making problems with such an option. numerical results further corroborate our theoretical findings.",,2024-02-23,,"['junwen yang', 'tianyuan jin', 'vincent y. f. tan']"
2402.15137,benchmarking observational studies with experimental data under   right-censoring,stat.me stat.ml,"drawing causal inferences from observational studies (os) requires unverifiable validity assumptions; however, one can falsify those assumptions by benchmarking the os with experimental data from a randomized controlled trial (rct). a major limitation of existing procedures is not accounting for censoring, despite the abundance of rcts and oses that report right-censored time-to-event outcomes. we consider two cases where censoring time (1) is independent of time-to-event and (2) depends on time-to-event the same way in os and rct. for the former, we adopt a censoring-doubly-robust signal for the conditional average treatment effect (cate) to facilitate an equivalence test of cates in os and rct, which serves as a proxy for testing if the validity assumptions hold. for the latter, we show that the same test can still be used even though unbiased cate estimation may not be possible. we verify the effectiveness of our censoring-aware tests via semi-synthetic experiments and analyze rct and os data from the women's health initiative study.",,2024-02-23,,"['ilker demirel', 'edward de brouwer', 'zeshan hussain', 'michael oberst', 'anthony philippakis', 'david sontag']"
2402.15171,covariance-adaptive least-squares algorithm for stochastic combinatorial   semi-bandits,cs.lg math.st stat.ml stat.th,"we address the problem of stochastic combinatorial semi-bandits, where a player can select from p subsets of a set containing d base items. most existing algorithms (e.g. cucb, escb, ols-ucb) require prior knowledge on the reward distribution, like an upper bound on a sub-gaussian proxy-variance, which is hard to estimate tightly. in this work, we design a variance-adaptive version of ols-ucb, relying on an online estimation of the covariance structure. estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. when covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms bandit feedback approaches, not only in exponential regimes where p $\gg$ d but also when p $\le$ d, which is not straightforward from most existing analyses.",,2024-02-23,,"['julien zhou', 'pierre gaillard', 'thibaud rahier', 'houssam zenati', 'julyan arbel']"
2402.15194,fine-tuning of continuous-time diffusion models as entropy-regularized   control,cs.lg cs.ai stat.ml,"diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. while diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). however, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. the last issue often occurs when the reward function is a learned model meant to approximate a ground-truth ""genuine"" reward, as is the case in many practical applications. these challenges, collectively termed ""reward collapse,"" pose a substantial obstacle. to address this reward collapse, we frame the finetuning problem as entropy-regularized control against the pretrained diffusion model, i.e., directly optimizing entropy-enhanced rewards with neural sdes. we present theoretical and empirical evidence that demonstrates our framework is capable of efficiently generating diverse samples with high genuine rewards, mitigating the overoptimization of imperfect reward models.",,2024-02-23,2024-02-28,"['masatoshi uehara', 'yulai zhao', 'kevin black', 'ehsan hajiramezanali', 'gabriele scalia', 'nathaniel lee diamant', 'alex m tseng', 'tommaso biancalani', 'sergey levine']"
2402.15232,classification of compact radio sources in the galactic plane with   supervised machine learning,astro-ph.im cs.lg stat.ml,"generation of science-ready data from processed data products is one of the major challenges in next-generation radio continuum surveys with the square kilometre array (ska) and its precursors, due to the expected data volume and the need to achieve a high degree of automated processing. source extraction, characterization, and classification are the major stages involved in this process. in this work we focus on the classification of compact radio sources in the galactic plane using both radio and infrared images as inputs. to this aim, we produced a curated dataset of ~20,000 images of compact sources of different astronomical classes, obtained from past radio and infrared surveys, and novel radio data from pilot surveys carried out with the australian ska pathfinder (askap). radio spectral index information was also obtained for a subset of the data. we then trained two different classifiers on the produced dataset. the first model uses gradient-boosted decision trees and is trained on a set of pre-computed features derived from the data, which include radio-infrared colour indices and the radio spectral index. the second model is trained directly on multi-channel images, employing convolutional neural networks. using a completely supervised procedure, we obtained a high classification accuracy (f1-score>90%) for separating galactic objects from the extragalactic background. individual class discrimination performances, ranging from 60% to 75%, increased by 10% when adding far-infrared and spectral index information, with extragalactic objects, pne and hii regions identified with higher accuracies. the implemented tools and trained models were publicly released, and made available to the radioastronomical community for future application on new radio data.",,2024-02-23,,"['s. riggi', 'g. umana', 'c. trigilio', 'c. bordiu', 'f. bufano', 'a. ingallinera', 'f. cavallaro', 'y. gordon', 'r. p. norris', 'g. gürkan', 'p. leto', 'c. buemi', 's. loru', 'a. m. hopkins', 'm. d. filipović', 't. cecconello']"
2402.15285,generative modelling with tensor train approximations of   hamilton--jacobi--bellman equations,stat.ml cs.lg math.st stat.th,"sampling from probability densities is a common challenge in fields such as uncertainty quantification (uq) and generative modelling (gm). in gm in particular, the use of reverse-time diffusion processes depending on the log-densities of ornstein-uhlenbeck forward processes are a popular sampling tool. in berner et al. [2022] the authors point out that these log-densities can be obtained by solution of a \textit{hamilton-jacobi-bellman} (hjb) equation known from stochastic optimal control. while this hjb equation is usually treated with indirect methods such as policy iteration and unsupervised training of black-box architectures like neural networks, we propose instead to solve the hjb equation by direct time integration, using compressed polynomials represented in the tensor train (tt) format for spatial discretization. crucially, this method is sample-free, agnostic to normalization constants and can avoid the curse of dimensionality due to the tt compression. we provide a complete derivation of the hjb equation's action on tensor train polynomials and demonstrate the performance of the proposed time-step-, rank- and degree-adaptive integration method on a nonlinear sampling task in 20 dimensions.",,2024-02-23,,"['david sommer', 'robert gruhlke', 'max kirstein', 'martin eigel', 'claudia schillings']"
2402.15332,categorical deep learning: an algebraic theory of architectures,cs.lg cs.ai math.ct math.ra stat.ml,"we present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal algebra of monads valued in a 2-category of parametric maps -- as a single theory elegantly subsuming both of these flavours of neural network design. to defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as rnns. we also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.",,2024-02-23,,"['bruno gavranović', 'paul lessard', 'andrew dudzik', 'tamara von glehn', 'joão g. m. araújo', 'petar veličković']"
2402.15344,iteration and stochastic first-order oracle complexities of stochastic   gradient descent using constant and decaying learning rates,stat.ml cs.lg,"the performance of stochastic gradient descent (sgd), which is the simplest first-order optimizer for training deep neural networks, depends on not only the learning rate but also the batch size. they both affect the number of iterations and the stochastic first-order oracle (sfo) complexity needed for training. in particular, the previous numerical results indicated that, for sgd using a constant learning rate, the number of iterations needed for training decreases when the batch size increases, and the sfo complexity needed for training is minimized at a critical batch size and that it increases once the batch size exceeds that size. here, we study the relationship between batch size and the iteration and sfo complexities needed for nonconvex optimization in deep learning with sgd using constant or decaying learning rates and show that sgd using the critical batch size minimizes the sfo complexity. we also provide numerical comparisons of sgd with the existing first-order optimizers and show the usefulness of sgd using a critical batch size. moreover, we show that measured critical batch sizes are close to the sizes estimated from our theoretical results.",,2024-02-23,,"['kento imaizumi', 'hideaki iiduka']"
2402.15345,fourier basis density model,cs.lg stat.ml,"we introduce a lightweight, flexible and end-to-end trainable probability density model parameterized by a constrained fourier basis. we assess its performance at approximating a range of multi-modal 1d densities, which are generally difficult to fit. in comparison to the deep factorized model introduced in [1], our model achieves a lower cross entropy at a similar computational budget. in addition, we also evaluate our method on a toy compression task, demonstrating its utility in learned compression.",,2024-02-23,,"['alfredo de la fuente', 'saurabh singh', 'johannes ballé']"
2402.15347,information-theoretic safe bayesian optimization,cs.lg cs.ai stat.ml,"we consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. a common approach is to place a gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. in this paper, we propose an information-theoretic safe exploration criterion that directly exploits the gp posterior to identify the most informative safe parameters to evaluate. the combination of this exploration criterion with a well known bayesian optimization acquisition function yields a novel safe bayesian optimization selection criterion. our approach is naturally applicable to continuous domains and does not require additional explicit hyperparameters. we theoretically analyze the method and show that we do not violate the safety constraint with high probability and that we learn about the value of the safe optimum up to arbitrary precision. empirical evaluations demonstrate improved data-efficiency and scalability.",,2024-02-23,,"['alessandro g. bottero', 'carlos e. luis', 'julia vinogradska', 'felix berkenkamp', 'jan peters']"
2402.15357,rapid bayesian identification of sparse nonlinear dynamics from scarce   and noisy data,stat.me nlin.cd stat.ml,"we propose a fast probabilistic framework for identifying differential equations governing the dynamics of observed data. we recast the sindy method within a bayesian framework and use gaussian approximations for the prior and likelihood to speed up computation. the resulting method, bayesian-sindy, not only quantifies uncertainty in the parameters estimated but also is more robust when learning the correct model from limited and noisy data. using both synthetic and real-life examples such as lynx-hare population dynamics, we demonstrate the effectiveness of the new framework in learning correct model equations and compare its computational and data efficiency with existing methods. because bayesian-sindy can quickly assimilate data and is robust against noise, it is particularly suitable for biological data and real-time system identification in control. its probabilistic framework also enables the calculation of information entropy, laying the foundation for an active learning strategy.",,2024-02-23,,"['lloyd fung', 'urban fasel', 'matthew p. juniper']"
2402.15365,efficient semi-supervised inference for logistic regression under   case-control studies,stat.ml cs.lg,"semi-supervised learning has received increasingly attention in statistics and machine learning. in semi-supervised learning settings, a labeled data set with both outcomes and covariates and an unlabeled data set with covariates only are collected. we consider an inference problem in semi-supervised settings where the outcome in the labeled data is binary and the labeled data is collected by case-control sampling. case-control sampling is an effective sampling scheme for alleviating imbalance structure in binary data. under the logistic model assumption, case-control data can still provide consistent estimator for the slope parameter of the regression model. however, the intercept parameter is not identifiable. consequently, the marginal case proportion cannot be estimated from case-control data. we find out that with the availability of the unlabeled data, the intercept parameter can be identified in semi-supervised learning setting. we construct the likelihood function of the observed labeled and unlabeled data and obtain the maximum likelihood estimator via an iterative algorithm. the proposed estimator is shown to be consistent, asymptotically normal, and semiparametrically efficient. extensive simulation studies are conducted to show the finite sample performance of the proposed method. the results imply that the unlabeled data not only helps to identify the intercept but also improves the estimation efficiency of the slope parameter. meanwhile, the marginal case proportion can be estimated accurately by the proposed method.",,2024-02-23,,"['zhuojun quan', 'yuanyuan lin', 'kani chen', 'wen yu']"
2402.15409,"lasso with latents: efficient estimation, covariate rescaling, and   computational-statistical gaps",stat.ml cs.cc cs.ds cs.lg math.st stat.th,"it is well-known that the statistical performance of lasso can suffer significantly when the covariates of interest have strong correlations. in particular, the prediction error of lasso becomes much worse than computationally inefficient alternatives like best subset selection. due to a large conjectured computational-statistical tradeoff in the problem of sparse linear regression, it may be impossible to close this gap in general.   in this work, we propose a natural sparse linear regression setting where strong correlations between covariates arise from unobserved latent variables. in this setting, we analyze the problem caused by strong correlations and design a surprisingly simple fix. while lasso with standard normalization of covariates fails, there exists a heterogeneous scaling of the covariates with which lasso will suddenly obtain strong provable guarantees for estimation. moreover, we design a simple, efficient procedure for computing such a ""smart scaling.""   the sample complexity of the resulting ""rescaled lasso"" algorithm incurs (in the worst case) quadratic dependence on the sparsity of the underlying signal. while this dependence is not information-theoretically necessary, we give evidence that it is optimal among the class of polynomial-time algorithms, via the method of low-degree polynomials. this argument reveals a new connection between sparse linear regression and a special version of sparse pca with a near-critical negative spike. the latter problem can be thought of as a real-valued analogue of learning a sparse parity. using it, we also establish the first computational-statistical gap for the closely related problem of learning a gaussian graphical model.",,2024-02-23,,"['jonathan kelner', 'frederic koehler', 'raghu meka', 'dhruv rohatgi']"
2402.15415,the impact of lora on the emergence of clusters in transformers,cs.lg math.ds stat.ml,"in this paper, we employ the mathematical framework on transformers developed by \citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical} to explore how variations in attention parameters and initial token values impact the structural dynamics of token clusters. our analysis demonstrates that while the clusters within a modified attention matrix dynamics can exhibit significant divergence from the original over extended periods, they maintain close similarities over shorter intervals, depending on the parameter differences. this work contributes to the fine-tuning field through practical applications to the lora algorithm \cite{hu2021lora,peft}, enhancing our understanding of the behavior of lora-enhanced transformer models.",,2024-02-23,,"['hugo koubbi', 'matthieu boussard', 'louis hernandez']"
2402.15432,universal lower bounds and optimal rates: achieving minimax clustering   error in sub-exponential mixture models,math.st cs.lg stat.ml stat.th,"clustering is a pivotal challenge in unsupervised machine learning and is often investigated through the lens of mixture models. the optimal error rate for recovering cluster labels in gaussian and sub-gaussian mixture models involves ad hoc signal-to-noise ratios. simple iterative algorithms, such as lloyd's algorithm, attain this optimal error rate. in this paper, we first establish a universal lower bound for the error rate in clustering any mixture model, expressed through a chernoff divergence, a more versatile measure of model information than signal-to-noise ratios. we then demonstrate that iterative algorithms attain this lower bound in mixture models with sub-exponential tails, notably emphasizing location-scale mixtures featuring laplace-distributed errors. additionally, for datasets better modelled by poisson or negative binomial mixtures, we study mixture models whose distributions belong to an exponential family. in such mixtures, we establish that bregman hard clustering, a variant of lloyd's algorithm employing a bregman divergence, is rate optimal.",,2024-02-23,,"['maximilien dreveton', 'alperen gözeten', 'matthias grossglauser', 'patrick thiran']"
2402.15442,gros: a general robust aggregation strategy,math.st stat.ap stat.ml stat.th,"a new, very general, robust procedure for combining estimators in metric spaces is introduced gros. the method is reminiscent of the well-known median of means, as described in \cite{devroye2016sub}. initially, the sample is divided into $k$ groups. subsequently, an estimator is computed for each group. finally, these $k$ estimators are combined using a robust procedure. we prove that this estimator is sub-gaussian and we get its break-down point, in the sense of donoho. the robust procedure involves a minimization problem on a general metric space, but we show that the same (up to a constant) sub-gaussianity is obtained if the minimization is taken over the sample, making gros feasible in practice. the performance of gros is evaluated through five simulation studies: the first one focuses on classification using $k$-means, the second one on the multi-armed bandit problem, the third one on the regression problem. the fourth one is the set estimation problem under a noisy model. lastly, we apply gros to get a robust persistent diagram.",,2024-02-23,,"['alejandro cholaquidis', 'emilien joly', 'leonardo moreno']"
2402.15478,"transformers are expressive, but are they expressive enough for   regression?",cs.lg stat.ml,"transformers have become pivotal in natural language processing, demonstrating remarkable success in applications like machine translation and summarization. given their widespread adoption, several works have attempted to analyze the expressivity of transformers. expressivity of a neural network is the class of functions it can approximate. a neural network is fully expressive if it can act as a universal function approximator. we attempt to analyze the same for transformers. contrary to existing claims, our findings reveal that transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. the central question emerges as: ""\textit{are transformers truly universal function approximators}?"" to address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. our contributions include a theoretical analysis pinpointing the root of transformers' limitation in function approximation and extensive experiments to verify the limitation. by shedding light on these challenges, we advocate a refined understanding of transformers' capabilities.",,2024-02-23,,"['swaroop nath', 'harshad khadilkar', 'pushpak bhattacharyya']"
2402.15587,a study of shape modeling against noise,cs.cv stat.ml,"shape modeling is a challenging task with many potential applications in computer vision and medical imaging. there are many shape modeling methods in the literature, each with its advantages and applications. however, many shape modeling methods have difficulties handling shapes that have missing pieces or outliers. in this regard, this paper introduces shape denoising, a fundamental problem in shape modeling that lies at the core of many computer vision and medical imaging applications and has not received enough attention in the literature. the paper introduces six types of noise that can be used to perturb shapes as well as an objective measure for the noise level and for comparing methods on their shape denoising capabilities. finally, the paper evaluates seven methods capable of accomplishing this task, of which six are based on deep learning, including some generative models.",,2024-02-23,,"['cheng long', 'adrian barbu']"
2402.15602,minimax optimality of score-based diffusion models: beyond the density   lower bound assumptions,math.st cs.lg stat.ml stat.th,"we study the asymptotic error of score-based diffusion model sampling in large-sample scenarios from a non-parametric statistics perspective. we show that a kernel-based score estimator achieves an optimal mean square error of $\widetilde{o}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$ for the score function of $p_0*\mathcal{n}(0,t\boldsymbol{i}_d)$, where $n$ and $d$ represent the sample size and the dimension, $t$ is bounded above and below by polynomials of $n$, and $p_0$ is an arbitrary sub-gaussian distribution. as a consequence, this yields an $\widetilde{o}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of the distribution of the sample generated by the diffusion model under a mere sub-gaussian assumption. if in addition, $p_0$ belongs to the nonparametric family of the $\beta$-sobolev space with $\beta\le 2$, by adopting an early stopping strategy, we obtain that the diffusion model is nearly (up to log factors) minimax optimal. this removes the crucial lower bound assumption on $p_0$ in previous proofs of the minimax optimality of the diffusion model for nonparametric families.",,2024-02-23,,"['kaihong zhang', 'heqi yin', 'feng liang', 'jingbo liu']"
2402.15603,differentially private fair binary classifications,cs.lg cs.cr cs.it math.it stat.ml,"in this work, we investigate binary classification under the constraints of both differential privacy and fairness. we first propose an algorithm based on the decoupling technique for learning a classifier with only fairness guarantee. this algorithm takes in classifiers trained on different demographic groups and generates a single classifier satisfying statistical parity. we then refine this algorithm to incorporate differential privacy. the performance of the final algorithm is rigorously examined in terms of privacy, fairness, and utility guarantees. empirical evaluations conducted on the adult and credit card datasets illustrate that our algorithm outperforms the state-of-the-art in terms of fairness guarantees, while maintaining the same level of privacy and utility.",,2024-02-23,,"['hrad ghoukasian', 'shahab asoodeh']"
2402.15625,learning cyclic causal models from incomplete data,stat.ml cs.ai cs.lg,"causal learning is a fundamental problem in statistics and science, offering insights into predicting the effects of unseen treatments on a system. despite recent advances in this topic, most existing causal discovery algorithms operate under two key assumptions: (i) the underlying graph is acyclic, and (ii) the available data is complete. these assumptions can be problematic as many real-world systems contain feedback loops (e.g., biological systems), and practical scenarios frequently involve missing data. in this work, we propose a novel framework, named missnodags, for learning cyclic causal graphs from partially missing data. under the additive noise model, missnodags learns the causal graph by alternating between imputing the missing data and maximizing the expected log-likelihood of the visible part of the data in each training step, following the principles of the expectation-maximization (em) framework. through synthetic experiments and real-world single-cell perturbation data, we demonstrate improved performance when compared to using state-of-the-art imputation techniques followed by causal learning on partially missing interventional data.",,2024-02-23,,"['muralikrishnna g. sethuraman', 'faramarz fekri']"
2402.15635,bagged deep image prior for recovering images in the presence of speckle   noise,cs.it cs.cv cs.lg eess.iv math.it stat.ap stat.ml,"we investigate both the theoretical and algorithmic aspects of likelihood-based methods for recovering a complex-valued signal from multiple sets of measurements, referred to as looks, affected by speckle (multiplicative) noise. our theoretical contributions include establishing the first existing theoretical upper bound on the mean squared error (mse) of the maximum likelihood estimator under the deep image prior hypothesis. our theoretical results capture the dependence of mse upon the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. on the algorithmic side, we introduce the concept of bagged deep image priors (bagged-dip) and integrate them with projected gradient descent. furthermore, we show how employing newton-schulz algorithm for calculating matrix inverses within the iterations of pgd reduces the computational complexity of the algorithm. we will show that this method achieves the state-of-the-art performance.",,2024-02-23,,"['xi chen', 'zhewen hou', 'christopher a. metzler', 'arian maleki', 'shirin jalali']"
2402.15691,orthogonal gradient boosting for simpler additive rule ensembles,cs.lg stat.ml,"gradient boosting of prediction rules is an efficient approach to learn potentially interpretable yet accurate probabilistic models. however, actual interpretability requires to limit the number and size of the generated rules, and existing boosting variants are not designed for this purpose. though corrective boosting refits all rule weights in each iteration to minimise prediction risk, the included rule conditions tend to be sub-optimal, because commonly used objective functions fail to anticipate this refitting. here, we address this issue by a new objective function that measures the angle between the risk gradient vector and the projection of the condition output vector onto the orthogonal complement of the already selected conditions. this approach correctly approximate the ideal update of adding the risk gradient itself to the model and favours the inclusion of more general and thus shorter rules. as we demonstrate using a wide range of prediction tasks, this significantly improves the comprehensibility/accuracy trade-off of the fitted ensemble. additionally, we show how objective values for related rule conditions can be computed incrementally to avoid any substantial computational overhead of the new method.",,2024-02-23,,"['fan yang', 'pierre le bodic', 'michael kamp', 'mario boley']"
2402.15703,is offline decision making possible with only few samples? reliable   decisions in data-starved bandits via trust region enhancement,cs.lg cs.ai stat.ml,"what can an agent learn in a stochastic multi-armed bandit (mab) problem from a dataset that contains just a single sample for each arm? surprisingly, in this work, we demonstrate that even in such a data-starved setting it may still be possible to find a policy competitive with the optimal one. this paves the way to reliable decision-making in settings where critical decisions must be made by relying only on a handful of samples.   our analysis reveals that \emph{stochastic policies can be substantially better} than deterministic ones for offline decision-making. focusing on offline multi-armed bandits, we design an algorithm called trust region of uncertainty for stochastic policy enhancement (trust) which is quite different from the predominant value-based lower confidence bound approach. its design is enabled by localization laws, critical radii, and relative pessimism. we prove that its sample complexity is comparable to that of lcb on minimax problems while being substantially lower on problems with very few samples.   finally, we consider an application to offline reinforcement learning in the special case where the logging policies are known.",,2024-02-23,,"['ruiqi zhang', 'yuexiang zhai', 'andrea zanette']"
2402.15710,a statistical analysis of wasserstein autoencoders for intrinsically   low-dimensional data,cs.lg math.st stat.ml stat.th,"variational autoencoders (vaes) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. this popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. wasserstein autoencoders (waes), a variant of vaes, aim to not only improve model efficiency but also interpretability. however, there has been limited focus on analyzing their statistical guarantees. the matter is further complicated by the fact that the data distributions to which waes are applied - such as natural images - are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. to bridge the gap between the theory and practice of waes, in this paper, we show that waes can learn the data distributions when the network architectures are properly chosen. we show that the convergence rates of the expected excess risk in the number of samples for waes are independent of the high feature dimension, instead relying only on the intrinsic dimension of the data distribution.",,2024-02-23,,"['saptarshi chakraborty', 'peter l. bartlett']"
2402.15718,a duality analysis of kernel ridge regression in the noiseless regime,stat.ml cs.lg,"in this paper, we conduct a comprehensive analysis of generalization properties of kernel ridge regression (krr) in the noiseless regime, a scenario crucial to scientific computing, where data are often generated via computer simulations. we prove that krr can attain the minimax optimal rate, which depends on both the eigenvalue decay of the associated kernel and the relative smoothness of target functions. particularly, when the eigenvalue decays exponentially fast, krr achieves the spectral accuracy, i.e., a convergence rate faster than any polynomial. moreover, the numerical experiments well corroborate our theoretical findings. our proof leverages a novel extension of the duality framework introduced by chen et al. (2023), which could be useful in analyzing kernel-based methods beyond the scope of this work.",,2024-02-23,,"['jihao long', 'xiaojun peng', 'lei wu']"
2402.15734,data-efficient operator learning via unsupervised pretraining and   in-context learning,cs.lg stat.ml,"recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insight for solving scientific problems based on partial differential equations (pdes). however, being data-intensive, these methods still require a large amount of pde data. this reintroduces the need for expensive numerical pde solutions, partially undermining the original goal of avoiding these expensive simulations. in this work, seeking data efficiency, we design unsupervised pretraining and in-context learning methods for pde operator learning. to reduce the need for training data with simulated solutions, we pretrain neural operators on unlabeled pde data using reconstruction-based proxy tasks. to improve out-of-distribution performance, we further assist neural operators in flexibly leveraging in-context learning methods, without incurring extra training costs or designs. extensive empirical evaluations on a diverse set of pdes demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models.",,2024-02-24,,"['wuyang chen', 'jialin song', 'pu ren', 'shashank subramanian', 'dmitriy morozov', 'michael w. mahoney']"
2402.15739,low-rank bandits via tight two-to-infinity singular subspace recovery,cs.lg stat.ml,"we study contextual bandits with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\in [m]\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. for such bandits, we present efficient algorithms for policy evaluation, best policy identification and regret minimization. for policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. for instance, the number of samples required to return an $\varepsilon$-optimal policy with probability at least $1-\delta$ typically scales as ${m+n\over \varepsilon^2}\log(1/\delta)$. our regret minimization algorithm enjoys minimax guarantees scaling as $r^{7/4}(m+n)^{3/4}\sqrt{t}$, which improves over existing algorithms. all the proposed algorithms consist of two phases: they first leverage spectral methods to estimate the left and right singular subspaces of the low-rank reward matrix. we show that these estimates enjoy tight error guarantees in the two-to-infinity norm. this in turn allows us to reformulate our problems as a misspecified linear bandit problem with dimension roughly $r(m+n)$ and misspecification controlled by the subspace recovery error, as well as to design the second phase of our algorithms efficiently.",,2024-02-24,,"['yassir jedra', 'william réveillard', 'stefan stojanovic', 'alexandre proutiere']"
2402.15757,batch active learning of reward functions from human preferences,cs.lg cs.ai cs.ro stat.ml,"data generation and labeling are often expensive in robot learning. preference-based learning is a concept that enables reliable labeling by querying users with preference questions. active querying methods are commonly employed in preference-based learning to generate more informative data at the expense of parallelization and computation time. in this paper, we develop a set of novel algorithms, batch active preference-based learning methods, that enable efficient learning of reward functions using as few data samples as possible while still having short query generation times and also retaining parallelizability. we introduce a method based on determinantal point processes (dpp) for active batch generation and several heuristic-based alternatives. finally, we present our experimental results for a variety of robotics tasks in simulation. our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. we showcase one of our algorithms in a study to learn human users' preferences.",,2024-02-24,,"['erdem bıyık', 'nima anari', 'dorsa sadigh']"
2402.15892,statistical games,math.st cs.ai cs.gt cs.lg econ.th stat.ml stat.th,"this work contains the mathematical exploration of a few prototypical games in which central concepts from statistics and probability theory naturally emerge. the first two kinds of games are termed fisher and bayesian games, which are connected to frequentist and bayesian statistics, respectively. later, a more general type of game is introduced, termed statistical game, in which a further parameter, the players' relative risk aversion, can be set. in this work, we show that fisher and bayesian games can be viewed as limiting cases of statistical games. therefore, statistical games can be viewed as a unified framework, incorporating both frequentist and bayesian statistics. furthermore, a philosophical framework is (re-)presented -- often referred to as minimax regret criterion -- as a general approach to decision making.   the main motivation for this work was to embed bayesian statistics into a broader decision-making framework, where, based on collected data, actions with consequences have to be made, which can be translated to utilities (or rewards/losses) of the decision-maker. the work starts with the simplest possible toy model, related to hypothesis testing and statistical inference. this choice has two main benefits: i.) it allows us to determine (conjecture) the behaviour of the equilibrium strategies in various limiting cases ii.) this way, we can introduce statistical games without requiring additional stochastic parameters. the work contains game theoretical methods related to two-player, non-cooperative games to determine and prove equilibrium strategies of fisher, bayesian and statistical games. it also relies on analytical tools for derivations concerning various limiting cases.",,2024-02-24,,['jozsef konczer']
2402.15926,large stepsize gradient descent for logistic loss: non-monotonicity of   the loss improves optimization efficiency,cs.lg stat.ml,"we consider gradient descent (gd) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\eta$ is so large that the loss initially oscillates. we show that gd exits this initial oscillatory phase rapidly -- in $\mathcal{o}(\eta)$ steps -- and subsequently achieves an $\tilde{\mathcal{o}}(1 / (\eta t) )$ convergence rate after $t$ additional steps. our results imply that, given a budget of $t$ steps, gd can achieve an accelerated loss of $\tilde{\mathcal{o}}(1/t^2)$ with an aggressive stepsize $\eta:= \theta( t)$, without any use of momentum or variable stepsize schedulers. our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\tilde{\mathcal{o}}(1/t^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (sgd) with a large stepsize, under suitable separability conditions.",,2024-02-24,,"['jingfeng wu', 'peter l. bartlett', 'matus telgarsky', 'bin yu']"
2402.15978,shaving weights with occam's razor: bayesian sparsification for neural   networks using the marginal likelihood,cs.lg stat.ml,"neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful ai models are becoming too large to na\""ively deploy on consumer hardware. while much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. we present sparsifiability via the marginal likelihood (spam), a pruning framework that highlights the effectiveness of using the bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. our approach implements an automatic occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. in addition, we demonstrate that the pre-computed posterior hessian approximation used in the laplace approximation can be re-used to define a cheap pruning criterion, which outperforms many existing (more expensive) approaches. we demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets.",,2024-02-24,,"['rayen dhahri', 'alexander immer', 'betrand charpentier', 'stephan günnemann', 'vincent fortuin']"
2402.15984,a unified fourier slice method to derive ridgelet transform for a   variety of depth-2 neural networks,cs.lg math.fa stat.ml,"to investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron. the ridgelet transform is a pseudo-inverse operator that maps a given function $f$ to the parameter distribution $\gamma$ so that a network $\mathtt{nn}[\gamma]$ reproduces $f$, i.e. $\mathtt{nn}[\gamma]=f$. for depth-2 fully-connected networks on a euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed. however, for a variety of modern neural network architectures, the closed-form expression has not been known. in this paper, we explain a systematic method using fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields $\mathbb{f}_p$, group convolutional networks on abstract hilbert space $\mathcal{h}$, fully-connected networks on noncompact symmetric spaces $g/k$, and pooling layers, or the $d$-plane ridgelet transform.",,2024-02-24,,"['sho sonoda', 'isao ishikawa', 'masahiro ikeda']"
2402.15995,improved hardness results for learning intersections of halfspaces,cs.cc cs.lg math.st stat.ml stat.th,"we show strong (and surprisingly simple) lower bounds for weakly learning intersections of halfspaces in the improper setting. strikingly little is known about this problem. for instance, it is not even known if there is a polynomial-time algorithm for learning the intersection of only two halfspaces. on the other hand, lower bounds based on well-established assumptions (such as approximating worst-case lattice problems or variants of feige's 3sat hypothesis) are only known (or are implied by existing results) for the intersection of super-logarithmically many halfspaces [ks09,ks06,dss16]. with intersections of fewer halfspaces being only ruled out under less standard assumptions [dv21] (such as the existence of local pseudo-random generators with large stretch). we significantly narrow this gap by showing that even learning $\omega(\log \log n)$ halfspaces in dimension $n$ takes super-polynomial time under standard assumptions on worst-case lattice problems (namely that svp and sivp are hard to approximate within polynomial factors). further, we give unconditional hardness results in the statistical query framework. specifically, we show that for any $k$ (even constant), learning $k$ halfspaces in dimension $n$ requires accuracy $n^{-\omega(k)}$, or exponentially many queries -- in particular ruling out sq algorithms with polynomial accuracy for $\omega(1)$ halfspaces. to the best of our knowledge this is the first unconditional hardness result for learning a super-constant number of halfspaces.   our lower bounds are obtained in a unified way via a novel connection we make between intersections of halfspaces and the so-called parallel pancakes distribution [dks17,blpr19,brst21] that has been at the heart of many lower bound constructions in (robust) high-dimensional statistics in the past few years.",,2024-02-25,,['stefan tiegel']
2402.16059,gradient-enhanced deep gaussian processes for multifidelity modelling,stat.ml cs.lg,"multifidelity models integrate data from multiple sources to produce a single approximator for the underlying process. dense low-fidelity samples are used to reduce interpolation error, while sparse high-fidelity samples are used to compensate for bias or noise in the low-fidelity samples. deep gaussian processes (gps) are attractive for multifidelity modelling as they are non-parametric, robust to overfitting, perform well for small datasets, and, critically, can capture nonlinear and input-dependent relationships between data of different fidelities. many datasets naturally contain gradient data, especially when they are generated by computational models that are compatible with automatic differentiation or have adjoint solutions. principally, this work extends deep gps to incorporate gradient data. we demonstrate this method on an analytical test problem and a realistic partial differential equation problem, where we predict the aerodynamic coefficients of a hypersonic flight vehicle over a range of flight conditions and geometries. in both examples, the gradient-enhanced deep gp outperforms a gradient-enhanced linear gp model and their non-gradient-enhanced counterparts.",,2024-02-25,,"['viv bone', 'chris van der heide', 'kieran mackle', 'ingo h. j. jahn', 'peter m. dower', 'chris manzie']"
2402.16126,a statistical method for crack detection in 3d concrete images,cs.cv stat.ap,"in practical applications, effectively segmenting cracks in large-scale computed tomography (ct) images holds significant importance for understanding the structural integrity of materials. however, classical methods and machine learning algorithms often incur high computational costs when dealing with the substantial size of input images. hence, a robust algorithm is needed to pre-detect crack regions, enabling focused analysis and reducing computational overhead. the proposed approach addresses this challenge by offering a streamlined method for identifying crack regions in ct images with high probability. by efficiently identifying areas of interest, our algorithm allows for a more focused examination of potential anomalies within the material structure. through comprehensive testing on both semi-synthetic and real 3d ct images, we validate the efficiency of our approach in enhancing crack segmentation while reducing computational resource requirements.",,2024-02-25,,"['vitalii makogin', 'duc nguyen', 'evgeny spodarev']"
2402.16131,a vae-based framework for learning multi-level neural granger-causal   connectivity,cs.lg stat.me stat.ml,"granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. in certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. this paper introduces a variational autoencoder (vae) based framework that jointly learns granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. the performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual system learning. the method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results.",,2024-02-25,,"['jiahe lin', 'huitian lei', 'george michailidis']"
2402.16158,distribution-free fair federated learning with small samples,stat.ml cs.cy cs.lg,"as federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important. however, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees. to address this issue, this paper introduces fedfairee, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples. our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes. we provide rigorous theoretical guarantees for both fairness and accuracy, and our experimental results further provide robust empirical validation for our proposed method.",,2024-02-25,,"['qichuan yin', 'junzhou huang', 'huaxiu yao', 'linjun zhang']"
2402.16300,conformalized selective regression,cs.lg stat.ml,"should prediction models always deliver a prediction? in the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty. selective regression, also known as the ""reject option,"" allows models to abstain from predictions in cases of considerable uncertainty. initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. however, this focus neglects the significant influence of model-specific biases on a model's performance. in this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. in addition, we propose a standardized evaluation framework to allow proper comparison of selective regression approaches. via an extensive experimental approach, we demonstrate how our proposed approach, conformalized selective regression, demonstrates an advantage over multiple state-of-the-art baselines.",,2024-02-25,,"['anna sokol', 'nuno moniz', 'nitesh chawla']"
2402.16359,feedback efficient online fine-tuning of diffusion models,cs.lg cs.ai q-bio.qm stat.ml,"diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. however, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. it is natural to frame this as a reinforcement learning (rl) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). in this work, we propose a novel reinforcement learning procedure that efficiently explores on the manifold of feasible samples. we present a theoretical analysis providing a regret guarantee, as well as empirical validation across three domains: images, biological sequences, and molecules.",,2024-02-26,2024-02-27,"['masatoshi uehara', 'yulai zhao', 'kevin black', 'ehsan hajiramezanali', 'gabriele scalia', 'nathaniel lee diamant', 'alex m tseng', 'sergey levine', 'tommaso biancalani']"
2402.16383,self supervised correlation-based permutations for multi-view clustering,cs.lg stat.ml,"fusing information from different modalities can enhance data analysis tasks, including clustering. however, existing multi-view clustering (mvc) solutions are limited to specific domains or rely on a suboptimal and computationally demanding two-stage procedure of representation and clustering. we propose an end-to-end deep learning-based mvc framework for general data (image, tabular, etc.). our approach involves learning meaningful fused data representations with a novel permutation-based canonical correlation objective. concurrently, we learn cluster assignments by identifying consistent pseudo-labels across multiple views. we demonstrate the effectiveness of our model using ten mvc benchmark datasets. theoretically, we show that our model approximates the supervised linear discrimination analysis (lda) representation. additionally, we provide an error bound induced by false-pseudo label annotations.",,2024-02-26,,"['ran eisenberg', 'jonathan svirsky', 'ofir lindenbaum']"
2402.16408,stable training of normalizing flows for high-dimensional variational   inference,stat.ml cs.lg,"variational inference with normalizing flows (nfs) is an increasingly popular alternative to mcmc methods. in particular, nfs based on coupling layers (real nvps) are frequently used due to their good empirical performance. in theory, increasing the depth of normalizing flows should lead to more accurate posterior approximations. however, in practice, training deep normalizing flows for approximating high-dimensional posterior distributions is often infeasible due to the high variance of the stochastic gradients. in this work, we show that previous methods for stabilizing the variance of stochastic gradient descent can be insufficient to achieve stable training of real nvps. as the source of the problem, we identify that, during training, samples often exhibit unusual high values. as a remedy, we propose a combination of two methods: (1) soft-thresholding of the scale in real nvps, and (2) a bijective soft log transformation of the samples. we evaluate these and other previously proposed modification on several challenging target distributions, including a high-dimensional horseshoe logistic regression model. our experiments show that with our modifications, stable training of real nvps for posteriors with several thousand dimensions is possible, allowing for more accurate marginal likelihood estimation via importance sampling. moreover, we evaluate several common training techniques and architecture choices and provide practical advise for training nfs for high-dimensional variational inference.",,2024-02-26,,['daniel andrade']
2402.16435,training implicit generative models via an invariant statistical loss,cs.lg cs.ai math.st stat.ml stat.th,"implicit generative models have the capability to learn arbitrary complex data distributions. on the downside, training requires telling apart real data from artificially-generated ones using adversarial discriminators, leading to unstable training and mode-dropping issues. as reported by zahee et al. (2017), even in the one-dimensional (1d) case, training a generative adversarial network (gan) is challenging and often suboptimal. in this work, we develop a discriminator-free method for training one-dimensional (1d) generative implicit models and subsequently expand this method to accommodate multivariate cases. our loss function is a discrepancy measure between a suitably chosen transformation of the model samples and a uniform distribution; hence, it is invariant with respect to the true distribution of the data. we first formulate our method for 1d random variables, providing an effective solution for approximate reparameterization of arbitrary complex distributions. then, we consider the temporal setting (both univariate and multivariate), in which we model the conditional distribution of each sample given the history of the process. we demonstrate through numerical simulations that this new method yields promising results, successfully learning true distributions in a variety of scenarios and mitigating some of the well-known problems that state-of-the-art implicit methods present.",,2024-02-26,,"['josé manuel de frutos', 'pablo m. olmos', 'manuel a. vázquez', 'joaquín míguez']"
2402.16481,a kernel-based analysis of laplacian eigenmaps,math.st math.pr math.sp stat.ml stat.th,"given i.i.d. observations uniformly distributed on a closed manifold $\mathcal{m}\subseteq \mathbb{r}^p$, we study the spectral properties of the associated empirical graph laplacian based on a gaussian kernel. our main results are non-asymptotic error bounds, showing that the eigenvalues and eigenspaces of the empirical graph laplacian are close to the eigenvalues and eigenspaces of the laplace-beltrami operator of $\mathcal{m}$. in our analysis, we connect the empirical graph laplacian to kernel principal component analysis, and consider the heat kernel of $\mathcal{m}$ as reproducing kernel feature map. this leads to novel points of view and allows to leverage results for empirical covariance operators in infinite dimensions.",,2024-02-26,,['martin wahl']
2402.16565,partial rankings of optimizers,cs.lg stat.ml,"we introduce a framework for benchmarking optimizers according to multiple criteria over various test functions. based on a recently introduced union-free generic depth function for partial orders/rankings, it fully exploits the ordinal information and allows for incomparability. our method describes the distribution of all partial orders/rankings, avoiding the notorious shortcomings of aggregation. this permits to identify test functions that produce central or outlying rankings of optimizers and to assess the quality of benchmarking suites.",,2024-02-26,,"['julian rodemann', 'hannah blocher']"
2402.16639,differentiable particle filtering using optimal placement resampling,cs.lg stat.co,"particle filters are a frequent choice for inference tasks in nonlinear and non-gaussian state-space models. they can either be used for state inference by approximating the filtering distribution or for parameter inference by approximating the marginal data (observation) likelihood. a good proposal distribution and a good resampling scheme are crucial to obtain low variance estimates. however, traditional methods like multinomial resampling introduce nondifferentiability in pf-based loss functions for parameter estimation, prohibiting gradient-based learning tasks. this work proposes a differentiable resampling scheme by deterministic sampling from an empirical cumulative distribution function. we evaluate our method on parameter inference tasks and proposal learning.",,2024-02-26,,"['domonkos csuzdi', 'olivér törő', 'tamás bécsi']"
2402.16661,penalized generative variable selection,stat.ml cs.lg stat.me,"deep networks are increasingly applied to a wide variety of data, including data with high-dimensional predictors. in such analysis, variable selection can be needed along with estimation/model building. many of the existing deep network studies that incorporate variable selection have been limited to methodological and numerical developments. in this study, we consider modeling/estimation using the conditional wasserstein generative adversarial networks. group lasso penalization is applied for variable selection, which may improve model estimation/prediction, interpretability, stability, etc. significantly advancing from the existing literature, the analysis of censored survival data is also considered. we establish the convergence rate for variable selection while considering the approximation error, and obtain a more efficient distribution estimation. simulations and the analysis of real experimental data demonstrate satisfactory practical utility of the proposed analysis.",,2024-02-26,,"['tong wang', 'jian huang', 'shuangge ma']"
2402.16683,re-envisioning numerical information field theory (nifty.re): a library   for gaussian processes and variational inference,astro-ph.im cs.lg stat.ml,"imaging is the process of transforming noisy, incomplete data into a space that humans can interpret. nifty is a bayesian framework for imaging and has already successfully been applied to many fields in astrophysics. previous design decisions held the performance and the development of methods in nifty back. we present a rewrite of nifty, coined nifty.re, which reworks the modeling principle, extends the inference strategies, and outsources much of the heavy lifting to jax. the rewrite dramatically accelerates models written in nifty, lays the foundation for new types of inference machineries, improves maintainability, and enables interoperability between nifty and the jax machine learning ecosystem.",,2024-02-26,,"['gordian edenhofer', 'philipp frank', 'jakob roth', 'reimar h. leike', 'massin guerdi', 'lukas i. scheel-platz', 'matteo guardiani', 'vincent eberle', 'margret westerkamp', 'torsten a. enßlin']"
2402.16688,on the connection between noise-contrastive estimation and contrastive   divergence,stat.ml cs.lg,"noise-contrastive estimation (nce) is a popular method for estimating unnormalised probabilistic models, such as energy-based models, which are effective for modelling complex data distributions. unlike classical maximum likelihood (ml) estimation that relies on importance sampling (resulting in ml-is) or mcmc (resulting in contrastive divergence, cd), nce uses a proxy criterion to avoid the need for evaluating an often intractable normalisation constant.   despite apparent conceptual differences, we show that two nce criteria, ranking nce (rnce) and conditional nce (cnce), can be viewed as ml estimation methods. specifically, rnce is equivalent to ml estimation combined with conditional importance sampling, and both rnce and cnce are special cases of cd. these findings bridge the gap between the two method classes and allow us to apply techniques from the ml-is and cd literature to nce, offering several advantageous extensions.",,2024-02-26,,"['amanda olmin', 'jakob lindqvist', 'lennart svensson', 'fredrik lindsten']"
2402.16710,cost aware best arm identification,cs.lg stat.ml,"in this paper, we study a best arm identification problem with dual objects. in addition to the classic reward, each arm is associated with a cost distribution and the goal is to identify the largest reward arm using the minimum expected cost. we call it \emph{cost aware best arm identification} (cabai), which captures the separation of testing and implementation phases in product development pipelines and models the objective shift between phases, i.e., cost for testing and reward for implementation. we first derive an theoretic lower bound for cabai and propose an algorithm called $\mathsf{ctas}$ to match it asymptotically. to reduce the computation of $\mathsf{ctas}$, we further propose a low-complexity algorithm called co, based on a square-root rule, which proves optimal in simplified two-armed models and generalizes surprisingly well in numerical experiments. our results show (i) ignoring the heterogeneous action cost results in sub-optimality in practice, and (ii) low-complexity algorithms deliver near-optimal performance over a wide range of problems.",,2024-02-26,,"['kellen kanarios', 'qining zhang', 'lei ying']"
2402.16792,rate-optimal rank aggregation with private pairwise rankings,stat.ml cs.cr cs.lg,"in various real-world scenarios like recommender systems and political surveys, pairwise rankings are commonly collected and utilized for rank aggregation to obtain an overall ranking of items. however, preference rankings can reveal individuals' personal preferences, underscoring the need to protect them before releasing for downstream analysis. in this paper, we address the challenge of preserving privacy while ensuring the utility of rank aggregation based on pairwise rankings generated from the bradley-terry-luce (btl) model. using the randomized response mechanism to perturb raw pairwise rankings is a common privacy protection strategy used in practice, but a critical challenge arises because the privatized rankings no longer adhere to the btl model, resulting in significant bias in downstream rank aggregation tasks. motivated from this, we propose a debiased randomized response mechanism to protect the raw pairwise rankings, ensuring consistent estimation of true preferences and rankings in downstream rank aggregation. theoretically, we offer insights into the relationship between overall privacy guarantees and estimation errors from private ranking data, and establish minimax rates for estimation errors. this enables the determination of optimal privacy guarantees that balance consistency in rank aggregation with robust privacy protection. we also investigate convergence rates of expected ranking errors for partial and full ranking recovery, quantifying how privacy protection influences the specification of top-$k$ item sets and complete rankings. our findings are validated through extensive simulations and a real application.",,2024-02-26,,"['shirong xu', 'will wei sun', 'guang cheng']"
2402.16793,failures and successes of cross-validation for early-stopped gradient   descent,math.st cs.lg stat.ml stat.th,"we analyze the statistical properties of generalized cross-validation (gcv) and leave-one-out cross-validation (loocv) applied to early-stopped gradient descent (gd) in high-dimensional least squares regression. we prove that gcv is generically inconsistent as an estimator of the prediction risk of early-stopped gd, even for a well-specified linear model with isotropic features. in contrast, we show that loocv converges uniformly along the gd trajectory to the prediction risk. our theory requires only mild assumptions on the data distribution and does not require the underlying regression function to be linear. furthermore, by leveraging the individual loocv errors, we construct consistent estimators for the entire prediction error distribution along the gd trajectory and consistent estimators for a wide class of error functionals. this in particular enables the construction of pathwise prediction intervals based on gd iterates that have asymptotically correct nominal coverage conditional on the training data.",,2024-02-26,,"['pratik patil', 'yuchen wu', 'ryan j. tibshirani']"
2402.16811,stopping bayesian optimization with probabilistic regret bounds,stat.ml cs.lg,"bayesian optimization is a popular framework for efficiently finding high-quality solutions to difficult problems based on limited prior information. as a rule, these algorithms operate by iteratively choosing what to try next until some predefined budget has been exhausted. we investigate replacing this de facto stopping rule with an $(\epsilon, \delta)$-criterion: stop when a solution has been found whose value is within $\epsilon > 0$ of the optimum with probability at least $1 - \delta$ under the model. given access to the prior distribution of problems, we show how to verify this condition in practice using a limited number of draws from the posterior. for gaussian process priors, we prove that bayesian optimization with the proposed criterion stops in finite time and returns a point that satisfies the $(\epsilon, \delta)$-criterion under mild assumptions. these findings are accompanied by extensive empirical results which demonstrate the strengths and weaknesses of this approach.",,2024-02-26,,['james t. wilson']
2402.16909,impact of physical activity on quality of life during pregnancy: a   causal ml approach,cs.lg stat.me,"the concept of quality of life (qol) refers to a holistic measurement of an individual's well-being, incorporating psychological and social aspects. pregnant women, especially those with obesity and stress, often experience lower qol. physical activity (pa) has shown the potential to enhance the qol. however, pregnant women who are overweight and obese rarely meet the recommended level of pa. studies have investigated the relationship between pa and qol during pregnancy using correlation-based approaches. these methods aim to discover spurious correlations between variables rather than causal relationships. besides, the existing methods mainly rely on physical activity parameters and neglect the use of different factors such as maternal (medical) history and context data, leading to biased estimates. furthermore, the estimations lack an understanding of mediators and counterfactual scenarios that might affect them. in this paper, we investigate the causal relationship between being physically active (treatment variable) and the qol (outcome) during pregnancy and postpartum. to estimate the causal effect, we develop a causal machine learning method, integrating causal discovery and causal inference components. the data for our investigation is derived from a long-term wearable-based health monitoring study focusing on overweight and obese pregnant women. the machine learning (meta-learner) estimation technique is used to estimate the causal effect. our result shows that performing adequate physical activity during pregnancy and postpartum improves the qol by units of 7.3 and 3.4 on average in physical health and psychological domains, respectively. in the final step, four refutation analysis techniques are employed to validate our estimation.",,2024-02-25,,"['kianoosh kazemi', 'iina ryhtä', 'iman azimi', 'hannakaisa niela-vilen', 'anna axelin', 'amir m. rahmani', 'pasi liljeberg']"
2402.16926,on the (in)feasibility of ml backdoor detection as an hypothesis testing   problem,cs.cr cs.ai cs.lg stat.ml,"we introduce a formal statistical definition for the problem of backdoor detection in machine learning systems and use it to analyze the feasibility of such problems, providing evidence for the utility and applicability of our definition. the main contributions of this work are an impossibility result and an achievability result for backdoor detection. we show a no-free-lunch theorem, proving that universal (adversary-unaware) backdoor detection is impossible, except for very small alphabet sizes. thus, we argue, that backdoor detection methods need to be either explicitly, or implicitly adversary-aware. however, our work does not imply that backdoor detection cannot work in specific scenarios, as evidenced by successful backdoor detection methods in the scientific literature. furthermore, we connect our definition to the probably approximately correct (pac) learnability of the out-of-distribution detection problem.",,2024-02-26,,"['georg pichler', 'marco romanelli', 'divya prakash manivannan', 'prashanth krishnamurthy', 'farshad khorrami', 'siddharth garg']"
2402.17036,iterated inla for state and parameter estimation in nonlinear dynamical   systems,stat.ml cs.lg,"data assimilation (da) methods use priors arising from differential equations to robustly interpolate and extrapolate data. popular techniques such as ensemble methods that handle high-dimensional, nonlinear pde priors focus mostly on state estimation, however can have difficulty learning the parameters accurately. on the other hand, machine learning based approaches can naturally learn the state and parameters, but their applicability can be limited, or produce uncertainties that are hard to interpret. inspired by the integrated nested laplace approximation (inla) method in spatial statistics, we propose an alternative approach to da based on iteratively linearising the dynamical model. this produces a gaussian markov random field at each iteration, enabling one to use inla to infer the state and parameters. our approach can be used for arbitrary nonlinear systems, while retaining interpretability, and is furthermore demonstrated to outperform existing methods on the da task. by providing a more nuanced approach to handling nonlinear pde priors, our methodology offers improved accuracy and robustness in predictions, especially where data sparsity is prevalent.",,2024-02-26,,"['rafael anderka', 'marc peter deisenroth', 'so takao']"
2402.17067,on independent samples along the langevin diffusion and the unadjusted   langevin algorithm,math.st cs.it math.it stat.ml stat.th,"we study the rate at which the initial and current random variables become independent along a markov chain, focusing on the langevin diffusion in continuous time and the unadjusted langevin algorithm (ula) in discrete time. we measure the dependence between random variables via their mutual information. for the langevin diffusion, we show the mutual information converges to $0$ exponentially fast when the target is strongly log-concave, and at a polynomial rate when the target is weakly log-concave. these rates are analogous to the mixing time of the langevin diffusion under similar assumptions. for the ula, we show the mutual information converges to $0$ exponentially fast when the target is strongly log-concave and smooth. we prove our results by developing the mutual version of the mixing time analyses of these markov chains. we also provide alternative proofs based on strong data processing inequalities for the langevin diffusion and the ula, and by showing regularity results for these processes in mutual information.",,2024-02-26,,"['jiaming liang', 'siddharth mitra', 'andre wibisono']"
2402.17087,a note on bayesian networks with latent root variables,stat.ml cs.ai cs.lg,"we characterise the likelihood function computed from a bayesian network with latent variables as root nodes. we show that the marginal distribution over the remaining, manifest, variables also factorises as a bayesian network, which we call empirical. a dataset of observations of the manifest variables allows us to quantify the parameters of the empirical bayesian net. we prove that (i) the likelihood of such a dataset from the original bayesian network is dominated by the global maximum of the likelihood from the empirical one; and that (ii) such a maximum is attained if and only if the parameters of the bayesian network are consistent with those of the empirical model.",,2024-02-26,,"['marco zaffalon', 'alessandro antonucci']"
2402.17089,learning high-dimensional targets by two-parameter models and gradient   flow,stat.ml cs.lg,"we explore the theoretical possibility of learning $d$-dimensional targets with $w$-parameter models by gradient flow (gf) when $w<d$. our main result shows that if the targets are described by a particular $d$-dimensional probability distribution, then there exist models with as few as two parameters that can learn the targets with arbitrarily high success probability. on the other hand, we show that for $w<d$ there is necessarily a large subset of gf-non-learnable targets. in particular, the set of learnable targets is not dense in $\mathbb r^d$, and any subset of $\mathbb r^d$ homeomorphic to the $w$-dimensional sphere contains non-learnable targets. finally, we observe that the model in our main theorem on almost guaranteed two-parameter learning is constructed using a hierarchical procedure and as a result is not expressible by a single elementary function. we show that this limitation is essential in the sense that such learnability can be ruled out for a large class of elementary functions.",,2024-02-26,,['dmitry yarotsky']
2402.17104,adversarial perturbations of physical signals,cs.lg cs.cr eess.sp math.oc stat.ml,"we investigate the vulnerability of computer-vision-based signal classifiers to adversarial perturbations of their inputs, where the signals and perturbations are subject to physical constraints. we consider a scenario in which a source and interferer emit signals that propagate as waves to a detector, which attempts to classify the source by analyzing the spectrogram of the signal it receives using a pre-trained neural network. by solving pde-constrained optimization problems, we construct interfering signals that cause the detector to misclassify the source even though the perturbations to the spectrogram of the received signal are nearly imperceptible. though such problems can have millions of decision variables, we introduce methods to solve them efficiently. our experiments demonstrate that one can compute effective and physically realizable adversarial perturbations for a variety of machine learning models under various physical conditions.",,2024-02-26,,"['robert l. bassett', 'austin van dellen', 'anthony p. austin']"
2402.17106,dataset fairness: achievable fairness on your data with utility   guarantees,stat.ml cs.cy cs.lg,"in machine learning fairness, training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. the severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases. therefore using a uniform fairness requirement across datasets remains questionable and can often lead to models with substantially low utility. to address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. by utilizing the you-only-train-once (yoto) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offering a statistically grounded perspective on the acceptable range of fairness violations for any given accuracy threshold. our empirical evaluation spanning tabular, image and language datasets underscores that our approach provides practitioners with a principled framework for dataset-specific fairness decisions across various data modalities.",,2024-02-26,,"['muhammad faaiz taufiq', 'jean-francois ton', 'yang liu']"
2402.17157,generative learning for forecasting the dynamics of complex systems,cs.lg physics.comp-ph physics.flu-dyn stat.ml,"we introduce generative models for accelerating simulations of complex systems through learning and evolving their effective dynamics. in the proposed generative learning of effective dynamics (g-led), instances of high dimensional data are down sampled to a lower dimensional manifold that is evolved through an auto-regressive attention mechanism. in turn, bayesian diffusion models, that map this low-dimensional manifold onto its corresponding high-dimensional space, capture the statistics of the system dynamics. we demonstrate the capabilities and drawbacks of g-led in simulations of several benchmark systems, including the kuramoto-sivashinsky (ks) equation, two-dimensional high reynolds number flow over a backward-facing step, and simulations of three-dimensional turbulent channel flow. the results demonstrate that generative learning offers new frontiers for the accurate forecasting of the statistical properties of complex systems at a reduced computational cost.",,2024-02-26,,"['han gao', 'sebastian kaltenbach', 'petros koumoutsakos']"
2402.17233,hybrid square neural ode causal modeling,cs.lg stat.ap stat.me,"hybrid models combine mechanistic ode-based dynamics with flexible and expressive neural network components. such models have grown rapidly in popularity, especially in scientific domains where such ode-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). the incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. unfortunately, as hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. we address this problem by leveraging another common source of domain knowledge: ranking of treatment effects for a set of interventions, even if the precise treatment effect is unknown. we encode this information in a causal loss that we combine with the standard predictive loss to arrive at a hybrid loss that biases our learning towards causally valid hybrid models. we demonstrate our ability to achieve a win-win -- state-of-the-art predictive performance and causal validity -- in the challenging task of modeling glucose dynamics during exercise.",,2024-02-27,,"['bob junyi zou', 'matthew e. levine', 'dessi p. zaharieva', 'ramesh johari', 'emily b. fox']"
2402.17258,stochastic approximation in infinite dimensions,math.st math.fa math.pr stat.ml stat.th,"stochastic approximation (sa) was introduced in the early 1950's and has been an active area of research for several decades. while the initial focus was on statistical questions, it was seen to have applications to signal processing, convex optimisation. %over the last decade, there has been a revival of interest in sa as in later years sa has found application in reinforced learning (rl) and led to revival of interest.   while bulk of the literature is on sa for the case when the observations are from a finite dimensional euclidian space, there has been interest in extending the same to infinite dimension. extension to hilbert spaces is relatively easier to do, but this is not so when we come to a banach space - since in the case of a banach space, even {\em law of large numbers} is not true in general. we consider some cases where approximation works in a banach space. our framework includes case when the banach space $\bb$ is $\cb([0,1],\r^d)$, as well as $\l^1([0,1],\r^d)$, the two cases which do not even have the radon-nikodym property.",,2024-02-27,,"['rajeeva laxman karandikar', 'bhamidi v rao']"
2402.17287,an interpretable evaluation of entropy-based novelty of generative   models,cs.lg cs.cv stat.ml,"the massive developments of generative model frameworks and architectures require principled methods for the evaluation of a model's novelty compared to a reference dataset or baseline generative models. while the recent literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a baseline model has not been adequately studied in the machine learning community. in this work, we focus on the novelty assessment under multi-modal generative models and attempt to answer the following question: given the samples of a generative model $\mathcal{g}$ and a reference dataset $\mathcal{s}$, how can we discover and count the modes expressed by $\mathcal{g}$ more frequently than in $\mathcal{s}$. we introduce a spectral approach to the described task and propose the kernel-based entropic novelty (ken) score to quantify the mode-based novelty of distribution $p_\mathcal{g}$ with respect to distribution $p_\mathcal{s}$. we analytically interpret the behavior of the ken score under mixture distributions with sub-gaussian components. next, we develop a method based on cholesky decomposition to compute the ken score from observed samples. we support the ken-based quantification of novelty by presenting several numerical results on synthetic and real image distributions. our numerical results indicate the success of the proposed approach in detecting the novel modes and the comparison of state-of-the-art generative models.",,2024-02-27,,"['jingwei zhang', 'cheuk ting li', 'farzan farnia']"
2402.17343,enhanced bayesian optimization via preferential modeling of abstract   properties,cs.lg stat.ml,"experimental (design) optimization is a key driver in designing and discovering new products and processes. bayesian optimization (bo) is an effective tool for optimizing expensive and black-box experimental design processes. while bayesian optimization is a principled data-driven approach to experimental optimization, it learns everything from scratch and could greatly benefit from the expertise of its human (domain) experts who often reason about systems at different abstraction levels using physical properties that are not necessarily directly measured (or measurable). in this paper, we propose a human-ai collaborative bayesian framework to incorporate expert preferences about unmeasured abstract properties into the surrogate modeling to further boost the performance of bo. we provide an efficient strategy that can also handle any incorrect/misleading expert bias in preferential judgments. we discuss the convergence behavior of our proposed framework. our experimental results involving synthetic functions and real-world datasets show the superiority of our method against the baselines.",,2024-02-27,,"['arun kumar a', 'alistair shilton', 'sunil gupta', 'santu rana', 'stewart greenhill', 'svetha venkatesh']"
2402.17539,the optimizing mode classification stabilization of sampled stochastic   jump systems via an improved hill-climbing algorithm based on q-learning,math.oc cs.sy eess.sy math.st stat.th,"this paper addresses the stabilization problem of stochastic jump systems (sjss) closed by a generally sampled controller. because of the controller's switching and state both sampled, it is challenging to study its stabilization. a new stabilizing method deeply depending on the mode classifications is proposed to deal with the above sampling situation, whose quantity is equal to a stirling number of the second kind. for the sake of finding the best stabilization effect among all the classifications, a convex optimization problem is developed, whose globally solution is proved to be existent and can be computed by an augmented lagrangian function. more importantly, in order to further reduce the computation complexity but retaining a better performance as much as possible, a novelly improved hill-climbing algorithm is established by applying the q-learning technique to provide an optimal attenuation coefficient. a numerical example is offered so as to verify the effectiveness and superiority of the methods proposed in this study.",,2024-02-27,,['guoliang wang']
2402.17570,sparse variational contaminated noise gaussian process regression for   forecasting geomagnetic perturbations,cs.lg stat.ap stat.me,"gaussian processes (gp) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures. in this paper, we present a novel extension to the gp framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise. we propose a scalable inference algorithm based on the sparse variational gaussian process (svgp) method for fitting sparse gaussian process regression models with contaminated normal noise on large datasets. we examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks. we show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline.",,2024-02-27,2024-02-27,"['daniel iong', 'matthew mcanear', 'yuezhou qu', 'shasha zou', 'gabor toth', 'yang chen']"
2402.17595,implicit regularization via spectral neural networks and non-linear   matrix sensing,cs.lg cs.ai cs.ne stat.ml,"the phenomenon of implicit regularization has attracted interest in recent years as a fundamental aspect of the remarkable generalizing ability of neural networks. in a nutshell, it entails that gradient descent dynamics in many neural nets, even without any explicit regularizer in the loss function, converges to the solution of a regularized learning problem. however, known results attempting to theoretically explain this phenomenon focus overwhelmingly on the setting of linear neural nets, and the simplicity of the linear structure is particularly crucial to existing arguments. in this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradient descent.in this vein, we contribute a network architecture called spectral neural networks (abbrv. snn) that is particularly suitable for matrix learning problems. conceptually, this entails coordinatizing the space of matrices by their singular values and singular vectors, as opposed to by their entries, a potentially fruitful perspective for matrix learning. we demonstrate that the snn architecture is inherently much more amenable to theoretical analysis than vanilla neural nets and confirm its effectiveness in the context of matrix sensing, via both mathematical guarantees and empirical investigations. we believe that the snn architecture has the potential to be of wide applicability in a broad class of matrix learning scenarios.",,2024-02-27,,"['hong t. m. chu', 'subhro ghosh', 'chi thanh lam', 'soumendu sundar mukherjee']"
2402.17599,dagnosis: localized identification of data inconsistencies using   structures,cs.lg cs.ai stat.ml,"identification and appropriate handling of inconsistencies in data at deployment time is crucial to reliably use machine learning models. while recent data-centric methods are able to identify such inconsistencies with respect to the training set, they suffer from two key limitations: (1) suboptimality in settings where features exhibit statistical independencies, due to their usage of compressive representations and (2) lack of localization to pin-point why a sample might be flagged as inconsistent, which is important to guide future data collection. we solve these two fundamental limitations using directed acyclic graphs (dags) to encode the training set's features probability distribution and independencies as a structure. our method, called dagnosis, leverages these structural interactions to bring valuable and insightful data-centric conclusions. dagnosis unlocks the localization of the causes of inconsistencies on a dag, an aspect overlooked by previous approaches. moreover, we show empirically that leveraging these interactions (1) leads to more accurate conclusions in detecting inconsistencies, as well as (2) provides more detailed insights into why some samples are flagged.",,2024-02-26,2024-02-28,"['nicolas huynh', 'jeroen berrevoets', 'nabeel seedat', 'jonathan crabbé', 'zhaozhi qian', 'mihaela van der schaar']"
2402.17641,variational learning is effective for large deep networks,cs.lg cs.ai cs.cl math.oc stat.ml,"we give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. we show that an optimizer called improved variational online newton (ivon) consistently matches or outperforms adam for training large networks such as gpt-2 and resnets from scratch. ivon's computational costs are nearly identical to adam but its predictive uncertainty is better. we show several new use cases of ivon where we improve fine-tuning and model merging in large language models, accurately predict generalization error, and faithfully estimate sensitivity to data. we find overwhelming evidence in support of effectiveness of variational learning.",,2024-02-27,,"['yuesong shen', 'nico daheim', 'bai cong', 'peter nickl', 'gian maria marconi', 'clement bazan', 'rio yokota', 'iryna gurevych', 'daniel cremers', 'mohammad emtiyaz khan', 'thomas möllenhoff']"
2402.17699,gradient-based discrete sampling with automatic cyclical scheduling,cs.lg stat.ml,"discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities. while gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information. to tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions. our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced"" proposals for given step sizes and high efficiency of the markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning. we prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions. extensive experiments demonstrate the superiority of our method in sampling complex multimodal discrete distributions.",,2024-02-27,,"['patrick pynadath', 'riddhiman bhattacharya', 'arun hariharan', 'ruqi zhang']"
2402.17704,transfer learning bayesian optimization to design competitor dna   molecules for use in diagnostic assays,q-bio.qm cs.lg stat.ml,"with the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences. often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization. this paper presents a transfer learning design of experiments workflow to make this development feasible. by combining a transfer learning surrogate model with bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks. we demonstrate the reduction in the number of experiments using data from the development of dna competitors for use in an amplification-based diagnostic assay. we use cross-validation to compare the predictive accuracy of different transfer learning models, and then compare the performance of the models for both single objective and penalized optimization tasks.",,2024-02-27,,"['ruby sedgwick', 'john p. goertz', 'molly m. stevens', 'ruth misener', 'mark van der wilk']"
2402.17732,batched nonparametric contextual bandits,math.st cs.lg stat.ml stat.th,"we study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. we establish a minimax regret lower bound for this setting and propose batched successive elimination with dynamic binning (basedb) that achieves optimal regret (up to logarithmic factors). in essence, basedb dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. we also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning. additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting.",,2024-02-27,,"['rong jiang', 'cong ma']"
2402.17756,robustly learning single-index models via alignment sharpness,cs.lg cs.ds math.oc math.st stat.ml stat.th,"we study the problem of learning single-index models under the $l_2^2$ loss in the agnostic model. we give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and lipschitz link functions. this is the first efficient constant factor approximate agnostic learner, even for gaussian data and for any nontrivial class of link functions. prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation. the main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest.",,2024-02-27,,"['nikos zarifis', 'puqian wang', 'ilias diakonikolas', 'jelena diakonikolas']"
2402.17806,material microstructure design using vae-regression with multimodal   prior,cs.lg cond-mat.mtrl-sci stat.ml,"we propose a variational autoencoder (vae)-based model for building forward and inverse structure-property linkages, a problem of paramount importance in computational materials science. our model systematically combines vae with regression, linking the two models through a two-level prior conditioned on the regression variables. the regression loss is optimized jointly with the reconstruction loss of the variational autoencoder, learning microstructure features relevant for property prediction and reconstruction. the resultant model can be used for both forward and inverse prediction i.e., for predicting the properties of a given microstructure as well as for predicting the microstructure required to obtain given properties. since the inverse problem is ill-posed (one-to-many), we derive the objective function using a multi-modal gaussian mixture prior enabling the model to infer multiple microstructures for a target set of properties. we show that for forward prediction, our model is as accurate as state-of-the-art forward-only models. additionally, our method enables direct inverse inference. we show that the microstructures inferred using our model achieve desired properties reasonably accurately, avoiding the need for expensive optimization loops.",,2024-02-27,,"['avadhut sardeshmukh', 'sreedhar reddy', 'bp gautham', 'pushpak bhattacharyya']"
2402.17826,prediction-powered ranking of large language models,cs.lg cs.ai cs.cl cs.cy cs.hc stat.ml,"large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. one of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. however, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. in this work, we develop a statistical framework to bridge this gap. given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provides a rank-set -- a set of possible ranking positions -- for each of the models under comparison. moreover, it guarantees that, with a probability greater than or equal to a user-specified value, the rank-sets cover the true ranking consistent with (the distribution of) human pairwise preferences. our framework is computationally efficient, easy to use, and does not make any assumption about the distribution of human preferences nor about the degree of alignment between the pairwise comparisons by the humans and the strong large language model.",,2024-02-27,,"['ivi chatzi', 'eleni straitouri', 'suhas thejaswi', 'manuel gomez rodriguez']"
2402.17834,stable lm 2 1.6b technical report,cs.cl stat.ml,"we introduce stablelm 2 1.6b, the first in a new generation of our language model series. in this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of stablelm 2 1.6b. the weights for both models are available via hugging face for anyone to download and use. the report contains thorough evaluations of these models, including zero- and few-shot benchmarks, multilingual benchmarks, and the mt benchmark focusing on multi-turn dialogues. at the time of publishing this report, stablelm 2 1.6b was the state-of-the-art open model under 2b parameters by a significant margin. given its appealing small size, we also provide throughput measurements on a number of edge devices. in addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.",,2024-02-27,,"['marco bellagente', 'jonathan tow', 'dakota mahan', 'duy phung', 'maksym zhuravinskyi', 'reshinth adithyan', 'james baicoianu', 'ben brooks', 'nathan cooper', 'ashish datta', 'meng lee', 'emad mostaque', 'michael pieler', 'nikhil pinnaparju', 'paulo rocha', 'harry saini', 'hannah teufel', 'niccolo zanichelli', 'carlos riquelme']"
2402.17870,stochastic approximation with biased mcmc for expectation maximization,stat.co cs.lg math.oc stat.ml,"the expectation maximization (em) algorithm is a widespread method for empirical bayesian inference, but its expectation step (e-step) is often intractable. employing a stochastic approximation scheme with markov chain monte carlo (mcmc) can circumvent this issue, resulting in an algorithm known as mcmc-saem. while theoretical guarantees for mcmc-saem have previously been established, these results are restricted to the case where asymptotically unbiased mcmc algorithms are used. in practice, mcmc-saem is often run with asymptotically biased mcmc, for which the consequences are theoretically less understood. in this work, we fill this gap by analyzing the asymptotics and non-asymptotics of saem with biased mcmc steps, particularly the effect of bias. we also provide numerical experiments comparing the metropolis-adjusted langevin algorithm (mala), which is asymptotically unbiased, and the unadjusted langevin algorithm (ula), which is asymptotically biased, on synthetic and real datasets. experimental results show that ula is more stable with respect to the choice of langevin stepsize and can sometimes result in faster convergence.",,2024-02-27,,"['samuel gruffaz', 'kyurae kim', 'alain oliviero durmus', 'jacob r. gardner']"
2402.17886,zeroth-order sampling methods for non-log-concave distributions:   alleviating metastability by denoising diffusion,stat.ml cs.lg math.pr math.st stat.me stat.th,"this paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. it first describes a framework, diffusion monte carlo (dmc), based on the simulation of a denoising diffusion process with its score function approximated by a generic monte carlo estimator. dmc is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a monte carlo score estimator. then we provide an implementation of this oracle, based on rejection sampling, and this turns dmc into a true algorithm, termed zeroth-order diffusion monte carlo (zod-mc). we provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for dmc, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. then we prove that zod-mc admits an inverse polynomial dependence on the desired sampling accuracy, albeit still suffering from the curse of dimensionality. consequently, for low dimensional distributions, zod-mc is a very efficient sampler, with performance exceeding latest samplers, including also-denoising-diffusion-based rdmc and rs-dmc. last, we experimentally demonstrate the insensitivity of zod-mc to increasingly higher barriers between modes or discontinuity in non-convex potential.",,2024-02-27,2024-02-29,"['ye he', 'kevin rojas', 'molei tao']"
2402.17920,generalized bayesian additive regression trees for restricted mean   survival time inference,stat.me,"prediction methods for time-to-event outcomes often utilize survival models that rely on strong assumptions about noninformative censoring or on how individual-level covariates and survival functions are related. when the main interest is in predicting individual-level restricted mean survival times (rmst), reliance on such assumptions can lead to poor predictive performance if these assumptions are not satisfied. we propose a generalized bayes framework that avoids full probability modeling of all survival outcomes by using an rmst-targeted loss function that depends on a collection of inverse probability of censoring weights (ipcw). in our generalized bayes formulation, we utilize a flexible additive tree regression model for the rmst function, and the posterior distribution of interest is obtained through model-averaging ipcw-conditional loss function-based pseudo-bayesian posteriors. because informative censoring can be captured by the ipcw-dependent loss function, our approach only requires one to specify a model for the censoring distribution, thereby obviating the need for complex joint modeling to handle informative censoring. we evaluate the performance of our method through a series of simulations that compare it with several well-known survival machine learning methods, and we illustrate the application of our method using a multi-site cohort of breast cancer patients with clinical and genomic covariates.",,2024-02-27,,"['mahsa ashouri', 'nicholas c. henderson']"
2402.17943,sequential transport maps using sos density estimation and   $\alpha$-divergences,stat.ml cs.lg,"transport-based density estimation methods are receiving growing interest because of their ability to efficiently generate samples from the approximated density. we further invertigate the sequential transport maps framework proposed from arxiv:2106.04170 arxiv:2303.02554, which builds on a sequence of composed knothe-rosenblatt (kr) maps. each of those maps are built by first estimating an intermediate density of moderate complexity, and then by computing the exact kr map from a reference density to the precomputed approximate density. in our work, we explore the use of sum-of-squares (sos) densities and $\alpha$-divergences for approximating the intermediate densities. combining sos densities with $\alpha$-divergence interestingly yields convex optimization problems which can be efficiently solved using semidefinite programming. the main advantage of $\alpha$-divergences is to enable working with unnormalized densities, which provides benefits both numerically and theoretically. in particular, we provide two new convergence analyses of the sequential transport maps: one based on a triangle-like inequality and the second on information geometric properties of $\alpha$-divergences for unnormalizied densities. the choice of intermediate densities is also crucial for the efficiency of the method. while tempered (or annealed) densities are the state-of-the-art, we introduce diffusion-based intermediate densities which permits to approximate densities known from samples only. such intermediate densities are well-established in machine learning for generative modeling. finally we propose and try different low-dimensional maps (or lazy maps) for dealing with high-dimensional problems and numerically demonstrate our methods on several benchmarks, including bayesian inference problems and unsupervised learning task.",,2024-02-27,,"['benjamin zanger', 'tiangang cui', 'martin schreiber', 'olivier zahm']"
2402.18149,provably efficient partially observable risk-sensitive reinforcement   learning with hindsight observation,cs.lg stat.ml,"this work pioneers regret analysis of risk-sensitive reinforcement learning in partially observable environments with hindsight observation, addressing a gap in theoretical exploration. we introduce a novel formulation that integrates hindsight observations into a partially observable markov decision process (pomdp) framework, where the goal is to optimize accumulated reward under the entropic risk measure. we develop the first provably efficient rl algorithm tailored for this setting. we also prove by rigorous analysis that our algorithm achieves polynomial regret $\tilde{o}\left(\frac{e^{|{\gamma}|h}-1}{|{\gamma}|h}h^2\sqrt{khs^2oa}\right)$, which outperforms or matches existing upper bounds when the model degenerates to risk-neutral or fully observable settings. we adopt the method of change-of-measure and develop a novel analytical tool of beta vectors to streamline mathematical derivations. these techniques are of particular interest to the theoretical study of reinforcement learning.",,2024-02-28,,"['tonghe zhang', 'yu chen', 'longbo huang']"
2402.18213,multi-objective differentiable neural architecture search,cs.lg cs.cv stat.ml,"pareto front profiling in multi-objective optimization (moo), i.e. finding a diverse set of pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. typically, in moo neural architecture search (nas), we aim to balance performance and hardware metrics across devices. prior nas approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the pareto front necessitates a search for each constraint. in this work, we propose a novel nas algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. to this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. extensive experiments with up to 19 hardware devices and 3 objectives showcase the effectiveness and scalability of our method. finally, we show that, without additional costs, our method outperforms existing moo nas methods across qualitatively different search spaces and datasets, including mobilenetv3 on imagenet-1k and a transformer space on machine translation.",,2024-02-28,,"['rhea sanjay sukthanker', 'arber zela', 'benedikt staffler', 'samuel dooley', 'josif grabocka', 'frank hutter']"
2402.18242,a network-constrain weibull aft model for biomarkers discovery,stat.ml math.st stat.me stat.th,"we propose aftnet, a novel network-constraint survival analysis method based on the weibull accelerated failure time (aft) model solved by a penalized likelihood approach for variable selection and estimation. when using the log-linear representation, the inference problem becomes a structured sparse regression problem for which we explicitly incorporate the correlation patterns among predictors using a double penalty that promotes both sparsity and grouping effect. moreover, we establish the theoretical consistency for the aftnet estimator and present an efficient iterative computational algorithm based on the proximal gradient descent method. finally, we evaluate aftnet performance both on synthetic and real data examples.",,2024-02-28,,"['claudia angelini', 'daniela de canditiis', 'italia de feis', 'antonella iuliano']"
2402.18313,levelling up learning: exploring the impact of gamification in flipped   classrooms,stat.ot physics.ed-ph,"in recent years, the integration of gamification into educational settings has garnered significant attention as a means to enhance student engagement and learning outcomes. by leveraging gamified elements such as points and leaderboards, educators aim to promote active participation, motivation, and deeper understanding among students. this study investigates the effects of gamification on student engagement in a flipped classroom environment. the findings suggest that gamification strategies, when effectively implemented, can have a positive impact on student motivation and engagement. this paper concludes with recommendations for educators, potential challenges such as superficial engagement and demotivation, and future directions for research to address these challenges and further explore the potential of gamification in fostering student success.",,2024-02-28,,"['eilidh jack', 'craig alexander', 'elinor m jones']"
2402.18392,unveiling the potential of robustness in evaluating causal inference   models,cs.lg cs.ai econ.em stat.ml,"the growing demand for personalized decision-making has led to a surge of interest in estimating the conditional average treatment effect (cate). the intersection of machine learning and causal inference has yielded various effective cate estimators. however, deploying these estimators in practice is often hindered by the absence of counterfactual labels, making it challenging to select the desirable cate estimator using conventional model selection procedures like cross-validation. existing approaches for cate estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges. firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners. secondly, they lack a specific focus on selecting a robust estimator. to address these challenges, this paper introduces a novel approach, the distributionally robust metric (drm), for cate estimator selection. the proposed drm not only eliminates the need to fit additional models but also excels at selecting a robust cate estimator. experimental studies demonstrate the efficacy of the drm method, showcasing its consistent effectiveness in identifying superior estimators while mitigating the risk of selecting inferior ones.",,2024-02-28,,"['yiyan huang', 'cheuk hang leung', 'siyi wang', 'yijun li', 'qi wu']"
2402.18477,signature kernel conditional independence tests in causal discovery for   stochastic processes,cs.lg cs.ai stat.ml,"inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. such processes can often be accurately modeled via stochastic differential equations (sdes), which naturally imply causal relationships via ""which variables enter the differential of which other variables"". in this paper, we develop a kernel-based test of conditional independence (ci) on ""path-space"" -- solutions to sdes -- by leveraging recent advances in signature kernels. we demonstrate strictly superior performance of our proposed ci test compared to existing approaches on path-space. then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. assuming faithfulness and a ci oracle, our algorithm is sound and complete. we empirically verify that our developed ci test in conjunction with the causal discovery algorithm reliably outperforms baselines across a range of settings.",,2024-02-28,,"['georg manten', 'cecilia casolo', 'emilio ferrucci', 'søren wengel mogensen', 'cristopher salvi', 'niki kilbertus']"
2402.18510,rnns are not transformers (yet): the key bottleneck on in-context   retrieval,cs.lg cs.cl stat.ml,"this paper investigates the gap in representation powers of recurrent neural networks (rnns) and transformers in the context of solving algorithmic problems. we focus on understanding whether rnns, known for their memory efficiency in handling long sequences, can match the performance of transformers, particularly when enhanced with chain-of-thought (cot) prompting. our theoretical analysis reveals that cot improves rnns but is insufficient to close the gap with transformers. a key bottleneck lies in the inability of rnns to perfectly retrieve information from the context, even with cot: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that rnns are not expressive enough to solve the tasks while transformers can solve them with ease. conversely, we prove that adopting techniques to enhance the in-context retrieval capability of rnns, including retrieval-augmented generation (rag) and adding a single transformer layer, can elevate rnns to be capable of solving all polynomial-time solvable problems with cot, hence closing the representation gap with transformers.",,2024-02-28,2024-02-29,"['kaiyue wen', 'xingyu dang', 'kaifeng lyu']"
2402.18551,implicit bias of next-token prediction,cs.lg cs.cl stat.ml,"next-token prediction (ntp), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. departing from traditional one-hot classification, in ntp, multiple tokens with varying frequencies follow each given context. this work frames ntp training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. it then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the ntp training loss reaches its lower bound (entropy)? specifically, for linear ntp models trained using gradient descent (gd), we make the following contributions: firstly, we determine ntp-separability conditions on the data, under which gd can attain its lower bound. we also demonstrate that these conditions hold under overparameterization. secondly, we establish that the parameters of gd projected onto an appropriate data subspace converge to the unique solution of a system of linear equations, which requires the logits' difference of in-support tokens to be equal to the log-ratio of their respective probabilities. meanwhile, on the orthogonal subspace, the parameters diverge and converge in the direction of the solution of a max-margin quadratic program, minimizing the euclidean norm of parameters satisfying the \ntp-separability conditions. akin to prior research on implicit bias of one-hot classification, our work opens exciting avenues for future research that can lead to better understanding optimization, generalization and robustness principles of models trained with ntp.",,2024-02-28,,['christos thrampoulidis']
2402.18591,stochastic contextual bandits with graph feedback: from independence   number to mas number,cs.lg cs.gt math.st stat.th,"we consider contextual bandits with graph feedback, a class of interactive learning problems with richer structures than vanilla contextual bandits, where taking an action reveals the rewards for all neighboring actions in the feedback graph under all contexts. unlike the multi-armed bandits setting where a growing literature has painted a near-complete understanding of graph feedback, much remains unexplored in the contextual bandits counterpart. in this paper, we make inroads into this inquiry by establishing a regret lower bound $\omega(\sqrt{\beta_m(g) t})$, where $m$ is the number of contexts, $g$ is the feedback graph, and $\beta_m(g)$ is our proposed graph-theoretical quantity that characterizes the fundamental learning limit for this class of problems. interestingly, $\beta_m(g)$ interpolates between $\alpha(g)$ (the independence number of the graph) and $\mathsf{m}(g)$ (the maximum acyclic subgraph (mas) number of the graph) as the number of contexts $m$ varies. we also provide algorithms that achieve near-optimal regrets for important classes of context sequences and/or feedback graphs, such as transitively closed graphs that find applications in auctions and inventory control. in particular, with many contexts, our results show that the mas number completely characterizes the statistical complexity for contextual bandits, as opposed to the independence number in multi-armed bandits.",,2024-02-12,,"['yuxiao wen', 'yanjun han', 'zhengyuan zhou']"
2402.18697,inferring dynamic networks from marginals with iterative proportional   fitting,stat.ml cs.lg cs.si math.oc math.st stat.th,"a common network inference problem, arising from real-world data constraints, is how to infer a dynamic network from its time-aggregated adjacency matrix and time-varying marginals (i.e., row and column sums). prior approaches to this problem have repurposed the classic iterative proportional fitting (ipf) procedure, also known as sinkhorn's algorithm, with promising empirical results. however, the statistical foundation for using ipf has not been well understood: under what settings does ipf provide principled estimation of a dynamic network from its marginals, and how well does it estimate the network? in this work, we establish such a setting, by identifying a generative network model whose maximum likelihood estimates are recovered by ipf. our model both reveals implicit assumptions on the use of ipf in such settings and enables new analyses, such as structure-dependent error bounds on ipf's parameter estimates. when ipf fails to converge on sparse network data, we introduce a principled algorithm that guarantees ipf converges under minimal changes to the network structure. finally, we conduct experiments with synthetic and real-world data, which demonstrate the practical value of our theoretical and algorithmic contributions.",,2024-02-28,,"['serina chang', 'frederic koehler', 'zhaonan qu', 'jure leskovec', 'johan ugander']"
2402.18724,learning associative memories with gradient descent,cs.lg cs.ai stat.ml,"this work focuses on the training dynamics of one associative memory module storing outer products of token embeddings. we reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings. through theory and experiments, we provide several insights. in overparameterized regimes, we obtain logarithmic growth of the ``classification margins.'' yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes. the oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence. in underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes. finally, we assess the validity of our findings on small transformer models.",,2024-02-28,,"['vivien cabannes', 'berfin simsek', 'alberto bietti']"
2402.18800,blockecho: retaining long-range dependencies for imputing block-wise   missing data,cs.lg stat.ml,"block-wise missing data poses significant challenges in real-world data imputation tasks. compared to scattered missing data, block-wise gaps exacerbate adverse effects on subsequent analytic and machine learning tasks, as the lack of local neighboring elements significantly reduces the interpolation capability and predictive power. however, this issue has not received adequate attention. most sota matrix completion methods appeared less effective, primarily due to overreliance on neighboring elements for predictions. we systematically analyze the issue and propose a novel matrix completion method ``blockecho"" for a more comprehensive solution. this method creatively integrates matrix factorization (mf) within generative adversarial networks (gan) to explicitly retain long-distance inter-element relationships in the original matrix. besides, we incorporate an additional discriminator for gan, comparing the generator's intermediate progress with pre-trained mf results to constrain high-order feature distributions. subsequently, we evaluate blockecho on public datasets across three domains. results demonstrate superior performance over both traditional and sota methods when imputing block-wise missing data, especially at higher missing rates. the advantage also holds for scattered missing data at high missing rates. we also contribute on the analyses in providing theoretical justification on the optimality and convergence of fusing mf and gan for missing block data.",,2024-02-28,,"['qiao han', 'mingqian li', 'yao yang', 'yiteng zhai']"
2402.18805,vec-sbm: optimal community detection with vectorial edges covariates,cs.si stat.ml,"social networks are often associated with rich side information, such as texts and images. while numerous methods have been developed to identify communities from pairwise interactions, they usually ignore such side information. in this work, we study an extension of the stochastic block model (sbm), a widely used statistical framework for community detection, that integrates vectorial edges covariates: the vectorial edges covariates stochastic block model (vec-sbm). we propose a novel algorithm based on iterative refinement techniques and show that it optimally recovers the latent communities under the vec-sbm. furthermore, we rigorously assess the added value of leveraging edge's side information in the community detection process. we complement our theoretical results with numerical experiments on synthetic and semi-synthetic data.",,2024-02-28,,"['guillaume braun', 'masashi sugiyama']"
2402.18851,applications of 0-1 neural networks in prescription and prediction,cs.lg cs.ai math.oc stat.ml,"a key challenge in medical decision making is learning treatment policies for patients with limited observational data. this challenge is particularly evident in personalized healthcare decision-making, where models need to take into account the intricate relationships between patient characteristics, treatment options, and health outcomes. to address this, we introduce prescriptive networks (pnns), shallow 0-1 neural networks trained with mixed integer programming that can be used with counterfactual estimation to optimize policies in medium data settings. these models offer greater interpretability than deep neural networks and can encode more complex policies than common models such as decision trees. we show that pnns can outperform existing methods in both synthetic data experiments and in a case study of assigning treatments for postpartum hypertension. in particular, pnns are shown to produce policies that could reduce peak blood pressure by 5.47 mm hg (p=0.02) over existing clinical practice, and by 2 mm hg (p=0.01) over the next best prescriptive modeling technique. moreover pnns were more likely than all other models to correctly identify clinically significant features while existing models relied on potentially dangerous features such as patient insurance information and race that could lead to bias in treatment.",,2024-02-29,,"['vrishabh patil', 'kara hoppe', 'yonatan mintz']"
2402.18884,supervised contrastive representation learning: landscape analysis with   unconstrained features,cs.lg stat.ml,"recent findings reveal that over-parameterized deep neural networks, trained beyond zero training-error, exhibit a distinctive structural pattern at the final layer, termed as neural-collapse (nc). these results indicate that the final hidden-layer outputs in such networks display minimal within-class variations over the training set. while existing research extensively investigates this phenomenon under cross-entropy loss, there are fewer studies focusing on its contrastive counterpart, supervised contrastive (sc) loss. through the lens of nc, this paper employs an analytical approach to study the solutions derived from optimizing the sc loss. we adopt the unconstrained features model (ufm) as a representative proxy for unveiling nc-related phenomena in sufficiently over-parameterized deep networks. we show that, despite the non-convexity of sc loss minimization, all local minima are global minima. furthermore, the minimizer is unique (up to a rotation). we prove our results by formalizing a tight convex relaxation of the ufm. finally, through this convex formulation, we delve deeper into characterizing the properties of global solutions under label-imbalanced training data.",,2024-02-29,,"['tina behnia', 'christos thrampoulidis']"
2402.18900,prognostic covariate adjustment for logistic regression in randomized   controlled trials,stat.me stat.ap stat.ml,"randomized controlled trials (rcts) with binary primary endpoints introduce novel challenges for inferring the causal effects of treatments. the most significant challenge is non-collapsibility, in which the conditional odds ratio estimand under covariate adjustment differs from the unconditional estimand in the logistic regression analysis of rct data. this issue gives rise to apparent paradoxes, such as the variance of the estimator for the conditional odds ratio from a covariate-adjusted model being greater than the variance of the estimator from the unadjusted model. we address this challenge in the context of adjustment based on predictions of control outcomes from generative artificial intelligence (ai) algorithms, which are referred to as prognostic scores. we demonstrate that prognostic score adjustment in logistic regression increases the power of the wald test for the conditional odds ratio under a fixed sample size, or alternatively reduces the necessary sample size to achieve a desired power, compared to the unadjusted analysis. we derive formulae for prospective calculations of the power gain and sample size reduction that can result from adjustment for the prognostic score. furthermore, we utilize g-computation to expand the scope of prognostic score adjustment to inferences on the marginal risk difference, relative risk, and odds ratio estimands. we demonstrate the validity of our formulae via extensive simulation studies that encompass different types of logistic regression model specifications. our simulation studies also indicate how prognostic score adjustment can reduce the variance of g-computation estimators for the marginal estimands while maintaining frequentist properties such as asymptotic unbiasedness and type i error rate control. our methodology can ultimately enable more definitive and conclusive analyses for rcts with binary primary endpoints.",,2024-02-29,,"['yunfan li', 'arman sabbaghi', 'jonathan r. walsh', 'charles k. fisher']"
2402.18910,digic: domain generalizable imitation learning by causal discovery,cs.lg cs.ai stat.me,"causality has been combined with machine learning to produce robust representations for domain generalization. most existing methods of this type require massive data from multiple domains to identify causal features by cross-domain variations, which can be expensive or even infeasible and may lead to misidentification in some cases. in this work, we make a different attempt by leveraging the demonstration data distribution to discover the causal features for a domain generalizable policy. we design a novel framework, called digic, to identify the causal features by finding the direct cause of the expert action from the demonstration data distribution via causal discovery. our framework can achieve domain generalizable imitation learning with only single-domain data and serve as a complement for cross-domain variation-based methods under non-structural assumptions on the underlying causal models. our empirical study in various control tasks shows that the proposed framework evidently improves the domain generalization performance and has comparable performance to the expert in the original domain simultaneously.",,2024-02-29,,"['yang chen', 'yitao liang', 'zhouchen lin']"
2402.18995,negative-binomial randomized gamma markov processes for heterogeneous   overdispersed count time series,cs.lg cs.ai stat.ml,"modeling count-valued time series has been receiving increasing attention since count time series naturally arise in physical and social domains. poisson gamma dynamical systems (pgdss) are newly-developed methods, which can well capture the expressive latent transition structure and bursty dynamics behind count sequences. in particular, pgdss demonstrate superior performance in terms of data imputation and prediction, compared with canonical linear dynamical system (lds) based methods. despite these advantages, pgds cannot capture the heterogeneous overdispersed behaviours of the underlying dynamic processes. to mitigate this defect, we propose a negative-binomial-randomized gamma markov process, which not only significantly improves the predictive performance of the proposed dynamical system, but also facilitates the fast convergence of the inference algorithm. moreover, we develop methods to estimate both factor-structured and graph-structured transition dynamics, which enable us to infer more explainable latent structure, compared with pgdss. finally, we demonstrate the explainable latent structure learned by the proposed method, and show its superior performance in imputing missing data and forecasting future observations, compared with the related models.",,2024-02-29,,"['rui huang', 'sikun yang', 'heinz koeppl']"
2402.19442,"training dynamics of multi-head softmax attention for in-context   learning: emergence, convergence, and optimality",cs.lg cs.ai math.oc math.st stat.ml stat.th,"we study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression. we establish the global convergence of gradient flow under suitable choices of initialization. in addition, we prove that an interesting ""task allocation"" phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. specifically, we prove that the gradient flow dynamics can be split into three phases -- a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flow is on par with the best possible multi-head softmax attention model up to a constant factor. our analysis also delineates a strict separation in terms of the prediction accuracy of icl between single-head and multi-head attention models. the key technique for our convergence analysis is to map the gradient flow dynamics in the parameter space to a set of ordinary differential equations in the spectral domain, where the relative magnitudes of the semi-singular values of the attention weights determines task allocation. to our best knowledge, our work provides the first convergence result for the multi-head softmax attention model.",,2024-02-29,,"['siyu chen', 'heejune sheen', 'tianhao wang', 'zhuoran yang']"
2402.19449,heavy-tailed class imbalance and why adam outperforms gradient descent   on language models,cs.lg cs.cl math.oc stat.ml,"adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens. we show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. when training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. as most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. on the other hand, adam and sign-based methods do not suffer from this problem and improve predictions on all classes. to establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision cnns, and linear models. we further study this phenomenon on a linear classification with cross-entropy loss, showing that heavy-tailed class imbalance leads to ill-conditioning, and that the normalization used by adam can counteract it.",,2024-02-29,,"['frederik kunstner', 'robin yadav', 'alan milligan', 'mark schmidt', 'alberto bietti']"
2402.19455,listening to the noise: blind denoising with gibbs diffusion,stat.ml astro-ph.co cs.cv cs.lg eess.sp,"in recent years, denoising problems have become intertwined with the development of deep generative models. in particular, diffusion models are trained like denoisers, and the distribution they model coincide with denoising priors in the bayesian picture. however, denoising through diffusion-based posterior sampling requires the noise level and covariance to be known, preventing blind denoising. we overcome this limitation by introducing gibbs diffusion (gdiff), a general methodology addressing posterior sampling of both the signal and the noise parameters. assuming arbitrary parametric gaussian noise, we develop a gibbs algorithm that alternates sampling steps from a conditional diffusion model trained to map the signal prior to the family of noise distributions, and a monte carlo sampler to infer the noise parameters. our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the gibbs stationary distribution caused by the diffusion model. we showcase our method for 1) blind denoising of natural images involving colored noises with unknown amplitude and spectral index, and 2) a cosmology problem, namely the analysis of cosmic microwave background data, where bayesian inference of ""noise"" parameters means constraining models of the evolution of the universe.",,2024-02-29,,"['david heurtel-depeiges', 'charles c. margossian', 'ruben ohana', 'bruno régaldo-saint blancard']"
2402.19460,benchmarking uncertainty disentanglement: specialized uncertainties for   specialized tasks,cs.lg stat.ml,"uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, out-of-distribution detection, and aleatoric uncertainty quantification. the latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one task. hence, there is a plethora of recent advances with different intentions - that often entirely deviate from practical behavior. this paper conducts a comprehensive evaluation of numerous uncertainty estimators across diverse tasks on imagenet. we find that, despite promising theoretical endeavors, disentanglement is not yet achieved in practice. additionally, we reveal which uncertainty estimators excel at which specific tasks, providing insights for practitioners and guiding future research toward task-centric and disentangled uncertainty estimation methods. our code is available at https://github.com/bmucsanyi/bud.",,2024-02-29,,"['bálint mucsányi', 'michael kirchhof', 'seong joon oh']"
